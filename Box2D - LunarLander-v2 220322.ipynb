{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "I'm using the environment in the OpenAI gym - Box2D to practice how to train an agent by reinforcement learning. In this notebook, the environment is a classic rocket trajectory optimization problem. ***The goal is to land the lunar lander moving from the top of the screen to the landing pad***\n",
    "\n",
    "I try differnet methods to complete this goal:\n",
    "\n",
    "***1. Create a Deep Learning Model with Keras*** (See Section 4)\n",
    "- After 1 milliom timesteps of learning, the agent received score between -10 to 277 (average 218.0). ***The Lunar Lander landing on the ground 4 out of 5 times properly***\n",
    "- Time spend 1hr 25min\n",
    "\n",
    "***2. Epsilon-Greedy Policy*** (See Section 5-6)\n",
    "- The ***Agent with Epsilon-Greedy Policy stop learning process at epoch #85 with average reward 302***\n",
    "- Time spend on receiving more than 300 average reward 1hr 19min\n",
    "\n",
    "***3. PPO Model in Stable_baseline3*** (See Section 7)\n",
    "- ***Average reward rise to above 160 consistently*** after 2.25M learning timesteps\n",
    "- Average length of episode after 2.25M learning timesteps is about 350\n",
    "- Time spend on 5 million learning timesteps in 1h 34min\n",
    "\n",
    "\n",
    "### ***Table of Content:***\n",
    "1. What is Lunar Lander\n",
    "2. Import Dependencies\n",
    "3. Understanding The Environment\n",
    "4. Create a Deep Learning Neural Network with Keras\n",
    "   - 4.1 Build Agent with Keras-RL\n",
    "   - 4.2 Train the Agents\n",
    "5. Epsilon-Greedy Policy\n",
    "6. Q-learning via Gradient Descent\n",
    "7. PPO Model in Stable_baseline3\n",
    "8. Video Record and Play the Final Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Lunar Lander?\n",
    "\n",
    "<img src='http://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/LunarLander-v2/poster.jpg' width='250px'/>\n",
    "\n",
    "This environment is a classic rocket trajectory optimization problem.\n",
    "\n",
    "According to Pontryagin's maximum principle, it is optimal to fire the engine at full throttle or turn it off. \n",
    "\n",
    "This is the reason why this environment has discrete actions: \n",
    "- engine on or off\n",
    "\n",
    "There are two environment versions: \n",
    "- discrete or continuous.\n",
    "\n",
    "- The landing pad is always at coordinates (0,0).\n",
    "- The coordinates are the first two numbers in the state vector.\n",
    "- Landing outside of the landing pad is possible. \n",
    "- Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n",
    "\n",
    "Source: https://gym.openai.com/envs/LunarLander-v2/\n",
    "\n",
    "\n",
    "### Action Space\n",
    "There are four discrete actions available: \n",
    "- do nothing, \n",
    "- fire left orientation engine, \n",
    "- fire main engine, \n",
    "- fire right orientation engine.\n",
    "\n",
    "\n",
    "### Observation Space\n",
    "There are 8 states: \n",
    "- the coordinates of the lander in `x` & `y`,\n",
    "- its linear velocities in `x` & `y`,\n",
    "- its angle, \n",
    "- its angular velocity, and\n",
    "- two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "### Rewards\n",
    "- moving from the top of the screen to the landing pad and coming to rest is about 100-140 points.\n",
    "- If the lander moves away from the landing pad, it loses reward.\n",
    "- If the lander crashes, it receives an additional -100 points. \n",
    "- If it comes to rest, it receives an additional +100 points. \n",
    "- Each leg with ground contact is +10 points.\n",
    "- Firing the main engine is -0.3 points each frame. \n",
    "- Firing the side engine is -0.03 points each frame. \n",
    "- Solved is 200 points.\n",
    "\n",
    "### Starting State\n",
    "- The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
    "\n",
    "### Episode Termination\n",
    "The episode finishes if:\n",
    "- the lander crashes (the lander body gets in contact with the moon);\n",
    "- the lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
    "- the lander is not awake. \n",
    "    \n",
    "A body which is not awake is a body which doesn't move and doesn't collide with any other body:\n",
    "- When Box2D determines that a body (or group of bodies) has come to rest,\n",
    "- the body enters a sleep state which has very little CPU overhead. \n",
    "- If a body is awake and collides with a sleeping body, then the sleeping body wakes up. \n",
    "- Bodies will also wake up if a joint or contact attached to them is destroyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zdDFE3eKB_EG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 12:00:38.709 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc65a4ffd40>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-21 12:00:38.709 python[2795:235555] Warning: Expected min height of view: (<NSButton: 0x7fc63a5fec40>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-21 12:00:38.712 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc5ea406cb0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-21 12:00:38.714 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc5ea410c80>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "Action space: Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")                       # Using Lunar Lander verion 2 in this notebook\n",
    "env.reset()                                            # reset the environment to initial default\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))                    # display the environment\n",
    "print(\"Observation space:\", env.observation_space)     # There are no coordinates in the state vector\n",
    "print(\"Action space:\", env.action_space)               # 4 actions are: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "29AZ1eL3B_EH",
    "outputId": "2db66c4e-6d20-411d-8614-70f9d427be06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<LunarLander<LunarLander-v2>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close                                    # close the opened environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjrCiZlLB_EJ",
    "outputId": "231b81d7-f0df-4e16-8e2e-a4ad781e9b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: [ 0.00382395  1.4118519   0.38731435  0.04140358 -0.00442427 -0.08773248\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(\"initial observation code:\", obs)\n",
    "\n",
    "# Note: There are 8 states in observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN06EBaGB_EJ",
    "outputId": "500a7cfd-e6ee-4cfe-e55a-534949e6f74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking action: 2\n",
      "new observation code: [ 0.01334715  1.384133    0.45757285 -0.39623848 -0.0112183  -0.05246214\n",
      "  0.          0.        ]\n",
      "reward: -0.023984999450721023\n",
      "is episode over?: False\n"
     ]
    }
   ],
   "source": [
    "# If taken action 2 'fire main engine' in the environment:\n",
    "action = env.action_space.sample()\n",
    "\n",
    "new_obs, reward, is_done, info = env.step(action) \n",
    "\n",
    "print(\"Taking action:\", action)\n",
    "\n",
    "# Then the environment will reply 3 things new_observation, reward and is_done to the agent\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is episode over?:\", is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Baseline Use Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01143932,  1.4158826 ,  0.38245684,  0.09894957, -0.01331872,\n",
       "       -0.09206639,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-121.90417940305434 Timesteps Spend:83\n",
      "Episode:2 Score:24.594325481854185 Timesteps Spend:74\n",
      "Episode:3 Score:-187.2061642336522 Timesteps Spend:64\n",
      "Episode:4 Score:-109.85857953706126 Timesteps Spend:75\n",
      "Episode:5 Score:-129.90604097939706 Timesteps Spend:86\n",
      "CPU times: user 329 ms, sys: 47.9 ms, total: 377 ms\n",
      "Wall time: 401 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    obs = env.reset()                                      # initial the set of observation\n",
    "    \n",
    "    done = False                                           # initial the game is over = False, until reach maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    timesteps_counter = 0                                  # count the number of timestep spend\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment\n",
    "        \n",
    "        action = env.action_space.sample()                 # random choose an action \n",
    "        \n",
    "        n_obs, reward, done, info = env.step(action)       # pass random actions into environment to get back\n",
    "                                                            # 1. next set of observation\n",
    "                                                            # 2. reward\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward                                    # accumulate each episodes' reward received into score\n",
    "        \n",
    "        timesteps_counter += 1                             # +1 in each timestep spend\n",
    "        \n",
    "    print('Episode:{} Score:{} Timesteps Spend:{}'.format(episode, score,timesteps_counter))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, by taken random actions ***The Lunar Lander not landing properly***. At the end of each episode score receive between -187 to 24 (average 104). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a Deep Learning Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build a Sequential Neural Network\n",
    "def build_neural_network(states, actions):\n",
    "    \n",
    "    neural_network = Sequential()\n",
    "    \n",
    "    neural_network.add(Flatten(input_shape = (1, states), name = 'States'))      # use Flatten layer as the input layer, input_shape = 1 x number of state \n",
    "    \n",
    "    neural_network.add(Dense(512, activation = 'relu', name = 'Dense_1'))        # use Dense layer with 512 units of tensor with relu activation function\n",
    "    \n",
    "    neural_network.add(Dense(256, activation = 'relu', name = 'Dense_2'))        # use another Dense layer with 256 units of tensor with relu activation function\n",
    "    \n",
    "    neural_network.add(Dense(actions, activation = 'linear', name = 'Actions'))  # the output layer with shape = number of actions with linear activation function\n",
    "                                                                                 # Output the probability for each actions \n",
    "    \n",
    "    return neural_network                                                        # build_neural_network return a neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SqUOJSU5YxzR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "States (Flatten)             (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Actions (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input_shape of the build_model function\n",
    "states = env.observation_space.shape[0]                         # use .shape[0] to return number of observation space\n",
    "\n",
    "# Set the output_shape of the build_model function\n",
    "actions = env.action_space.n                            \n",
    "\n",
    "# Run the build_model function to build the model\n",
    "neural_network = build_neural_network(states, actions)                            \n",
    "\n",
    "# See the model summary\n",
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Build Agent with Keras-RL\n",
    "\n",
    "***Two Types of Policy in Reinforcement Learning (RL):***\n",
    "1. Value Base RL\n",
    "2. Policy Base RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from rl.agents import DQNAgent                                                      # used DQNAgent here, should try other agents: SARSAAgent      \n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy      # used Policy base RL  \n",
    "from rl.memory import SequentialMemory\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an Agent to learn from the model\n",
    "def build_agent(neural_network, actions):            \n",
    "    \n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),    # use EpsGreedyQPolicy()\n",
    "                                  attr='eps',\n",
    "                                  value_max = 1,\n",
    "                                  value_min = .1,\n",
    "                                  value_test = 0.2,\n",
    "                                  nb_steps = 10000\n",
    "    )                    \n",
    "\n",
    "    memory = SequentialMemory(limit = 1000,              # buffer limit: number of episode\n",
    "                              window_length = 1          # store pass 1 window for 1000 episode \n",
    "                             )\n",
    "    \n",
    "    dqn = DQNAgent(model = neural_network, \n",
    "                   memory = memory,\n",
    "                   policy = policy,\n",
    "                   enable_dueling_network = True,        # Dueling Networks split value and advantage, help the model learn when to take action and when not to bother\n",
    "                   dueling_type = 'avg',\n",
    "                   nb_actions = actions,                 # 4 actions to learn\n",
    "                   nb_steps_warmup = 1000,           \n",
    "                   target_model_update = 1e-2\n",
    "                  )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to save the log\n",
    "log_path = os.path.join(\"Training\", \"Logs\", \"LunarLander-v2_Keras-RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "    116/1000000: episode: 1, duration: 0.131s, episode steps: 116, steps per second: 883, episode reward: -435.381, mean reward: -3.753 [-100.000, 40.408], mean action: 1.603 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    228/1000000: episode: 2, duration: 0.063s, episode steps: 112, steps per second: 1779, episode reward: -193.390, mean reward: -1.727 [-100.000,  1.272], mean action: 1.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    339/1000000: episode: 3, duration: 0.064s, episode steps: 111, steps per second: 1737, episode reward: -403.567, mean reward: -3.636 [-100.000,  1.353], mean action: 1.270 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    448/1000000: episode: 4, duration: 0.059s, episode steps: 109, steps per second: 1837, episode reward: -219.864, mean reward: -2.017 [-100.000, 102.671], mean action: 1.514 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    535/1000000: episode: 5, duration: 0.050s, episode steps:  87, steps per second: 1746, episode reward: -312.962, mean reward: -3.597 [-100.000, 30.823], mean action: 1.322 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    635/1000000: episode: 6, duration: 0.059s, episode steps: 100, steps per second: 1690, episode reward: -89.122, mean reward: -0.891 [-100.000, 16.751], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    707/1000000: episode: 7, duration: 0.041s, episode steps:  72, steps per second: 1761, episode reward: -120.334, mean reward: -1.671 [-100.000, 10.443], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    814/1000000: episode: 8, duration: 0.065s, episode steps: 107, steps per second: 1650, episode reward: -241.436, mean reward: -2.256 [-100.000, 39.460], mean action: 1.411 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    915/1000000: episode: 9, duration: 0.056s, episode steps: 101, steps per second: 1810, episode reward: -122.621, mean reward: -1.214 [-100.000, 25.923], mean action: 1.327 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1022/1000000: episode: 10, duration: 0.558s, episode steps: 107, steps per second: 192, episode reward: -444.970, mean reward: -4.159 [-100.000,  1.028], mean action: 1.589 [0.000, 3.000],  loss: 62.097049, mae: 3.612580, mean_q: -2.032317, mean_eps: 0.909010\n",
      "   1175/1000000: episode: 11, duration: 0.751s, episode steps: 153, steps per second: 204, episode reward: -105.443, mean reward: -0.689 [-100.000,  8.564], mean action: 1.595 [0.000, 3.000],  loss: 43.784955, mae: 4.043119, mean_q: -3.089873, mean_eps: 0.901180\n",
      "   1279/1000000: episode: 12, duration: 0.570s, episode steps: 104, steps per second: 182, episode reward: -110.598, mean reward: -1.063 [-100.000, 29.974], mean action: 1.519 [0.000, 3.000],  loss: 29.670988, mae: 4.251080, mean_q: -3.257826, mean_eps: 0.889615\n",
      "   1365/1000000: episode: 13, duration: 0.413s, episode steps:  86, steps per second: 208, episode reward: -302.575, mean reward: -3.518 [-100.000, 17.550], mean action: 1.488 [0.000, 3.000],  loss: 35.819861, mae: 4.756309, mean_q: -3.231151, mean_eps: 0.881065\n",
      "   1444/1000000: episode: 14, duration: 0.386s, episode steps:  79, steps per second: 204, episode reward: -129.021, mean reward: -1.633 [-100.000,  7.801], mean action: 1.532 [0.000, 3.000],  loss: 28.634081, mae: 4.792944, mean_q: -3.063875, mean_eps: 0.873640\n",
      "   1557/1000000: episode: 15, duration: 0.587s, episode steps: 113, steps per second: 193, episode reward: -145.695, mean reward: -1.289 [-100.000,  8.219], mean action: 1.504 [0.000, 3.000],  loss: 25.831471, mae: 4.978975, mean_q: -2.480229, mean_eps: 0.865000\n",
      "   1647/1000000: episode: 16, duration: 0.427s, episode steps:  90, steps per second: 211, episode reward: -103.309, mean reward: -1.148 [-100.000,  6.418], mean action: 1.444 [0.000, 3.000],  loss: 27.169509, mae: 5.359346, mean_q: -2.250278, mean_eps: 0.855865\n",
      "   1776/1000000: episode: 17, duration: 0.644s, episode steps: 129, steps per second: 200, episode reward: -77.732, mean reward: -0.603 [-100.000,  9.148], mean action: 1.566 [0.000, 3.000],  loss: 25.201958, mae: 6.138164, mean_q: -2.322360, mean_eps: 0.846010\n",
      "   1874/1000000: episode: 18, duration: 0.473s, episode steps:  98, steps per second: 207, episode reward: -319.797, mean reward: -3.263 [-100.000,  1.589], mean action: 1.704 [0.000, 3.000],  loss: 29.258460, mae: 6.578364, mean_q: -1.227662, mean_eps: 0.835795\n",
      "   1976/1000000: episode: 19, duration: 0.553s, episode steps: 102, steps per second: 185, episode reward: -350.444, mean reward: -3.436 [-100.000,  9.197], mean action: 1.578 [0.000, 3.000],  loss: 32.045230, mae: 7.228441, mean_q: -1.542046, mean_eps: 0.826795\n",
      "   2079/1000000: episode: 20, duration: 0.542s, episode steps: 103, steps per second: 190, episode reward: -62.522, mean reward: -0.607 [-100.000, 18.184], mean action: 1.621 [0.000, 3.000],  loss: 25.666531, mae: 7.176262, mean_q: -0.499329, mean_eps: 0.817570\n",
      "   2203/1000000: episode: 21, duration: 0.601s, episode steps: 124, steps per second: 206, episode reward: -180.966, mean reward: -1.459 [-100.000,  7.832], mean action: 1.653 [0.000, 3.000],  loss: 19.491966, mae: 6.956630, mean_q: -0.159492, mean_eps: 0.807355\n",
      "   2266/1000000: episode: 22, duration: 0.305s, episode steps:  63, steps per second: 207, episode reward: -180.495, mean reward: -2.865 [-100.000,  5.974], mean action: 1.238 [0.000, 3.000],  loss: 22.409618, mae: 8.261713, mean_q: -2.451849, mean_eps: 0.798940\n",
      "   2384/1000000: episode: 23, duration: 0.581s, episode steps: 118, steps per second: 203, episode reward: -325.382, mean reward: -2.757 [-100.000,  2.659], mean action: 1.593 [0.000, 3.000],  loss: 14.181412, mae: 7.767071, mean_q: -1.077323, mean_eps: 0.790795\n",
      "   2522/1000000: episode: 24, duration: 0.676s, episode steps: 138, steps per second: 204, episode reward: -63.019, mean reward: -0.457 [-100.000, 20.968], mean action: 1.623 [0.000, 3.000],  loss: 8.843096, mae: 8.120845, mean_q: -1.515869, mean_eps: 0.779275\n",
      "   2621/1000000: episode: 25, duration: 0.474s, episode steps:  99, steps per second: 209, episode reward: -109.099, mean reward: -1.102 [-100.000, 17.762], mean action: 1.667 [0.000, 3.000],  loss: 11.934035, mae: 8.082648, mean_q: -0.162214, mean_eps: 0.768610\n",
      "   2750/1000000: episode: 26, duration: 0.693s, episode steps: 129, steps per second: 186, episode reward: -275.202, mean reward: -2.133 [-100.000, 48.561], mean action: 1.659 [0.000, 3.000],  loss: 9.718997, mae: 9.185500, mean_q: -1.626159, mean_eps: 0.758350\n",
      "   2854/1000000: episode: 27, duration: 0.510s, episode steps: 104, steps per second: 204, episode reward: -204.971, mean reward: -1.971 [-100.000, 24.532], mean action: 1.596 [0.000, 3.000],  loss: 8.230507, mae: 10.349162, mean_q: -3.945087, mean_eps: 0.747865\n",
      "   2974/1000000: episode: 28, duration: 0.608s, episode steps: 120, steps per second: 197, episode reward: -170.356, mean reward: -1.420 [-100.000, 25.596], mean action: 1.617 [0.000, 3.000],  loss: 7.182925, mae: 10.286741, mean_q: -3.130738, mean_eps: 0.737785\n",
      "   3105/1000000: episode: 29, duration: 0.627s, episode steps: 131, steps per second: 209, episode reward: -76.076, mean reward: -0.581 [-100.000,  8.657], mean action: 1.695 [0.000, 3.000],  loss: 7.555928, mae: 10.600909, mean_q: -4.047347, mean_eps: 0.726490\n",
      "   3245/1000000: episode: 30, duration: 0.673s, episode steps: 140, steps per second: 208, episode reward: -86.696, mean reward: -0.619 [-100.000, 16.034], mean action: 1.757 [0.000, 3.000],  loss: 8.697736, mae: 11.045326, mean_q: -3.932620, mean_eps: 0.714295\n",
      "   3375/1000000: episode: 31, duration: 0.621s, episode steps: 130, steps per second: 209, episode reward: -157.717, mean reward: -1.213 [-100.000, 10.404], mean action: 1.677 [0.000, 3.000],  loss: 6.512296, mae: 10.948247, mean_q: -3.062741, mean_eps: 0.702145\n",
      "   3513/1000000: episode: 32, duration: 0.663s, episode steps: 138, steps per second: 208, episode reward: -139.843, mean reward: -1.013 [-100.000, 10.700], mean action: 1.855 [0.000, 3.000],  loss: 5.214148, mae: 10.659594, mean_q: -2.390854, mean_eps: 0.690085\n",
      "   3597/1000000: episode: 33, duration: 0.402s, episode steps:  84, steps per second: 209, episode reward: -128.343, mean reward: -1.528 [-100.000, 10.693], mean action: 1.857 [0.000, 3.000],  loss: 5.509600, mae: 10.864516, mean_q: -3.687329, mean_eps: 0.680095\n",
      "   3728/1000000: episode: 34, duration: 0.637s, episode steps: 131, steps per second: 206, episode reward: -110.939, mean reward: -0.847 [-100.000, 13.045], mean action: 1.748 [0.000, 3.000],  loss: 6.329144, mae: 10.834918, mean_q: -3.509267, mean_eps: 0.670420\n",
      "   3848/1000000: episode: 35, duration: 0.580s, episode steps: 120, steps per second: 207, episode reward:  1.112, mean reward:  0.009 [-100.000, 66.688], mean action: 1.525 [0.000, 3.000],  loss: 3.942243, mae: 10.823577, mean_q: -2.503361, mean_eps: 0.659125\n",
      "   3984/1000000: episode: 36, duration: 0.676s, episode steps: 136, steps per second: 201, episode reward: -21.371, mean reward: -0.157 [-100.000, 12.790], mean action: 1.676 [0.000, 3.000],  loss: 6.896196, mae: 11.488280, mean_q: -2.440390, mean_eps: 0.647605\n",
      "   4093/1000000: episode: 37, duration: 0.525s, episode steps: 109, steps per second: 208, episode reward: -136.573, mean reward: -1.253 [-100.000, 16.405], mean action: 1.514 [0.000, 3.000],  loss: 5.306137, mae: 11.181761, mean_q: -1.351535, mean_eps: 0.636580\n",
      "   4276/1000000: episode: 38, duration: 0.906s, episode steps: 183, steps per second: 202, episode reward: -218.848, mean reward: -1.196 [-100.000, 49.245], mean action: 1.820 [0.000, 3.000],  loss: 5.058899, mae: 11.752134, mean_q: -0.815344, mean_eps: 0.623440\n",
      "   4429/1000000: episode: 39, duration: 0.749s, episode steps: 153, steps per second: 204, episode reward: -114.472, mean reward: -0.748 [-100.000,  5.913], mean action: 1.641 [0.000, 3.000],  loss: 6.100724, mae: 12.450558, mean_q: 0.143792, mean_eps: 0.608320\n",
      "   4540/1000000: episode: 40, duration: 0.541s, episode steps: 111, steps per second: 205, episode reward: -229.846, mean reward: -2.071 [-100.000,  7.717], mean action: 1.685 [0.000, 3.000],  loss: 6.120188, mae: 12.900395, mean_q: -0.549536, mean_eps: 0.596440\n",
      "   4688/1000000: episode: 41, duration: 0.714s, episode steps: 148, steps per second: 207, episode reward: -74.178, mean reward: -0.501 [-100.000, 64.016], mean action: 1.635 [0.000, 3.000],  loss: 9.476920, mae: 14.075490, mean_q: -0.355439, mean_eps: 0.584785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4836/1000000: episode: 42, duration: 0.713s, episode steps: 148, steps per second: 208, episode reward: -85.244, mean reward: -0.576 [-100.000, 13.701], mean action: 1.784 [0.000, 3.000],  loss: 10.014818, mae: 14.403841, mean_q: -1.249108, mean_eps: 0.571465\n",
      "   5000/1000000: episode: 43, duration: 0.782s, episode steps: 164, steps per second: 210, episode reward: -30.628, mean reward: -0.187 [-100.000, 22.829], mean action: 1.677 [0.000, 3.000],  loss: 8.248719, mae: 13.689221, mean_q: 0.502490, mean_eps: 0.557425\n",
      "   5100/1000000: episode: 44, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -83.857, mean reward: -0.839 [-100.000, 16.815], mean action: 1.690 [0.000, 3.000],  loss: 15.122543, mae: 13.379645, mean_q: 0.372218, mean_eps: 0.545545\n",
      "   5254/1000000: episode: 45, duration: 0.744s, episode steps: 154, steps per second: 207, episode reward: -39.728, mean reward: -0.258 [-100.000, 11.337], mean action: 1.617 [0.000, 3.000],  loss: 11.670069, mae: 13.241310, mean_q: 0.479612, mean_eps: 0.534115\n",
      "   5394/1000000: episode: 46, duration: 0.707s, episode steps: 140, steps per second: 198, episode reward: -166.059, mean reward: -1.186 [-100.000,  4.292], mean action: 1.893 [0.000, 3.000],  loss: 11.419762, mae: 11.972322, mean_q: 2.795474, mean_eps: 0.520885\n",
      "   5480/1000000: episode: 47, duration: 0.464s, episode steps:  86, steps per second: 185, episode reward: -77.808, mean reward: -0.905 [-100.000,  8.718], mean action: 1.547 [0.000, 3.000],  loss: 8.900971, mae: 12.417670, mean_q: 3.551778, mean_eps: 0.510715\n",
      "   5573/1000000: episode: 48, duration: 0.501s, episode steps:  93, steps per second: 186, episode reward: -107.963, mean reward: -1.161 [-100.000,  9.693], mean action: 1.548 [0.000, 3.000],  loss: 5.682558, mae: 11.979794, mean_q: 4.404097, mean_eps: 0.502660\n",
      "   5716/1000000: episode: 49, duration: 0.743s, episode steps: 143, steps per second: 192, episode reward: -233.907, mean reward: -1.636 [-100.000, 14.120], mean action: 1.224 [0.000, 3.000],  loss: 4.722569, mae: 12.413148, mean_q: 4.484511, mean_eps: 0.492040\n",
      "   5873/1000000: episode: 50, duration: 0.884s, episode steps: 157, steps per second: 178, episode reward: -122.891, mean reward: -0.783 [-100.000, 16.687], mean action: 1.828 [0.000, 3.000],  loss: 6.682986, mae: 12.969907, mean_q: 4.845964, mean_eps: 0.478540\n",
      "   6043/1000000: episode: 51, duration: 0.820s, episode steps: 170, steps per second: 207, episode reward: -27.815, mean reward: -0.164 [-100.000, 16.724], mean action: 1.724 [0.000, 3.000],  loss: 4.324400, mae: 14.358635, mean_q: 4.358756, mean_eps: 0.463825\n",
      "   6180/1000000: episode: 52, duration: 0.650s, episode steps: 137, steps per second: 211, episode reward: -44.197, mean reward: -0.323 [-100.000, 15.933], mean action: 1.701 [0.000, 3.000],  loss: 4.893442, mae: 14.669009, mean_q: 5.042146, mean_eps: 0.450010\n",
      "   6507/1000000: episode: 53, duration: 1.641s, episode steps: 327, steps per second: 199, episode reward: -29.734, mean reward: -0.091 [-100.000, 20.374], mean action: 1.801 [0.000, 3.000],  loss: 3.450220, mae: 14.452841, mean_q: 6.024075, mean_eps: 0.429130\n",
      "   6694/1000000: episode: 54, duration: 1.098s, episode steps: 187, steps per second: 170, episode reward: -150.456, mean reward: -0.805 [-100.000,  9.842], mean action: 1.759 [0.000, 3.000],  loss: 4.497822, mae: 13.977805, mean_q: 7.105681, mean_eps: 0.406000\n",
      "   6787/1000000: episode: 55, duration: 0.521s, episode steps:  93, steps per second: 178, episode reward: -45.530, mean reward: -0.490 [-100.000, 20.618], mean action: 1.710 [0.000, 3.000],  loss: 4.728312, mae: 14.247748, mean_q: 8.608838, mean_eps: 0.393400\n",
      "   6921/1000000: episode: 56, duration: 0.694s, episode steps: 134, steps per second: 193, episode reward: -50.062, mean reward: -0.374 [-100.000,  8.920], mean action: 1.739 [0.000, 3.000],  loss: 5.570724, mae: 14.688620, mean_q: 9.568556, mean_eps: 0.383185\n",
      "   7205/1000000: episode: 57, duration: 1.559s, episode steps: 284, steps per second: 182, episode reward: -49.444, mean reward: -0.174 [-100.000, 12.362], mean action: 1.870 [0.000, 3.000],  loss: 4.005521, mae: 14.312606, mean_q: 10.272919, mean_eps: 0.364375\n",
      "   8205/1000000: episode: 58, duration: 5.912s, episode steps: 1000, steps per second: 169, episode reward: 56.980, mean reward:  0.057 [-20.810, 30.874], mean action: 1.878 [0.000, 3.000],  loss: 3.643517, mae: 15.147327, mean_q: 0.811018, mean_eps: 0.306595\n",
      "   8906/1000000: episode: 59, duration: 4.162s, episode steps: 701, steps per second: 168, episode reward: 252.868, mean reward:  0.361 [-19.577, 100.000], mean action: 1.494 [0.000, 3.000],  loss: 4.740750, mae: 15.033049, mean_q: -12.099320, mean_eps: 0.230050\n",
      "   9906/1000000: episode: 60, duration: 5.665s, episode steps: 1000, steps per second: 177, episode reward: -43.603, mean reward: -0.044 [-10.450,  9.964], mean action: 1.631 [0.000, 3.000],  loss: 7.901357, mae: 14.021828, mean_q: -5.124217, mean_eps: 0.153505\n",
      "  10505/1000000: episode: 61, duration: 3.192s, episode steps: 599, steps per second: 188, episode reward: 188.108, mean reward:  0.314 [-13.395, 100.000], mean action: 2.165 [0.000, 3.000],  loss: 1.160618, mae: 11.794089, mean_q: 0.342837, mean_eps: 0.100671\n",
      "  11505/1000000: episode: 62, duration: 5.809s, episode steps: 1000, steps per second: 172, episode reward: -81.742, mean reward: -0.082 [-3.877,  5.156], mean action: 1.723 [0.000, 3.000],  loss: 7.037578, mae: 11.745269, mean_q: 8.594490, mean_eps: 0.100000\n",
      "  12505/1000000: episode: 63, duration: 5.446s, episode steps: 1000, steps per second: 184, episode reward: -44.633, mean reward: -0.045 [-3.904,  4.990], mean action: 1.805 [0.000, 3.000],  loss: 0.563941, mae: 7.603877, mean_q: 4.773535, mean_eps: 0.100000\n",
      "  13505/1000000: episode: 64, duration: 6.077s, episode steps: 1000, steps per second: 165, episode reward: -44.265, mean reward: -0.044 [-3.890,  4.719], mean action: 1.936 [0.000, 3.000],  loss: 0.640795, mae: 11.197054, mean_q: 3.300596, mean_eps: 0.100000\n",
      "  14505/1000000: episode: 65, duration: 5.784s, episode steps: 1000, steps per second: 173, episode reward: -53.691, mean reward: -0.054 [-3.331,  4.361], mean action: 1.579 [0.000, 3.000],  loss: 0.379468, mae: 10.778876, mean_q: 12.538569, mean_eps: 0.100000\n",
      "  15505/1000000: episode: 66, duration: 5.975s, episode steps: 1000, steps per second: 167, episode reward: 11.145, mean reward:  0.011 [-3.941,  6.830], mean action: 1.780 [0.000, 3.000],  loss: 0.359819, mae: 12.512293, mean_q: 17.054015, mean_eps: 0.100000\n",
      "  16505/1000000: episode: 67, duration: 5.957s, episode steps: 1000, steps per second: 168, episode reward: -42.080, mean reward: -0.042 [-3.250,  4.394], mean action: 1.600 [0.000, 3.000],  loss: 0.276509, mae: 10.184718, mean_q: 13.823812, mean_eps: 0.100000\n",
      "  17505/1000000: episode: 68, duration: 5.976s, episode steps: 1000, steps per second: 167, episode reward: -52.432, mean reward: -0.052 [-3.578,  4.287], mean action: 1.631 [0.000, 3.000],  loss: 0.267513, mae: 9.232174, mean_q: 12.544527, mean_eps: 0.100000\n",
      "  18505/1000000: episode: 69, duration: 5.639s, episode steps: 1000, steps per second: 177, episode reward: -35.127, mean reward: -0.035 [-4.288,  4.119], mean action: 1.913 [0.000, 3.000],  loss: 0.397161, mae: 10.194845, mean_q: -0.687526, mean_eps: 0.100000\n",
      "  19505/1000000: episode: 70, duration: 5.342s, episode steps: 1000, steps per second: 187, episode reward: -43.316, mean reward: -0.043 [-3.115,  3.934], mean action: 1.871 [0.000, 3.000],  loss: 0.628594, mae: 11.707828, mean_q: -5.079780, mean_eps: 0.100000\n",
      "  20505/1000000: episode: 71, duration: 5.891s, episode steps: 1000, steps per second: 170, episode reward: -60.493, mean reward: -0.060 [-2.975,  4.553], mean action: 1.678 [0.000, 3.000],  loss: 0.326441, mae: 6.945617, mean_q: 3.907061, mean_eps: 0.100000\n",
      "  21505/1000000: episode: 72, duration: 5.377s, episode steps: 1000, steps per second: 186, episode reward: -36.169, mean reward: -0.036 [-3.048,  4.308], mean action: 1.840 [0.000, 3.000],  loss: 0.240561, mae: 5.210290, mean_q: 2.906428, mean_eps: 0.100000\n",
      "  21964/1000000: episode: 73, duration: 2.378s, episode steps: 459, steps per second: 193, episode reward: 242.363, mean reward:  0.528 [-18.948, 100.000], mean action: 1.538 [0.000, 3.000],  loss: 0.458688, mae: 7.152333, mean_q: 4.184549, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  22227/1000000: episode: 74, duration: 1.305s, episode steps: 263, steps per second: 201, episode reward: -76.563, mean reward: -0.291 [-100.000,  4.062], mean action: 1.856 [0.000, 3.000],  loss: 4.333753, mae: 10.204486, mean_q: 8.967933, mean_eps: 0.100000\n",
      "  23227/1000000: episode: 75, duration: 5.331s, episode steps: 1000, steps per second: 188, episode reward: 46.372, mean reward:  0.046 [-18.645, 22.343], mean action: 1.631 [0.000, 3.000],  loss: 8.939444, mae: 11.450695, mean_q: 8.704385, mean_eps: 0.100000\n",
      "  23797/1000000: episode: 76, duration: 2.993s, episode steps: 570, steps per second: 190, episode reward: 152.529, mean reward:  0.268 [-16.319, 100.000], mean action: 2.196 [0.000, 3.000],  loss: 4.083459, mae: 9.893718, mean_q: 11.285403, mean_eps: 0.100000\n",
      "  23897/1000000: episode: 77, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -124.066, mean reward: -1.241 [-100.000,  9.382], mean action: 1.220 [0.000, 3.000],  loss: 7.172027, mae: 13.456568, mean_q: 18.873622, mean_eps: 0.100000\n",
      "  24332/1000000: episode: 78, duration: 2.271s, episode steps: 435, steps per second: 192, episode reward: 177.287, mean reward:  0.408 [-17.077, 100.000], mean action: 1.614 [0.000, 3.000],  loss: 7.489156, mae: 16.928170, mean_q: 23.187858, mean_eps: 0.100000\n",
      "  24424/1000000: episode: 79, duration: 0.444s, episode steps:  92, steps per second: 207, episode reward: -53.753, mean reward: -0.584 [-100.000, 13.070], mean action: 1.859 [0.000, 3.000],  loss: 12.998549, mae: 16.538469, mean_q: 22.808973, mean_eps: 0.100000\n",
      "  24703/1000000: episode: 80, duration: 1.350s, episode steps: 279, steps per second: 207, episode reward: -63.671, mean reward: -0.228 [-100.000,  9.862], mean action: 1.756 [0.000, 3.000],  loss: 14.494696, mae: 20.041886, mean_q: 27.360966, mean_eps: 0.100000\n",
      "  24779/1000000: episode: 81, duration: 0.361s, episode steps:  76, steps per second: 210, episode reward: -105.631, mean reward: -1.390 [-100.000, 11.353], mean action: 1.434 [0.000, 3.000],  loss: 20.320876, mae: 23.122084, mean_q: 30.767943, mean_eps: 0.100000\n",
      "  25080/1000000: episode: 82, duration: 1.478s, episode steps: 301, steps per second: 204, episode reward: 234.033, mean reward:  0.778 [-14.702, 100.000], mean action: 1.439 [0.000, 3.000],  loss: 14.979646, mae: 23.823013, mean_q: 31.459966, mean_eps: 0.100000\n",
      "  25181/1000000: episode: 83, duration: 0.484s, episode steps: 101, steps per second: 209, episode reward: -120.943, mean reward: -1.197 [-100.000, 17.172], mean action: 1.673 [0.000, 3.000],  loss: 13.420540, mae: 24.314437, mean_q: 32.178417, mean_eps: 0.100000\n",
      "  25283/1000000: episode: 84, duration: 0.485s, episode steps: 102, steps per second: 210, episode reward: -100.451, mean reward: -0.985 [-100.000, 10.315], mean action: 1.676 [0.000, 3.000],  loss: 26.392960, mae: 27.172745, mean_q: 35.108154, mean_eps: 0.100000\n",
      "  25490/1000000: episode: 85, duration: 0.993s, episode steps: 207, steps per second: 209, episode reward: -77.264, mean reward: -0.373 [-100.000,  8.877], mean action: 1.957 [0.000, 3.000],  loss: 13.211105, mae: 28.362691, mean_q: 34.759275, mean_eps: 0.100000\n",
      "  25812/1000000: episode: 86, duration: 1.619s, episode steps: 322, steps per second: 199, episode reward: -59.319, mean reward: -0.184 [-100.000,  9.281], mean action: 1.630 [0.000, 3.000],  loss: 15.380590, mae: 27.099490, mean_q: 31.993855, mean_eps: 0.100000\n",
      "  25932/1000000: episode: 87, duration: 0.595s, episode steps: 120, steps per second: 202, episode reward: -83.569, mean reward: -0.696 [-100.000, 16.919], mean action: 1.625 [0.000, 3.000],  loss: 22.667159, mae: 25.520643, mean_q: 29.334519, mean_eps: 0.100000\n",
      "  26030/1000000: episode: 88, duration: 0.484s, episode steps:  98, steps per second: 203, episode reward: -36.607, mean reward: -0.374 [-100.000, 18.735], mean action: 1.500 [0.000, 3.000],  loss: 10.154645, mae: 25.784997, mean_q: 28.226742, mean_eps: 0.100000\n",
      "  26178/1000000: episode: 89, duration: 0.716s, episode steps: 148, steps per second: 207, episode reward: -85.215, mean reward: -0.576 [-100.000, 10.070], mean action: 1.426 [0.000, 3.000],  loss: 14.233186, mae: 26.687824, mean_q: 26.892164, mean_eps: 0.100000\n",
      "  26523/1000000: episode: 90, duration: 1.710s, episode steps: 345, steps per second: 202, episode reward: 193.803, mean reward:  0.562 [-3.005, 100.000], mean action: 1.194 [0.000, 3.000],  loss: 9.055555, mae: 25.317125, mean_q: 26.970778, mean_eps: 0.100000\n",
      "  26656/1000000: episode: 91, duration: 0.633s, episode steps: 133, steps per second: 210, episode reward: -65.684, mean reward: -0.494 [-100.000, 59.071], mean action: 1.316 [0.000, 3.000],  loss: 13.141947, mae: 24.642179, mean_q: 26.121650, mean_eps: 0.100000\n",
      "  26752/1000000: episode: 92, duration: 0.458s, episode steps:  96, steps per second: 210, episode reward: -44.569, mean reward: -0.464 [-100.000, 22.588], mean action: 1.490 [0.000, 3.000],  loss: 8.538732, mae: 26.005446, mean_q: 22.889564, mean_eps: 0.100000\n",
      "  26927/1000000: episode: 93, duration: 0.861s, episode steps: 175, steps per second: 203, episode reward: -98.768, mean reward: -0.564 [-100.000, 12.102], mean action: 1.566 [0.000, 3.000],  loss: 7.111617, mae: 27.318231, mean_q: 24.957142, mean_eps: 0.100000\n",
      "  27029/1000000: episode: 94, duration: 0.490s, episode steps: 102, steps per second: 208, episode reward: -96.099, mean reward: -0.942 [-100.000,  8.575], mean action: 1.500 [0.000, 3.000],  loss: 10.330432, mae: 27.647590, mean_q: 22.702087, mean_eps: 0.100000\n",
      "  27144/1000000: episode: 95, duration: 0.560s, episode steps: 115, steps per second: 205, episode reward: -123.801, mean reward: -1.077 [-100.000,  9.720], mean action: 1.513 [0.000, 3.000],  loss: 7.276242, mae: 27.692364, mean_q: 22.530846, mean_eps: 0.100000\n",
      "  28144/1000000: episode: 96, duration: 6.227s, episode steps: 1000, steps per second: 161, episode reward: -102.444, mean reward: -0.102 [-3.063,  3.726], mean action: 1.869 [0.000, 3.000],  loss: 4.655811, mae: 22.155217, mean_q: 18.167396, mean_eps: 0.100000\n",
      "  29144/1000000: episode: 97, duration: 5.630s, episode steps: 1000, steps per second: 178, episode reward: -17.176, mean reward: -0.017 [-19.031, 17.010], mean action: 1.262 [0.000, 3.000],  loss: 3.646858, mae: 9.707571, mean_q: 10.387590, mean_eps: 0.100000\n",
      "  29333/1000000: episode: 98, duration: 0.917s, episode steps: 189, steps per second: 206, episode reward: -59.126, mean reward: -0.313 [-100.000,  3.615], mean action: 1.899 [0.000, 3.000],  loss: 9.249850, mae: 10.169001, mean_q: 10.971613, mean_eps: 0.100000\n",
      "  29634/1000000: episode: 99, duration: 1.539s, episode steps: 301, steps per second: 196, episode reward: -45.385, mean reward: -0.151 [-100.000, 11.250], mean action: 1.708 [0.000, 3.000],  loss: 11.346211, mae: 13.163629, mean_q: 15.563212, mean_eps: 0.100000\n",
      "  30291/1000000: episode: 100, duration: 3.726s, episode steps: 657, steps per second: 176, episode reward: 211.426, mean reward:  0.322 [-3.216, 100.000], mean action: 1.174 [0.000, 3.000],  loss: 11.033360, mae: 18.756808, mean_q: 21.301057, mean_eps: 0.100000\n",
      "  31291/1000000: episode: 101, duration: 5.734s, episode steps: 1000, steps per second: 174, episode reward: 54.624, mean reward:  0.055 [-18.865, 21.103], mean action: 1.696 [0.000, 3.000],  loss: 7.674236, mae: 13.367105, mean_q: 3.966191, mean_eps: 0.100000\n",
      "  32291/1000000: episode: 102, duration: 5.962s, episode steps: 1000, steps per second: 168, episode reward: -32.109, mean reward: -0.032 [-8.634, 10.515], mean action: 1.846 [0.000, 3.000],  loss: 1.166083, mae: 10.415523, mean_q: -0.594489, mean_eps: 0.100000\n",
      "  32554/1000000: episode: 103, duration: 1.272s, episode steps: 263, steps per second: 207, episode reward: -13.308, mean reward: -0.051 [-100.000, 13.827], mean action: 1.525 [0.000, 3.000],  loss: 0.614320, mae: 8.494873, mean_q: 6.933065, mean_eps: 0.100000\n",
      "  33029/1000000: episode: 104, duration: 2.495s, episode steps: 475, steps per second: 190, episode reward: 262.504, mean reward:  0.553 [-10.862, 100.000], mean action: 1.269 [0.000, 3.000],  loss: 2.403947, mae: 13.447990, mean_q: 11.331666, mean_eps: 0.100000\n",
      "  33380/1000000: episode: 105, duration: 1.767s, episode steps: 351, steps per second: 199, episode reward: 228.258, mean reward:  0.650 [-19.708, 100.000], mean action: 1.145 [0.000, 3.000],  loss: 7.428994, mae: 17.855175, mean_q: 16.670419, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  33743/1000000: episode: 106, duration: 1.786s, episode steps: 363, steps per second: 203, episode reward: 295.955, mean reward:  0.815 [-16.575, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 7.697164, mae: 15.013220, mean_q: 11.694701, mean_eps: 0.100000\n",
      "  34209/1000000: episode: 107, duration: 2.391s, episode steps: 466, steps per second: 195, episode reward: 240.999, mean reward:  0.517 [-17.666, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 13.604781, mae: 14.456079, mean_q: 15.515842, mean_eps: 0.100000\n",
      "  35209/1000000: episode: 108, duration: 5.360s, episode steps: 1000, steps per second: 187, episode reward: -15.376, mean reward: -0.015 [-3.348,  3.823], mean action: 1.587 [0.000, 3.000],  loss: 7.767347, mae: 14.677855, mean_q: 19.226097, mean_eps: 0.100000\n",
      "  35932/1000000: episode: 109, duration: 3.782s, episode steps: 723, steps per second: 191, episode reward: 147.516, mean reward:  0.204 [-10.225, 100.000], mean action: 1.700 [0.000, 3.000],  loss: 0.595985, mae: 13.935055, mean_q: 18.116485, mean_eps: 0.100000\n",
      "  36932/1000000: episode: 110, duration: 5.527s, episode steps: 1000, steps per second: 181, episode reward: 14.209, mean reward:  0.014 [-13.672, 23.306], mean action: 1.895 [0.000, 3.000],  loss: 3.851370, mae: 8.926039, mean_q: 8.859019, mean_eps: 0.100000\n",
      "  37181/1000000: episode: 111, duration: 1.201s, episode steps: 249, steps per second: 207, episode reward: -64.266, mean reward: -0.258 [-100.000, 11.301], mean action: 1.237 [0.000, 3.000],  loss: 1.807147, mae: 8.147569, mean_q: 9.913672, mean_eps: 0.100000\n",
      "  37659/1000000: episode: 112, duration: 2.431s, episode steps: 478, steps per second: 197, episode reward: -188.356, mean reward: -0.394 [-100.000, 23.591], mean action: 1.619 [0.000, 3.000],  loss: 4.899334, mae: 14.888537, mean_q: 20.526206, mean_eps: 0.100000\n",
      "  37786/1000000: episode: 113, duration: 0.612s, episode steps: 127, steps per second: 207, episode reward: -92.531, mean reward: -0.729 [-100.000,  4.227], mean action: 1.874 [0.000, 3.000],  loss: 10.403895, mae: 20.386548, mean_q: 27.678016, mean_eps: 0.100000\n",
      "  37920/1000000: episode: 114, duration: 0.638s, episode steps: 134, steps per second: 210, episode reward: -217.546, mean reward: -1.623 [-100.000,  4.278], mean action: 1.657 [0.000, 3.000],  loss: 14.838005, mae: 23.489932, mean_q: 27.926191, mean_eps: 0.100000\n",
      "  38350/1000000: episode: 115, duration: 2.138s, episode steps: 430, steps per second: 201, episode reward: -301.942, mean reward: -0.702 [-100.000, 21.195], mean action: 1.298 [0.000, 3.000],  loss: 10.250043, mae: 24.048970, mean_q: 21.839484, mean_eps: 0.100000\n",
      "  38679/1000000: episode: 116, duration: 1.621s, episode steps: 329, steps per second: 203, episode reward: 220.893, mean reward:  0.671 [-10.538, 100.000], mean action: 1.149 [0.000, 3.000],  loss: 7.192896, mae: 25.836647, mean_q: 12.751761, mean_eps: 0.100000\n",
      "  39679/1000000: episode: 117, duration: 5.456s, episode steps: 1000, steps per second: 183, episode reward: -108.096, mean reward: -0.108 [-20.642, 21.000], mean action: 1.357 [0.000, 3.000],  loss: 4.889099, mae: 23.015434, mean_q: 22.138495, mean_eps: 0.100000\n",
      "  39776/1000000: episode: 118, duration: 0.470s, episode steps:  97, steps per second: 206, episode reward: -160.492, mean reward: -1.655 [-100.000,  9.240], mean action: 2.072 [0.000, 3.000],  loss: 1.925000, mae: 19.237470, mean_q: 26.196796, mean_eps: 0.100000\n",
      "  40570/1000000: episode: 119, duration: 4.412s, episode steps: 794, steps per second: 180, episode reward: 126.145, mean reward:  0.159 [-20.627, 100.000], mean action: 2.060 [0.000, 3.000],  loss: 3.754881, mae: 19.437340, mean_q: 25.110973, mean_eps: 0.100000\n",
      "  41039/1000000: episode: 120, duration: 2.421s, episode steps: 469, steps per second: 194, episode reward: 178.968, mean reward:  0.382 [-14.092, 100.000], mean action: 2.098 [0.000, 3.000],  loss: 4.768512, mae: 19.107832, mean_q: 25.660646, mean_eps: 0.100000\n",
      "  41703/1000000: episode: 121, duration: 3.579s, episode steps: 664, steps per second: 186, episode reward: 181.901, mean reward:  0.274 [-18.941, 100.000], mean action: 2.321 [0.000, 3.000],  loss: 6.166726, mae: 19.661026, mean_q: 27.188523, mean_eps: 0.100000\n",
      "  42178/1000000: episode: 122, duration: 2.418s, episode steps: 475, steps per second: 196, episode reward: 213.408, mean reward:  0.449 [-18.558, 100.000], mean action: 1.619 [0.000, 3.000],  loss: 7.770684, mae: 20.928465, mean_q: 28.957073, mean_eps: 0.100000\n",
      "  43124/1000000: episode: 123, duration: 5.651s, episode steps: 946, steps per second: 167, episode reward: 122.511, mean reward:  0.130 [-19.181, 100.000], mean action: 1.744 [0.000, 3.000],  loss: 5.735376, mae: 22.759721, mean_q: 31.397361, mean_eps: 0.100000\n",
      "  44012/1000000: episode: 124, duration: 5.252s, episode steps: 888, steps per second: 169, episode reward: 136.251, mean reward:  0.153 [-17.643, 100.000], mean action: 1.524 [0.000, 3.000],  loss: 5.376674, mae: 20.714411, mean_q: 28.260018, mean_eps: 0.100000\n",
      "  44806/1000000: episode: 125, duration: 4.259s, episode steps: 794, steps per second: 186, episode reward: 110.046, mean reward:  0.139 [-11.225, 100.000], mean action: 1.455 [0.000, 3.000],  loss: 3.766828, mae: 20.975092, mean_q: 28.838002, mean_eps: 0.100000\n",
      "  45806/1000000: episode: 126, duration: 5.345s, episode steps: 1000, steps per second: 187, episode reward: 66.508, mean reward:  0.067 [-20.515, 22.337], mean action: 1.628 [0.000, 3.000],  loss: 3.904756, mae: 19.893929, mean_q: 27.283699, mean_eps: 0.100000\n",
      "  45897/1000000: episode: 127, duration: 0.436s, episode steps:  91, steps per second: 209, episode reward:  1.981, mean reward:  0.022 [-100.000, 16.244], mean action: 1.846 [0.000, 3.000],  loss: 1.434778, mae: 20.061914, mean_q: 27.494176, mean_eps: 0.100000\n",
      "  46227/1000000: episode: 128, duration: 1.641s, episode steps: 330, steps per second: 201, episode reward: 232.249, mean reward:  0.704 [-17.508, 100.000], mean action: 1.464 [0.000, 3.000],  loss: 6.279785, mae: 22.519041, mean_q: 30.913659, mean_eps: 0.100000\n",
      "  46382/1000000: episode: 129, duration: 0.742s, episode steps: 155, steps per second: 209, episode reward: 36.723, mean reward:  0.237 [-100.000, 25.624], mean action: 1.858 [0.000, 3.000],  loss: 7.199375, mae: 25.880723, mean_q: 35.524745, mean_eps: 0.100000\n",
      "  46977/1000000: episode: 130, duration: 3.083s, episode steps: 595, steps per second: 193, episode reward: 207.281, mean reward:  0.348 [-19.243, 100.000], mean action: 1.306 [0.000, 3.000],  loss: 12.057730, mae: 29.658301, mean_q: 40.464419, mean_eps: 0.100000\n",
      "  47418/1000000: episode: 131, duration: 2.296s, episode steps: 441, steps per second: 192, episode reward: 233.916, mean reward:  0.530 [-13.370, 100.000], mean action: 1.333 [0.000, 3.000],  loss: 9.373307, mae: 27.738786, mean_q: 38.282702, mean_eps: 0.100000\n",
      "  47884/1000000: episode: 132, duration: 2.368s, episode steps: 466, steps per second: 197, episode reward: 281.569, mean reward:  0.604 [-18.256, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 5.704997, mae: 25.626776, mean_q: 35.629351, mean_eps: 0.100000\n",
      "  48201/1000000: episode: 133, duration: 1.606s, episode steps: 317, steps per second: 197, episode reward: 259.297, mean reward:  0.818 [-2.795, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 6.452657, mae: 26.296135, mean_q: 36.328599, mean_eps: 0.100000\n",
      "  48518/1000000: episode: 134, duration: 1.578s, episode steps: 317, steps per second: 201, episode reward: 218.773, mean reward:  0.690 [-8.723, 100.000], mean action: 1.230 [0.000, 3.000],  loss: 8.788433, mae: 26.703109, mean_q: 36.601686, mean_eps: 0.100000\n",
      "  48732/1000000: episode: 135, duration: 1.038s, episode steps: 214, steps per second: 206, episode reward: -13.174, mean reward: -0.062 [-100.000, 13.556], mean action: 1.874 [0.000, 3.000],  loss: 7.664051, mae: 28.379722, mean_q: 38.627442, mean_eps: 0.100000\n",
      "  49169/1000000: episode: 136, duration: 2.187s, episode steps: 437, steps per second: 200, episode reward: -20.884, mean reward: -0.048 [-100.000, 12.920], mean action: 1.476 [0.000, 3.000],  loss: 16.472722, mae: 30.196155, mean_q: 41.143776, mean_eps: 0.100000\n",
      "  49288/1000000: episode: 137, duration: 0.566s, episode steps: 119, steps per second: 210, episode reward: -23.147, mean reward: -0.195 [-100.000, 10.765], mean action: 1.597 [0.000, 3.000],  loss: 15.134744, mae: 31.283478, mean_q: 42.729399, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50073/1000000: episode: 138, duration: 4.289s, episode steps: 785, steps per second: 183, episode reward: 155.040, mean reward:  0.198 [-7.898, 100.000], mean action: 1.980 [0.000, 3.000],  loss: 14.340638, mae: 28.552230, mean_q: 38.691316, mean_eps: 0.100000\n",
      "  50370/1000000: episode: 139, duration: 1.468s, episode steps: 297, steps per second: 202, episode reward: 234.814, mean reward:  0.791 [-17.612, 100.000], mean action: 2.226 [0.000, 3.000],  loss: 6.323265, mae: 26.974830, mean_q: 33.677852, mean_eps: 0.100000\n",
      "  50503/1000000: episode: 140, duration: 0.637s, episode steps: 133, steps per second: 209, episode reward: -15.159, mean reward: -0.114 [-100.000, 16.695], mean action: 1.421 [0.000, 3.000],  loss: 6.489678, mae: 27.739092, mean_q: 28.250813, mean_eps: 0.100000\n",
      "  50861/1000000: episode: 141, duration: 1.778s, episode steps: 358, steps per second: 201, episode reward: -218.706, mean reward: -0.611 [-100.000, 15.939], mean action: 1.897 [0.000, 3.000],  loss: 15.120761, mae: 28.828953, mean_q: 28.374727, mean_eps: 0.100000\n",
      "  51232/1000000: episode: 142, duration: 1.841s, episode steps: 371, steps per second: 202, episode reward: -62.880, mean reward: -0.169 [-100.000, 15.066], mean action: 1.623 [0.000, 3.000],  loss: 16.094404, mae: 28.225447, mean_q: 21.330238, mean_eps: 0.100000\n",
      "  51405/1000000: episode: 143, duration: 0.822s, episode steps: 173, steps per second: 210, episode reward: 27.504, mean reward:  0.159 [-100.000, 19.365], mean action: 1.520 [0.000, 3.000],  loss: 12.099706, mae: 30.220424, mean_q: 27.444383, mean_eps: 0.100000\n",
      "  52013/1000000: episode: 144, duration: 3.261s, episode steps: 608, steps per second: 186, episode reward: 145.940, mean reward:  0.240 [-14.528, 100.000], mean action: 1.562 [0.000, 3.000],  loss: 8.063845, mae: 28.516744, mean_q: 31.670362, mean_eps: 0.100000\n",
      "  53013/1000000: episode: 145, duration: 6.056s, episode steps: 1000, steps per second: 165, episode reward: -28.453, mean reward: -0.028 [-13.368, 13.260], mean action: 1.567 [0.000, 3.000],  loss: 6.117328, mae: 22.617819, mean_q: 30.827685, mean_eps: 0.100000\n",
      "  53462/1000000: episode: 146, duration: 2.226s, episode steps: 449, steps per second: 202, episode reward: 274.828, mean reward:  0.612 [-20.235, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 1.281666, mae: 25.468830, mean_q: 34.656313, mean_eps: 0.100000\n",
      "  54016/1000000: episode: 147, duration: 2.935s, episode steps: 554, steps per second: 189, episode reward: 248.153, mean reward:  0.448 [-17.460, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 7.157520, mae: 25.521387, mean_q: 34.900482, mean_eps: 0.100000\n",
      "  54442/1000000: episode: 148, duration: 2.136s, episode steps: 426, steps per second: 199, episode reward: 248.224, mean reward:  0.583 [-19.891, 100.000], mean action: 0.981 [0.000, 3.000],  loss: 8.135234, mae: 24.819388, mean_q: 33.550871, mean_eps: 0.100000\n",
      "  54712/1000000: episode: 149, duration: 1.316s, episode steps: 270, steps per second: 205, episode reward: 255.191, mean reward:  0.945 [-12.035, 100.000], mean action: 1.485 [0.000, 3.000],  loss: 8.347881, mae: 25.090382, mean_q: 33.763010, mean_eps: 0.100000\n",
      "  55190/1000000: episode: 150, duration: 2.529s, episode steps: 478, steps per second: 189, episode reward: 219.137, mean reward:  0.458 [-20.203, 100.000], mean action: 1.416 [0.000, 3.000],  loss: 8.826092, mae: 25.791272, mean_q: 35.018814, mean_eps: 0.100000\n",
      "  55585/1000000: episode: 151, duration: 1.974s, episode steps: 395, steps per second: 200, episode reward: 237.303, mean reward:  0.601 [-10.063, 100.000], mean action: 0.965 [0.000, 3.000],  loss: 10.052823, mae: 27.363841, mean_q: 37.174179, mean_eps: 0.100000\n",
      "  56360/1000000: episode: 152, duration: 4.121s, episode steps: 775, steps per second: 188, episode reward: 192.010, mean reward:  0.248 [-20.033, 100.000], mean action: 2.045 [0.000, 3.000],  loss: 6.186620, mae: 28.019478, mean_q: 38.187201, mean_eps: 0.100000\n",
      "  56877/1000000: episode: 153, duration: 2.699s, episode steps: 517, steps per second: 192, episode reward: 235.764, mean reward:  0.456 [-17.525, 100.000], mean action: 1.228 [0.000, 3.000],  loss: 2.836388, mae: 31.001403, mean_q: 42.153048, mean_eps: 0.100000\n",
      "  57424/1000000: episode: 154, duration: 2.802s, episode steps: 547, steps per second: 195, episode reward: 148.895, mean reward:  0.272 [-20.942, 100.000], mean action: 1.700 [0.000, 3.000],  loss: 4.114651, mae: 31.246349, mean_q: 42.548471, mean_eps: 0.100000\n",
      "  58404/1000000: episode: 155, duration: 5.307s, episode steps: 980, steps per second: 185, episode reward: 136.121, mean reward:  0.139 [-18.621, 100.000], mean action: 2.143 [0.000, 3.000],  loss: 2.447138, mae: 31.252268, mean_q: 42.723537, mean_eps: 0.100000\n",
      "  59339/1000000: episode: 156, duration: 4.921s, episode steps: 935, steps per second: 190, episode reward: 70.669, mean reward:  0.076 [-11.630, 100.000], mean action: 1.873 [0.000, 3.000],  loss: 1.656973, mae: 27.638744, mean_q: 37.719688, mean_eps: 0.100000\n",
      "  59802/1000000: episode: 157, duration: 2.433s, episode steps: 463, steps per second: 190, episode reward: 147.155, mean reward:  0.318 [-17.145, 100.000], mean action: 1.924 [0.000, 3.000],  loss: 2.554498, mae: 24.029975, mean_q: 32.540773, mean_eps: 0.100000\n",
      "  60551/1000000: episode: 158, duration: 3.832s, episode steps: 749, steps per second: 195, episode reward: 98.769, mean reward:  0.132 [-13.223, 100.000], mean action: 1.955 [0.000, 3.000],  loss: 2.561782, mae: 30.221187, mean_q: 41.214027, mean_eps: 0.100000\n",
      "  60875/1000000: episode: 159, duration: 1.571s, episode steps: 324, steps per second: 206, episode reward: 263.228, mean reward:  0.812 [-18.855, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 2.056516, mae: 32.714841, mean_q: 45.151880, mean_eps: 0.100000\n",
      "  61266/1000000: episode: 160, duration: 1.978s, episode steps: 391, steps per second: 198, episode reward: 191.095, mean reward:  0.489 [-14.644, 100.000], mean action: 1.422 [0.000, 3.000],  loss: 3.203910, mae: 33.766604, mean_q: 46.600181, mean_eps: 0.100000\n",
      "  61544/1000000: episode: 161, duration: 1.349s, episode steps: 278, steps per second: 206, episode reward: 270.635, mean reward:  0.974 [-10.257, 100.000], mean action: 1.122 [0.000, 3.000],  loss: 5.324404, mae: 37.313687, mean_q: 51.015891, mean_eps: 0.100000\n",
      "  62544/1000000: episode: 162, duration: 5.390s, episode steps: 1000, steps per second: 186, episode reward: 79.340, mean reward:  0.079 [-23.927, 21.463], mean action: 1.740 [0.000, 3.000],  loss: 3.861938, mae: 36.520889, mean_q: 49.638896, mean_eps: 0.100000\n",
      "  62800/1000000: episode: 163, duration: 1.238s, episode steps: 256, steps per second: 207, episode reward: 249.573, mean reward:  0.975 [-2.753, 100.000], mean action: 1.508 [0.000, 3.000],  loss: 2.235867, mae: 35.747972, mean_q: 49.243426, mean_eps: 0.100000\n",
      "  63703/1000000: episode: 164, duration: 5.016s, episode steps: 903, steps per second: 180, episode reward: 180.326, mean reward:  0.200 [-18.758, 100.000], mean action: 2.120 [0.000, 3.000],  loss: 2.840996, mae: 35.908135, mean_q: 49.037912, mean_eps: 0.100000\n",
      "  64055/1000000: episode: 165, duration: 1.735s, episode steps: 352, steps per second: 203, episode reward: 233.141, mean reward:  0.662 [-17.456, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 2.398982, mae: 36.075962, mean_q: 49.467806, mean_eps: 0.100000\n",
      "  64300/1000000: episode: 166, duration: 1.186s, episode steps: 245, steps per second: 207, episode reward: 275.653, mean reward:  1.125 [-3.124, 100.000], mean action: 1.559 [0.000, 3.000],  loss: 2.465146, mae: 38.782366, mean_q: 53.344347, mean_eps: 0.100000\n",
      "  64857/1000000: episode: 167, duration: 2.890s, episode steps: 557, steps per second: 193, episode reward: 289.888, mean reward:  0.520 [-19.659, 100.000], mean action: 1.156 [0.000, 3.000],  loss: 6.545315, mae: 45.005107, mean_q: 61.483783, mean_eps: 0.100000\n",
      "  65225/1000000: episode: 168, duration: 1.819s, episode steps: 368, steps per second: 202, episode reward: 266.283, mean reward:  0.724 [-20.627, 100.000], mean action: 0.851 [0.000, 3.000],  loss: 5.078096, mae: 47.489549, mean_q: 64.866839, mean_eps: 0.100000\n",
      "  65511/1000000: episode: 169, duration: 1.388s, episode steps: 286, steps per second: 206, episode reward: 251.535, mean reward:  0.879 [-11.016, 100.000], mean action: 1.140 [0.000, 3.000],  loss: 4.178332, mae: 48.363053, mean_q: 66.195469, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  65862/1000000: episode: 170, duration: 1.748s, episode steps: 351, steps per second: 201, episode reward: 218.387, mean reward:  0.622 [-2.799, 100.000], mean action: 1.376 [0.000, 3.000],  loss: 4.032458, mae: 47.334263, mean_q: 64.611072, mean_eps: 0.100000\n",
      "  66205/1000000: episode: 171, duration: 1.792s, episode steps: 343, steps per second: 191, episode reward: 271.433, mean reward:  0.791 [-20.663, 100.000], mean action: 0.857 [0.000, 3.000],  loss: 6.162441, mae: 46.950796, mean_q: 63.896057, mean_eps: 0.100000\n",
      "  66719/1000000: episode: 172, duration: 2.749s, episode steps: 514, steps per second: 187, episode reward: 253.855, mean reward:  0.494 [-20.271, 100.000], mean action: 1.490 [0.000, 3.000],  loss: 3.826026, mae: 43.093229, mean_q: 58.631339, mean_eps: 0.100000\n",
      "  66898/1000000: episode: 173, duration: 0.949s, episode steps: 179, steps per second: 189, episode reward: -4.736, mean reward: -0.026 [-100.000, 13.122], mean action: 1.810 [0.000, 3.000],  loss: 4.160287, mae: 43.026567, mean_q: 58.765209, mean_eps: 0.100000\n",
      "  67228/1000000: episode: 174, duration: 1.692s, episode steps: 330, steps per second: 195, episode reward: 177.943, mean reward:  0.539 [-20.860, 100.000], mean action: 1.818 [0.000, 3.000],  loss: 12.217050, mae: 43.046992, mean_q: 58.590633, mean_eps: 0.100000\n",
      "  67858/1000000: episode: 175, duration: 3.456s, episode steps: 630, steps per second: 182, episode reward: 189.962, mean reward:  0.302 [-11.694, 100.000], mean action: 1.729 [0.000, 3.000],  loss: 15.994931, mae: 45.337334, mean_q: 61.568958, mean_eps: 0.100000\n",
      "  68211/1000000: episode: 176, duration: 1.760s, episode steps: 353, steps per second: 201, episode reward: 240.519, mean reward:  0.681 [-10.419, 100.000], mean action: 0.949 [0.000, 3.000],  loss: 3.176027, mae: 43.626682, mean_q: 59.517505, mean_eps: 0.100000\n",
      "  69037/1000000: episode: 177, duration: 4.739s, episode steps: 826, steps per second: 174, episode reward: 231.192, mean reward:  0.280 [-19.073, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 1.816320, mae: 40.597006, mean_q: 55.212971, mean_eps: 0.100000\n",
      "  69296/1000000: episode: 178, duration: 1.323s, episode steps: 259, steps per second: 196, episode reward: 281.031, mean reward:  1.085 [-7.748, 100.000], mean action: 1.440 [0.000, 3.000],  loss: 3.032545, mae: 37.692914, mean_q: 51.426870, mean_eps: 0.100000\n",
      "  69690/1000000: episode: 179, duration: 2.002s, episode steps: 394, steps per second: 197, episode reward: 212.292, mean reward:  0.539 [-19.463, 100.000], mean action: 1.335 [0.000, 3.000],  loss: 3.485973, mae: 40.684936, mean_q: 55.477596, mean_eps: 0.100000\n",
      "  70006/1000000: episode: 180, duration: 1.574s, episode steps: 316, steps per second: 201, episode reward: 272.208, mean reward:  0.861 [-17.338, 100.000], mean action: 0.943 [0.000, 3.000],  loss: 6.015317, mae: 46.521742, mean_q: 63.164364, mean_eps: 0.100000\n",
      "  70273/1000000: episode: 181, duration: 1.324s, episode steps: 267, steps per second: 202, episode reward: 285.232, mean reward:  1.068 [-8.851, 100.000], mean action: 1.378 [0.000, 3.000],  loss: 4.769417, mae: 49.092844, mean_q: 66.626516, mean_eps: 0.100000\n",
      "  70629/1000000: episode: 182, duration: 1.711s, episode steps: 356, steps per second: 208, episode reward: 290.019, mean reward:  0.815 [-19.485, 100.000], mean action: 0.966 [0.000, 3.000],  loss: 3.244037, mae: 50.736291, mean_q: 69.007126, mean_eps: 0.100000\n",
      "  70744/1000000: episode: 183, duration: 0.546s, episode steps: 115, steps per second: 211, episode reward: 70.290, mean reward:  0.611 [-100.000, 13.861], mean action: 1.835 [0.000, 3.000],  loss: 3.626915, mae: 54.352926, mean_q: 73.853812, mean_eps: 0.100000\n",
      "  71215/1000000: episode: 184, duration: 2.379s, episode steps: 471, steps per second: 198, episode reward: 264.425, mean reward:  0.561 [-18.655, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 14.882447, mae: 56.265096, mean_q: 75.987858, mean_eps: 0.100000\n",
      "  71331/1000000: episode: 185, duration: 0.551s, episode steps: 116, steps per second: 210, episode reward: -144.463, mean reward: -1.245 [-100.000, 33.267], mean action: 1.491 [0.000, 3.000],  loss: 9.816221, mae: 55.087031, mean_q: 74.346593, mean_eps: 0.100000\n",
      "  71471/1000000: episode: 186, duration: 0.656s, episode steps: 140, steps per second: 213, episode reward: -35.503, mean reward: -0.254 [-100.000, 19.028], mean action: 1.400 [0.000, 3.000],  loss: 33.534813, mae: 55.031626, mean_q: 74.513117, mean_eps: 0.100000\n",
      "  71581/1000000: episode: 187, duration: 0.517s, episode steps: 110, steps per second: 213, episode reward: -24.343, mean reward: -0.221 [-100.000, 19.029], mean action: 1.345 [0.000, 3.000],  loss: 43.301343, mae: 56.230636, mean_q: 76.110326, mean_eps: 0.100000\n",
      "  71662/1000000: episode: 188, duration: 0.387s, episode steps:  81, steps per second: 209, episode reward: 42.522, mean reward:  0.525 [-100.000, 20.410], mean action: 1.852 [0.000, 3.000],  loss: 53.334845, mae: 58.872100, mean_q: 79.455873, mean_eps: 0.100000\n",
      "  71760/1000000: episode: 189, duration: 0.464s, episode steps:  98, steps per second: 211, episode reward:  2.463, mean reward:  0.025 [-100.000, 12.581], mean action: 1.857 [0.000, 3.000],  loss: 79.185496, mae: 58.507073, mean_q: 79.026834, mean_eps: 0.100000\n",
      "  71947/1000000: episode: 190, duration: 0.891s, episode steps: 187, steps per second: 210, episode reward: -18.147, mean reward: -0.097 [-100.000, 15.054], mean action: 1.620 [0.000, 3.000],  loss: 57.690414, mae: 57.990787, mean_q: 78.848790, mean_eps: 0.100000\n",
      "  72051/1000000: episode: 191, duration: 0.487s, episode steps: 104, steps per second: 214, episode reward: -187.929, mean reward: -1.807 [-100.000, 17.194], mean action: 1.000 [0.000, 2.000],  loss: 36.252782, mae: 59.165090, mean_q: 80.355183, mean_eps: 0.100000\n",
      "  72163/1000000: episode: 192, duration: 0.528s, episode steps: 112, steps per second: 212, episode reward: 21.619, mean reward:  0.193 [-100.000, 16.657], mean action: 1.312 [0.000, 3.000],  loss: 37.728810, mae: 62.367199, mean_q: 84.105047, mean_eps: 0.100000\n",
      "  72261/1000000: episode: 193, duration: 0.471s, episode steps:  98, steps per second: 208, episode reward: 30.395, mean reward:  0.310 [-100.000, 18.011], mean action: 1.857 [0.000, 3.000],  loss: 49.416644, mae: 65.515594, mean_q: 87.565101, mean_eps: 0.100000\n",
      "  72379/1000000: episode: 194, duration: 0.557s, episode steps: 118, steps per second: 212, episode reward: -42.200, mean reward: -0.358 [-100.000,  9.327], mean action: 1.381 [0.000, 3.000],  loss: 52.337596, mae: 65.356331, mean_q: 87.160927, mean_eps: 0.100000\n",
      "  72509/1000000: episode: 195, duration: 0.632s, episode steps: 130, steps per second: 206, episode reward: -61.236, mean reward: -0.471 [-100.000,  9.111], mean action: 1.608 [0.000, 3.000],  loss: 47.825977, mae: 66.246351, mean_q: 87.581655, mean_eps: 0.100000\n",
      "  72629/1000000: episode: 196, duration: 0.583s, episode steps: 120, steps per second: 206, episode reward: -62.839, mean reward: -0.524 [-100.000, 15.815], mean action: 1.483 [0.000, 3.000],  loss: 36.767109, mae: 64.807975, mean_q: 85.212069, mean_eps: 0.100000\n",
      "  72732/1000000: episode: 197, duration: 0.517s, episode steps: 103, steps per second: 199, episode reward: -99.189, mean reward: -0.963 [-100.000,  8.887], mean action: 1.816 [0.000, 3.000],  loss: 30.646261, mae: 62.096617, mean_q: 81.038865, mean_eps: 0.100000\n",
      "  72897/1000000: episode: 198, duration: 0.869s, episode steps: 165, steps per second: 190, episode reward: 32.498, mean reward:  0.197 [-100.000, 15.595], mean action: 1.679 [0.000, 3.000],  loss: 25.725869, mae: 61.449562, mean_q: 79.273415, mean_eps: 0.100000\n",
      "  73024/1000000: episode: 199, duration: 0.641s, episode steps: 127, steps per second: 198, episode reward: -14.208, mean reward: -0.112 [-100.000, 13.392], mean action: 1.535 [0.000, 3.000],  loss: 30.041520, mae: 62.154721, mean_q: 78.657666, mean_eps: 0.100000\n",
      "  73182/1000000: episode: 200, duration: 0.789s, episode steps: 158, steps per second: 200, episode reward: -3.946, mean reward: -0.025 [-100.000, 16.375], mean action: 1.589 [0.000, 3.000],  loss: 15.782418, mae: 62.081094, mean_q: 78.456347, mean_eps: 0.100000\n",
      "  73331/1000000: episode: 201, duration: 0.850s, episode steps: 149, steps per second: 175, episode reward: 38.179, mean reward:  0.256 [-100.000, 14.841], mean action: 1.846 [0.000, 3.000],  loss: 14.518434, mae: 58.817438, mean_q: 73.370237, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  73506/1000000: episode: 202, duration: 0.941s, episode steps: 175, steps per second: 186, episode reward: 37.894, mean reward:  0.217 [-100.000, 23.693], mean action: 1.486 [0.000, 3.000],  loss: 11.510886, mae: 57.614340, mean_q: 69.501952, mean_eps: 0.100000\n",
      "  73792/1000000: episode: 203, duration: 1.433s, episode steps: 286, steps per second: 200, episode reward: -199.968, mean reward: -0.699 [-100.000, 15.242], mean action: 1.486 [0.000, 3.000],  loss: 12.399951, mae: 57.096511, mean_q: 62.881879, mean_eps: 0.100000\n",
      "  74252/1000000: episode: 204, duration: 2.576s, episode steps: 460, steps per second: 179, episode reward: 243.866, mean reward:  0.530 [-21.505, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 14.810784, mae: 53.136794, mean_q: 37.577767, mean_eps: 0.100000\n",
      "  74499/1000000: episode: 205, duration: 1.283s, episode steps: 247, steps per second: 192, episode reward: 185.589, mean reward:  0.751 [-19.881, 100.000], mean action: 1.769 [0.000, 3.000],  loss: 22.213419, mae: 42.297784, mean_q: 18.285488, mean_eps: 0.100000\n",
      "  75499/1000000: episode: 206, duration: 6.004s, episode steps: 1000, steps per second: 167, episode reward: 67.735, mean reward:  0.068 [-20.825, 24.432], mean action: 1.074 [0.000, 3.000],  loss: 16.717768, mae: 25.056258, mean_q: 12.319795, mean_eps: 0.100000\n",
      "  75606/1000000: episode: 207, duration: 0.513s, episode steps: 107, steps per second: 209, episode reward: -22.391, mean reward: -0.209 [-100.000, 21.223], mean action: 1.785 [0.000, 3.000],  loss: 1.789135, mae: 16.239552, mean_q: 0.137346, mean_eps: 0.100000\n",
      "  75692/1000000: episode: 208, duration: 0.409s, episode steps:  86, steps per second: 210, episode reward: -106.842, mean reward: -1.242 [-100.000, 11.096], mean action: 1.919 [0.000, 3.000],  loss: 3.760360, mae: 19.557181, mean_q: 4.731349, mean_eps: 0.100000\n",
      "  75823/1000000: episode: 209, duration: 0.618s, episode steps: 131, steps per second: 212, episode reward: 29.576, mean reward:  0.226 [-100.000, 21.054], mean action: 1.374 [0.000, 3.000],  loss: 4.672872, mae: 25.729919, mean_q: 16.046693, mean_eps: 0.100000\n",
      "  76296/1000000: episode: 210, duration: 2.516s, episode steps: 473, steps per second: 188, episode reward: 216.416, mean reward:  0.458 [-12.567, 100.000], mean action: 1.163 [0.000, 3.000],  loss: 4.065046, mae: 34.438480, mean_q: 32.990026, mean_eps: 0.100000\n",
      "  76380/1000000: episode: 211, duration: 0.405s, episode steps:  84, steps per second: 207, episode reward:  2.831, mean reward:  0.034 [-100.000, 13.229], mean action: 1.417 [0.000, 3.000],  loss: 12.989164, mae: 35.362280, mean_q: 39.218407, mean_eps: 0.100000\n",
      "  76843/1000000: episode: 212, duration: 2.349s, episode steps: 463, steps per second: 197, episode reward: 276.345, mean reward:  0.597 [-19.320, 100.000], mean action: 1.380 [0.000, 3.000],  loss: 8.302065, mae: 35.436824, mean_q: 40.061828, mean_eps: 0.100000\n",
      "  77843/1000000: episode: 213, duration: 5.508s, episode steps: 1000, steps per second: 182, episode reward: -32.405, mean reward: -0.032 [-5.425,  5.030], mean action: 1.936 [0.000, 3.000],  loss: 10.814866, mae: 31.147220, mean_q: -1.280234, mean_eps: 0.100000\n",
      "  78843/1000000: episode: 214, duration: 5.403s, episode steps: 1000, steps per second: 185, episode reward: -108.800, mean reward: -0.109 [-21.121, 20.147], mean action: 1.824 [0.000, 3.000],  loss: 2.646262, mae: 36.572232, mean_q: -30.144196, mean_eps: 0.100000\n",
      "  79843/1000000: episode: 215, duration: 6.031s, episode steps: 1000, steps per second: 166, episode reward: -7.494, mean reward: -0.007 [-4.542,  5.013], mean action: 1.826 [0.000, 3.000],  loss: 1.277830, mae: 25.878261, mean_q: -4.327217, mean_eps: 0.100000\n",
      "  79916/1000000: episode: 216, duration: 0.351s, episode steps:  73, steps per second: 208, episode reward: -19.889, mean reward: -0.272 [-100.000,  5.504], mean action: 1.822 [0.000, 3.000],  loss: 0.541969, mae: 15.213617, mean_q: 20.842555, mean_eps: 0.100000\n",
      "  80916/1000000: episode: 217, duration: 5.920s, episode steps: 1000, steps per second: 169, episode reward:  8.636, mean reward:  0.009 [-18.861, 21.517], mean action: 1.599 [0.000, 3.000],  loss: 1.656418, mae: 16.428657, mean_q: 21.501244, mean_eps: 0.100000\n",
      "  81916/1000000: episode: 218, duration: 5.539s, episode steps: 1000, steps per second: 181, episode reward: 22.343, mean reward:  0.022 [-23.751, 18.907], mean action: 1.925 [0.000, 3.000],  loss: 1.715060, mae: 12.498710, mean_q: 10.236085, mean_eps: 0.100000\n",
      "  82916/1000000: episode: 219, duration: 5.618s, episode steps: 1000, steps per second: 178, episode reward: 22.859, mean reward:  0.023 [-4.622,  4.977], mean action: 1.744 [0.000, 3.000],  loss: 0.508365, mae: 11.157969, mean_q: 11.925484, mean_eps: 0.100000\n",
      "  83916/1000000: episode: 220, duration: 5.736s, episode steps: 1000, steps per second: 174, episode reward: 11.164, mean reward:  0.011 [-5.003,  5.037], mean action: 1.813 [0.000, 3.000],  loss: 0.173723, mae: 13.561811, mean_q: 18.590214, mean_eps: 0.100000\n",
      "  84916/1000000: episode: 221, duration: 6.117s, episode steps: 1000, steps per second: 163, episode reward: 36.553, mean reward:  0.037 [-13.539, 15.404], mean action: 1.872 [0.000, 3.000],  loss: 0.502529, mae: 12.385315, mean_q: 16.927435, mean_eps: 0.100000\n",
      "  85916/1000000: episode: 222, duration: 5.897s, episode steps: 1000, steps per second: 170, episode reward: -12.185, mean reward: -0.012 [-24.574, 24.316], mean action: 1.705 [0.000, 3.000],  loss: 0.667132, mae: 9.924786, mean_q: 12.312528, mean_eps: 0.100000\n",
      "  86365/1000000: episode: 223, duration: 2.336s, episode steps: 449, steps per second: 192, episode reward: 264.282, mean reward:  0.589 [-17.060, 100.000], mean action: 1.399 [0.000, 3.000],  loss: 0.653360, mae: 11.631191, mean_q: 11.158041, mean_eps: 0.100000\n",
      "  86603/1000000: episode: 224, duration: 1.169s, episode steps: 238, steps per second: 204, episode reward: 265.993, mean reward:  1.118 [-8.963, 100.000], mean action: 1.647 [0.000, 3.000],  loss: 13.378065, mae: 17.563389, mean_q: 13.682253, mean_eps: 0.100000\n",
      "  86906/1000000: episode: 225, duration: 1.500s, episode steps: 303, steps per second: 202, episode reward: 264.022, mean reward:  0.871 [-13.000, 100.000], mean action: 1.627 [0.000, 3.000],  loss: 12.954920, mae: 23.412186, mean_q: 21.950813, mean_eps: 0.100000\n",
      "  87270/1000000: episode: 226, duration: 1.839s, episode steps: 364, steps per second: 198, episode reward: 233.069, mean reward:  0.640 [-20.275, 100.000], mean action: 1.445 [0.000, 3.000],  loss: 22.765850, mae: 24.584230, mean_q: 25.422456, mean_eps: 0.100000\n",
      "  87483/1000000: episode: 227, duration: 1.036s, episode steps: 213, steps per second: 206, episode reward: 288.105, mean reward:  1.353 [-8.626, 100.000], mean action: 1.601 [0.000, 3.000],  loss: 19.552924, mae: 26.407730, mean_q: 33.614221, mean_eps: 0.100000\n",
      "  87703/1000000: episode: 228, duration: 1.077s, episode steps: 220, steps per second: 204, episode reward: 280.844, mean reward:  1.277 [-18.069, 100.000], mean action: 1.368 [0.000, 3.000],  loss: 18.270609, mae: 26.622613, mean_q: 35.394090, mean_eps: 0.100000\n",
      "  88064/1000000: episode: 229, duration: 1.803s, episode steps: 361, steps per second: 200, episode reward: 262.014, mean reward:  0.726 [-17.778, 100.000], mean action: 0.936 [0.000, 3.000],  loss: 15.864573, mae: 27.133550, mean_q: 36.999894, mean_eps: 0.100000\n",
      "  88430/1000000: episode: 230, duration: 1.800s, episode steps: 366, steps per second: 203, episode reward: 249.490, mean reward:  0.682 [-20.093, 100.000], mean action: 0.869 [0.000, 3.000],  loss: 16.863824, mae: 27.288831, mean_q: 37.409531, mean_eps: 0.100000\n",
      "  88662/1000000: episode: 231, duration: 1.121s, episode steps: 232, steps per second: 207, episode reward: 298.000, mean reward:  1.284 [-8.701, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 13.511233, mae: 27.319231, mean_q: 37.697972, mean_eps: 0.100000\n",
      "  88765/1000000: episode: 232, duration: 0.503s, episode steps: 103, steps per second: 205, episode reward: 56.123, mean reward:  0.545 [-100.000, 22.723], mean action: 1.913 [0.000, 3.000],  loss: 11.106401, mae: 28.467939, mean_q: 39.306682, mean_eps: 0.100000\n",
      "  89127/1000000: episode: 233, duration: 1.794s, episode steps: 362, steps per second: 202, episode reward: 272.666, mean reward:  0.753 [-19.472, 100.000], mean action: 1.160 [0.000, 3.000],  loss: 13.200369, mae: 31.867389, mean_q: 43.820277, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  89317/1000000: episode: 234, duration: 0.915s, episode steps: 190, steps per second: 208, episode reward: 43.990, mean reward:  0.232 [-100.000, 14.789], mean action: 1.384 [0.000, 3.000],  loss: 17.462583, mae: 32.588996, mean_q: 44.943785, mean_eps: 0.100000\n",
      "  89460/1000000: episode: 235, duration: 0.681s, episode steps: 143, steps per second: 210, episode reward: -39.582, mean reward: -0.277 [-100.000,  7.246], mean action: 1.273 [0.000, 3.000],  loss: 16.693248, mae: 37.135342, mean_q: 50.980883, mean_eps: 0.100000\n",
      "  89765/1000000: episode: 236, duration: 1.519s, episode steps: 305, steps per second: 201, episode reward: 267.962, mean reward:  0.879 [-12.633, 100.000], mean action: 0.925 [0.000, 3.000],  loss: 19.946491, mae: 38.124768, mean_q: 52.228328, mean_eps: 0.100000\n",
      "  90087/1000000: episode: 237, duration: 1.603s, episode steps: 322, steps per second: 201, episode reward: 238.889, mean reward:  0.742 [-17.841, 100.000], mean action: 1.342 [0.000, 3.000],  loss: 12.896945, mae: 35.715185, mean_q: 49.034786, mean_eps: 0.100000\n",
      "  90280/1000000: episode: 238, duration: 0.935s, episode steps: 193, steps per second: 206, episode reward: 29.882, mean reward:  0.155 [-100.000, 11.743], mean action: 1.845 [0.000, 3.000],  loss: 11.137349, mae: 37.094847, mean_q: 50.180232, mean_eps: 0.100000\n",
      "  90701/1000000: episode: 239, duration: 2.161s, episode steps: 421, steps per second: 195, episode reward: 269.034, mean reward:  0.639 [-18.392, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 11.377083, mae: 34.955743, mean_q: 47.963090, mean_eps: 0.100000\n",
      "  90905/1000000: episode: 240, duration: 0.993s, episode steps: 204, steps per second: 205, episode reward: 11.739, mean reward:  0.058 [-100.000, 11.706], mean action: 1.833 [0.000, 3.000],  loss: 16.156970, mae: 33.904835, mean_q: 46.553361, mean_eps: 0.100000\n",
      "  91607/1000000: episode: 241, duration: 3.627s, episode steps: 702, steps per second: 194, episode reward: 266.214, mean reward:  0.379 [-23.302, 100.000], mean action: 0.830 [0.000, 3.000],  loss: 13.218280, mae: 33.614676, mean_q: 46.341664, mean_eps: 0.100000\n",
      "  91748/1000000: episode: 242, duration: 0.676s, episode steps: 141, steps per second: 209, episode reward: -36.587, mean reward: -0.259 [-100.000, 10.177], mean action: 1.468 [0.000, 3.000],  loss: 14.406336, mae: 31.875681, mean_q: 44.151915, mean_eps: 0.100000\n",
      "  92134/1000000: episode: 243, duration: 1.961s, episode steps: 386, steps per second: 197, episode reward: 265.262, mean reward:  0.687 [-2.562, 100.000], mean action: 1.461 [0.000, 3.000],  loss: 8.804241, mae: 33.239114, mean_q: 45.665207, mean_eps: 0.100000\n",
      "  92568/1000000: episode: 244, duration: 2.193s, episode steps: 434, steps per second: 198, episode reward: 235.909, mean reward:  0.544 [-9.315, 100.000], mean action: 1.429 [0.000, 3.000],  loss: 6.346028, mae: 34.728528, mean_q: 47.211534, mean_eps: 0.100000\n",
      "  92786/1000000: episode: 245, duration: 1.046s, episode steps: 218, steps per second: 208, episode reward: -31.196, mean reward: -0.143 [-100.000, 15.895], mean action: 1.404 [0.000, 3.000],  loss: 6.263736, mae: 36.306174, mean_q: 49.010940, mean_eps: 0.100000\n",
      "  93190/1000000: episode: 246, duration: 2.074s, episode steps: 404, steps per second: 195, episode reward: 251.435, mean reward:  0.622 [-18.630, 100.000], mean action: 0.975 [0.000, 3.000],  loss: 9.430949, mae: 33.802683, mean_q: 45.502409, mean_eps: 0.100000\n",
      "  93636/1000000: episode: 247, duration: 2.257s, episode steps: 446, steps per second: 198, episode reward: 243.723, mean reward:  0.546 [-8.733, 100.000], mean action: 1.430 [0.000, 3.000],  loss: 7.963109, mae: 32.561206, mean_q: 44.106194, mean_eps: 0.100000\n",
      "  94167/1000000: episode: 248, duration: 2.786s, episode steps: 531, steps per second: 191, episode reward: 221.085, mean reward:  0.416 [-9.869, 100.000], mean action: 1.299 [0.000, 3.000],  loss: 7.871434, mae: 33.403290, mean_q: 45.607530, mean_eps: 0.100000\n",
      "  94693/1000000: episode: 249, duration: 2.745s, episode steps: 526, steps per second: 192, episode reward: 206.856, mean reward:  0.393 [-9.703, 100.000], mean action: 1.249 [0.000, 3.000],  loss: 3.972588, mae: 33.618644, mean_q: 45.709840, mean_eps: 0.100000\n",
      "  95279/1000000: episode: 250, duration: 3.106s, episode steps: 586, steps per second: 189, episode reward: 213.948, mean reward:  0.365 [-10.739, 100.000], mean action: 1.007 [0.000, 3.000],  loss: 4.305670, mae: 32.472486, mean_q: 44.075301, mean_eps: 0.100000\n",
      "  95541/1000000: episode: 251, duration: 1.298s, episode steps: 262, steps per second: 202, episode reward:  8.529, mean reward:  0.033 [-100.000, 11.732], mean action: 1.496 [0.000, 3.000],  loss: 5.220638, mae: 32.523990, mean_q: 44.146680, mean_eps: 0.100000\n",
      "  96352/1000000: episode: 252, duration: 4.377s, episode steps: 811, steps per second: 185, episode reward: 159.136, mean reward:  0.196 [-19.495, 100.000], mean action: 1.656 [0.000, 3.000],  loss: 9.783083, mae: 32.667273, mean_q: 44.001394, mean_eps: 0.100000\n",
      "  96926/1000000: episode: 253, duration: 2.941s, episode steps: 574, steps per second: 195, episode reward: 235.217, mean reward:  0.410 [-19.124, 100.000], mean action: 1.322 [0.000, 3.000],  loss: 4.238774, mae: 33.563184, mean_q: 45.188630, mean_eps: 0.100000\n",
      "  97472/1000000: episode: 254, duration: 2.933s, episode steps: 546, steps per second: 186, episode reward: 264.628, mean reward:  0.485 [-12.535, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 5.690897, mae: 35.151928, mean_q: 47.523689, mean_eps: 0.100000\n",
      "  97786/1000000: episode: 255, duration: 1.559s, episode steps: 314, steps per second: 201, episode reward: 257.865, mean reward:  0.821 [-2.687, 100.000], mean action: 1.532 [0.000, 3.000],  loss: 4.829288, mae: 36.151910, mean_q: 49.109008, mean_eps: 0.100000\n",
      "  98030/1000000: episode: 256, duration: 1.196s, episode steps: 244, steps per second: 204, episode reward: -26.854, mean reward: -0.110 [-100.000,  8.290], mean action: 1.811 [0.000, 3.000],  loss: 3.749339, mae: 36.028138, mean_q: 48.900483, mean_eps: 0.100000\n",
      "  98137/1000000: episode: 257, duration: 0.512s, episode steps: 107, steps per second: 209, episode reward: -33.987, mean reward: -0.318 [-100.000, 13.769], mean action: 1.150 [0.000, 3.000],  loss: 17.636419, mae: 36.286902, mean_q: 49.443492, mean_eps: 0.100000\n",
      "  98689/1000000: episode: 258, duration: 2.740s, episode steps: 552, steps per second: 201, episode reward: 203.625, mean reward:  0.369 [-10.695, 100.000], mean action: 1.190 [0.000, 3.000],  loss: 18.395092, mae: 38.091097, mean_q: 51.892443, mean_eps: 0.100000\n",
      "  98802/1000000: episode: 259, duration: 0.539s, episode steps: 113, steps per second: 210, episode reward: -83.898, mean reward: -0.742 [-100.000,  9.195], mean action: 1.602 [0.000, 3.000],  loss: 11.444271, mae: 37.938425, mean_q: 52.357779, mean_eps: 0.100000\n",
      "  99395/1000000: episode: 260, duration: 3.091s, episode steps: 593, steps per second: 192, episode reward: 194.807, mean reward:  0.329 [-21.413, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 9.906598, mae: 37.701765, mean_q: 52.075793, mean_eps: 0.100000\n",
      " 100395/1000000: episode: 261, duration: 5.616s, episode steps: 1000, steps per second: 178, episode reward: -30.801, mean reward: -0.031 [-13.403, 12.812], mean action: 1.637 [0.000, 3.000],  loss: 3.833086, mae: 35.647810, mean_q: 48.599141, mean_eps: 0.100000\n",
      " 100771/1000000: episode: 262, duration: 1.887s, episode steps: 376, steps per second: 199, episode reward: 243.810, mean reward:  0.648 [-18.242, 100.000], mean action: 1.888 [0.000, 3.000],  loss: 2.513926, mae: 34.463629, mean_q: 46.725746, mean_eps: 0.100000\n",
      " 100924/1000000: episode: 263, duration: 0.739s, episode steps: 153, steps per second: 207, episode reward: 52.866, mean reward:  0.346 [-100.000, 15.623], mean action: 1.739 [0.000, 3.000],  loss: 5.753696, mae: 34.375651, mean_q: 46.719386, mean_eps: 0.100000\n",
      " 101532/1000000: episode: 264, duration: 3.235s, episode steps: 608, steps per second: 188, episode reward: 175.779, mean reward:  0.289 [-11.824, 100.000], mean action: 1.405 [0.000, 3.000],  loss: 16.407384, mae: 34.539543, mean_q: 47.145114, mean_eps: 0.100000\n",
      " 102335/1000000: episode: 265, duration: 4.317s, episode steps: 803, steps per second: 186, episode reward: 230.301, mean reward:  0.287 [-17.751, 100.000], mean action: 0.938 [0.000, 3.000],  loss: 6.214362, mae: 33.539739, mean_q: 46.282321, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 102970/1000000: episode: 266, duration: 3.418s, episode steps: 635, steps per second: 186, episode reward: 165.696, mean reward:  0.261 [-20.604, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 5.397411, mae: 33.332310, mean_q: 45.695838, mean_eps: 0.100000\n",
      " 103488/1000000: episode: 267, duration: 2.681s, episode steps: 518, steps per second: 193, episode reward: 199.094, mean reward:  0.384 [-11.411, 100.000], mean action: 1.288 [0.000, 3.000],  loss: 4.477270, mae: 35.249902, mean_q: 48.449222, mean_eps: 0.100000\n",
      " 103843/1000000: episode: 268, duration: 1.791s, episode steps: 355, steps per second: 198, episode reward: 244.958, mean reward:  0.690 [-12.892, 100.000], mean action: 1.690 [0.000, 3.000],  loss: 4.268493, mae: 36.671774, mean_q: 50.334443, mean_eps: 0.100000\n",
      " 104515/1000000: episode: 269, duration: 3.463s, episode steps: 672, steps per second: 194, episode reward: 228.467, mean reward:  0.340 [-20.170, 100.000], mean action: 1.378 [0.000, 3.000],  loss: 5.060366, mae: 36.959479, mean_q: 50.402469, mean_eps: 0.100000\n",
      " 104958/1000000: episode: 270, duration: 2.233s, episode steps: 443, steps per second: 198, episode reward: 262.012, mean reward:  0.591 [-19.414, 100.000], mean action: 1.149 [0.000, 3.000],  loss: 6.212292, mae: 36.764010, mean_q: 50.276047, mean_eps: 0.100000\n",
      " 105399/1000000: episode: 271, duration: 2.188s, episode steps: 441, steps per second: 202, episode reward: 252.000, mean reward:  0.571 [-19.465, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 5.506982, mae: 37.643556, mean_q: 51.501277, mean_eps: 0.100000\n",
      " 105620/1000000: episode: 272, duration: 1.072s, episode steps: 221, steps per second: 206, episode reward:  0.720, mean reward:  0.003 [-100.000, 12.979], mean action: 1.878 [0.000, 3.000],  loss: 4.752815, mae: 40.211847, mean_q: 54.707840, mean_eps: 0.100000\n",
      " 105910/1000000: episode: 273, duration: 1.412s, episode steps: 290, steps per second: 205, episode reward: 234.882, mean reward:  0.810 [-8.508, 100.000], mean action: 1.369 [0.000, 3.000],  loss: 14.681975, mae: 40.163037, mean_q: 54.926773, mean_eps: 0.100000\n",
      " 106247/1000000: episode: 274, duration: 1.689s, episode steps: 337, steps per second: 200, episode reward: 217.546, mean reward:  0.646 [-19.948, 100.000], mean action: 1.039 [0.000, 3.000],  loss: 11.917482, mae: 40.612697, mean_q: 55.736063, mean_eps: 0.100000\n",
      " 106502/1000000: episode: 275, duration: 1.254s, episode steps: 255, steps per second: 203, episode reward: 250.362, mean reward:  0.982 [-12.692, 100.000], mean action: 1.937 [0.000, 3.000],  loss: 8.813662, mae: 40.555159, mean_q: 55.608175, mean_eps: 0.100000\n",
      " 106852/1000000: episode: 276, duration: 1.706s, episode steps: 350, steps per second: 205, episode reward: 263.651, mean reward:  0.753 [-10.319, 100.000], mean action: 0.826 [0.000, 3.000],  loss: 8.344802, mae: 41.867937, mean_q: 57.677237, mean_eps: 0.100000\n",
      " 107122/1000000: episode: 277, duration: 1.319s, episode steps: 270, steps per second: 205, episode reward: 310.224, mean reward:  1.149 [-10.214, 100.000], mean action: 1.426 [0.000, 3.000],  loss: 7.413720, mae: 43.440698, mean_q: 59.622445, mean_eps: 0.100000\n",
      " 107406/1000000: episode: 278, duration: 1.411s, episode steps: 284, steps per second: 201, episode reward: 258.308, mean reward:  0.910 [-10.110, 100.000], mean action: 1.183 [0.000, 3.000],  loss: 4.772839, mae: 46.543309, mean_q: 63.562508, mean_eps: 0.100000\n",
      " 107693/1000000: episode: 279, duration: 1.387s, episode steps: 287, steps per second: 207, episode reward: 294.510, mean reward:  1.026 [-2.418, 100.000], mean action: 1.143 [0.000, 3.000],  loss: 4.315057, mae: 46.192002, mean_q: 63.046274, mean_eps: 0.100000\n",
      " 107970/1000000: episode: 280, duration: 1.374s, episode steps: 277, steps per second: 202, episode reward: 273.732, mean reward:  0.988 [-9.497, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 6.123446, mae: 47.890338, mean_q: 65.117160, mean_eps: 0.100000\n",
      " 108259/1000000: episode: 281, duration: 1.424s, episode steps: 289, steps per second: 203, episode reward: 275.868, mean reward:  0.955 [-4.237, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 4.312576, mae: 47.772989, mean_q: 64.762324, mean_eps: 0.100000\n",
      " 108512/1000000: episode: 282, duration: 1.253s, episode steps: 253, steps per second: 202, episode reward: 284.364, mean reward:  1.124 [-11.800, 100.000], mean action: 1.451 [0.000, 3.000],  loss: 5.833146, mae: 48.700673, mean_q: 65.868424, mean_eps: 0.100000\n",
      " 109067/1000000: episode: 283, duration: 2.843s, episode steps: 555, steps per second: 195, episode reward: 301.518, mean reward:  0.543 [-18.390, 100.000], mean action: 1.018 [0.000, 3.000],  loss: 4.742256, mae: 49.618394, mean_q: 67.004471, mean_eps: 0.100000\n",
      " 109601/1000000: episode: 284, duration: 2.735s, episode steps: 534, steps per second: 195, episode reward: 243.974, mean reward:  0.457 [-19.584, 100.000], mean action: 2.011 [0.000, 3.000],  loss: 2.965106, mae: 48.836204, mean_q: 65.995127, mean_eps: 0.100000\n",
      " 110131/1000000: episode: 285, duration: 2.707s, episode steps: 530, steps per second: 196, episode reward: 270.375, mean reward:  0.510 [-10.591, 100.000], mean action: 1.674 [0.000, 3.000],  loss: 2.199520, mae: 46.307237, mean_q: 62.672744, mean_eps: 0.100000\n",
      " 110497/1000000: episode: 286, duration: 1.808s, episode steps: 366, steps per second: 202, episode reward: 251.372, mean reward:  0.687 [-19.496, 100.000], mean action: 2.281 [0.000, 3.000],  loss: 4.009479, mae: 46.945699, mean_q: 63.697691, mean_eps: 0.100000\n",
      " 110893/1000000: episode: 287, duration: 1.965s, episode steps: 396, steps per second: 201, episode reward: 254.906, mean reward:  0.644 [-17.682, 100.000], mean action: 0.838 [0.000, 3.000],  loss: 3.002866, mae: 48.527597, mean_q: 66.028914, mean_eps: 0.100000\n",
      " 111207/1000000: episode: 288, duration: 1.574s, episode steps: 314, steps per second: 199, episode reward: 236.518, mean reward:  0.753 [-17.476, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 3.061018, mae: 46.917615, mean_q: 63.877079, mean_eps: 0.100000\n",
      " 111485/1000000: episode: 289, duration: 1.380s, episode steps: 278, steps per second: 201, episode reward: 226.526, mean reward:  0.815 [-18.884, 100.000], mean action: 1.241 [0.000, 3.000],  loss: 3.383780, mae: 45.142778, mean_q: 61.194438, mean_eps: 0.100000\n",
      " 111996/1000000: episode: 290, duration: 2.629s, episode steps: 511, steps per second: 194, episode reward: 221.405, mean reward:  0.433 [-20.319, 100.000], mean action: 0.898 [0.000, 3.000],  loss: 4.320397, mae: 44.086932, mean_q: 59.663845, mean_eps: 0.100000\n",
      " 112511/1000000: episode: 291, duration: 2.661s, episode steps: 515, steps per second: 194, episode reward: 241.768, mean reward:  0.469 [-18.866, 100.000], mean action: 0.773 [0.000, 3.000],  loss: 4.182670, mae: 42.496728, mean_q: 57.713364, mean_eps: 0.100000\n",
      " 112793/1000000: episode: 292, duration: 1.377s, episode steps: 282, steps per second: 205, episode reward: 232.320, mean reward:  0.824 [-8.244, 100.000], mean action: 1.259 [0.000, 3.000],  loss: 3.853715, mae: 40.025680, mean_q: 54.564934, mean_eps: 0.100000\n",
      " 113106/1000000: episode: 293, duration: 1.541s, episode steps: 313, steps per second: 203, episode reward: 267.602, mean reward:  0.855 [-17.809, 100.000], mean action: 1.348 [0.000, 3.000],  loss: 3.893838, mae: 43.988409, mean_q: 59.767038, mean_eps: 0.100000\n",
      " 113585/1000000: episode: 294, duration: 2.428s, episode steps: 479, steps per second: 197, episode reward: 223.224, mean reward:  0.466 [-13.089, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 3.706272, mae: 48.481858, mean_q: 65.663823, mean_eps: 0.100000\n",
      " 113942/1000000: episode: 295, duration: 1.830s, episode steps: 357, steps per second: 195, episode reward: 235.558, mean reward:  0.660 [-12.993, 100.000], mean action: 1.347 [0.000, 3.000],  loss: 2.982518, mae: 48.900237, mean_q: 66.160543, mean_eps: 0.100000\n",
      " 114394/1000000: episode: 296, duration: 2.368s, episode steps: 452, steps per second: 191, episode reward: 231.061, mean reward:  0.511 [-13.130, 100.000], mean action: 1.584 [0.000, 3.000],  loss: 3.199529, mae: 45.896838, mean_q: 62.021295, mean_eps: 0.100000\n",
      " 115132/1000000: episode: 297, duration: 4.060s, episode steps: 738, steps per second: 182, episode reward: 236.801, mean reward:  0.321 [-17.634, 100.000], mean action: 1.985 [0.000, 3.000],  loss: 2.324359, mae: 45.711155, mean_q: 61.771657, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 115506/1000000: episode: 298, duration: 1.861s, episode steps: 374, steps per second: 201, episode reward: 274.969, mean reward:  0.735 [-19.822, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 2.066929, mae: 45.512817, mean_q: 61.926710, mean_eps: 0.100000\n",
      " 115606/1000000: episode: 299, duration: 0.472s, episode steps: 100, steps per second: 212, episode reward: -48.321, mean reward: -0.483 [-100.000, 10.554], mean action: 1.220 [0.000, 3.000],  loss: 3.200796, mae: 44.823940, mean_q: 61.075129, mean_eps: 0.100000\n",
      " 116004/1000000: episode: 300, duration: 1.954s, episode steps: 398, steps per second: 204, episode reward: 226.614, mean reward:  0.569 [-17.490, 100.000], mean action: 1.764 [0.000, 3.000],  loss: 7.126574, mae: 46.327602, mean_q: 63.123956, mean_eps: 0.100000\n",
      " 116482/1000000: episode: 301, duration: 2.473s, episode steps: 478, steps per second: 193, episode reward: 235.254, mean reward:  0.492 [-18.139, 100.000], mean action: 1.471 [0.000, 3.000],  loss: 3.751967, mae: 47.111456, mean_q: 63.673800, mean_eps: 0.100000\n",
      " 116885/1000000: episode: 302, duration: 2.075s, episode steps: 403, steps per second: 194, episode reward: 257.665, mean reward:  0.639 [-15.704, 100.000], mean action: 1.397 [0.000, 3.000],  loss: 2.988965, mae: 45.794878, mean_q: 62.028006, mean_eps: 0.100000\n",
      " 117852/1000000: episode: 303, duration: 5.244s, episode steps: 967, steps per second: 184, episode reward: 241.764, mean reward:  0.250 [-22.651, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 2.864237, mae: 46.315458, mean_q: 62.696488, mean_eps: 0.100000\n",
      " 118199/1000000: episode: 304, duration: 1.742s, episode steps: 347, steps per second: 199, episode reward: 179.593, mean reward:  0.518 [-20.165, 100.000], mean action: 2.173 [0.000, 3.000],  loss: 1.941672, mae: 43.490301, mean_q: 59.538425, mean_eps: 0.100000\n",
      " 118577/1000000: episode: 305, duration: 1.899s, episode steps: 378, steps per second: 199, episode reward: 241.503, mean reward:  0.639 [-9.776, 100.000], mean action: 1.577 [0.000, 3.000],  loss: 3.179987, mae: 42.810009, mean_q: 58.901103, mean_eps: 0.100000\n",
      " 118672/1000000: episode: 306, duration: 0.456s, episode steps:  95, steps per second: 208, episode reward: -104.668, mean reward: -1.102 [-100.000, 11.898], mean action: 1.432 [0.000, 3.000],  loss: 4.051018, mae: 46.244656, mean_q: 63.227862, mean_eps: 0.100000\n",
      " 119021/1000000: episode: 307, duration: 1.712s, episode steps: 349, steps per second: 204, episode reward: 296.940, mean reward:  0.851 [-21.907, 100.000], mean action: 1.183 [0.000, 3.000],  loss: 4.991541, mae: 48.376049, mean_q: 65.598305, mean_eps: 0.100000\n",
      " 119148/1000000: episode: 308, duration: 0.604s, episode steps: 127, steps per second: 210, episode reward: -371.176, mean reward: -2.923 [-100.000, 91.952], mean action: 1.110 [0.000, 3.000],  loss: 6.155401, mae: 50.003506, mean_q: 67.732135, mean_eps: 0.100000\n",
      " 119433/1000000: episode: 309, duration: 1.400s, episode steps: 285, steps per second: 204, episode reward: 218.482, mean reward:  0.767 [-9.401, 100.000], mean action: 1.389 [0.000, 3.000],  loss: 14.665937, mae: 52.447119, mean_q: 69.042369, mean_eps: 0.100000\n",
      " 119560/1000000: episode: 310, duration: 0.611s, episode steps: 127, steps per second: 208, episode reward: -14.460, mean reward: -0.114 [-100.000, 20.434], mean action: 1.874 [0.000, 3.000],  loss: 12.538846, mae: 53.726206, mean_q: 69.634621, mean_eps: 0.100000\n",
      " 119903/1000000: episode: 311, duration: 1.706s, episode steps: 343, steps per second: 201, episode reward: 217.757, mean reward:  0.635 [-19.515, 100.000], mean action: 1.099 [0.000, 3.000],  loss: 10.653899, mae: 54.146390, mean_q: 70.030679, mean_eps: 0.100000\n",
      " 120046/1000000: episode: 312, duration: 0.687s, episode steps: 143, steps per second: 208, episode reward:  6.697, mean reward:  0.047 [-100.000, 20.124], mean action: 1.608 [0.000, 3.000],  loss: 7.836175, mae: 54.418787, mean_q: 70.129384, mean_eps: 0.100000\n",
      " 120504/1000000: episode: 313, duration: 2.321s, episode steps: 458, steps per second: 197, episode reward: 180.317, mean reward:  0.394 [-10.477, 100.000], mean action: 1.199 [0.000, 3.000],  loss: 6.712661, mae: 53.455135, mean_q: 70.592745, mean_eps: 0.100000\n",
      " 120959/1000000: episode: 314, duration: 2.319s, episode steps: 455, steps per second: 196, episode reward: 244.877, mean reward:  0.538 [-19.925, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 5.390856, mae: 47.188764, mean_q: 63.209768, mean_eps: 0.100000\n",
      " 121728/1000000: episode: 315, duration: 4.074s, episode steps: 769, steps per second: 189, episode reward: 227.367, mean reward:  0.296 [-23.464, 100.000], mean action: 1.372 [0.000, 3.000],  loss: 4.637260, mae: 46.928638, mean_q: 63.324811, mean_eps: 0.100000\n",
      " 121879/1000000: episode: 316, duration: 0.722s, episode steps: 151, steps per second: 209, episode reward: 11.120, mean reward:  0.074 [-100.000, 17.243], mean action: 1.450 [0.000, 3.000],  loss: 4.562154, mae: 48.312275, mean_q: 65.287335, mean_eps: 0.100000\n",
      " 122058/1000000: episode: 317, duration: 0.854s, episode steps: 179, steps per second: 210, episode reward: -4.301, mean reward: -0.024 [-100.000, 15.033], mean action: 1.268 [0.000, 3.000],  loss: 4.337755, mae: 49.645244, mean_q: 66.909612, mean_eps: 0.100000\n",
      " 122410/1000000: episode: 318, duration: 1.743s, episode steps: 352, steps per second: 202, episode reward: 268.918, mean reward:  0.764 [-3.873, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 14.118423, mae: 48.299931, mean_q: 64.768685, mean_eps: 0.100000\n",
      " 122918/1000000: episode: 319, duration: 2.744s, episode steps: 508, steps per second: 185, episode reward: 233.801, mean reward:  0.460 [-11.039, 100.000], mean action: 1.587 [0.000, 3.000],  loss: 6.101565, mae: 48.176469, mean_q: 64.481990, mean_eps: 0.100000\n",
      " 123398/1000000: episode: 320, duration: 2.492s, episode steps: 480, steps per second: 193, episode reward: 203.548, mean reward:  0.424 [-17.888, 100.000], mean action: 1.735 [0.000, 3.000],  loss: 4.512214, mae: 48.524893, mean_q: 65.434949, mean_eps: 0.100000\n",
      " 123859/1000000: episode: 321, duration: 2.364s, episode steps: 461, steps per second: 195, episode reward: 186.768, mean reward:  0.405 [-12.394, 100.000], mean action: 1.503 [0.000, 3.000],  loss: 2.890010, mae: 48.232360, mean_q: 65.149184, mean_eps: 0.100000\n",
      " 124342/1000000: episode: 322, duration: 2.503s, episode steps: 483, steps per second: 193, episode reward: 211.529, mean reward:  0.438 [-18.835, 100.000], mean action: 1.983 [0.000, 3.000],  loss: 3.438912, mae: 47.065754, mean_q: 63.751501, mean_eps: 0.100000\n",
      " 124813/1000000: episode: 323, duration: 2.427s, episode steps: 471, steps per second: 194, episode reward: 209.835, mean reward:  0.446 [-17.962, 100.000], mean action: 2.178 [0.000, 3.000],  loss: 3.318391, mae: 48.796386, mean_q: 66.417779, mean_eps: 0.100000\n",
      " 125214/1000000: episode: 324, duration: 2.103s, episode steps: 401, steps per second: 191, episode reward: 189.760, mean reward:  0.473 [-15.024, 100.000], mean action: 1.397 [0.000, 3.000],  loss: 2.543524, mae: 48.905042, mean_q: 66.719890, mean_eps: 0.100000\n",
      " 125828/1000000: episode: 325, duration: 3.227s, episode steps: 614, steps per second: 190, episode reward: 196.035, mean reward:  0.319 [-10.164, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 1.838453, mae: 44.745485, mean_q: 60.951905, mean_eps: 0.100000\n",
      " 126340/1000000: episode: 326, duration: 2.611s, episode steps: 512, steps per second: 196, episode reward: 241.982, mean reward:  0.473 [-17.500, 100.000], mean action: 1.893 [0.000, 3.000],  loss: 1.806503, mae: 43.897098, mean_q: 59.514101, mean_eps: 0.100000\n",
      " 126865/1000000: episode: 327, duration: 2.825s, episode steps: 525, steps per second: 186, episode reward: 237.916, mean reward:  0.453 [-19.291, 100.000], mean action: 1.347 [0.000, 3.000],  loss: 2.465346, mae: 48.155503, mean_q: 65.150504, mean_eps: 0.100000\n",
      " 127687/1000000: episode: 328, duration: 4.392s, episode steps: 822, steps per second: 187, episode reward: 190.381, mean reward:  0.232 [-24.533, 100.000], mean action: 2.151 [0.000, 3.000],  loss: 2.371767, mae: 45.290887, mean_q: 61.257170, mean_eps: 0.100000\n",
      " 128291/1000000: episode: 329, duration: 3.303s, episode steps: 604, steps per second: 183, episode reward: 216.551, mean reward:  0.359 [-20.424, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 1.821854, mae: 43.560888, mean_q: 59.276556, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 128712/1000000: episode: 330, duration: 2.122s, episode steps: 421, steps per second: 198, episode reward: 275.900, mean reward:  0.655 [-17.437, 100.000], mean action: 1.342 [0.000, 3.000],  loss: 2.914382, mae: 43.678286, mean_q: 59.239567, mean_eps: 0.100000\n",
      " 129114/1000000: episode: 331, duration: 2.035s, episode steps: 402, steps per second: 198, episode reward: 262.237, mean reward:  0.652 [-18.603, 100.000], mean action: 1.480 [0.000, 3.000],  loss: 2.141713, mae: 43.597847, mean_q: 58.894395, mean_eps: 0.100000\n",
      " 129544/1000000: episode: 332, duration: 2.181s, episode steps: 430, steps per second: 197, episode reward: 258.047, mean reward:  0.600 [-2.864, 100.000], mean action: 1.107 [0.000, 3.000],  loss: 2.516116, mae: 44.560851, mean_q: 60.000491, mean_eps: 0.100000\n",
      " 129951/1000000: episode: 333, duration: 2.063s, episode steps: 407, steps per second: 197, episode reward: 251.409, mean reward:  0.618 [-3.010, 100.000], mean action: 1.170 [0.000, 3.000],  loss: 2.555131, mae: 45.999900, mean_q: 61.889356, mean_eps: 0.100000\n",
      " 130278/1000000: episode: 334, duration: 1.599s, episode steps: 327, steps per second: 204, episode reward: 279.020, mean reward:  0.853 [-11.550, 100.000], mean action: 1.275 [0.000, 3.000],  loss: 2.637805, mae: 46.466383, mean_q: 62.564438, mean_eps: 0.100000\n",
      " 130843/1000000: episode: 335, duration: 2.875s, episode steps: 565, steps per second: 197, episode reward: 295.209, mean reward:  0.522 [-17.373, 100.000], mean action: 1.182 [0.000, 3.000],  loss: 3.112678, mae: 48.083313, mean_q: 64.965301, mean_eps: 0.100000\n",
      " 131266/1000000: episode: 336, duration: 2.102s, episode steps: 423, steps per second: 201, episode reward: 226.390, mean reward:  0.535 [-19.854, 100.000], mean action: 1.357 [0.000, 3.000],  loss: 2.595871, mae: 48.268702, mean_q: 65.625931, mean_eps: 0.100000\n",
      " 131829/1000000: episode: 337, duration: 2.832s, episode steps: 563, steps per second: 199, episode reward: 240.811, mean reward:  0.428 [-19.670, 100.000], mean action: 1.112 [0.000, 3.000],  loss: 3.570795, mae: 48.635170, mean_q: 66.342236, mean_eps: 0.100000\n",
      " 132098/1000000: episode: 338, duration: 1.324s, episode steps: 269, steps per second: 203, episode reward: 251.023, mean reward:  0.933 [-14.018, 100.000], mean action: 2.082 [0.000, 3.000],  loss: 2.621270, mae: 48.787770, mean_q: 66.430530, mean_eps: 0.100000\n",
      " 132655/1000000: episode: 339, duration: 2.917s, episode steps: 557, steps per second: 191, episode reward: 261.525, mean reward:  0.470 [-19.129, 100.000], mean action: 1.257 [0.000, 3.000],  loss: 2.902618, mae: 48.879655, mean_q: 66.361396, mean_eps: 0.100000\n",
      " 132923/1000000: episode: 340, duration: 1.306s, episode steps: 268, steps per second: 205, episode reward: 257.759, mean reward:  0.962 [-17.877, 100.000], mean action: 1.183 [0.000, 3.000],  loss: 2.733052, mae: 49.349976, mean_q: 66.848200, mean_eps: 0.100000\n",
      " 133157/1000000: episode: 341, duration: 1.142s, episode steps: 234, steps per second: 205, episode reward: 277.868, mean reward:  1.187 [-16.350, 100.000], mean action: 1.402 [0.000, 3.000],  loss: 4.411440, mae: 50.414715, mean_q: 68.216500, mean_eps: 0.100000\n",
      " 133641/1000000: episode: 342, duration: 2.486s, episode steps: 484, steps per second: 195, episode reward: 312.407, mean reward:  0.645 [-18.967, 100.000], mean action: 0.917 [0.000, 3.000],  loss: 3.425382, mae: 51.763807, mean_q: 70.061518, mean_eps: 0.100000\n",
      " 133987/1000000: episode: 343, duration: 1.707s, episode steps: 346, steps per second: 203, episode reward: 250.481, mean reward:  0.724 [-17.547, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 3.776427, mae: 52.496352, mean_q: 70.913584, mean_eps: 0.100000\n",
      " 134230/1000000: episode: 344, duration: 1.174s, episode steps: 243, steps per second: 207, episode reward: 260.710, mean reward:  1.073 [-3.003, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 3.069909, mae: 50.897547, mean_q: 68.753859, mean_eps: 0.100000\n",
      " 134471/1000000: episode: 345, duration: 1.168s, episode steps: 241, steps per second: 206, episode reward: 243.042, mean reward:  1.008 [-3.057, 100.000], mean action: 1.170 [0.000, 3.000],  loss: 2.895870, mae: 50.846538, mean_q: 68.497681, mean_eps: 0.100000\n",
      " 134787/1000000: episode: 346, duration: 1.558s, episode steps: 316, steps per second: 203, episode reward: 258.730, mean reward:  0.819 [-18.170, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 3.699382, mae: 52.817536, mean_q: 71.141293, mean_eps: 0.100000\n",
      " 135058/1000000: episode: 347, duration: 1.309s, episode steps: 271, steps per second: 207, episode reward: 244.592, mean reward:  0.903 [-18.095, 100.000], mean action: 0.967 [0.000, 3.000],  loss: 3.302020, mae: 53.193456, mean_q: 71.751945, mean_eps: 0.100000\n",
      " 135857/1000000: episode: 348, duration: 4.235s, episode steps: 799, steps per second: 189, episode reward: 247.831, mean reward:  0.310 [-19.139, 100.000], mean action: 2.131 [0.000, 3.000],  loss: 2.304949, mae: 52.322467, mean_q: 70.937057, mean_eps: 0.100000\n",
      " 136096/1000000: episode: 349, duration: 1.160s, episode steps: 239, steps per second: 206, episode reward: 257.581, mean reward:  1.078 [-9.608, 100.000], mean action: 1.406 [0.000, 3.000],  loss: 1.321908, mae: 49.885989, mean_q: 67.878287, mean_eps: 0.100000\n",
      " 136344/1000000: episode: 350, duration: 1.194s, episode steps: 248, steps per second: 208, episode reward: 296.563, mean reward:  1.196 [-11.674, 100.000], mean action: 1.431 [0.000, 3.000],  loss: 1.684832, mae: 49.864695, mean_q: 67.769727, mean_eps: 0.100000\n",
      " 136588/1000000: episode: 351, duration: 1.188s, episode steps: 244, steps per second: 205, episode reward: 254.561, mean reward:  1.043 [-18.547, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 2.653564, mae: 53.386703, mean_q: 72.375337, mean_eps: 0.100000\n",
      " 136996/1000000: episode: 352, duration: 2.021s, episode steps: 408, steps per second: 202, episode reward: 299.138, mean reward:  0.733 [-19.371, 100.000], mean action: 1.272 [0.000, 3.000],  loss: 3.433951, mae: 57.538071, mean_q: 77.889446, mean_eps: 0.100000\n",
      " 137227/1000000: episode: 353, duration: 1.116s, episode steps: 231, steps per second: 207, episode reward: 241.762, mean reward:  1.047 [-19.089, 100.000], mean action: 1.229 [0.000, 3.000],  loss: 2.995356, mae: 57.938597, mean_q: 78.519320, mean_eps: 0.100000\n",
      " 138136/1000000: episode: 354, duration: 4.736s, episode steps: 909, steps per second: 192, episode reward: 219.381, mean reward:  0.241 [-18.098, 100.000], mean action: 0.920 [0.000, 3.000],  loss: 2.697380, mae: 51.637035, mean_q: 70.242779, mean_eps: 0.100000\n",
      " 138390/1000000: episode: 355, duration: 1.245s, episode steps: 254, steps per second: 204, episode reward: 252.001, mean reward:  0.992 [-16.937, 100.000], mean action: 1.248 [0.000, 3.000],  loss: 2.280518, mae: 44.377987, mean_q: 60.502757, mean_eps: 0.100000\n",
      " 139390/1000000: episode: 356, duration: 5.287s, episode steps: 1000, steps per second: 189, episode reward: 98.624, mean reward:  0.099 [-19.683, 21.458], mean action: 1.084 [0.000, 3.000],  loss: 2.339111, mae: 45.121196, mean_q: 61.527912, mean_eps: 0.100000\n",
      " 139670/1000000: episode: 357, duration: 1.354s, episode steps: 280, steps per second: 207, episode reward: 275.326, mean reward:  0.983 [-8.911, 100.000], mean action: 0.868 [0.000, 3.000],  loss: 1.350557, mae: 39.505494, mean_q: 53.920346, mean_eps: 0.100000\n",
      " 139900/1000000: episode: 358, duration: 1.120s, episode steps: 230, steps per second: 205, episode reward: 276.224, mean reward:  1.201 [-12.270, 100.000], mean action: 1.400 [0.000, 3.000],  loss: 1.982220, mae: 44.559041, mean_q: 60.514410, mean_eps: 0.100000\n",
      " 140469/1000000: episode: 359, duration: 2.915s, episode steps: 569, steps per second: 195, episode reward: 234.188, mean reward:  0.412 [-19.014, 100.000], mean action: 0.837 [0.000, 3.000],  loss: 2.308337, mae: 52.286220, mean_q: 70.617649, mean_eps: 0.100000\n",
      " 140941/1000000: episode: 360, duration: 2.396s, episode steps: 472, steps per second: 197, episode reward: 230.553, mean reward:  0.488 [-18.406, 100.000], mean action: 2.407 [0.000, 3.000],  loss: 2.505015, mae: 52.389777, mean_q: 71.325782, mean_eps: 0.100000\n",
      " 141167/1000000: episode: 361, duration: 1.099s, episode steps: 226, steps per second: 206, episode reward: 274.176, mean reward:  1.213 [-10.742, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 1.362558, mae: 53.528377, mean_q: 73.061712, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 141467/1000000: episode: 362, duration: 1.477s, episode steps: 300, steps per second: 203, episode reward: 293.736, mean reward:  0.979 [-9.047, 100.000], mean action: 1.247 [0.000, 3.000],  loss: 1.506511, mae: 59.186331, mean_q: 80.216852, mean_eps: 0.100000\n",
      " 141769/1000000: episode: 363, duration: 1.462s, episode steps: 302, steps per second: 207, episode reward: 285.875, mean reward:  0.947 [-18.832, 100.000], mean action: 0.887 [0.000, 3.000],  loss: 2.125913, mae: 62.351476, mean_q: 84.283034, mean_eps: 0.100000\n",
      " 142769/1000000: episode: 364, duration: 5.266s, episode steps: 1000, steps per second: 190, episode reward: 123.246, mean reward:  0.123 [-18.005, 22.792], mean action: 0.923 [0.000, 3.000],  loss: 2.381090, mae: 52.798674, mean_q: 71.507713, mean_eps: 0.100000\n",
      " 142986/1000000: episode: 365, duration: 1.040s, episode steps: 217, steps per second: 209, episode reward: 242.470, mean reward:  1.117 [-9.589, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 1.242714, mae: 41.307283, mean_q: 56.187248, mean_eps: 0.100000\n",
      " 143151/1000000: episode: 366, duration: 0.796s, episode steps: 165, steps per second: 207, episode reward:  8.340, mean reward:  0.051 [-100.000,  8.283], mean action: 1.891 [0.000, 3.000],  loss: 1.962511, mae: 44.499974, mean_q: 60.314067, mean_eps: 0.100000\n",
      " 143365/1000000: episode: 367, duration: 1.039s, episode steps: 214, steps per second: 206, episode reward: 278.963, mean reward:  1.304 [-7.857, 100.000], mean action: 1.276 [0.000, 3.000],  loss: 11.412149, mae: 50.825879, mean_q: 68.979591, mean_eps: 0.100000\n",
      " 143467/1000000: episode: 368, duration: 0.493s, episode steps: 102, steps per second: 207, episode reward: 37.943, mean reward:  0.372 [-100.000, 13.912], mean action: 1.912 [0.000, 3.000],  loss: 9.390647, mae: 56.122616, mean_q: 76.153056, mean_eps: 0.100000\n",
      " 143888/1000000: episode: 369, duration: 2.065s, episode steps: 421, steps per second: 204, episode reward: -141.539, mean reward: -0.336 [-100.000, 24.337], mean action: 1.192 [0.000, 3.000],  loss: 15.432589, mae: 61.804335, mean_q: 83.831297, mean_eps: 0.100000\n",
      " 144230/1000000: episode: 370, duration: 1.657s, episode steps: 342, steps per second: 206, episode reward: 228.502, mean reward:  0.668 [-19.777, 100.000], mean action: 0.974 [0.000, 3.000],  loss: 6.799310, mae: 54.200361, mean_q: 69.079901, mean_eps: 0.100000\n",
      " 144553/1000000: episode: 371, duration: 1.572s, episode steps: 323, steps per second: 205, episode reward: -75.265, mean reward: -0.233 [-100.000, 14.120], mean action: 1.307 [0.000, 3.000],  loss: 7.223183, mae: 45.112805, mean_q: 55.354198, mean_eps: 0.100000\n",
      " 145044/1000000: episode: 372, duration: 2.482s, episode steps: 491, steps per second: 198, episode reward: 160.572, mean reward:  0.327 [-19.186, 100.000], mean action: 0.792 [0.000, 3.000],  loss: 10.599534, mae: 38.785600, mean_q: 45.250856, mean_eps: 0.100000\n",
      " 145136/1000000: episode: 373, duration: 0.444s, episode steps:  92, steps per second: 207, episode reward:  8.206, mean reward:  0.089 [-100.000, 17.193], mean action: 1.761 [0.000, 3.000],  loss: 11.667759, mae: 35.133978, mean_q: 43.018048, mean_eps: 0.100000\n",
      " 146136/1000000: episode: 374, duration: 5.373s, episode steps: 1000, steps per second: 186, episode reward: 159.491, mean reward:  0.159 [-19.513, 21.982], mean action: 1.537 [0.000, 3.000],  loss: 7.549792, mae: 44.849840, mean_q: 58.279055, mean_eps: 0.100000\n",
      " 146591/1000000: episode: 375, duration: 2.355s, episode steps: 455, steps per second: 193, episode reward: 232.958, mean reward:  0.512 [-17.570, 100.000], mean action: 2.262 [0.000, 3.000],  loss: 1.888304, mae: 48.231263, mean_q: 64.951952, mean_eps: 0.100000\n",
      " 147139/1000000: episode: 376, duration: 2.859s, episode steps: 548, steps per second: 192, episode reward: 274.663, mean reward:  0.501 [-20.285, 100.000], mean action: 1.560 [0.000, 3.000],  loss: 2.375215, mae: 50.647771, mean_q: 68.519147, mean_eps: 0.100000\n",
      " 147394/1000000: episode: 377, duration: 1.254s, episode steps: 255, steps per second: 203, episode reward: 231.952, mean reward:  0.910 [-10.164, 100.000], mean action: 1.161 [0.000, 3.000],  loss: 1.576173, mae: 49.767310, mean_q: 67.335533, mean_eps: 0.100000\n",
      " 147795/1000000: episode: 378, duration: 2.045s, episode steps: 401, steps per second: 196, episode reward: 273.952, mean reward:  0.683 [-18.394, 100.000], mean action: 0.923 [0.000, 3.000],  loss: 3.576425, mae: 50.859249, mean_q: 68.630975, mean_eps: 0.100000\n",
      " 148101/1000000: episode: 379, duration: 1.507s, episode steps: 306, steps per second: 203, episode reward: 276.134, mean reward:  0.902 [-10.043, 100.000], mean action: 1.595 [0.000, 3.000],  loss: 3.589487, mae: 51.706024, mean_q: 69.607759, mean_eps: 0.100000\n",
      " 148277/1000000: episode: 380, duration: 0.840s, episode steps: 176, steps per second: 210, episode reward: 27.426, mean reward:  0.156 [-100.000, 12.205], mean action: 1.352 [0.000, 3.000],  loss: 3.479787, mae: 53.658999, mean_q: 72.197184, mean_eps: 0.100000\n",
      " 148769/1000000: episode: 381, duration: 2.507s, episode steps: 492, steps per second: 196, episode reward: 256.166, mean reward:  0.521 [-19.556, 100.000], mean action: 1.524 [0.000, 3.000],  loss: 17.203359, mae: 52.577161, mean_q: 71.100849, mean_eps: 0.100000\n",
      " 148920/1000000: episode: 382, duration: 0.730s, episode steps: 151, steps per second: 207, episode reward: -42.223, mean reward: -0.280 [-100.000, 11.411], mean action: 1.629 [0.000, 3.000],  loss: 9.039860, mae: 52.944042, mean_q: 71.514842, mean_eps: 0.100000\n",
      " 149117/1000000: episode: 383, duration: 0.948s, episode steps: 197, steps per second: 208, episode reward: 250.184, mean reward:  1.270 [-7.788, 100.000], mean action: 1.614 [0.000, 3.000],  loss: 8.887692, mae: 53.044102, mean_q: 70.823017, mean_eps: 0.100000\n",
      " 149227/1000000: episode: 384, duration: 0.532s, episode steps: 110, steps per second: 207, episode reward: -30.965, mean reward: -0.282 [-100.000,  9.660], mean action: 2.127 [0.000, 3.000],  loss: 9.981910, mae: 54.250450, mean_q: 72.206163, mean_eps: 0.100000\n",
      " 149476/1000000: episode: 385, duration: 1.210s, episode steps: 249, steps per second: 206, episode reward: 242.719, mean reward:  0.975 [-16.931, 100.000], mean action: 1.390 [0.000, 3.000],  loss: 18.255289, mae: 54.594730, mean_q: 72.503124, mean_eps: 0.100000\n",
      " 149789/1000000: episode: 386, duration: 1.568s, episode steps: 313, steps per second: 200, episode reward: 223.730, mean reward:  0.715 [-5.659, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 15.104489, mae: 55.832269, mean_q: 73.770219, mean_eps: 0.100000\n",
      " 150188/1000000: episode: 387, duration: 1.995s, episode steps: 399, steps per second: 200, episode reward: 180.680, mean reward:  0.453 [-18.628, 100.000], mean action: 1.910 [0.000, 3.000],  loss: 9.249298, mae: 54.320109, mean_q: 72.766817, mean_eps: 0.100000\n",
      " 150462/1000000: episode: 388, duration: 1.374s, episode steps: 274, steps per second: 199, episode reward: 238.158, mean reward:  0.869 [-8.814, 100.000], mean action: 1.752 [0.000, 3.000],  loss: 6.135022, mae: 50.731816, mean_q: 68.714758, mean_eps: 0.100000\n",
      " 150879/1000000: episode: 389, duration: 2.080s, episode steps: 417, steps per second: 201, episode reward: 275.771, mean reward:  0.661 [-19.924, 100.000], mean action: 1.417 [0.000, 3.000],  loss: 4.051301, mae: 52.497459, mean_q: 70.861615, mean_eps: 0.100000\n",
      " 151263/1000000: episode: 390, duration: 1.892s, episode steps: 384, steps per second: 203, episode reward: 271.551, mean reward:  0.707 [-3.005, 100.000], mean action: 1.357 [0.000, 3.000],  loss: 3.588069, mae: 54.550755, mean_q: 73.491780, mean_eps: 0.100000\n",
      " 151987/1000000: episode: 391, duration: 3.729s, episode steps: 724, steps per second: 194, episode reward: 174.741, mean reward:  0.241 [-18.153, 100.000], mean action: 1.443 [0.000, 3.000],  loss: 4.570479, mae: 46.403167, mean_q: 61.381794, mean_eps: 0.100000\n",
      " 152453/1000000: episode: 392, duration: 2.486s, episode steps: 466, steps per second: 187, episode reward: 252.742, mean reward:  0.542 [-18.046, 100.000], mean action: 1.408 [0.000, 3.000],  loss: 9.263082, mae: 31.241104, mean_q: 38.631773, mean_eps: 0.100000\n",
      " 152911/1000000: episode: 393, duration: 2.400s, episode steps: 458, steps per second: 191, episode reward: 210.807, mean reward:  0.460 [-12.791, 100.000], mean action: 1.583 [0.000, 3.000],  loss: 6.539845, mae: 39.551787, mean_q: 51.874886, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 153911/1000000: episode: 394, duration: 5.945s, episode steps: 1000, steps per second: 168, episode reward: 71.151, mean reward:  0.071 [-19.257, 22.433], mean action: 1.249 [0.000, 3.000],  loss: 2.831769, mae: 46.310511, mean_q: 62.894165, mean_eps: 0.100000\n",
      " 154166/1000000: episode: 395, duration: 1.246s, episode steps: 255, steps per second: 205, episode reward: 298.791, mean reward:  1.172 [-7.372, 100.000], mean action: 1.373 [0.000, 3.000],  loss: 3.540755, mae: 44.580317, mean_q: 60.578352, mean_eps: 0.100000\n",
      " 154927/1000000: episode: 396, duration: 3.964s, episode steps: 761, steps per second: 192, episode reward: 228.043, mean reward:  0.300 [-20.725, 100.000], mean action: 0.925 [0.000, 3.000],  loss: 3.828515, mae: 48.269765, mean_q: 65.610568, mean_eps: 0.100000\n",
      " 155043/1000000: episode: 397, duration: 0.553s, episode steps: 116, steps per second: 210, episode reward: 25.615, mean reward:  0.221 [-100.000, 14.514], mean action: 1.526 [0.000, 3.000],  loss: 4.804380, mae: 47.620536, mean_q: 64.900335, mean_eps: 0.100000\n",
      " 155141/1000000: episode: 398, duration: 0.467s, episode steps:  98, steps per second: 210, episode reward: 14.161, mean reward:  0.145 [-100.000, 16.750], mean action: 1.327 [0.000, 3.000],  loss: 19.305700, mae: 48.497776, mean_q: 66.917806, mean_eps: 0.100000\n",
      " 155237/1000000: episode: 399, duration: 0.455s, episode steps:  96, steps per second: 211, episode reward: -39.457, mean reward: -0.411 [-100.000, 22.529], mean action: 1.521 [0.000, 3.000],  loss: 17.669367, mae: 50.596237, mean_q: 69.639640, mean_eps: 0.100000\n",
      " 155463/1000000: episode: 400, duration: 1.092s, episode steps: 226, steps per second: 207, episode reward: 291.774, mean reward:  1.291 [-3.811, 100.000], mean action: 1.106 [0.000, 3.000],  loss: 19.956207, mae: 51.575579, mean_q: 70.547354, mean_eps: 0.100000\n",
      " 155552/1000000: episode: 401, duration: 0.427s, episode steps:  89, steps per second: 208, episode reward: -158.860, mean reward: -1.785 [-100.000, 51.323], mean action: 1.944 [0.000, 3.000],  loss: 22.064227, mae: 54.287382, mean_q: 73.742750, mean_eps: 0.100000\n",
      " 156104/1000000: episode: 402, duration: 2.951s, episode steps: 552, steps per second: 187, episode reward: 256.689, mean reward:  0.465 [-18.097, 100.000], mean action: 1.533 [0.000, 3.000],  loss: 16.325817, mae: 59.012579, mean_q: 79.679068, mean_eps: 0.100000\n",
      " 156442/1000000: episode: 403, duration: 1.672s, episode steps: 338, steps per second: 202, episode reward: 289.021, mean reward:  0.855 [-17.689, 100.000], mean action: 1.219 [0.000, 3.000],  loss: 8.190998, mae: 54.817990, mean_q: 74.024543, mean_eps: 0.100000\n",
      " 156538/1000000: episode: 404, duration: 0.459s, episode steps:  96, steps per second: 209, episode reward: 21.520, mean reward:  0.224 [-100.000, 25.615], mean action: 1.469 [0.000, 3.000],  loss: 5.265184, mae: 55.326969, mean_q: 74.385479, mean_eps: 0.100000\n",
      " 156767/1000000: episode: 405, duration: 1.104s, episode steps: 229, steps per second: 207, episode reward: 271.757, mean reward:  1.187 [-9.985, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 10.005400, mae: 55.111894, mean_q: 74.702934, mean_eps: 0.100000\n",
      " 157042/1000000: episode: 406, duration: 1.342s, episode steps: 275, steps per second: 205, episode reward: 237.996, mean reward:  0.865 [-9.114, 100.000], mean action: 1.113 [0.000, 3.000],  loss: 7.833060, mae: 56.607592, mean_q: 76.640249, mean_eps: 0.100000\n",
      " 157383/1000000: episode: 407, duration: 1.688s, episode steps: 341, steps per second: 202, episode reward: 261.204, mean reward:  0.766 [-10.844, 100.000], mean action: 1.399 [0.000, 3.000],  loss: 7.353353, mae: 55.267619, mean_q: 74.608184, mean_eps: 0.100000\n",
      " 157535/1000000: episode: 408, duration: 0.724s, episode steps: 152, steps per second: 210, episode reward: 23.374, mean reward:  0.154 [-100.000, 16.939], mean action: 1.546 [0.000, 3.000],  loss: 7.447080, mae: 56.530369, mean_q: 76.066586, mean_eps: 0.100000\n",
      " 158317/1000000: episode: 409, duration: 4.261s, episode steps: 782, steps per second: 184, episode reward: 248.076, mean reward:  0.317 [-20.561, 100.000], mean action: 1.364 [0.000, 3.000],  loss: 11.350540, mae: 53.863163, mean_q: 72.998326, mean_eps: 0.100000\n",
      " 158741/1000000: episode: 410, duration: 2.182s, episode steps: 424, steps per second: 194, episode reward: 250.802, mean reward:  0.592 [-19.777, 100.000], mean action: 1.158 [0.000, 3.000],  loss: 5.411440, mae: 49.239869, mean_q: 66.824541, mean_eps: 0.100000\n",
      " 159265/1000000: episode: 411, duration: 2.623s, episode steps: 524, steps per second: 200, episode reward: 281.540, mean reward:  0.537 [-18.715, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 4.353889, mae: 46.225330, mean_q: 62.843709, mean_eps: 0.100000\n",
      " 159631/1000000: episode: 412, duration: 1.882s, episode steps: 366, steps per second: 194, episode reward: 226.498, mean reward:  0.619 [-9.944, 100.000], mean action: 1.596 [0.000, 3.000],  loss: 5.129909, mae: 44.171430, mean_q: 60.145383, mean_eps: 0.100000\n",
      " 160007/1000000: episode: 413, duration: 1.888s, episode steps: 376, steps per second: 199, episode reward: 262.310, mean reward:  0.698 [-9.979, 100.000], mean action: 1.362 [0.000, 3.000],  loss: 4.148308, mae: 46.047667, mean_q: 62.376649, mean_eps: 0.100000\n",
      " 160355/1000000: episode: 414, duration: 1.746s, episode steps: 348, steps per second: 199, episode reward: 273.885, mean reward:  0.787 [-10.902, 100.000], mean action: 1.325 [0.000, 3.000],  loss: 3.926050, mae: 49.740757, mean_q: 67.115766, mean_eps: 0.100000\n",
      " 160816/1000000: episode: 415, duration: 2.308s, episode steps: 461, steps per second: 200, episode reward: 257.032, mean reward:  0.558 [-19.632, 100.000], mean action: 1.351 [0.000, 3.000],  loss: 3.275038, mae: 48.641015, mean_q: 65.667946, mean_eps: 0.100000\n",
      " 161270/1000000: episode: 416, duration: 2.394s, episode steps: 454, steps per second: 190, episode reward: 210.126, mean reward:  0.463 [-10.888, 100.000], mean action: 1.676 [0.000, 3.000],  loss: 3.130153, mae: 45.980385, mean_q: 62.028715, mean_eps: 0.100000\n",
      " 161697/1000000: episode: 417, duration: 2.155s, episode steps: 427, steps per second: 198, episode reward: 271.683, mean reward:  0.636 [-17.732, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 3.349810, mae: 44.623203, mean_q: 60.237926, mean_eps: 0.100000\n",
      " 162029/1000000: episode: 418, duration: 1.673s, episode steps: 332, steps per second: 198, episode reward: 277.226, mean reward:  0.835 [-9.755, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 3.032539, mae: 48.361723, mean_q: 65.328143, mean_eps: 0.100000\n",
      " 162325/1000000: episode: 419, duration: 1.480s, episode steps: 296, steps per second: 200, episode reward: 282.781, mean reward:  0.955 [-8.714, 100.000], mean action: 1.463 [0.000, 3.000],  loss: 3.462087, mae: 51.532302, mean_q: 69.677970, mean_eps: 0.100000\n",
      " 162573/1000000: episode: 420, duration: 1.199s, episode steps: 248, steps per second: 207, episode reward: 247.241, mean reward:  0.997 [-3.201, 100.000], mean action: 1.254 [0.000, 3.000],  loss: 3.292139, mae: 51.652387, mean_q: 69.980471, mean_eps: 0.100000\n",
      " 162836/1000000: episode: 421, duration: 1.266s, episode steps: 263, steps per second: 208, episode reward: 281.549, mean reward:  1.071 [-2.924, 100.000], mean action: 1.289 [0.000, 3.000],  loss: 4.890364, mae: 52.147814, mean_q: 70.670049, mean_eps: 0.100000\n",
      " 163138/1000000: episode: 422, duration: 1.481s, episode steps: 302, steps per second: 204, episode reward: 236.835, mean reward:  0.784 [-17.374, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 5.213986, mae: 52.328657, mean_q: 70.707394, mean_eps: 0.100000\n",
      " 163489/1000000: episode: 423, duration: 1.714s, episode steps: 351, steps per second: 205, episode reward: 278.727, mean reward:  0.794 [-23.142, 100.000], mean action: 0.966 [0.000, 3.000],  loss: 4.283910, mae: 53.682991, mean_q: 72.553257, mean_eps: 0.100000\n",
      " 163747/1000000: episode: 424, duration: 1.301s, episode steps: 258, steps per second: 198, episode reward: 294.657, mean reward:  1.142 [-9.032, 100.000], mean action: 1.605 [0.000, 3.000],  loss: 3.528628, mae: 53.687334, mean_q: 72.689146, mean_eps: 0.100000\n",
      " 164078/1000000: episode: 425, duration: 1.706s, episode steps: 331, steps per second: 194, episode reward: 270.067, mean reward:  0.816 [-11.660, 100.000], mean action: 1.387 [0.000, 3.000],  loss: 2.847906, mae: 52.730170, mean_q: 71.357595, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 164596/1000000: episode: 426, duration: 2.670s, episode steps: 518, steps per second: 194, episode reward: 260.243, mean reward:  0.502 [-10.383, 100.000], mean action: 1.689 [0.000, 3.000],  loss: 3.145832, mae: 50.837563, mean_q: 68.572029, mean_eps: 0.100000\n",
      " 164915/1000000: episode: 427, duration: 1.565s, episode steps: 319, steps per second: 204, episode reward: 279.994, mean reward:  0.878 [-10.281, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 3.108337, mae: 49.712043, mean_q: 67.137988, mean_eps: 0.100000\n",
      " 165151/1000000: episode: 428, duration: 1.170s, episode steps: 236, steps per second: 202, episode reward: 266.427, mean reward:  1.129 [-11.434, 100.000], mean action: 1.564 [0.000, 3.000],  loss: 3.230393, mae: 52.114249, mean_q: 70.397493, mean_eps: 0.100000\n",
      " 165404/1000000: episode: 429, duration: 1.239s, episode steps: 253, steps per second: 204, episode reward: 266.652, mean reward:  1.054 [-18.526, 100.000], mean action: 1.043 [0.000, 3.000],  loss: 4.430514, mae: 54.251173, mean_q: 73.231500, mean_eps: 0.100000\n",
      " 165787/1000000: episode: 430, duration: 1.879s, episode steps: 383, steps per second: 204, episode reward: 232.207, mean reward:  0.606 [-19.304, 100.000], mean action: 1.133 [0.000, 3.000],  loss: 3.750086, mae: 57.313674, mean_q: 77.243799, mean_eps: 0.100000\n",
      " 166016/1000000: episode: 431, duration: 1.104s, episode steps: 229, steps per second: 208, episode reward: 248.695, mean reward:  1.086 [-8.533, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 5.091431, mae: 54.331421, mean_q: 73.444456, mean_eps: 0.100000\n",
      " 166110/1000000: episode: 432, duration: 0.451s, episode steps:  94, steps per second: 209, episode reward: 27.125, mean reward:  0.289 [-100.000, 20.143], mean action: 1.819 [0.000, 3.000],  loss: 3.841409, mae: 54.368295, mean_q: 73.612772, mean_eps: 0.100000\n",
      " 166404/1000000: episode: 433, duration: 1.423s, episode steps: 294, steps per second: 207, episode reward: 278.727, mean reward:  0.948 [-17.436, 100.000], mean action: 1.259 [0.000, 3.000],  loss: 4.635363, mae: 55.143667, mean_q: 74.075871, mean_eps: 0.100000\n",
      " 166986/1000000: episode: 434, duration: 2.993s, episode steps: 582, steps per second: 194, episode reward: 266.391, mean reward:  0.458 [-19.636, 100.000], mean action: 0.641 [0.000, 3.000],  loss: 4.194905, mae: 55.731676, mean_q: 74.797449, mean_eps: 0.100000\n",
      " 167243/1000000: episode: 435, duration: 1.260s, episode steps: 257, steps per second: 204, episode reward: 253.124, mean reward:  0.985 [-17.462, 100.000], mean action: 1.117 [0.000, 3.000],  loss: 1.743991, mae: 54.697906, mean_q: 73.685411, mean_eps: 0.100000\n",
      " 167887/1000000: episode: 436, duration: 3.228s, episode steps: 644, steps per second: 200, episode reward: 289.112, mean reward:  0.449 [-18.853, 100.000], mean action: 1.455 [0.000, 3.000],  loss: 2.406441, mae: 52.882303, mean_q: 71.528768, mean_eps: 0.100000\n",
      " 168284/1000000: episode: 437, duration: 2.002s, episode steps: 397, steps per second: 198, episode reward: 257.529, mean reward:  0.649 [-18.608, 100.000], mean action: 1.889 [0.000, 3.000],  loss: 2.329579, mae: 51.545167, mean_q: 69.923896, mean_eps: 0.100000\n",
      " 168738/1000000: episode: 438, duration: 2.318s, episode steps: 454, steps per second: 196, episode reward: 244.932, mean reward:  0.539 [-10.407, 100.000], mean action: 0.874 [0.000, 3.000],  loss: 2.323961, mae: 50.461863, mean_q: 68.316057, mean_eps: 0.100000\n",
      " 169404/1000000: episode: 439, duration: 3.435s, episode steps: 666, steps per second: 194, episode reward: 285.911, mean reward:  0.429 [-19.307, 100.000], mean action: 1.179 [0.000, 3.000],  loss: 1.759599, mae: 51.797784, mean_q: 69.946399, mean_eps: 0.100000\n",
      " 169622/1000000: episode: 440, duration: 1.050s, episode steps: 218, steps per second: 208, episode reward: 263.617, mean reward:  1.209 [-11.365, 100.000], mean action: 1.362 [0.000, 3.000],  loss: 1.955932, mae: 49.948330, mean_q: 67.435510, mean_eps: 0.100000\n",
      " 169978/1000000: episode: 441, duration: 1.803s, episode steps: 356, steps per second: 197, episode reward: 272.735, mean reward:  0.766 [-17.963, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 2.364248, mae: 52.766975, mean_q: 71.161656, mean_eps: 0.100000\n",
      " 170398/1000000: episode: 442, duration: 2.110s, episode steps: 420, steps per second: 199, episode reward: 268.185, mean reward:  0.639 [-11.920, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 3.193337, mae: 53.343583, mean_q: 71.994402, mean_eps: 0.100000\n",
      " 170655/1000000: episode: 443, duration: 1.258s, episode steps: 257, steps per second: 204, episode reward: 275.262, mean reward:  1.071 [-13.999, 100.000], mean action: 1.650 [0.000, 3.000],  loss: 2.035728, mae: 55.972997, mean_q: 75.501840, mean_eps: 0.100000\n",
      " 170860/1000000: episode: 444, duration: 0.994s, episode steps: 205, steps per second: 206, episode reward: 243.458, mean reward:  1.188 [-11.028, 100.000], mean action: 1.171 [0.000, 3.000],  loss: 3.287983, mae: 56.210066, mean_q: 75.849432, mean_eps: 0.100000\n",
      " 171254/1000000: episode: 445, duration: 1.964s, episode steps: 394, steps per second: 201, episode reward: 284.166, mean reward:  0.721 [-18.040, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 3.430033, mae: 59.152810, mean_q: 79.843733, mean_eps: 0.100000\n",
      " 171576/1000000: episode: 446, duration: 1.626s, episode steps: 322, steps per second: 198, episode reward: 265.201, mean reward:  0.824 [-19.279, 100.000], mean action: 1.280 [0.000, 3.000],  loss: 3.283240, mae: 59.404974, mean_q: 80.199668, mean_eps: 0.100000\n",
      " 171795/1000000: episode: 447, duration: 1.056s, episode steps: 219, steps per second: 207, episode reward: 277.593, mean reward:  1.268 [-11.033, 100.000], mean action: 1.237 [0.000, 3.000],  loss: 3.749758, mae: 58.615219, mean_q: 79.133436, mean_eps: 0.100000\n",
      " 172043/1000000: episode: 448, duration: 1.192s, episode steps: 248, steps per second: 208, episode reward: 312.030, mean reward:  1.258 [-12.752, 100.000], mean action: 1.544 [0.000, 3.000],  loss: 2.519837, mae: 58.831007, mean_q: 79.373476, mean_eps: 0.100000\n",
      " 172606/1000000: episode: 449, duration: 2.863s, episode steps: 563, steps per second: 197, episode reward: 239.614, mean reward:  0.426 [-19.122, 100.000], mean action: 2.494 [0.000, 3.000],  loss: 3.657915, mae: 58.688354, mean_q: 79.319635, mean_eps: 0.100000\n",
      " 172693/1000000: episode: 450, duration: 0.426s, episode steps:  87, steps per second: 204, episode reward: -11.606, mean reward: -0.133 [-100.000, 17.867], mean action: 1.379 [0.000, 3.000],  loss: 2.761448, mae: 54.854525, mean_q: 74.437249, mean_eps: 0.100000\n",
      " 173693/1000000: episode: 451, duration: 5.360s, episode steps: 1000, steps per second: 187, episode reward: 95.924, mean reward:  0.096 [-18.866, 13.322], mean action: 2.447 [0.000, 3.000],  loss: 4.428589, mae: 48.847240, mean_q: 66.273420, mean_eps: 0.100000\n",
      " 173953/1000000: episode: 452, duration: 1.310s, episode steps: 260, steps per second: 198, episode reward: 221.480, mean reward:  0.852 [-12.152, 100.000], mean action: 1.627 [0.000, 3.000],  loss: 1.691502, mae: 40.505024, mean_q: 55.005689, mean_eps: 0.100000\n",
      " 174133/1000000: episode: 453, duration: 0.876s, episode steps: 180, steps per second: 205, episode reward: 234.481, mean reward:  1.303 [-3.115, 100.000], mean action: 1.606 [0.000, 3.000],  loss: 3.372922, mae: 42.911849, mean_q: 58.134572, mean_eps: 0.100000\n",
      " 174418/1000000: episode: 454, duration: 1.411s, episode steps: 285, steps per second: 202, episode reward: 224.460, mean reward:  0.788 [-3.225, 100.000], mean action: 1.565 [0.000, 3.000],  loss: 3.561148, mae: 46.555858, mean_q: 62.843854, mean_eps: 0.100000\n",
      " 174613/1000000: episode: 455, duration: 0.939s, episode steps: 195, steps per second: 208, episode reward: 14.367, mean reward:  0.074 [-100.000, 16.487], mean action: 1.641 [0.000, 3.000],  loss: 3.685744, mae: 50.235815, mean_q: 67.652741, mean_eps: 0.100000\n",
      " 175297/1000000: episode: 456, duration: 3.499s, episode steps: 684, steps per second: 196, episode reward: 154.191, mean reward:  0.225 [-18.826, 100.000], mean action: 2.360 [0.000, 3.000],  loss: 6.505683, mae: 48.628837, mean_q: 65.556320, mean_eps: 0.100000\n",
      " 175665/1000000: episode: 457, duration: 1.826s, episode steps: 368, steps per second: 202, episode reward: 260.572, mean reward:  0.708 [-10.742, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 5.857759, mae: 38.444365, mean_q: 52.790650, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 176054/1000000: episode: 458, duration: 1.910s, episode steps: 389, steps per second: 204, episode reward: 310.578, mean reward:  0.798 [-9.547, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 3.781977, mae: 39.981228, mean_q: 55.380982, mean_eps: 0.100000\n",
      " 176393/1000000: episode: 459, duration: 1.667s, episode steps: 339, steps per second: 203, episode reward: 260.937, mean reward:  0.770 [-22.548, 100.000], mean action: 1.174 [0.000, 3.000],  loss: 4.617505, mae: 52.638162, mean_q: 71.296023, mean_eps: 0.100000\n",
      " 176791/1000000: episode: 460, duration: 1.990s, episode steps: 398, steps per second: 200, episode reward: 291.649, mean reward:  0.733 [-18.533, 100.000], mean action: 1.173 [0.000, 3.000],  loss: 2.962950, mae: 54.451180, mean_q: 73.526986, mean_eps: 0.100000\n",
      " 177063/1000000: episode: 461, duration: 1.357s, episode steps: 272, steps per second: 200, episode reward: 289.434, mean reward:  1.064 [-9.813, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 3.961459, mae: 54.566373, mean_q: 73.685778, mean_eps: 0.100000\n",
      " 177562/1000000: episode: 462, duration: 2.523s, episode steps: 499, steps per second: 198, episode reward: 275.252, mean reward:  0.552 [-18.729, 100.000], mean action: 0.752 [0.000, 3.000],  loss: 3.429403, mae: 55.823610, mean_q: 75.436009, mean_eps: 0.100000\n",
      " 177702/1000000: episode: 463, duration: 0.666s, episode steps: 140, steps per second: 210, episode reward: 34.099, mean reward:  0.244 [-100.000, 11.625], mean action: 1.571 [0.000, 3.000],  loss: 4.315436, mae: 54.849184, mean_q: 74.307193, mean_eps: 0.100000\n",
      " 177872/1000000: episode: 464, duration: 0.812s, episode steps: 170, steps per second: 209, episode reward: 260.009, mean reward:  1.529 [-11.172, 100.000], mean action: 1.206 [0.000, 3.000],  loss: 7.861853, mae: 57.261723, mean_q: 77.520279, mean_eps: 0.100000\n",
      " 177996/1000000: episode: 465, duration: 0.594s, episode steps: 124, steps per second: 209, episode reward: -7.983, mean reward: -0.064 [-100.000, 10.886], mean action: 1.653 [0.000, 3.000],  loss: 15.809415, mae: 58.421920, mean_q: 78.962665, mean_eps: 0.100000\n",
      " 178286/1000000: episode: 466, duration: 1.423s, episode steps: 290, steps per second: 204, episode reward: 275.952, mean reward:  0.952 [-6.012, 100.000], mean action: 0.886 [0.000, 3.000],  loss: 34.848883, mae: 58.681685, mean_q: 80.031396, mean_eps: 0.100000\n",
      " 178404/1000000: episode: 467, duration: 0.564s, episode steps: 118, steps per second: 209, episode reward:  6.793, mean reward:  0.058 [-100.000, 13.852], mean action: 1.441 [0.000, 3.000],  loss: 22.013209, mae: 59.744988, mean_q: 81.812626, mean_eps: 0.100000\n",
      " 178518/1000000: episode: 468, duration: 0.546s, episode steps: 114, steps per second: 209, episode reward: 24.998, mean reward:  0.219 [-100.000,  8.718], mean action: 1.632 [0.000, 3.000],  loss: 22.527900, mae: 62.579947, mean_q: 85.078866, mean_eps: 0.100000\n",
      " 178809/1000000: episode: 469, duration: 1.431s, episode steps: 291, steps per second: 203, episode reward: 274.745, mean reward:  0.944 [-24.449, 100.000], mean action: 0.993 [0.000, 3.000],  loss: 22.749846, mae: 63.296529, mean_q: 85.612181, mean_eps: 0.100000\n",
      " 179568/1000000: episode: 470, duration: 3.962s, episode steps: 759, steps per second: 192, episode reward: 260.481, mean reward:  0.343 [-19.308, 100.000], mean action: 0.792 [0.000, 3.000],  loss: 8.152226, mae: 50.103974, mean_q: 68.040206, mean_eps: 0.100000\n",
      " 179854/1000000: episode: 471, duration: 1.404s, episode steps: 286, steps per second: 204, episode reward: 233.659, mean reward:  0.817 [-18.085, 100.000], mean action: 1.234 [0.000, 3.000],  loss: 4.979499, mae: 37.019949, mean_q: 50.414172, mean_eps: 0.100000\n",
      " 180203/1000000: episode: 472, duration: 1.723s, episode steps: 349, steps per second: 203, episode reward: 293.129, mean reward:  0.840 [-11.082, 100.000], mean action: 1.410 [0.000, 3.000],  loss: 4.064701, mae: 40.292408, mean_q: 54.447826, mean_eps: 0.100000\n",
      " 180608/1000000: episode: 473, duration: 2.151s, episode steps: 405, steps per second: 188, episode reward: 265.154, mean reward:  0.655 [-19.861, 100.000], mean action: 1.074 [0.000, 3.000],  loss: 5.615083, mae: 51.092264, mean_q: 68.895605, mean_eps: 0.100000\n",
      " 180960/1000000: episode: 474, duration: 1.779s, episode steps: 352, steps per second: 198, episode reward: 269.789, mean reward:  0.766 [-19.355, 100.000], mean action: 0.932 [0.000, 3.000],  loss: 3.679922, mae: 54.592571, mean_q: 73.596873, mean_eps: 0.100000\n",
      " 181238/1000000: episode: 475, duration: 1.366s, episode steps: 278, steps per second: 204, episode reward: 272.827, mean reward:  0.981 [-11.660, 100.000], mean action: 1.554 [0.000, 3.000],  loss: 3.813651, mae: 52.526162, mean_q: 70.761833, mean_eps: 0.100000\n",
      " 181510/1000000: episode: 476, duration: 1.323s, episode steps: 272, steps per second: 206, episode reward: 247.986, mean reward:  0.912 [-8.046, 100.000], mean action: 1.335 [0.000, 3.000],  loss: 3.184804, mae: 53.337448, mean_q: 71.854243, mean_eps: 0.100000\n",
      " 181991/1000000: episode: 477, duration: 2.577s, episode steps: 481, steps per second: 187, episode reward: 280.524, mean reward:  0.583 [-19.194, 100.000], mean action: 0.879 [0.000, 3.000],  loss: 3.484017, mae: 55.116993, mean_q: 74.256717, mean_eps: 0.100000\n",
      " 182232/1000000: episode: 478, duration: 1.174s, episode steps: 241, steps per second: 205, episode reward: 302.513, mean reward:  1.255 [-2.386, 100.000], mean action: 1.581 [0.000, 3.000],  loss: 2.655840, mae: 56.677229, mean_q: 76.377220, mean_eps: 0.100000\n",
      " 182503/1000000: episode: 479, duration: 1.335s, episode steps: 271, steps per second: 203, episode reward: 298.179, mean reward:  1.100 [-2.584, 100.000], mean action: 1.646 [0.000, 3.000],  loss: 2.278830, mae: 58.030152, mean_q: 78.103437, mean_eps: 0.100000\n",
      " 182768/1000000: episode: 480, duration: 1.299s, episode steps: 265, steps per second: 204, episode reward: 227.504, mean reward:  0.859 [-3.249, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 3.364778, mae: 57.177706, mean_q: 76.984796, mean_eps: 0.100000\n",
      " 183109/1000000: episode: 481, duration: 1.681s, episode steps: 341, steps per second: 203, episode reward: 245.038, mean reward:  0.719 [-17.675, 100.000], mean action: 0.903 [0.000, 3.000],  loss: 3.686815, mae: 57.161360, mean_q: 77.018058, mean_eps: 0.100000\n",
      " 183395/1000000: episode: 482, duration: 1.391s, episode steps: 286, steps per second: 206, episode reward: 246.909, mean reward:  0.863 [-10.626, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 4.865316, mae: 53.888770, mean_q: 72.772218, mean_eps: 0.100000\n",
      " 183620/1000000: episode: 483, duration: 1.095s, episode steps: 225, steps per second: 206, episode reward: 249.515, mean reward:  1.109 [-3.011, 100.000], mean action: 1.342 [0.000, 3.000],  loss: 5.498809, mae: 53.049426, mean_q: 71.577975, mean_eps: 0.100000\n",
      " 183878/1000000: episode: 484, duration: 1.249s, episode steps: 258, steps per second: 207, episode reward: 285.655, mean reward:  1.107 [-4.559, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 6.066987, mae: 55.409016, mean_q: 74.631581, mean_eps: 0.100000\n",
      " 184078/1000000: episode: 485, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: 251.232, mean reward:  1.256 [-3.397, 100.000], mean action: 1.390 [0.000, 3.000],  loss: 4.522527, mae: 57.915525, mean_q: 77.990257, mean_eps: 0.100000\n",
      " 184437/1000000: episode: 486, duration: 1.770s, episode steps: 359, steps per second: 203, episode reward: 278.921, mean reward:  0.777 [-14.995, 100.000], mean action: 2.145 [0.000, 3.000],  loss: 3.216145, mae: 59.235008, mean_q: 79.753680, mean_eps: 0.100000\n",
      " 184572/1000000: episode: 487, duration: 0.649s, episode steps: 135, steps per second: 208, episode reward: 43.008, mean reward:  0.319 [-100.000, 14.878], mean action: 1.793 [0.000, 3.000],  loss: 3.256295, mae: 58.284249, mean_q: 78.749955, mean_eps: 0.100000\n",
      " 184675/1000000: episode: 488, duration: 0.489s, episode steps: 103, steps per second: 211, episode reward: -63.755, mean reward: -0.619 [-100.000, 11.049], mean action: 1.146 [0.000, 3.000],  loss: 22.551799, mae: 59.457042, mean_q: 80.275318, mean_eps: 0.100000\n",
      " 185263/1000000: episode: 489, duration: 3.019s, episode steps: 588, steps per second: 195, episode reward: 265.630, mean reward:  0.452 [-19.256, 100.000], mean action: 0.690 [0.000, 3.000],  loss: 10.970532, mae: 54.386530, mean_q: 73.527366, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 185979/1000000: episode: 490, duration: 3.674s, episode steps: 716, steps per second: 195, episode reward: 239.438, mean reward:  0.334 [-19.885, 100.000], mean action: 1.145 [0.000, 3.000],  loss: 6.009063, mae: 44.917849, mean_q: 60.976016, mean_eps: 0.100000\n",
      " 186350/1000000: episode: 491, duration: 1.819s, episode steps: 371, steps per second: 204, episode reward: 271.630, mean reward:  0.732 [-12.106, 100.000], mean action: 1.356 [0.000, 3.000],  loss: 3.292842, mae: 47.202646, mean_q: 63.880198, mean_eps: 0.100000\n",
      " 186675/1000000: episode: 492, duration: 1.606s, episode steps: 325, steps per second: 202, episode reward: 286.199, mean reward:  0.881 [-19.694, 100.000], mean action: 1.422 [0.000, 3.000],  loss: 2.720353, mae: 52.965042, mean_q: 71.374366, mean_eps: 0.100000\n",
      " 186990/1000000: episode: 493, duration: 1.574s, episode steps: 315, steps per second: 200, episode reward: 243.133, mean reward:  0.772 [-11.318, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 4.022982, mae: 57.555110, mean_q: 77.555896, mean_eps: 0.100000\n",
      " 187306/1000000: episode: 494, duration: 1.577s, episode steps: 316, steps per second: 200, episode reward: 255.278, mean reward:  0.808 [-13.233, 100.000], mean action: 1.209 [0.000, 3.000],  loss: 3.118235, mae: 57.616735, mean_q: 77.592547, mean_eps: 0.100000\n",
      " 187573/1000000: episode: 495, duration: 1.347s, episode steps: 267, steps per second: 198, episode reward: 220.759, mean reward:  0.827 [-3.235, 100.000], mean action: 1.453 [0.000, 3.000],  loss: 3.595790, mae: 55.938105, mean_q: 75.345464, mean_eps: 0.100000\n",
      " 187972/1000000: episode: 496, duration: 2.079s, episode steps: 399, steps per second: 192, episode reward: 266.997, mean reward:  0.669 [-12.073, 100.000], mean action: 1.008 [0.000, 3.000],  loss: 3.410132, mae: 54.784380, mean_q: 73.703616, mean_eps: 0.100000\n",
      " 188972/1000000: episode: 497, duration: 5.432s, episode steps: 1000, steps per second: 184, episode reward: 119.726, mean reward:  0.120 [-20.656, 12.534], mean action: 1.718 [0.000, 3.000],  loss: 2.038242, mae: 53.267161, mean_q: 71.825147, mean_eps: 0.100000\n",
      " 189362/1000000: episode: 498, duration: 1.974s, episode steps: 390, steps per second: 198, episode reward: 254.190, mean reward:  0.652 [-17.574, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 1.516961, mae: 47.674953, mean_q: 64.694466, mean_eps: 0.100000\n",
      " 189758/1000000: episode: 499, duration: 1.996s, episode steps: 396, steps per second: 198, episode reward: 235.365, mean reward:  0.594 [-12.073, 100.000], mean action: 0.987 [0.000, 3.000],  loss: 3.273217, mae: 46.618160, mean_q: 63.167159, mean_eps: 0.100000\n",
      " 190046/1000000: episode: 500, duration: 1.425s, episode steps: 288, steps per second: 202, episode reward: 249.854, mean reward:  0.868 [-6.631, 100.000], mean action: 1.410 [0.000, 3.000],  loss: 4.545363, mae: 49.193612, mean_q: 66.287511, mean_eps: 0.100000\n",
      " 190580/1000000: episode: 501, duration: 2.737s, episode steps: 534, steps per second: 195, episode reward: 262.060, mean reward:  0.491 [-17.171, 100.000], mean action: 1.129 [0.000, 3.000],  loss: 3.377483, mae: 51.741045, mean_q: 69.747156, mean_eps: 0.100000\n",
      " 191262/1000000: episode: 502, duration: 3.753s, episode steps: 682, steps per second: 182, episode reward: 254.064, mean reward:  0.373 [-19.582, 100.000], mean action: 1.054 [0.000, 3.000],  loss: 1.864161, mae: 52.234970, mean_q: 70.635195, mean_eps: 0.100000\n",
      " 191625/1000000: episode: 503, duration: 1.812s, episode steps: 363, steps per second: 200, episode reward: 262.428, mean reward:  0.723 [-17.528, 100.000], mean action: 1.143 [0.000, 3.000],  loss: 1.660045, mae: 50.649032, mean_q: 68.356888, mean_eps: 0.100000\n",
      " 192004/1000000: episode: 504, duration: 1.922s, episode steps: 379, steps per second: 197, episode reward: 266.536, mean reward:  0.703 [-11.740, 100.000], mean action: 1.528 [0.000, 3.000],  loss: 2.752300, mae: 51.086476, mean_q: 69.059505, mean_eps: 0.100000\n",
      " 192247/1000000: episode: 505, duration: 1.182s, episode steps: 243, steps per second: 206, episode reward: 279.541, mean reward:  1.150 [-10.573, 100.000], mean action: 1.457 [0.000, 3.000],  loss: 3.862963, mae: 54.928706, mean_q: 74.111893, mean_eps: 0.100000\n",
      " 192747/1000000: episode: 506, duration: 2.516s, episode steps: 500, steps per second: 199, episode reward: 255.861, mean reward:  0.512 [-19.240, 100.000], mean action: 0.866 [0.000, 3.000],  loss: 2.219506, mae: 51.675044, mean_q: 69.649519, mean_eps: 0.100000\n",
      " 193094/1000000: episode: 507, duration: 1.704s, episode steps: 347, steps per second: 204, episode reward: 262.487, mean reward:  0.756 [-18.328, 100.000], mean action: 1.138 [0.000, 3.000],  loss: 2.407754, mae: 46.181365, mean_q: 62.364521, mean_eps: 0.100000\n",
      " 193395/1000000: episode: 508, duration: 1.508s, episode steps: 301, steps per second: 200, episode reward: 240.769, mean reward:  0.800 [-13.806, 100.000], mean action: 1.372 [0.000, 3.000],  loss: 3.502107, mae: 43.226308, mean_q: 58.516865, mean_eps: 0.100000\n",
      " 193640/1000000: episode: 509, duration: 1.182s, episode steps: 245, steps per second: 207, episode reward: 284.875, mean reward:  1.163 [-10.222, 100.000], mean action: 1.273 [0.000, 3.000],  loss: 3.926397, mae: 46.071047, mean_q: 62.407320, mean_eps: 0.100000\n",
      " 194262/1000000: episode: 510, duration: 3.213s, episode steps: 622, steps per second: 194, episode reward: 254.760, mean reward:  0.410 [-19.705, 100.000], mean action: 0.982 [0.000, 3.000],  loss: 3.609079, mae: 51.370282, mean_q: 69.492855, mean_eps: 0.100000\n",
      " 194675/1000000: episode: 511, duration: 2.109s, episode steps: 413, steps per second: 196, episode reward: 233.331, mean reward:  0.565 [-17.297, 100.000], mean action: 2.024 [0.000, 3.000],  loss: 2.585601, mae: 52.270112, mean_q: 70.641791, mean_eps: 0.100000\n",
      " 195191/1000000: episode: 512, duration: 2.687s, episode steps: 516, steps per second: 192, episode reward: 227.661, mean reward:  0.441 [-18.387, 100.000], mean action: 1.409 [0.000, 3.000],  loss: 1.917659, mae: 47.056671, mean_q: 63.729063, mean_eps: 0.100000\n",
      " 195584/1000000: episode: 513, duration: 1.986s, episode steps: 393, steps per second: 198, episode reward: 271.841, mean reward:  0.692 [-20.607, 100.000], mean action: 1.081 [0.000, 3.000],  loss: 3.595036, mae: 45.979777, mean_q: 61.968938, mean_eps: 0.100000\n",
      " 195835/1000000: episode: 514, duration: 1.218s, episode steps: 251, steps per second: 206, episode reward: 267.424, mean reward:  1.065 [-8.961, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 2.519665, mae: 48.333080, mean_q: 65.043519, mean_eps: 0.100000\n",
      " 196199/1000000: episode: 515, duration: 1.851s, episode steps: 364, steps per second: 197, episode reward: 195.395, mean reward:  0.537 [-13.948, 100.000], mean action: 1.651 [0.000, 3.000],  loss: 3.286046, mae: 50.064164, mean_q: 67.462101, mean_eps: 0.100000\n",
      " 196599/1000000: episode: 516, duration: 1.984s, episode steps: 400, steps per second: 202, episode reward: 305.418, mean reward:  0.764 [-12.889, 100.000], mean action: 1.087 [0.000, 3.000],  loss: 3.470099, mae: 50.445038, mean_q: 68.289782, mean_eps: 0.100000\n",
      " 196831/1000000: episode: 517, duration: 1.130s, episode steps: 232, steps per second: 205, episode reward: 262.582, mean reward:  1.132 [-11.432, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 3.668228, mae: 50.313553, mean_q: 68.070610, mean_eps: 0.100000\n",
      " 197012/1000000: episode: 518, duration: 0.881s, episode steps: 181, steps per second: 205, episode reward: 225.212, mean reward:  1.244 [-14.592, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 3.973543, mae: 51.912017, mean_q: 70.345522, mean_eps: 0.100000\n",
      " 197336/1000000: episode: 519, duration: 1.625s, episode steps: 324, steps per second: 199, episode reward: 230.396, mean reward:  0.711 [-13.497, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 2.697200, mae: 55.197247, mean_q: 74.859894, mean_eps: 0.100000\n",
      " 197507/1000000: episode: 520, duration: 0.824s, episode steps: 171, steps per second: 208, episode reward: 238.557, mean reward:  1.395 [-2.701, 100.000], mean action: 1.269 [0.000, 3.000],  loss: 4.612715, mae: 54.275861, mean_q: 73.683281, mean_eps: 0.100000\n",
      " 197748/1000000: episode: 521, duration: 1.160s, episode steps: 241, steps per second: 208, episode reward: 250.770, mean reward:  1.041 [-18.892, 100.000], mean action: 0.884 [0.000, 3.000],  loss: 4.246492, mae: 56.969436, mean_q: 77.239323, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 197943/1000000: episode: 522, duration: 0.937s, episode steps: 195, steps per second: 208, episode reward: 252.933, mean reward:  1.297 [-2.644, 100.000], mean action: 1.226 [0.000, 3.000],  loss: 3.828770, mae: 56.431427, mean_q: 76.601825, mean_eps: 0.100000\n",
      " 198137/1000000: episode: 523, duration: 0.937s, episode steps: 194, steps per second: 207, episode reward: 271.533, mean reward:  1.400 [-12.435, 100.000], mean action: 1.582 [0.000, 3.000],  loss: 5.034804, mae: 55.643466, mean_q: 75.411039, mean_eps: 0.100000\n",
      " 199137/1000000: episode: 524, duration: 5.242s, episode steps: 1000, steps per second: 191, episode reward: 142.015, mean reward:  0.142 [-23.747, 22.220], mean action: 2.129 [0.000, 3.000],  loss: 3.578843, mae: 53.319674, mean_q: 72.053179, mean_eps: 0.100000\n",
      " 199293/1000000: episode: 525, duration: 0.741s, episode steps: 156, steps per second: 210, episode reward:  5.071, mean reward:  0.033 [-100.000,  8.374], mean action: 1.513 [0.000, 3.000],  loss: 2.135468, mae: 44.494419, mean_q: 60.187368, mean_eps: 0.100000\n",
      " 199679/1000000: episode: 526, duration: 1.898s, episode steps: 386, steps per second: 203, episode reward: 287.258, mean reward:  0.744 [-13.499, 100.000], mean action: 0.899 [0.000, 3.000],  loss: 6.091034, mae: 46.190261, mean_q: 62.845074, mean_eps: 0.100000\n",
      " 200030/1000000: episode: 527, duration: 1.748s, episode steps: 351, steps per second: 201, episode reward: 225.348, mean reward:  0.642 [-2.992, 100.000], mean action: 1.373 [0.000, 3.000],  loss: 5.830847, mae: 51.127853, mean_q: 69.401157, mean_eps: 0.100000\n",
      " 200247/1000000: episode: 528, duration: 1.045s, episode steps: 217, steps per second: 208, episode reward: 259.230, mean reward:  1.195 [-2.752, 100.000], mean action: 1.240 [0.000, 3.000],  loss: 4.760618, mae: 54.139446, mean_q: 73.338114, mean_eps: 0.100000\n",
      " 200550/1000000: episode: 529, duration: 1.507s, episode steps: 303, steps per second: 201, episode reward: 266.798, mean reward:  0.881 [-9.200, 100.000], mean action: 1.802 [0.000, 3.000],  loss: 4.872724, mae: 53.888975, mean_q: 73.136788, mean_eps: 0.100000\n",
      " 200805/1000000: episode: 530, duration: 1.248s, episode steps: 255, steps per second: 204, episode reward: 306.861, mean reward:  1.203 [-18.093, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 4.628723, mae: 52.833947, mean_q: 71.731554, mean_eps: 0.100000\n",
      " 201048/1000000: episode: 531, duration: 1.195s, episode steps: 243, steps per second: 203, episode reward: 261.958, mean reward:  1.078 [-17.946, 100.000], mean action: 1.547 [0.000, 3.000],  loss: 6.459956, mae: 53.405429, mean_q: 72.398462, mean_eps: 0.100000\n",
      " 201310/1000000: episode: 532, duration: 1.284s, episode steps: 262, steps per second: 204, episode reward: 273.355, mean reward:  1.043 [-9.172, 100.000], mean action: 1.107 [0.000, 3.000],  loss: 5.850199, mae: 53.578538, mean_q: 72.598569, mean_eps: 0.100000\n",
      " 201491/1000000: episode: 533, duration: 0.880s, episode steps: 181, steps per second: 206, episode reward: 276.842, mean reward:  1.530 [-2.790, 100.000], mean action: 2.232 [0.000, 3.000],  loss: 6.568163, mae: 52.156314, mean_q: 70.671415, mean_eps: 0.100000\n",
      " 201653/1000000: episode: 534, duration: 0.832s, episode steps: 162, steps per second: 195, episode reward: -25.564, mean reward: -0.158 [-100.000, 12.375], mean action: 1.883 [0.000, 3.000],  loss: 5.388650, mae: 55.358947, mean_q: 74.755954, mean_eps: 0.100000\n",
      " 202051/1000000: episode: 535, duration: 2.093s, episode steps: 398, steps per second: 190, episode reward: 250.446, mean reward:  0.629 [-19.948, 100.000], mean action: 0.706 [0.000, 3.000],  loss: 23.802892, mae: 54.662121, mean_q: 74.035048, mean_eps: 0.100000\n",
      " 202439/1000000: episode: 536, duration: 2.035s, episode steps: 388, steps per second: 191, episode reward: 249.332, mean reward:  0.643 [-19.243, 100.000], mean action: 1.090 [0.000, 3.000],  loss: 18.472798, mae: 51.297062, mean_q: 69.730577, mean_eps: 0.100000\n",
      " 203120/1000000: episode: 537, duration: 3.600s, episode steps: 681, steps per second: 189, episode reward: 274.450, mean reward:  0.403 [-18.614, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 3.968594, mae: 48.834844, mean_q: 66.484858, mean_eps: 0.100000\n",
      " 203357/1000000: episode: 538, duration: 1.177s, episode steps: 237, steps per second: 201, episode reward: 257.630, mean reward:  1.087 [-17.681, 100.000], mean action: 1.110 [0.000, 3.000],  loss: 1.904182, mae: 49.578541, mean_q: 67.289192, mean_eps: 0.100000\n",
      " 203534/1000000: episode: 539, duration: 0.884s, episode steps: 177, steps per second: 200, episode reward: 305.299, mean reward:  1.725 [-9.247, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 2.734085, mae: 52.953128, mean_q: 71.655448, mean_eps: 0.100000\n",
      " 203743/1000000: episode: 540, duration: 1.063s, episode steps: 209, steps per second: 197, episode reward: 248.405, mean reward:  1.189 [-9.601, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 3.502016, mae: 53.786754, mean_q: 72.638187, mean_eps: 0.100000\n",
      " 203935/1000000: episode: 541, duration: 1.202s, episode steps: 192, steps per second: 160, episode reward: 37.986, mean reward:  0.198 [-100.000, 13.477], mean action: 1.932 [0.000, 3.000],  loss: 2.851103, mae: 58.133177, mean_q: 78.379233, mean_eps: 0.100000\n",
      " 204124/1000000: episode: 542, duration: 1.057s, episode steps: 189, steps per second: 179, episode reward: 274.539, mean reward:  1.453 [-10.042, 100.000], mean action: 1.206 [0.000, 3.000],  loss: 16.997988, mae: 63.021519, mean_q: 85.164368, mean_eps: 0.100000\n",
      " 204370/1000000: episode: 543, duration: 1.267s, episode steps: 246, steps per second: 194, episode reward: 263.686, mean reward:  1.072 [-9.627, 100.000], mean action: 1.093 [0.000, 3.000],  loss: 13.988502, mae: 63.979778, mean_q: 86.792274, mean_eps: 0.100000\n",
      " 204476/1000000: episode: 544, duration: 0.585s, episode steps: 106, steps per second: 181, episode reward: -12.818, mean reward: -0.121 [-100.000, 16.005], mean action: 1.594 [0.000, 3.000],  loss: 10.951275, mae: 63.294674, mean_q: 85.945165, mean_eps: 0.100000\n",
      " 204641/1000000: episode: 545, duration: 0.870s, episode steps: 165, steps per second: 190, episode reward: 274.883, mean reward:  1.666 [-7.140, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 13.079688, mae: 64.765583, mean_q: 87.952487, mean_eps: 0.100000\n",
      " 204750/1000000: episode: 546, duration: 0.535s, episode steps: 109, steps per second: 204, episode reward: -42.909, mean reward: -0.394 [-100.000, 18.519], mean action: 1.560 [0.000, 3.000],  loss: 12.329632, mae: 64.714792, mean_q: 87.672690, mean_eps: 0.100000\n",
      " 205051/1000000: episode: 547, duration: 1.558s, episode steps: 301, steps per second: 193, episode reward: 263.266, mean reward:  0.875 [-19.269, 100.000], mean action: 0.920 [0.000, 3.000],  loss: 8.730255, mae: 64.140923, mean_q: 86.057247, mean_eps: 0.100000\n",
      " 205644/1000000: episode: 548, duration: 3.246s, episode steps: 593, steps per second: 183, episode reward: 274.766, mean reward:  0.463 [-18.950, 100.000], mean action: 0.535 [0.000, 3.000],  loss: 4.205944, mae: 61.164736, mean_q: 82.342862, mean_eps: 0.100000\n",
      " 205839/1000000: episode: 549, duration: 1.001s, episode steps: 195, steps per second: 195, episode reward: 266.947, mean reward:  1.369 [-8.476, 100.000], mean action: 1.179 [0.000, 3.000],  loss: 3.812173, mae: 57.069546, mean_q: 77.191318, mean_eps: 0.100000\n",
      " 206156/1000000: episode: 550, duration: 1.596s, episode steps: 317, steps per second: 199, episode reward: 264.715, mean reward:  0.835 [-19.837, 100.000], mean action: 1.032 [0.000, 3.000],  loss: 2.795846, mae: 58.111934, mean_q: 78.589893, mean_eps: 0.100000\n",
      " 206408/1000000: episode: 551, duration: 1.298s, episode steps: 252, steps per second: 194, episode reward: 263.061, mean reward:  1.044 [-18.284, 100.000], mean action: 1.123 [0.000, 3.000],  loss: 2.656205, mae: 58.496590, mean_q: 78.777335, mean_eps: 0.100000\n",
      " 206659/1000000: episode: 552, duration: 1.247s, episode steps: 251, steps per second: 201, episode reward: 277.238, mean reward:  1.105 [-18.863, 100.000], mean action: 1.124 [0.000, 3.000],  loss: 2.748570, mae: 63.652569, mean_q: 85.540920, mean_eps: 0.100000\n",
      " 206891/1000000: episode: 553, duration: 1.124s, episode steps: 232, steps per second: 206, episode reward: 246.944, mean reward:  1.064 [-10.159, 100.000], mean action: 1.151 [0.000, 3.000],  loss: 2.941986, mae: 63.377231, mean_q: 85.334602, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 207306/1000000: episode: 554, duration: 2.430s, episode steps: 415, steps per second: 171, episode reward: 296.007, mean reward:  0.713 [-9.719, 100.000], mean action: 1.292 [0.000, 3.000],  loss: 2.737268, mae: 62.890925, mean_q: 84.923114, mean_eps: 0.100000\n",
      " 207502/1000000: episode: 555, duration: 1.171s, episode steps: 196, steps per second: 167, episode reward: 264.919, mean reward:  1.352 [-10.199, 100.000], mean action: 0.980 [0.000, 3.000],  loss: 3.588177, mae: 62.113611, mean_q: 84.032675, mean_eps: 0.100000\n",
      " 207759/1000000: episode: 556, duration: 1.267s, episode steps: 257, steps per second: 203, episode reward: 291.669, mean reward:  1.135 [-10.749, 100.000], mean action: 1.553 [0.000, 3.000],  loss: 3.616306, mae: 62.119967, mean_q: 83.966799, mean_eps: 0.100000\n",
      " 207976/1000000: episode: 557, duration: 1.050s, episode steps: 217, steps per second: 207, episode reward: 270.038, mean reward:  1.244 [-18.365, 100.000], mean action: 1.157 [0.000, 3.000],  loss: 3.148867, mae: 62.783156, mean_q: 84.768747, mean_eps: 0.100000\n",
      " 208290/1000000: episode: 558, duration: 1.544s, episode steps: 314, steps per second: 203, episode reward: 307.891, mean reward:  0.981 [-12.411, 100.000], mean action: 1.675 [0.000, 3.000],  loss: 3.143882, mae: 62.420300, mean_q: 84.230673, mean_eps: 0.100000\n",
      " 208955/1000000: episode: 559, duration: 3.322s, episode steps: 665, steps per second: 200, episode reward: 287.811, mean reward:  0.433 [-16.982, 100.000], mean action: 1.032 [0.000, 3.000],  loss: 2.974500, mae: 62.471453, mean_q: 84.398417, mean_eps: 0.100000\n",
      " 209172/1000000: episode: 560, duration: 1.047s, episode steps: 217, steps per second: 207, episode reward: 268.938, mean reward:  1.239 [-17.807, 100.000], mean action: 1.226 [0.000, 3.000],  loss: 2.780398, mae: 62.255834, mean_q: 84.031721, mean_eps: 0.100000\n",
      " 209468/1000000: episode: 561, duration: 1.439s, episode steps: 296, steps per second: 206, episode reward: 289.705, mean reward:  0.979 [-17.550, 100.000], mean action: 0.939 [0.000, 3.000],  loss: 2.564950, mae: 62.812123, mean_q: 84.749271, mean_eps: 0.100000\n",
      " 209565/1000000: episode: 562, duration: 0.464s, episode steps:  97, steps per second: 209, episode reward:  5.465, mean reward:  0.056 [-100.000, 16.087], mean action: 1.443 [0.000, 3.000],  loss: 4.081590, mae: 62.374063, mean_q: 84.087359, mean_eps: 0.100000\n",
      " 210273/1000000: episode: 563, duration: 3.567s, episode steps: 708, steps per second: 198, episode reward: 160.199, mean reward:  0.226 [-20.337, 100.000], mean action: 0.970 [0.000, 3.000],  loss: 13.952262, mae: 57.244611, mean_q: 76.877465, mean_eps: 0.100000\n",
      " 210396/1000000: episode: 564, duration: 0.594s, episode steps: 123, steps per second: 207, episode reward: 56.354, mean reward:  0.458 [-100.000, 40.171], mean action: 1.854 [0.000, 3.000],  loss: 20.792876, mae: 40.326029, mean_q: 53.151024, mean_eps: 0.100000\n",
      " 210616/1000000: episode: 565, duration: 1.050s, episode steps: 220, steps per second: 209, episode reward: 271.854, mean reward:  1.236 [-10.244, 100.000], mean action: 0.741 [0.000, 3.000],  loss: 39.557687, mae: 42.504460, mean_q: 56.239455, mean_eps: 0.100000\n",
      " 210837/1000000: episode: 566, duration: 1.055s, episode steps: 221, steps per second: 209, episode reward: 249.664, mean reward:  1.130 [-10.333, 100.000], mean action: 0.900 [0.000, 3.000],  loss: 16.063219, mae: 41.712244, mean_q: 55.576549, mean_eps: 0.100000\n",
      " 211751/1000000: episode: 567, duration: 4.795s, episode steps: 914, steps per second: 191, episode reward: 203.083, mean reward:  0.222 [-19.676, 100.000], mean action: 0.779 [0.000, 3.000],  loss: 10.958444, mae: 49.141079, mean_q: 67.128190, mean_eps: 0.100000\n",
      " 212212/1000000: episode: 568, duration: 2.391s, episode steps: 461, steps per second: 193, episode reward: 272.478, mean reward:  0.591 [-19.388, 100.000], mean action: 1.633 [0.000, 3.000],  loss: 5.355622, mae: 33.722579, mean_q: 46.266564, mean_eps: 0.100000\n",
      " 212468/1000000: episode: 569, duration: 1.253s, episode steps: 256, steps per second: 204, episode reward: 257.242, mean reward:  1.005 [-17.109, 100.000], mean action: 1.828 [0.000, 3.000],  loss: 4.214970, mae: 41.291935, mean_q: 56.295851, mean_eps: 0.100000\n",
      " 213037/1000000: episode: 570, duration: 2.845s, episode steps: 569, steps per second: 200, episode reward: 241.963, mean reward:  0.425 [-21.015, 100.000], mean action: 2.112 [0.000, 3.000],  loss: 3.898049, mae: 52.546114, mean_q: 71.334602, mean_eps: 0.100000\n",
      " 213252/1000000: episode: 571, duration: 1.023s, episode steps: 215, steps per second: 210, episode reward: 265.857, mean reward:  1.237 [-18.066, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 4.341380, mae: 52.663961, mean_q: 71.529494, mean_eps: 0.100000\n",
      " 213373/1000000: episode: 572, duration: 0.577s, episode steps: 121, steps per second: 210, episode reward: -148.667, mean reward: -1.229 [-100.000, 27.043], mean action: 1.876 [0.000, 3.000],  loss: 4.006704, mae: 53.257971, mean_q: 72.335873, mean_eps: 0.100000\n",
      " 213730/1000000: episode: 573, duration: 1.759s, episode steps: 357, steps per second: 203, episode reward: 260.153, mean reward:  0.729 [-17.504, 100.000], mean action: 1.020 [0.000, 3.000],  loss: 14.631076, mae: 55.971258, mean_q: 75.833288, mean_eps: 0.100000\n",
      " 213828/1000000: episode: 574, duration: 0.473s, episode steps:  98, steps per second: 207, episode reward:  8.245, mean reward:  0.084 [-100.000, 11.305], mean action: 1.663 [0.000, 3.000],  loss: 15.836357, mae: 58.203636, mean_q: 78.763368, mean_eps: 0.100000\n",
      " 214065/1000000: episode: 575, duration: 1.169s, episode steps: 237, steps per second: 203, episode reward: 244.874, mean reward:  1.033 [-17.382, 100.000], mean action: 2.000 [0.000, 3.000],  loss: 17.310923, mae: 65.075576, mean_q: 87.917530, mean_eps: 0.100000\n",
      " 214269/1000000: episode: 576, duration: 0.997s, episode steps: 204, steps per second: 205, episode reward: 263.008, mean reward:  1.289 [-18.319, 100.000], mean action: 2.279 [0.000, 3.000],  loss: 14.748942, mae: 64.720810, mean_q: 87.341874, mean_eps: 0.100000\n",
      " 214496/1000000: episode: 577, duration: 1.099s, episode steps: 227, steps per second: 206, episode reward: 299.405, mean reward:  1.319 [-18.653, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 8.400795, mae: 64.584035, mean_q: 87.277312, mean_eps: 0.100000\n",
      " 214715/1000000: episode: 578, duration: 1.046s, episode steps: 219, steps per second: 209, episode reward: 264.263, mean reward:  1.207 [-18.315, 100.000], mean action: 1.064 [0.000, 3.000],  loss: 10.032637, mae: 64.431728, mean_q: 86.984859, mean_eps: 0.100000\n",
      " 214993/1000000: episode: 579, duration: 1.345s, episode steps: 278, steps per second: 207, episode reward: 283.274, mean reward:  1.019 [-11.466, 100.000], mean action: 1.302 [0.000, 3.000],  loss: 8.099593, mae: 63.009675, mean_q: 84.974056, mean_eps: 0.100000\n",
      " 215157/1000000: episode: 580, duration: 0.779s, episode steps: 164, steps per second: 211, episode reward: 247.347, mean reward:  1.508 [-8.729, 100.000], mean action: 1.110 [0.000, 3.000],  loss: 5.848957, mae: 65.931033, mean_q: 89.069675, mean_eps: 0.100000\n",
      " 215617/1000000: episode: 581, duration: 2.353s, episode steps: 460, steps per second: 195, episode reward: 265.120, mean reward:  0.576 [-18.539, 100.000], mean action: 1.087 [0.000, 3.000],  loss: 6.177534, mae: 60.822670, mean_q: 82.007236, mean_eps: 0.100000\n",
      " 215827/1000000: episode: 582, duration: 1.003s, episode steps: 210, steps per second: 209, episode reward: 262.489, mean reward:  1.250 [-17.349, 100.000], mean action: 0.848 [0.000, 3.000],  loss: 5.869826, mae: 56.748207, mean_q: 76.572673, mean_eps: 0.100000\n",
      " 216201/1000000: episode: 583, duration: 1.879s, episode steps: 374, steps per second: 199, episode reward: 239.850, mean reward:  0.641 [-19.920, 100.000], mean action: 0.719 [0.000, 3.000],  loss: 4.594022, mae: 54.163828, mean_q: 73.195419, mean_eps: 0.100000\n",
      " 216392/1000000: episode: 584, duration: 0.908s, episode steps: 191, steps per second: 210, episode reward: 267.053, mean reward:  1.398 [-2.267, 100.000], mean action: 0.749 [0.000, 3.000],  loss: 4.703752, mae: 54.322969, mean_q: 73.645812, mean_eps: 0.100000\n",
      " 216624/1000000: episode: 585, duration: 1.126s, episode steps: 232, steps per second: 206, episode reward: 254.577, mean reward:  1.097 [-9.447, 100.000], mean action: 1.259 [0.000, 3.000],  loss: 5.327719, mae: 61.429385, mean_q: 83.335011, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 216797/1000000: episode: 586, duration: 0.827s, episode steps: 173, steps per second: 209, episode reward: 252.072, mean reward:  1.457 [-6.398, 100.000], mean action: 1.312 [0.000, 3.000],  loss: 1.885548, mae: 63.962957, mean_q: 86.727510, mean_eps: 0.100000\n",
      " 216935/1000000: episode: 587, duration: 0.665s, episode steps: 138, steps per second: 208, episode reward: 21.560, mean reward:  0.156 [-100.000, 17.875], mean action: 1.659 [0.000, 3.000],  loss: 4.268258, mae: 63.696331, mean_q: 86.255287, mean_eps: 0.100000\n",
      " 217232/1000000: episode: 588, duration: 1.458s, episode steps: 297, steps per second: 204, episode reward: 303.314, mean reward:  1.021 [-19.742, 100.000], mean action: 1.037 [0.000, 3.000],  loss: 15.517594, mae: 65.546021, mean_q: 89.163676, mean_eps: 0.100000\n",
      " 217523/1000000: episode: 589, duration: 1.417s, episode steps: 291, steps per second: 205, episode reward: 278.720, mean reward:  0.958 [-17.611, 100.000], mean action: 0.704 [0.000, 3.000],  loss: 8.598040, mae: 64.905575, mean_q: 88.616893, mean_eps: 0.100000\n",
      " 217859/1000000: episode: 590, duration: 1.621s, episode steps: 336, steps per second: 207, episode reward: 296.910, mean reward:  0.884 [-17.436, 100.000], mean action: 0.735 [0.000, 3.000],  loss: 8.426290, mae: 62.501449, mean_q: 85.745284, mean_eps: 0.100000\n",
      " 218265/1000000: episode: 591, duration: 1.960s, episode steps: 406, steps per second: 207, episode reward: 236.630, mean reward:  0.583 [-18.441, 100.000], mean action: 0.702 [0.000, 3.000],  loss: 3.994185, mae: 57.380664, mean_q: 78.952195, mean_eps: 0.100000\n",
      " 218375/1000000: episode: 592, duration: 0.529s, episode steps: 110, steps per second: 208, episode reward: -28.184, mean reward: -0.256 [-100.000, 12.390], mean action: 1.827 [0.000, 3.000],  loss: 6.556154, mae: 49.255579, mean_q: 67.554009, mean_eps: 0.100000\n",
      " 218863/1000000: episode: 593, duration: 2.439s, episode steps: 488, steps per second: 200, episode reward: 266.350, mean reward:  0.546 [-19.997, 100.000], mean action: 1.098 [0.000, 3.000],  loss: 19.393525, mae: 51.540632, mean_q: 70.652847, mean_eps: 0.100000\n",
      " 218962/1000000: episode: 594, duration: 0.475s, episode steps:  99, steps per second: 208, episode reward: 12.102, mean reward:  0.122 [-100.000,  7.740], mean action: 1.778 [0.000, 3.000],  loss: 20.822513, mae: 49.215791, mean_q: 67.256200, mean_eps: 0.100000\n",
      " 219398/1000000: episode: 595, duration: 2.170s, episode steps: 436, steps per second: 201, episode reward: 254.253, mean reward:  0.583 [-10.506, 100.000], mean action: 0.661 [0.000, 3.000],  loss: 21.973545, mae: 53.331390, mean_q: 72.423452, mean_eps: 0.100000\n",
      " 219493/1000000: episode: 596, duration: 0.462s, episode steps:  95, steps per second: 206, episode reward:  6.206, mean reward:  0.065 [-100.000,  7.554], mean action: 1.811 [0.000, 3.000],  loss: 23.786771, mae: 48.488409, mean_q: 66.144055, mean_eps: 0.100000\n",
      " 219592/1000000: episode: 597, duration: 0.478s, episode steps:  99, steps per second: 207, episode reward: -11.217, mean reward: -0.113 [-100.000, 14.220], mean action: 1.273 [0.000, 3.000],  loss: 29.555111, mae: 49.312139, mean_q: 67.387395, mean_eps: 0.100000\n",
      " 219682/1000000: episode: 598, duration: 0.431s, episode steps:  90, steps per second: 209, episode reward: -1.173, mean reward: -0.013 [-100.000, 19.808], mean action: 1.211 [0.000, 3.000],  loss: 42.404510, mae: 52.751128, mean_q: 72.052561, mean_eps: 0.100000\n",
      " 220003/1000000: episode: 599, duration: 1.558s, episode steps: 321, steps per second: 206, episode reward: 202.887, mean reward:  0.632 [-14.660, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 32.492553, mae: 52.612798, mean_q: 71.679813, mean_eps: 0.100000\n",
      " 220079/1000000: episode: 600, duration: 0.363s, episode steps:  76, steps per second: 209, episode reward: -25.245, mean reward: -0.332 [-100.000,  9.447], mean action: 1.908 [0.000, 3.000],  loss: 30.777228, mae: 47.460977, mean_q: 64.421178, mean_eps: 0.100000\n",
      " 220163/1000000: episode: 601, duration: 0.403s, episode steps:  84, steps per second: 209, episode reward: -13.744, mean reward: -0.164 [-100.000, 12.279], mean action: 1.738 [0.000, 3.000],  loss: 54.087436, mae: 51.502053, mean_q: 69.954288, mean_eps: 0.100000\n",
      " 220244/1000000: episode: 602, duration: 0.388s, episode steps:  81, steps per second: 209, episode reward: -39.735, mean reward: -0.491 [-100.000, 13.520], mean action: 1.259 [0.000, 3.000],  loss: 47.697966, mae: 55.906540, mean_q: 75.441783, mean_eps: 0.100000\n",
      " 220350/1000000: episode: 603, duration: 0.498s, episode steps: 106, steps per second: 213, episode reward: 10.263, mean reward:  0.097 [-100.000, 19.960], mean action: 1.208 [0.000, 3.000],  loss: 46.222183, mae: 62.329463, mean_q: 83.769319, mean_eps: 0.100000\n",
      " 220470/1000000: episode: 604, duration: 0.565s, episode steps: 120, steps per second: 213, episode reward: -32.176, mean reward: -0.268 [-100.000,  9.767], mean action: 1.308 [0.000, 3.000],  loss: 37.550839, mae: 65.913920, mean_q: 87.847200, mean_eps: 0.100000\n",
      " 220599/1000000: episode: 605, duration: 0.610s, episode steps: 129, steps per second: 211, episode reward: 21.243, mean reward:  0.165 [-100.000, 12.887], mean action: 1.279 [0.000, 3.000],  loss: 30.500119, mae: 65.384567, mean_q: 86.835682, mean_eps: 0.100000\n",
      " 220692/1000000: episode: 606, duration: 0.450s, episode steps:  93, steps per second: 207, episode reward: -1.451, mean reward: -0.016 [-100.000, 14.666], mean action: 1.957 [0.000, 3.000],  loss: 27.196452, mae: 66.712040, mean_q: 89.043630, mean_eps: 0.100000\n",
      " 220794/1000000: episode: 607, duration: 0.491s, episode steps: 102, steps per second: 208, episode reward: -70.434, mean reward: -0.691 [-100.000,  5.272], mean action: 2.059 [0.000, 3.000],  loss: 25.025443, mae: 67.969648, mean_q: 89.719496, mean_eps: 0.100000\n",
      " 220898/1000000: episode: 608, duration: 0.498s, episode steps: 104, steps per second: 209, episode reward: 25.206, mean reward:  0.242 [-100.000, 13.232], mean action: 1.769 [0.000, 3.000],  loss: 27.708311, mae: 70.589121, mean_q: 92.873523, mean_eps: 0.100000\n",
      " 220993/1000000: episode: 609, duration: 0.456s, episode steps:  95, steps per second: 208, episode reward: -19.036, mean reward: -0.200 [-100.000,  7.119], mean action: 1.853 [0.000, 3.000],  loss: 24.904213, mae: 74.518275, mean_q: 97.399997, mean_eps: 0.100000\n",
      " 221181/1000000: episode: 610, duration: 0.906s, episode steps: 188, steps per second: 208, episode reward: -5.985, mean reward: -0.032 [-100.000, 10.742], mean action: 1.771 [0.000, 3.000],  loss: 18.363106, mae: 72.385185, mean_q: 92.343468, mean_eps: 0.100000\n",
      " 221284/1000000: episode: 611, duration: 0.499s, episode steps: 103, steps per second: 206, episode reward: 36.988, mean reward:  0.359 [-100.000, 17.110], mean action: 1.903 [0.000, 3.000],  loss: 15.630840, mae: 71.719629, mean_q: 89.301256, mean_eps: 0.100000\n",
      " 221423/1000000: episode: 612, duration: 0.670s, episode steps: 139, steps per second: 208, episode reward: -50.970, mean reward: -0.367 [-100.000,  7.647], mean action: 1.799 [0.000, 3.000],  loss: 18.311701, mae: 70.572965, mean_q: 86.824664, mean_eps: 0.100000\n",
      " 221576/1000000: episode: 613, duration: 0.739s, episode steps: 153, steps per second: 207, episode reward: -29.878, mean reward: -0.195 [-100.000, 14.708], mean action: 1.869 [0.000, 3.000],  loss: 17.379167, mae: 67.938022, mean_q: 80.494346, mean_eps: 0.100000\n",
      " 221741/1000000: episode: 614, duration: 0.795s, episode steps: 165, steps per second: 208, episode reward: 13.094, mean reward:  0.079 [-100.000,  7.690], mean action: 1.770 [0.000, 3.000],  loss: 19.805106, mae: 66.295413, mean_q: 79.655195, mean_eps: 0.100000\n",
      " 222162/1000000: episode: 615, duration: 2.122s, episode steps: 421, steps per second: 198, episode reward: 128.123, mean reward:  0.304 [-21.022, 100.000], mean action: 1.620 [0.000, 3.000],  loss: 19.683956, mae: 57.771616, mean_q: 65.954485, mean_eps: 0.100000\n",
      " 222462/1000000: episode: 616, duration: 1.477s, episode steps: 300, steps per second: 203, episode reward: -8.113, mean reward: -0.027 [-100.000,  4.715], mean action: 1.960 [0.000, 3.000],  loss: 21.591222, mae: 45.280345, mean_q: 46.255492, mean_eps: 0.100000\n",
      " 223193/1000000: episode: 617, duration: 3.876s, episode steps: 731, steps per second: 189, episode reward: 126.341, mean reward:  0.173 [-18.207, 100.000], mean action: 1.834 [0.000, 3.000],  loss: 19.801471, mae: 30.836956, mean_q: 28.622207, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 223381/1000000: episode: 618, duration: 0.909s, episode steps: 188, steps per second: 207, episode reward: -44.418, mean reward: -0.236 [-100.000, 30.371], mean action: 1.601 [0.000, 3.000],  loss: 14.171947, mae: 26.012463, mean_q: 19.198776, mean_eps: 0.100000\n",
      " 223643/1000000: episode: 619, duration: 1.265s, episode steps: 262, steps per second: 207, episode reward: 300.915, mean reward:  1.149 [-9.780, 100.000], mean action: 1.359 [0.000, 3.000],  loss: 17.050337, mae: 29.062831, mean_q: 23.149575, mean_eps: 0.100000\n",
      " 223737/1000000: episode: 620, duration: 0.448s, episode steps:  94, steps per second: 210, episode reward: -1.018, mean reward: -0.011 [-100.000, 10.018], mean action: 1.596 [0.000, 3.000],  loss: 15.540859, mae: 30.188794, mean_q: 28.372210, mean_eps: 0.100000\n",
      " 223825/1000000: episode: 621, duration: 0.424s, episode steps:  88, steps per second: 207, episode reward: 33.040, mean reward:  0.375 [-100.000, 16.294], mean action: 1.830 [0.000, 3.000],  loss: 12.408398, mae: 36.754202, mean_q: 35.988248, mean_eps: 0.100000\n",
      " 224100/1000000: episode: 622, duration: 1.354s, episode steps: 275, steps per second: 203, episode reward: 253.228, mean reward:  0.921 [-10.237, 100.000], mean action: 1.378 [0.000, 3.000],  loss: 26.131772, mae: 45.385062, mean_q: 51.048730, mean_eps: 0.100000\n",
      " 225100/1000000: episode: 623, duration: 5.434s, episode steps: 1000, steps per second: 184, episode reward: -70.128, mean reward: -0.070 [-23.178, 22.412], mean action: 1.537 [0.000, 3.000],  loss: 13.521026, mae: 39.704362, mean_q: 50.335557, mean_eps: 0.100000\n",
      " 225219/1000000: episode: 624, duration: 0.563s, episode steps: 119, steps per second: 211, episode reward: 32.135, mean reward:  0.270 [-100.000, 24.987], mean action: 1.101 [0.000, 3.000],  loss: 4.583406, mae: 25.817891, mean_q: 33.079592, mean_eps: 0.100000\n",
      " 225416/1000000: episode: 625, duration: 0.935s, episode steps: 197, steps per second: 211, episode reward: 248.692, mean reward:  1.262 [-2.755, 100.000], mean action: 0.822 [0.000, 3.000],  loss: 5.243507, mae: 29.806797, mean_q: 37.437487, mean_eps: 0.100000\n",
      " 225793/1000000: episode: 626, duration: 1.865s, episode steps: 377, steps per second: 202, episode reward: 198.208, mean reward:  0.526 [-19.550, 100.000], mean action: 1.170 [0.000, 3.000],  loss: 7.062550, mae: 37.298695, mean_q: 49.721408, mean_eps: 0.100000\n",
      " 225987/1000000: episode: 627, duration: 0.932s, episode steps: 194, steps per second: 208, episode reward: 279.882, mean reward:  1.443 [-2.818, 100.000], mean action: 1.098 [0.000, 3.000],  loss: 7.427464, mae: 41.983491, mean_q: 55.557965, mean_eps: 0.100000\n",
      " 226380/1000000: episode: 628, duration: 1.994s, episode steps: 393, steps per second: 197, episode reward: 227.483, mean reward:  0.579 [-12.292, 100.000], mean action: 1.344 [0.000, 3.000],  loss: 7.567397, mae: 43.420897, mean_q: 57.421053, mean_eps: 0.100000\n",
      " 226764/1000000: episode: 629, duration: 1.913s, episode steps: 384, steps per second: 201, episode reward: 271.590, mean reward:  0.707 [-19.563, 100.000], mean action: 0.922 [0.000, 3.000],  loss: 9.418012, mae: 39.496949, mean_q: 51.312856, mean_eps: 0.100000\n",
      " 227124/1000000: episode: 630, duration: 1.784s, episode steps: 360, steps per second: 202, episode reward: 222.861, mean reward:  0.619 [-19.761, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 8.521973, mae: 37.085331, mean_q: 48.118671, mean_eps: 0.100000\n",
      " 227399/1000000: episode: 631, duration: 1.356s, episode steps: 275, steps per second: 203, episode reward: 251.014, mean reward:  0.913 [-19.592, 100.000], mean action: 1.738 [0.000, 3.000],  loss: 9.211246, mae: 39.506010, mean_q: 53.332930, mean_eps: 0.100000\n",
      " 227932/1000000: episode: 632, duration: 2.779s, episode steps: 533, steps per second: 192, episode reward: 240.099, mean reward:  0.450 [-22.302, 100.000], mean action: 1.045 [0.000, 3.000],  loss: 9.791052, mae: 38.398690, mean_q: 52.085241, mean_eps: 0.100000\n",
      " 228433/1000000: episode: 633, duration: 2.499s, episode steps: 501, steps per second: 200, episode reward: 260.824, mean reward:  0.521 [-17.276, 100.000], mean action: 1.108 [0.000, 3.000],  loss: 6.190680, mae: 41.403657, mean_q: 56.241926, mean_eps: 0.100000\n",
      " 228914/1000000: episode: 634, duration: 2.442s, episode steps: 481, steps per second: 197, episode reward: 261.656, mean reward:  0.544 [-18.630, 100.000], mean action: 1.682 [0.000, 3.000],  loss: 4.236808, mae: 37.708504, mean_q: 51.522154, mean_eps: 0.100000\n",
      " 229532/1000000: episode: 635, duration: 3.261s, episode steps: 618, steps per second: 190, episode reward: 233.967, mean reward:  0.379 [-17.393, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 3.490917, mae: 39.117673, mean_q: 53.338041, mean_eps: 0.100000\n",
      " 229781/1000000: episode: 636, duration: 1.209s, episode steps: 249, steps per second: 206, episode reward: 259.791, mean reward:  1.043 [-8.017, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 3.434956, mae: 42.670232, mean_q: 57.958479, mean_eps: 0.100000\n",
      " 230272/1000000: episode: 637, duration: 2.515s, episode steps: 491, steps per second: 195, episode reward: 246.187, mean reward:  0.501 [-20.058, 100.000], mean action: 1.305 [0.000, 3.000],  loss: 3.850981, mae: 45.305801, mean_q: 61.457716, mean_eps: 0.100000\n",
      " 230675/1000000: episode: 638, duration: 2.032s, episode steps: 403, steps per second: 198, episode reward: 285.138, mean reward:  0.708 [-19.153, 100.000], mean action: 1.268 [0.000, 3.000],  loss: 3.693719, mae: 48.540538, mean_q: 65.847947, mean_eps: 0.100000\n",
      " 231159/1000000: episode: 639, duration: 2.399s, episode steps: 484, steps per second: 202, episode reward: 288.029, mean reward:  0.595 [-17.763, 100.000], mean action: 1.217 [0.000, 3.000],  loss: 3.170231, mae: 47.739975, mean_q: 64.944720, mean_eps: 0.100000\n",
      " 231434/1000000: episode: 640, duration: 1.345s, episode steps: 275, steps per second: 204, episode reward: 246.247, mean reward:  0.895 [-3.009, 100.000], mean action: 0.924 [0.000, 3.000],  loss: 4.743069, mae: 48.604863, mean_q: 66.146182, mean_eps: 0.100000\n",
      " 231678/1000000: episode: 641, duration: 1.172s, episode steps: 244, steps per second: 208, episode reward: 268.443, mean reward:  1.100 [-9.905, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 4.494942, mae: 48.500606, mean_q: 65.861214, mean_eps: 0.100000\n",
      " 231851/1000000: episode: 642, duration: 0.830s, episode steps: 173, steps per second: 208, episode reward: 293.633, mean reward:  1.697 [-1.719, 100.000], mean action: 1.277 [0.000, 3.000],  loss: 3.640736, mae: 50.035155, mean_q: 67.915143, mean_eps: 0.100000\n",
      " 232064/1000000: episode: 643, duration: 1.034s, episode steps: 213, steps per second: 206, episode reward: 243.580, mean reward:  1.144 [-2.964, 100.000], mean action: 1.408 [0.000, 3.000],  loss: 6.065882, mae: 52.779153, mean_q: 71.488715, mean_eps: 0.100000\n",
      " 232245/1000000: episode: 644, duration: 0.877s, episode steps: 181, steps per second: 206, episode reward: 273.499, mean reward:  1.511 [-14.654, 100.000], mean action: 2.221 [0.000, 3.000],  loss: 7.803881, mae: 56.784019, mean_q: 76.938420, mean_eps: 0.100000\n",
      " 232454/1000000: episode: 645, duration: 1.009s, episode steps: 209, steps per second: 207, episode reward: 303.787, mean reward:  1.454 [-18.532, 100.000], mean action: 1.177 [0.000, 3.000],  loss: 4.719006, mae: 58.546960, mean_q: 79.522913, mean_eps: 0.100000\n",
      " 232858/1000000: episode: 646, duration: 2.029s, episode steps: 404, steps per second: 199, episode reward: 263.798, mean reward:  0.653 [-17.554, 100.000], mean action: 0.824 [0.000, 3.000],  loss: 5.661799, mae: 58.650903, mean_q: 79.720052, mean_eps: 0.100000\n",
      " 233205/1000000: episode: 647, duration: 1.734s, episode steps: 347, steps per second: 200, episode reward: 259.897, mean reward:  0.749 [-24.690, 100.000], mean action: 1.037 [0.000, 3.000],  loss: 4.255875, mae: 57.760000, mean_q: 78.479026, mean_eps: 0.100000\n",
      " 233416/1000000: episode: 648, duration: 1.032s, episode steps: 211, steps per second: 205, episode reward: 272.837, mean reward:  1.293 [-7.115, 100.000], mean action: 1.431 [0.000, 3.000],  loss: 2.729152, mae: 54.897331, mean_q: 74.330775, mean_eps: 0.100000\n",
      " 233722/1000000: episode: 649, duration: 1.503s, episode steps: 306, steps per second: 204, episode reward: 255.443, mean reward:  0.835 [-17.695, 100.000], mean action: 0.791 [0.000, 3.000],  loss: 3.655545, mae: 53.875523, mean_q: 72.741861, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 234009/1000000: episode: 650, duration: 1.387s, episode steps: 287, steps per second: 207, episode reward: 293.824, mean reward:  1.024 [-16.800, 100.000], mean action: 1.247 [0.000, 3.000],  loss: 4.629867, mae: 54.506188, mean_q: 73.555575, mean_eps: 0.100000\n",
      " 234248/1000000: episode: 651, duration: 1.151s, episode steps: 239, steps per second: 208, episode reward: 269.474, mean reward:  1.128 [-17.549, 100.000], mean action: 0.812 [0.000, 3.000],  loss: 5.040196, mae: 54.029905, mean_q: 73.041124, mean_eps: 0.100000\n",
      " 234339/1000000: episode: 652, duration: 0.431s, episode steps:  91, steps per second: 211, episode reward: -29.402, mean reward: -0.323 [-100.000, 10.848], mean action: 1.044 [0.000, 3.000],  loss: 5.274687, mae: 53.932091, mean_q: 72.999423, mean_eps: 0.100000\n",
      " 234657/1000000: episode: 653, duration: 1.564s, episode steps: 318, steps per second: 203, episode reward: 283.742, mean reward:  0.892 [-9.720, 100.000], mean action: 1.167 [0.000, 3.000],  loss: 7.178581, mae: 57.729180, mean_q: 78.123628, mean_eps: 0.100000\n",
      " 234774/1000000: episode: 654, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -24.766, mean reward: -0.212 [-100.000, 10.941], mean action: 1.709 [0.000, 3.000],  loss: 7.719735, mae: 56.770008, mean_q: 76.675485, mean_eps: 0.100000\n",
      " 234871/1000000: episode: 655, duration: 0.464s, episode steps:  97, steps per second: 209, episode reward:  6.231, mean reward:  0.064 [-100.000, 15.334], mean action: 1.309 [0.000, 3.000],  loss: 16.296674, mae: 56.913922, mean_q: 76.912642, mean_eps: 0.100000\n",
      " 234998/1000000: episode: 656, duration: 0.606s, episode steps: 127, steps per second: 210, episode reward: 29.733, mean reward:  0.234 [-100.000, 18.676], mean action: 1.205 [0.000, 3.000],  loss: 24.011182, mae: 59.793845, mean_q: 80.619067, mean_eps: 0.100000\n",
      " 235087/1000000: episode: 657, duration: 0.456s, episode steps:  89, steps per second: 195, episode reward:  3.475, mean reward:  0.039 [-100.000, 15.346], mean action: 1.225 [0.000, 3.000],  loss: 31.896004, mae: 63.799029, mean_q: 86.001951, mean_eps: 0.100000\n",
      " 235219/1000000: episode: 658, duration: 0.632s, episode steps: 132, steps per second: 209, episode reward: -0.788, mean reward: -0.006 [-100.000, 10.929], mean action: 1.667 [0.000, 3.000],  loss: 40.791628, mae: 66.554670, mean_q: 89.467645, mean_eps: 0.100000\n",
      " 235337/1000000: episode: 659, duration: 0.579s, episode steps: 118, steps per second: 204, episode reward: -141.232, mean reward: -1.197 [-100.000,  5.674], mean action: 1.415 [0.000, 3.000],  loss: 39.947177, mae: 69.143894, mean_q: 92.486607, mean_eps: 0.100000\n",
      " 235479/1000000: episode: 660, duration: 0.709s, episode steps: 142, steps per second: 200, episode reward: 23.615, mean reward:  0.166 [-100.000, 19.637], mean action: 1.570 [0.000, 3.000],  loss: 33.821127, mae: 67.942483, mean_q: 90.646880, mean_eps: 0.100000\n",
      " 235608/1000000: episode: 661, duration: 0.618s, episode steps: 129, steps per second: 209, episode reward: -12.404, mean reward: -0.096 [-100.000,  9.875], mean action: 1.767 [0.000, 3.000],  loss: 30.975227, mae: 68.553425, mean_q: 91.348803, mean_eps: 0.100000\n",
      " 235749/1000000: episode: 662, duration: 0.676s, episode steps: 141, steps per second: 209, episode reward:  4.709, mean reward:  0.033 [-100.000, 11.212], mean action: 1.390 [0.000, 3.000],  loss: 30.230377, mae: 70.783857, mean_q: 93.418453, mean_eps: 0.100000\n",
      " 235865/1000000: episode: 663, duration: 0.554s, episode steps: 116, steps per second: 209, episode reward: 37.196, mean reward:  0.321 [-100.000, 10.778], mean action: 1.388 [0.000, 3.000],  loss: 30.866826, mae: 71.201129, mean_q: 93.352810, mean_eps: 0.100000\n",
      " 236036/1000000: episode: 664, duration: 0.820s, episode steps: 171, steps per second: 209, episode reward: -17.805, mean reward: -0.104 [-100.000, 20.333], mean action: 1.637 [0.000, 3.000],  loss: 25.501745, mae: 69.576584, mean_q: 90.003403, mean_eps: 0.100000\n",
      " 236144/1000000: episode: 665, duration: 0.512s, episode steps: 108, steps per second: 211, episode reward: -25.874, mean reward: -0.240 [-100.000, 10.309], mean action: 1.157 [0.000, 3.000],  loss: 27.363663, mae: 68.828020, mean_q: 88.923810, mean_eps: 0.100000\n",
      " 236282/1000000: episode: 666, duration: 0.661s, episode steps: 138, steps per second: 209, episode reward:  5.000, mean reward:  0.036 [-100.000,  8.564], mean action: 1.442 [0.000, 3.000],  loss: 23.832910, mae: 69.718431, mean_q: 88.653824, mean_eps: 0.100000\n",
      " 236464/1000000: episode: 667, duration: 0.877s, episode steps: 182, steps per second: 207, episode reward: -29.129, mean reward: -0.160 [-100.000,  9.296], mean action: 1.473 [0.000, 3.000],  loss: 20.496274, mae: 68.185649, mean_q: 86.795921, mean_eps: 0.100000\n",
      " 237140/1000000: episode: 668, duration: 3.682s, episode steps: 676, steps per second: 184, episode reward: 241.205, mean reward:  0.357 [-17.595, 100.000], mean action: 0.957 [0.000, 3.000],  loss: 15.947280, mae: 61.073876, mean_q: 78.557572, mean_eps: 0.100000\n",
      " 237859/1000000: episode: 669, duration: 4.016s, episode steps: 719, steps per second: 179, episode reward: 184.898, mean reward:  0.257 [-10.654, 100.000], mean action: 1.606 [0.000, 3.000],  loss: 5.477054, mae: 41.830315, mean_q: 56.843837, mean_eps: 0.100000\n",
      " 238859/1000000: episode: 670, duration: 6.111s, episode steps: 1000, steps per second: 164, episode reward: 13.865, mean reward:  0.014 [-18.884, 12.101], mean action: 1.659 [0.000, 3.000],  loss: 2.969941, mae: 41.806996, mean_q: 56.372855, mean_eps: 0.100000\n",
      " 239859/1000000: episode: 671, duration: 6.139s, episode steps: 1000, steps per second: 163, episode reward: 12.022, mean reward:  0.012 [-11.121, 11.189], mean action: 1.758 [0.000, 3.000],  loss: 1.516856, mae: 37.103941, mean_q: 50.153752, mean_eps: 0.100000\n",
      " 240859/1000000: episode: 672, duration: 5.650s, episode steps: 1000, steps per second: 177, episode reward: 61.588, mean reward:  0.062 [-20.790, 24.884], mean action: 1.709 [0.000, 3.000],  loss: 1.561595, mae: 35.934788, mean_q: 48.469492, mean_eps: 0.100000\n",
      " 241859/1000000: episode: 673, duration: 5.486s, episode steps: 1000, steps per second: 182, episode reward: 151.222, mean reward:  0.151 [-19.756, 20.309], mean action: 1.922 [0.000, 3.000],  loss: 0.692948, mae: 30.318847, mean_q: 40.994639, mean_eps: 0.100000\n",
      " 242153/1000000: episode: 674, duration: 1.463s, episode steps: 294, steps per second: 201, episode reward: 256.935, mean reward:  0.874 [-2.732, 100.000], mean action: 1.156 [0.000, 3.000],  loss: 0.318681, mae: 23.045167, mean_q: 31.126170, mean_eps: 0.100000\n",
      " 242427/1000000: episode: 675, duration: 1.349s, episode steps: 274, steps per second: 203, episode reward: 252.002, mean reward:  0.920 [-12.124, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 2.685041, mae: 26.988299, mean_q: 36.557966, mean_eps: 0.100000\n",
      " 242957/1000000: episode: 676, duration: 2.674s, episode steps: 530, steps per second: 198, episode reward: 276.548, mean reward:  0.522 [-19.554, 100.000], mean action: 1.147 [0.000, 3.000],  loss: 8.001730, mae: 39.483100, mean_q: 53.500154, mean_eps: 0.100000\n",
      " 243390/1000000: episode: 677, duration: 2.246s, episode steps: 433, steps per second: 193, episode reward: 260.334, mean reward:  0.601 [-17.772, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 5.761450, mae: 39.854267, mean_q: 54.123033, mean_eps: 0.100000\n",
      " 243654/1000000: episode: 678, duration: 1.281s, episode steps: 264, steps per second: 206, episode reward: 243.271, mean reward:  0.921 [-13.518, 100.000], mean action: 1.277 [0.000, 3.000],  loss: 6.906619, mae: 35.034827, mean_q: 47.840495, mean_eps: 0.100000\n",
      " 243989/1000000: episode: 679, duration: 1.641s, episode steps: 335, steps per second: 204, episode reward: 291.130, mean reward:  0.869 [-10.582, 100.000], mean action: 1.570 [0.000, 3.000],  loss: 9.456539, mae: 39.899676, mean_q: 54.224455, mean_eps: 0.100000\n",
      " 244311/1000000: episode: 680, duration: 1.588s, episode steps: 322, steps per second: 203, episode reward: 254.518, mean reward:  0.790 [-10.712, 100.000], mean action: 1.317 [0.000, 3.000],  loss: 6.925419, mae: 43.638148, mean_q: 59.046372, mean_eps: 0.100000\n",
      " 244621/1000000: episode: 681, duration: 1.539s, episode steps: 310, steps per second: 201, episode reward: 241.579, mean reward:  0.779 [-11.697, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 8.574940, mae: 47.431475, mean_q: 63.914306, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 245000/1000000: episode: 682, duration: 1.876s, episode steps: 379, steps per second: 202, episode reward: 262.116, mean reward:  0.692 [-17.336, 100.000], mean action: 1.029 [0.000, 3.000],  loss: 7.523999, mae: 45.849415, mean_q: 61.968095, mean_eps: 0.100000\n",
      " 245417/1000000: episode: 683, duration: 2.087s, episode steps: 417, steps per second: 200, episode reward: 236.248, mean reward:  0.567 [-20.874, 100.000], mean action: 0.722 [0.000, 3.000],  loss: 6.979153, mae: 43.332104, mean_q: 58.839812, mean_eps: 0.100000\n",
      " 245823/1000000: episode: 684, duration: 2.099s, episode steps: 406, steps per second: 193, episode reward: 224.765, mean reward:  0.554 [-17.457, 100.000], mean action: 1.650 [0.000, 3.000],  loss: 6.441795, mae: 43.819154, mean_q: 59.327692, mean_eps: 0.100000\n",
      " 246461/1000000: episode: 685, duration: 3.273s, episode steps: 638, steps per second: 195, episode reward: 210.509, mean reward:  0.330 [-18.875, 100.000], mean action: 0.840 [0.000, 3.000],  loss: 5.318318, mae: 41.766462, mean_q: 56.513225, mean_eps: 0.100000\n",
      " 246869/1000000: episode: 686, duration: 2.062s, episode steps: 408, steps per second: 198, episode reward: 256.664, mean reward:  0.629 [-18.658, 100.000], mean action: 0.975 [0.000, 3.000],  loss: 3.458326, mae: 39.500285, mean_q: 53.800191, mean_eps: 0.100000\n",
      " 247245/1000000: episode: 687, duration: 1.881s, episode steps: 376, steps per second: 200, episode reward: 276.693, mean reward:  0.736 [-18.109, 100.000], mean action: 0.949 [0.000, 3.000],  loss: 3.637608, mae: 40.322223, mean_q: 55.042041, mean_eps: 0.100000\n",
      " 247817/1000000: episode: 688, duration: 2.874s, episode steps: 572, steps per second: 199, episode reward: 257.969, mean reward:  0.451 [-21.350, 100.000], mean action: 0.582 [0.000, 3.000],  loss: 5.940073, mae: 40.294362, mean_q: 54.936461, mean_eps: 0.100000\n",
      " 248015/1000000: episode: 689, duration: 0.950s, episode steps: 198, steps per second: 208, episode reward: 285.516, mean reward:  1.442 [-9.512, 100.000], mean action: 1.273 [0.000, 3.000],  loss: 5.198755, mae: 36.020527, mean_q: 49.146158, mean_eps: 0.100000\n",
      " 248277/1000000: episode: 690, duration: 1.277s, episode steps: 262, steps per second: 205, episode reward: 279.285, mean reward:  1.066 [-10.249, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 8.375458, mae: 37.960713, mean_q: 51.574006, mean_eps: 0.100000\n",
      " 248526/1000000: episode: 691, duration: 1.213s, episode steps: 249, steps per second: 205, episode reward: 234.720, mean reward:  0.943 [-9.519, 100.000], mean action: 1.153 [0.000, 3.000],  loss: 7.986212, mae: 40.799529, mean_q: 55.332689, mean_eps: 0.100000\n",
      " 248690/1000000: episode: 692, duration: 0.783s, episode steps: 164, steps per second: 209, episode reward: 35.547, mean reward:  0.217 [-100.000, 23.736], mean action: 1.829 [0.000, 3.000],  loss: 7.801161, mae: 46.339518, mean_q: 62.591977, mean_eps: 0.100000\n",
      " 248995/1000000: episode: 693, duration: 1.494s, episode steps: 305, steps per second: 204, episode reward: 292.256, mean reward:  0.958 [-9.890, 100.000], mean action: 0.987 [0.000, 3.000],  loss: 12.692307, mae: 50.679629, mean_q: 68.363846, mean_eps: 0.100000\n",
      " 249160/1000000: episode: 694, duration: 0.794s, episode steps: 165, steps per second: 208, episode reward: -12.570, mean reward: -0.076 [-100.000, 15.883], mean action: 1.758 [0.000, 3.000],  loss: 13.562436, mae: 46.967127, mean_q: 63.596235, mean_eps: 0.100000\n",
      " 249495/1000000: episode: 695, duration: 1.659s, episode steps: 335, steps per second: 202, episode reward: 277.393, mean reward:  0.828 [-17.614, 100.000], mean action: 0.922 [0.000, 3.000],  loss: 9.470917, mae: 44.033175, mean_q: 58.937422, mean_eps: 0.100000\n",
      " 249694/1000000: episode: 696, duration: 0.947s, episode steps: 199, steps per second: 210, episode reward: 260.747, mean reward:  1.310 [-10.796, 100.000], mean action: 1.040 [0.000, 3.000],  loss: 6.788574, mae: 44.705865, mean_q: 59.443366, mean_eps: 0.100000\n",
      " 250070/1000000: episode: 697, duration: 1.879s, episode steps: 376, steps per second: 200, episode reward: 307.620, mean reward:  0.818 [-17.936, 100.000], mean action: 0.798 [0.000, 3.000],  loss: 6.857875, mae: 46.405390, mean_q: 61.830027, mean_eps: 0.100000\n",
      " 250257/1000000: episode: 698, duration: 0.899s, episode steps: 187, steps per second: 208, episode reward: 264.020, mean reward:  1.412 [-9.549, 100.000], mean action: 1.182 [0.000, 3.000],  loss: 3.759890, mae: 51.715775, mean_q: 69.386108, mean_eps: 0.100000\n",
      " 250441/1000000: episode: 699, duration: 0.877s, episode steps: 184, steps per second: 210, episode reward: 263.997, mean reward:  1.435 [-9.999, 100.000], mean action: 1.397 [0.000, 3.000],  loss: 3.230545, mae: 53.532180, mean_q: 72.507453, mean_eps: 0.100000\n",
      " 251102/1000000: episode: 700, duration: 3.423s, episode steps: 661, steps per second: 193, episode reward: 277.365, mean reward:  0.420 [-19.172, 100.000], mean action: 1.517 [0.000, 3.000],  loss: 4.236971, mae: 55.432744, mean_q: 75.034578, mean_eps: 0.100000\n",
      " 251301/1000000: episode: 701, duration: 0.947s, episode steps: 199, steps per second: 210, episode reward: 230.781, mean reward:  1.160 [-9.346, 100.000], mean action: 1.035 [0.000, 3.000],  loss: 1.796344, mae: 54.186801, mean_q: 73.352706, mean_eps: 0.100000\n",
      " 251445/1000000: episode: 702, duration: 0.688s, episode steps: 144, steps per second: 209, episode reward: 23.999, mean reward:  0.167 [-100.000, 15.720], mean action: 1.410 [0.000, 3.000],  loss: 3.034045, mae: 54.620537, mean_q: 73.879550, mean_eps: 0.100000\n",
      " 251817/1000000: episode: 703, duration: 1.802s, episode steps: 372, steps per second: 206, episode reward: 260.417, mean reward:  0.700 [-10.636, 100.000], mean action: 0.753 [0.000, 3.000],  loss: 8.175137, mae: 55.526131, mean_q: 75.386763, mean_eps: 0.100000\n",
      " 251926/1000000: episode: 704, duration: 0.521s, episode steps: 109, steps per second: 209, episode reward: -44.182, mean reward: -0.405 [-100.000, 17.980], mean action: 1.404 [0.000, 3.000],  loss: 11.017065, mae: 56.648160, mean_q: 77.124653, mean_eps: 0.100000\n",
      " 252029/1000000: episode: 705, duration: 0.499s, episode steps: 103, steps per second: 207, episode reward: -0.385, mean reward: -0.004 [-100.000, 15.845], mean action: 1.621 [0.000, 3.000],  loss: 35.884117, mae: 60.510584, mean_q: 82.268704, mean_eps: 0.100000\n",
      " 252660/1000000: episode: 706, duration: 3.292s, episode steps: 631, steps per second: 192, episode reward: 206.802, mean reward:  0.328 [-20.447, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 31.612781, mae: 55.856926, mean_q: 76.449403, mean_eps: 0.100000\n",
      " 252979/1000000: episode: 707, duration: 1.547s, episode steps: 319, steps per second: 206, episode reward: 250.036, mean reward:  0.784 [-12.117, 100.000], mean action: 0.984 [0.000, 3.000],  loss: 23.291204, mae: 50.612411, mean_q: 69.103268, mean_eps: 0.100000\n",
      " 253211/1000000: episode: 708, duration: 1.121s, episode steps: 232, steps per second: 207, episode reward: 294.286, mean reward:  1.268 [-12.059, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 5.694968, mae: 48.136400, mean_q: 65.771464, mean_eps: 0.100000\n",
      " 253376/1000000: episode: 709, duration: 0.795s, episode steps: 165, steps per second: 208, episode reward:  4.773, mean reward:  0.029 [-100.000, 14.472], mean action: 1.752 [0.000, 3.000],  loss: 6.236770, mae: 49.422420, mean_q: 67.487027, mean_eps: 0.100000\n",
      " 254376/1000000: episode: 710, duration: 5.459s, episode steps: 1000, steps per second: 183, episode reward: 147.682, mean reward:  0.148 [-21.466, 22.877], mean action: 1.178 [0.000, 3.000],  loss: 12.425985, mae: 50.374848, mean_q: 68.095728, mean_eps: 0.100000\n",
      " 255022/1000000: episode: 711, duration: 3.252s, episode steps: 646, steps per second: 199, episode reward: 237.412, mean reward:  0.368 [-20.627, 100.000], mean action: 0.817 [0.000, 3.000],  loss: 1.414674, mae: 40.711854, mean_q: 55.083248, mean_eps: 0.100000\n",
      " 255250/1000000: episode: 712, duration: 1.092s, episode steps: 228, steps per second: 209, episode reward: 274.781, mean reward:  1.205 [-6.979, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 2.427385, mae: 43.264343, mean_q: 58.557707, mean_eps: 0.100000\n",
      " 255454/1000000: episode: 713, duration: 0.987s, episode steps: 204, steps per second: 207, episode reward: 293.449, mean reward:  1.438 [-17.754, 100.000], mean action: 1.284 [0.000, 3.000],  loss: 2.187031, mae: 47.679825, mean_q: 64.423111, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 255727/1000000: episode: 714, duration: 1.357s, episode steps: 273, steps per second: 201, episode reward: 266.084, mean reward:  0.975 [-17.439, 100.000], mean action: 1.505 [0.000, 3.000],  loss: 4.950209, mae: 47.680153, mean_q: 64.415278, mean_eps: 0.100000\n",
      " 256081/1000000: episode: 715, duration: 1.768s, episode steps: 354, steps per second: 200, episode reward: 278.386, mean reward:  0.786 [-10.648, 100.000], mean action: 0.944 [0.000, 3.000],  loss: 6.434489, mae: 52.974680, mean_q: 71.594033, mean_eps: 0.100000\n",
      " 256589/1000000: episode: 716, duration: 2.549s, episode steps: 508, steps per second: 199, episode reward: 275.057, mean reward:  0.541 [-18.722, 100.000], mean action: 1.201 [0.000, 3.000],  loss: 5.746656, mae: 51.449035, mean_q: 69.623002, mean_eps: 0.100000\n",
      " 256934/1000000: episode: 717, duration: 1.714s, episode steps: 345, steps per second: 201, episode reward: 290.441, mean reward:  0.842 [-10.540, 100.000], mean action: 0.986 [0.000, 3.000],  loss: 4.158002, mae: 48.559646, mean_q: 65.937161, mean_eps: 0.100000\n",
      " 257134/1000000: episode: 718, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: 284.129, mean reward:  1.421 [-8.669, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 4.282570, mae: 50.055290, mean_q: 67.938361, mean_eps: 0.100000\n",
      " 257396/1000000: episode: 719, duration: 1.242s, episode steps: 262, steps per second: 211, episode reward: 285.232, mean reward:  1.089 [-12.424, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 2.757182, mae: 50.577041, mean_q: 68.531169, mean_eps: 0.100000\n",
      " 257591/1000000: episode: 720, duration: 0.932s, episode steps: 195, steps per second: 209, episode reward: 264.988, mean reward:  1.359 [-2.792, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 5.446695, mae: 54.792694, mean_q: 74.107975, mean_eps: 0.100000\n",
      " 257941/1000000: episode: 721, duration: 1.715s, episode steps: 350, steps per second: 204, episode reward: 280.802, mean reward:  0.802 [-23.162, 100.000], mean action: 1.063 [0.000, 3.000],  loss: 4.570552, mae: 57.137181, mean_q: 77.149439, mean_eps: 0.100000\n",
      " 258121/1000000: episode: 722, duration: 0.868s, episode steps: 180, steps per second: 207, episode reward: 292.019, mean reward:  1.622 [-17.787, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 8.535098, mae: 56.132772, mean_q: 75.994712, mean_eps: 0.100000\n",
      " 258503/1000000: episode: 723, duration: 1.953s, episode steps: 382, steps per second: 196, episode reward: 246.212, mean reward:  0.645 [-19.603, 100.000], mean action: 0.908 [0.000, 3.000],  loss: 5.919138, mae: 56.721253, mean_q: 76.928086, mean_eps: 0.100000\n",
      " 258594/1000000: episode: 724, duration: 0.457s, episode steps:  91, steps per second: 199, episode reward: 19.759, mean reward:  0.217 [-100.000, 20.222], mean action: 1.769 [0.000, 3.000],  loss: 6.075581, mae: 54.560278, mean_q: 74.093778, mean_eps: 0.100000\n",
      " 259076/1000000: episode: 725, duration: 2.407s, episode steps: 482, steps per second: 200, episode reward: 292.201, mean reward:  0.606 [-17.623, 100.000], mean action: 0.942 [0.000, 3.000],  loss: 7.877320, mae: 56.316384, mean_q: 76.725226, mean_eps: 0.100000\n",
      " 259404/1000000: episode: 726, duration: 1.629s, episode steps: 328, steps per second: 201, episode reward: 262.746, mean reward:  0.801 [-13.329, 100.000], mean action: 1.293 [0.000, 3.000],  loss: 5.505436, mae: 53.144472, mean_q: 72.276879, mean_eps: 0.100000\n",
      " 259716/1000000: episode: 727, duration: 1.513s, episode steps: 312, steps per second: 206, episode reward: 288.410, mean reward:  0.924 [-17.328, 100.000], mean action: 0.997 [0.000, 3.000],  loss: 4.889480, mae: 53.789596, mean_q: 73.346207, mean_eps: 0.100000\n",
      " 259914/1000000: episode: 728, duration: 0.948s, episode steps: 198, steps per second: 209, episode reward:  3.281, mean reward:  0.017 [-100.000, 21.040], mean action: 1.258 [0.000, 3.000],  loss: 4.505134, mae: 50.984624, mean_q: 69.359816, mean_eps: 0.100000\n",
      " 259996/1000000: episode: 729, duration: 0.393s, episode steps:  82, steps per second: 209, episode reward: 53.780, mean reward:  0.656 [-100.000, 21.588], mean action: 1.451 [0.000, 3.000],  loss: 21.309630, mae: 56.995211, mean_q: 77.579355, mean_eps: 0.100000\n",
      " 260169/1000000: episode: 730, duration: 0.827s, episode steps: 173, steps per second: 209, episode reward: 46.150, mean reward:  0.267 [-100.000, 14.722], mean action: 1.301 [0.000, 3.000],  loss: 33.802470, mae: 59.481068, mean_q: 81.131353, mean_eps: 0.100000\n",
      " 260273/1000000: episode: 731, duration: 0.497s, episode steps: 104, steps per second: 209, episode reward:  3.122, mean reward:  0.030 [-100.000, 21.670], mean action: 1.317 [0.000, 3.000],  loss: 39.136586, mae: 61.076316, mean_q: 83.408538, mean_eps: 0.100000\n",
      " 260349/1000000: episode: 732, duration: 0.364s, episode steps:  76, steps per second: 209, episode reward: -21.727, mean reward: -0.286 [-100.000, 13.161], mean action: 1.579 [0.000, 3.000],  loss: 41.888101, mae: 64.624368, mean_q: 87.594889, mean_eps: 0.100000\n",
      " 260463/1000000: episode: 733, duration: 0.538s, episode steps: 114, steps per second: 212, episode reward: -4.569, mean reward: -0.040 [-100.000, 16.583], mean action: 0.974 [0.000, 3.000],  loss: 19.086477, mae: 67.549783, mean_q: 91.794471, mean_eps: 0.100000\n",
      " 260564/1000000: episode: 734, duration: 0.486s, episode steps: 101, steps per second: 208, episode reward: 20.728, mean reward:  0.205 [-100.000, 11.431], mean action: 1.554 [0.000, 3.000],  loss: 28.835005, mae: 67.370386, mean_q: 90.951277, mean_eps: 0.100000\n",
      " 260702/1000000: episode: 735, duration: 0.654s, episode steps: 138, steps per second: 211, episode reward: -102.444, mean reward: -0.742 [-100.000, 17.524], mean action: 1.370 [0.000, 3.000],  loss: 46.950050, mae: 69.603780, mean_q: 92.895576, mean_eps: 0.100000\n",
      " 260823/1000000: episode: 736, duration: 0.581s, episode steps: 121, steps per second: 208, episode reward: 28.211, mean reward:  0.233 [-100.000, 19.342], mean action: 1.405 [0.000, 3.000],  loss: 34.363138, mae: 73.679865, mean_q: 97.611700, mean_eps: 0.100000\n",
      " 260935/1000000: episode: 737, duration: 0.535s, episode steps: 112, steps per second: 209, episode reward: 30.685, mean reward:  0.274 [-100.000, 13.877], mean action: 1.375 [0.000, 3.000],  loss: 24.667852, mae: 74.845791, mean_q: 98.090831, mean_eps: 0.100000\n",
      " 261166/1000000: episode: 738, duration: 1.112s, episode steps: 231, steps per second: 208, episode reward: 284.495, mean reward:  1.232 [-4.808, 100.000], mean action: 1.130 [0.000, 3.000],  loss: 24.694744, mae: 74.452352, mean_q: 97.986245, mean_eps: 0.100000\n",
      " 261321/1000000: episode: 739, duration: 0.738s, episode steps: 155, steps per second: 210, episode reward: -13.133, mean reward: -0.085 [-100.000, 18.213], mean action: 1.684 [0.000, 3.000],  loss: 13.934757, mae: 70.019501, mean_q: 92.112815, mean_eps: 0.100000\n",
      " 261539/1000000: episode: 740, duration: 1.038s, episode steps: 218, steps per second: 210, episode reward: 35.774, mean reward:  0.164 [-100.000, 14.551], mean action: 1.436 [0.000, 3.000],  loss: 10.100726, mae: 66.548238, mean_q: 87.170391, mean_eps: 0.100000\n",
      " 261667/1000000: episode: 741, duration: 0.618s, episode steps: 128, steps per second: 207, episode reward: 72.250, mean reward:  0.564 [-100.000, 16.088], mean action: 1.922 [0.000, 3.000],  loss: 11.334680, mae: 66.012350, mean_q: 86.062585, mean_eps: 0.100000\n",
      " 261777/1000000: episode: 742, duration: 0.531s, episode steps: 110, steps per second: 207, episode reward:  9.790, mean reward:  0.089 [-100.000, 15.229], mean action: 1.945 [0.000, 3.000],  loss: 29.383756, mae: 64.830692, mean_q: 85.125994, mean_eps: 0.100000\n",
      " 261980/1000000: episode: 743, duration: 0.983s, episode steps: 203, steps per second: 206, episode reward:  4.556, mean reward:  0.022 [-100.000,  8.197], mean action: 1.793 [0.000, 3.000],  loss: 18.376854, mae: 63.344319, mean_q: 84.037892, mean_eps: 0.100000\n",
      " 262385/1000000: episode: 744, duration: 2.074s, episode steps: 405, steps per second: 195, episode reward: 266.714, mean reward:  0.659 [-9.087, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 15.692763, mae: 56.129606, mean_q: 73.521586, mean_eps: 0.100000\n",
      " 262515/1000000: episode: 745, duration: 0.627s, episode steps: 130, steps per second: 207, episode reward: -11.310, mean reward: -0.087 [-100.000, 15.450], mean action: 1.600 [0.000, 3.000],  loss: 18.511934, mae: 51.054886, mean_q: 67.464121, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 262923/1000000: episode: 746, duration: 2.018s, episode steps: 408, steps per second: 202, episode reward: 276.762, mean reward:  0.678 [-18.804, 100.000], mean action: 1.201 [0.000, 3.000],  loss: 8.388819, mae: 46.599067, mean_q: 60.919376, mean_eps: 0.100000\n",
      " 263079/1000000: episode: 747, duration: 0.750s, episode steps: 156, steps per second: 208, episode reward: -13.871, mean reward: -0.089 [-100.000, 14.027], mean action: 1.885 [0.000, 3.000],  loss: 5.613089, mae: 42.593491, mean_q: 57.059184, mean_eps: 0.100000\n",
      " 263547/1000000: episode: 748, duration: 2.303s, episode steps: 468, steps per second: 203, episode reward: 294.561, mean reward:  0.629 [-17.969, 100.000], mean action: 0.596 [0.000, 3.000],  loss: 8.325245, mae: 45.919758, mean_q: 60.681642, mean_eps: 0.100000\n",
      " 264120/1000000: episode: 749, duration: 2.894s, episode steps: 573, steps per second: 198, episode reward: 297.370, mean reward:  0.519 [-17.515, 100.000], mean action: 0.887 [0.000, 3.000],  loss: 5.737151, mae: 43.201688, mean_q: 58.165023, mean_eps: 0.100000\n",
      " 264349/1000000: episode: 750, duration: 1.109s, episode steps: 229, steps per second: 206, episode reward: 259.916, mean reward:  1.135 [-3.087, 100.000], mean action: 1.262 [0.000, 3.000],  loss: 5.663009, mae: 39.391163, mean_q: 53.770168, mean_eps: 0.100000\n",
      " 264801/1000000: episode: 751, duration: 2.314s, episode steps: 452, steps per second: 195, episode reward: 239.177, mean reward:  0.529 [-17.498, 100.000], mean action: 1.310 [0.000, 3.000],  loss: 7.087217, mae: 44.606386, mean_q: 60.690748, mean_eps: 0.100000\n",
      " 265373/1000000: episode: 752, duration: 3.045s, episode steps: 572, steps per second: 188, episode reward: 232.515, mean reward:  0.406 [-20.219, 100.000], mean action: 1.309 [0.000, 3.000],  loss: 3.991018, mae: 45.984653, mean_q: 62.616360, mean_eps: 0.100000\n",
      " 265696/1000000: episode: 753, duration: 1.616s, episode steps: 323, steps per second: 200, episode reward: 265.107, mean reward:  0.821 [-18.256, 100.000], mean action: 0.969 [0.000, 3.000],  loss: 3.932518, mae: 43.078294, mean_q: 58.862176, mean_eps: 0.100000\n",
      " 265992/1000000: episode: 754, duration: 1.472s, episode steps: 296, steps per second: 201, episode reward: 256.277, mean reward:  0.866 [-10.078, 100.000], mean action: 1.520 [0.000, 3.000],  loss: 3.429444, mae: 44.828816, mean_q: 61.056348, mean_eps: 0.100000\n",
      " 266317/1000000: episode: 755, duration: 1.629s, episode steps: 325, steps per second: 199, episode reward: 261.003, mean reward:  0.803 [-2.536, 100.000], mean action: 1.006 [0.000, 3.000],  loss: 5.493550, mae: 48.174185, mean_q: 65.191559, mean_eps: 0.100000\n",
      " 266664/1000000: episode: 756, duration: 1.689s, episode steps: 347, steps per second: 205, episode reward: 292.179, mean reward:  0.842 [-18.987, 100.000], mean action: 1.231 [0.000, 3.000],  loss: 4.309432, mae: 49.957111, mean_q: 67.563259, mean_eps: 0.100000\n",
      " 267019/1000000: episode: 757, duration: 1.819s, episode steps: 355, steps per second: 195, episode reward: 220.823, mean reward:  0.622 [-17.569, 100.000], mean action: 1.451 [0.000, 3.000],  loss: 5.240181, mae: 48.876649, mean_q: 66.311768, mean_eps: 0.100000\n",
      " 267302/1000000: episode: 758, duration: 1.400s, episode steps: 283, steps per second: 202, episode reward: 251.459, mean reward:  0.889 [-18.424, 100.000], mean action: 1.406 [0.000, 3.000],  loss: 5.745478, mae: 47.526054, mean_q: 64.732203, mean_eps: 0.100000\n",
      " 267534/1000000: episode: 759, duration: 1.131s, episode steps: 232, steps per second: 205, episode reward: 260.007, mean reward:  1.121 [-12.061, 100.000], mean action: 1.302 [0.000, 3.000],  loss: 4.935970, mae: 48.347507, mean_q: 65.814270, mean_eps: 0.100000\n",
      " 267715/1000000: episode: 760, duration: 0.868s, episode steps: 181, steps per second: 209, episode reward: 243.659, mean reward:  1.346 [-9.585, 100.000], mean action: 1.309 [0.000, 3.000],  loss: 8.041684, mae: 48.956692, mean_q: 66.403746, mean_eps: 0.100000\n",
      " 267961/1000000: episode: 761, duration: 1.215s, episode steps: 246, steps per second: 202, episode reward: 264.802, mean reward:  1.076 [-7.522, 100.000], mean action: 1.407 [0.000, 3.000],  loss: 6.545476, mae: 50.784424, mean_q: 68.903025, mean_eps: 0.100000\n",
      " 268263/1000000: episode: 762, duration: 1.479s, episode steps: 302, steps per second: 204, episode reward: 292.157, mean reward:  0.967 [-9.491, 100.000], mean action: 1.520 [0.000, 3.000],  loss: 5.713577, mae: 52.709795, mean_q: 71.373379, mean_eps: 0.100000\n",
      " 268520/1000000: episode: 763, duration: 1.248s, episode steps: 257, steps per second: 206, episode reward: 271.073, mean reward:  1.055 [-2.677, 100.000], mean action: 1.366 [0.000, 3.000],  loss: 4.597409, mae: 52.808694, mean_q: 71.365675, mean_eps: 0.100000\n",
      " 268703/1000000: episode: 764, duration: 0.875s, episode steps: 183, steps per second: 209, episode reward: 287.038, mean reward:  1.569 [-12.971, 100.000], mean action: 1.202 [0.000, 3.000],  loss: 6.561095, mae: 52.742939, mean_q: 71.232663, mean_eps: 0.100000\n",
      " 268908/1000000: episode: 765, duration: 0.999s, episode steps: 205, steps per second: 205, episode reward: 305.377, mean reward:  1.490 [-10.593, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 6.380091, mae: 52.130824, mean_q: 70.598874, mean_eps: 0.100000\n",
      " 269259/1000000: episode: 766, duration: 1.717s, episode steps: 351, steps per second: 204, episode reward: 224.833, mean reward:  0.641 [-21.197, 100.000], mean action: 1.274 [0.000, 3.000],  loss: 5.836494, mae: 51.779256, mean_q: 69.769927, mean_eps: 0.100000\n",
      " 270259/1000000: episode: 767, duration: 5.152s, episode steps: 1000, steps per second: 194, episode reward: 173.663, mean reward:  0.174 [-19.382, 80.389], mean action: 0.999 [0.000, 3.000],  loss: 12.525402, mae: 46.282393, mean_q: 62.037844, mean_eps: 0.100000\n",
      " 270647/1000000: episode: 768, duration: 1.934s, episode steps: 388, steps per second: 201, episode reward: 285.938, mean reward:  0.737 [-10.551, 100.000], mean action: 1.157 [0.000, 3.000],  loss: 2.552593, mae: 44.767765, mean_q: 60.988718, mean_eps: 0.100000\n",
      " 270795/1000000: episode: 769, duration: 0.714s, episode steps: 148, steps per second: 207, episode reward: 29.250, mean reward:  0.198 [-100.000, 15.063], mean action: 1.824 [0.000, 3.000],  loss: 3.274253, mae: 45.569731, mean_q: 61.806957, mean_eps: 0.100000\n",
      " 271382/1000000: episode: 770, duration: 3.039s, episode steps: 587, steps per second: 193, episode reward: 191.291, mean reward:  0.326 [-19.946, 100.000], mean action: 1.102 [0.000, 3.000],  loss: 14.065696, mae: 47.235248, mean_q: 64.338204, mean_eps: 0.100000\n",
      " 271709/1000000: episode: 771, duration: 1.632s, episode steps: 327, steps per second: 200, episode reward: 282.791, mean reward:  0.865 [-17.352, 100.000], mean action: 1.440 [0.000, 3.000],  loss: 12.097309, mae: 39.942801, mean_q: 55.002449, mean_eps: 0.100000\n",
      " 271829/1000000: episode: 772, duration: 0.588s, episode steps: 120, steps per second: 204, episode reward:  3.487, mean reward:  0.029 [-100.000, 12.129], mean action: 1.925 [0.000, 3.000],  loss: 5.374290, mae: 41.563345, mean_q: 56.985640, mean_eps: 0.100000\n",
      " 272155/1000000: episode: 773, duration: 1.629s, episode steps: 326, steps per second: 200, episode reward: 304.960, mean reward:  0.935 [-18.252, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 16.821445, mae: 42.704742, mean_q: 58.476497, mean_eps: 0.100000\n",
      " 272378/1000000: episode: 774, duration: 1.069s, episode steps: 223, steps per second: 209, episode reward: 292.297, mean reward:  1.311 [-9.231, 100.000], mean action: 1.318 [0.000, 3.000],  loss: 18.591676, mae: 50.821974, mean_q: 69.460984, mean_eps: 0.100000\n",
      " 272626/1000000: episode: 775, duration: 1.191s, episode steps: 248, steps per second: 208, episode reward: 273.928, mean reward:  1.105 [-8.465, 100.000], mean action: 1.065 [0.000, 3.000],  loss: 10.515431, mae: 54.735681, mean_q: 75.020051, mean_eps: 0.100000\n",
      " 273021/1000000: episode: 776, duration: 1.933s, episode steps: 395, steps per second: 204, episode reward: 238.926, mean reward:  0.605 [-19.613, 100.000], mean action: 0.878 [0.000, 3.000],  loss: 6.880484, mae: 51.528545, mean_q: 70.694531, mean_eps: 0.100000\n",
      " 273262/1000000: episode: 777, duration: 1.180s, episode steps: 241, steps per second: 204, episode reward: 250.735, mean reward:  1.040 [-10.786, 100.000], mean action: 1.216 [0.000, 3.000],  loss: 6.093236, mae: 48.349396, mean_q: 66.158874, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 273464/1000000: episode: 778, duration: 0.969s, episode steps: 202, steps per second: 209, episode reward: 279.100, mean reward:  1.382 [-19.044, 100.000], mean action: 1.104 [0.000, 3.000],  loss: 5.882971, mae: 50.465661, mean_q: 69.001142, mean_eps: 0.100000\n",
      " 273573/1000000: episode: 779, duration: 0.525s, episode steps: 109, steps per second: 208, episode reward:  8.082, mean reward:  0.074 [-100.000, 10.175], mean action: 1.972 [0.000, 3.000],  loss: 5.785688, mae: 51.173633, mean_q: 69.992652, mean_eps: 0.100000\n",
      " 273788/1000000: episode: 780, duration: 1.028s, episode steps: 215, steps per second: 209, episode reward: 261.427, mean reward:  1.216 [-3.022, 100.000], mean action: 1.326 [0.000, 3.000],  loss: 13.599524, mae: 52.991793, mean_q: 72.337347, mean_eps: 0.100000\n",
      " 274104/1000000: episode: 781, duration: 1.541s, episode steps: 316, steps per second: 205, episode reward: 282.903, mean reward:  0.895 [-9.593, 100.000], mean action: 1.241 [0.000, 3.000],  loss: 9.812052, mae: 58.904615, mean_q: 79.653575, mean_eps: 0.100000\n",
      " 274335/1000000: episode: 782, duration: 1.114s, episode steps: 231, steps per second: 207, episode reward: 247.623, mean reward:  1.072 [-12.358, 100.000], mean action: 1.225 [0.000, 3.000],  loss: 12.116048, mae: 61.014398, mean_q: 82.377340, mean_eps: 0.100000\n",
      " 274510/1000000: episode: 783, duration: 0.841s, episode steps: 175, steps per second: 208, episode reward: 296.972, mean reward:  1.697 [-17.640, 100.000], mean action: 1.069 [0.000, 3.000],  loss: 12.407742, mae: 61.205799, mean_q: 82.481073, mean_eps: 0.100000\n",
      " 274874/1000000: episode: 784, duration: 1.827s, episode steps: 364, steps per second: 199, episode reward: 256.761, mean reward:  0.705 [-9.486, 100.000], mean action: 1.146 [0.000, 3.000],  loss: 5.108201, mae: 59.637673, mean_q: 80.639761, mean_eps: 0.100000\n",
      " 275297/1000000: episode: 785, duration: 2.065s, episode steps: 423, steps per second: 205, episode reward: 287.699, mean reward:  0.680 [-21.218, 100.000], mean action: 1.019 [0.000, 3.000],  loss: 3.421388, mae: 58.620975, mean_q: 79.492736, mean_eps: 0.100000\n",
      " 275409/1000000: episode: 786, duration: 0.532s, episode steps: 112, steps per second: 210, episode reward: -9.898, mean reward: -0.088 [-100.000, 11.793], mean action: 1.446 [0.000, 3.000],  loss: 2.585975, mae: 57.103656, mean_q: 77.540266, mean_eps: 0.100000\n",
      " 275526/1000000: episode: 787, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: 20.365, mean reward:  0.174 [-100.000, 20.974], mean action: 1.521 [0.000, 3.000],  loss: 8.550504, mae: 58.302243, mean_q: 78.940826, mean_eps: 0.100000\n",
      " 275652/1000000: episode: 788, duration: 0.604s, episode steps: 126, steps per second: 209, episode reward: -6.518, mean reward: -0.052 [-100.000, 18.983], mean action: 1.405 [0.000, 3.000],  loss: 12.718055, mae: 59.897217, mean_q: 80.928238, mean_eps: 0.100000\n",
      " 275763/1000000: episode: 789, duration: 0.528s, episode steps: 111, steps per second: 210, episode reward: -48.445, mean reward: -0.436 [-100.000, 11.550], mean action: 1.577 [0.000, 3.000],  loss: 9.624635, mae: 61.155110, mean_q: 82.484867, mean_eps: 0.100000\n",
      " 275851/1000000: episode: 790, duration: 0.419s, episode steps:  88, steps per second: 210, episode reward: 23.719, mean reward:  0.270 [-100.000, 19.254], mean action: 1.648 [0.000, 3.000],  loss: 17.185985, mae: 64.781932, mean_q: 86.438599, mean_eps: 0.100000\n",
      " 275969/1000000: episode: 791, duration: 0.563s, episode steps: 118, steps per second: 210, episode reward:  7.155, mean reward:  0.061 [-100.000, 18.079], mean action: 1.492 [0.000, 3.000],  loss: 12.877207, mae: 65.990626, mean_q: 87.333685, mean_eps: 0.100000\n",
      " 276319/1000000: episode: 792, duration: 1.732s, episode steps: 350, steps per second: 202, episode reward: 257.670, mean reward:  0.736 [-10.485, 100.000], mean action: 1.443 [0.000, 3.000],  loss: 9.838544, mae: 65.632864, mean_q: 84.821622, mean_eps: 0.100000\n",
      " 276409/1000000: episode: 793, duration: 0.435s, episode steps:  90, steps per second: 207, episode reward: 25.161, mean reward:  0.280 [-100.000, 16.770], mean action: 1.878 [0.000, 3.000],  loss: 7.695412, mae: 64.574000, mean_q: 82.611506, mean_eps: 0.100000\n",
      " 276517/1000000: episode: 794, duration: 0.516s, episode steps: 108, steps per second: 209, episode reward: -82.388, mean reward: -0.763 [-100.000, 12.892], mean action: 1.556 [0.000, 3.000],  loss: 8.888717, mae: 63.444756, mean_q: 81.243510, mean_eps: 0.100000\n",
      " 276651/1000000: episode: 795, duration: 0.643s, episode steps: 134, steps per second: 208, episode reward: 55.435, mean reward:  0.414 [-100.000, 12.388], mean action: 1.761 [0.000, 3.000],  loss: 7.869018, mae: 62.960160, mean_q: 80.548318, mean_eps: 0.100000\n",
      " 277202/1000000: episode: 796, duration: 2.730s, episode steps: 551, steps per second: 202, episode reward: 275.551, mean reward:  0.500 [-17.907, 100.000], mean action: 0.512 [0.000, 3.000],  loss: 19.079813, mae: 57.270447, mean_q: 74.203991, mean_eps: 0.100000\n",
      " 277627/1000000: episode: 797, duration: 2.111s, episode steps: 425, steps per second: 201, episode reward: 257.571, mean reward:  0.606 [-19.013, 100.000], mean action: 1.191 [0.000, 3.000],  loss: 11.897120, mae: 53.378750, mean_q: 70.929781, mean_eps: 0.100000\n",
      " 278056/1000000: episode: 798, duration: 2.145s, episode steps: 429, steps per second: 200, episode reward: 295.461, mean reward:  0.689 [-18.906, 100.000], mean action: 1.210 [0.000, 3.000],  loss: 3.071192, mae: 53.375950, mean_q: 71.987424, mean_eps: 0.100000\n",
      " 278247/1000000: episode: 799, duration: 0.913s, episode steps: 191, steps per second: 209, episode reward: 313.776, mean reward:  1.643 [-12.358, 100.000], mean action: 1.319 [0.000, 3.000],  loss: 4.234959, mae: 57.671486, mean_q: 77.764274, mean_eps: 0.100000\n",
      " 278578/1000000: episode: 800, duration: 1.652s, episode steps: 331, steps per second: 200, episode reward: 272.107, mean reward:  0.822 [-13.311, 100.000], mean action: 1.317 [0.000, 3.000],  loss: 3.744219, mae: 57.895512, mean_q: 78.268394, mean_eps: 0.100000\n",
      " 278866/1000000: episode: 801, duration: 1.407s, episode steps: 288, steps per second: 205, episode reward: 284.731, mean reward:  0.989 [-9.520, 100.000], mean action: 1.642 [0.000, 3.000],  loss: 3.766526, mae: 58.403941, mean_q: 78.921593, mean_eps: 0.100000\n",
      " 279108/1000000: episode: 802, duration: 1.187s, episode steps: 242, steps per second: 204, episode reward: 287.678, mean reward:  1.189 [-6.199, 100.000], mean action: 1.603 [0.000, 3.000],  loss: 4.855005, mae: 58.132367, mean_q: 78.617572, mean_eps: 0.100000\n",
      " 279491/1000000: episode: 803, duration: 1.902s, episode steps: 383, steps per second: 201, episode reward: 269.023, mean reward:  0.702 [-19.562, 100.000], mean action: 0.966 [0.000, 3.000],  loss: 3.167908, mae: 58.827676, mean_q: 79.483633, mean_eps: 0.100000\n",
      " 279899/1000000: episode: 804, duration: 2.069s, episode steps: 408, steps per second: 197, episode reward: 240.685, mean reward:  0.590 [-18.155, 100.000], mean action: 0.865 [0.000, 3.000],  loss: 3.153765, mae: 57.132658, mean_q: 77.208157, mean_eps: 0.100000\n",
      " 280392/1000000: episode: 805, duration: 2.519s, episode steps: 493, steps per second: 196, episode reward: 273.016, mean reward:  0.554 [-23.773, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 3.331732, mae: 52.932458, mean_q: 71.681039, mean_eps: 0.100000\n",
      " 280665/1000000: episode: 806, duration: 1.340s, episode steps: 273, steps per second: 204, episode reward: 241.678, mean reward:  0.885 [-19.913, 100.000], mean action: 1.407 [0.000, 3.000],  loss: 2.838443, mae: 52.475887, mean_q: 71.015792, mean_eps: 0.100000\n",
      " 280995/1000000: episode: 807, duration: 1.608s, episode steps: 330, steps per second: 205, episode reward: 285.150, mean reward:  0.864 [-17.431, 100.000], mean action: 1.036 [0.000, 3.000],  loss: 3.668062, mae: 52.492786, mean_q: 71.151835, mean_eps: 0.100000\n",
      " 281995/1000000: episode: 808, duration: 5.210s, episode steps: 1000, steps per second: 192, episode reward: 170.206, mean reward:  0.170 [-21.063, 23.186], mean action: 0.858 [0.000, 3.000],  loss: 3.002852, mae: 49.280204, mean_q: 66.646854, mean_eps: 0.100000\n",
      " 282372/1000000: episode: 809, duration: 1.879s, episode steps: 377, steps per second: 201, episode reward: 264.460, mean reward:  0.701 [-19.068, 100.000], mean action: 1.021 [0.000, 3.000],  loss: 2.388480, mae: 45.180962, mean_q: 60.967695, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 283372/1000000: episode: 810, duration: 5.376s, episode steps: 1000, steps per second: 186, episode reward: 148.359, mean reward:  0.148 [-19.818, 23.554], mean action: 1.554 [0.000, 3.000],  loss: 2.314772, mae: 47.854448, mean_q: 64.813772, mean_eps: 0.100000\n",
      " 284372/1000000: episode: 811, duration: 5.222s, episode steps: 1000, steps per second: 191, episode reward: 175.501, mean reward:  0.176 [-20.259, 22.368], mean action: 1.409 [0.000, 3.000],  loss: 1.594596, mae: 43.157363, mean_q: 58.323021, mean_eps: 0.100000\n",
      " 284466/1000000: episode: 812, duration: 0.457s, episode steps:  94, steps per second: 206, episode reward: 41.130, mean reward:  0.438 [-100.000, 19.769], mean action: 1.894 [0.000, 3.000],  loss: 2.405283, mae: 43.945624, mean_q: 59.127850, mean_eps: 0.100000\n",
      " 284621/1000000: episode: 813, duration: 0.741s, episode steps: 155, steps per second: 209, episode reward: 281.792, mean reward:  1.818 [-11.017, 100.000], mean action: 1.516 [0.000, 3.000],  loss: 7.194188, mae: 44.399900, mean_q: 59.854904, mean_eps: 0.100000\n",
      " 284688/1000000: episode: 814, duration: 0.317s, episode steps:  67, steps per second: 211, episode reward: -55.285, mean reward: -0.825 [-100.000, 10.540], mean action: 0.791 [0.000, 3.000],  loss: 13.013152, mae: 45.444508, mean_q: 61.246189, mean_eps: 0.100000\n",
      " 284766/1000000: episode: 815, duration: 0.368s, episode steps:  78, steps per second: 212, episode reward: -25.173, mean reward: -0.323 [-100.000, 19.142], mean action: 0.859 [0.000, 3.000],  loss: 14.446034, mae: 47.674328, mean_q: 64.311479, mean_eps: 0.100000\n",
      " 284871/1000000: episode: 816, duration: 0.500s, episode steps: 105, steps per second: 210, episode reward: -5.724, mean reward: -0.055 [-100.000, 18.161], mean action: 1.305 [0.000, 3.000],  loss: 11.424508, mae: 51.270778, mean_q: 68.899542, mean_eps: 0.100000\n",
      " 285036/1000000: episode: 817, duration: 0.791s, episode steps: 165, steps per second: 209, episode reward: 13.669, mean reward:  0.083 [-100.000, 17.938], mean action: 1.745 [0.000, 3.000],  loss: 9.598177, mae: 56.978225, mean_q: 75.375539, mean_eps: 0.100000\n",
      " 285308/1000000: episode: 818, duration: 1.326s, episode steps: 272, steps per second: 205, episode reward: 291.050, mean reward:  1.070 [-18.128, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 8.612259, mae: 64.258782, mean_q: 83.759106, mean_eps: 0.100000\n",
      " 285706/1000000: episode: 819, duration: 1.959s, episode steps: 398, steps per second: 203, episode reward: 300.092, mean reward:  0.754 [-17.934, 100.000], mean action: 1.344 [0.000, 3.000],  loss: 7.342006, mae: 65.255494, mean_q: 85.670719, mean_eps: 0.100000\n",
      " 285907/1000000: episode: 820, duration: 0.975s, episode steps: 201, steps per second: 206, episode reward: 253.367, mean reward:  1.261 [-12.165, 100.000], mean action: 1.244 [0.000, 3.000],  loss: 5.676519, mae: 61.874918, mean_q: 82.553296, mean_eps: 0.100000\n",
      " 286158/1000000: episode: 821, duration: 1.216s, episode steps: 251, steps per second: 206, episode reward: 236.806, mean reward:  0.943 [-8.503, 100.000], mean action: 1.195 [0.000, 3.000],  loss: 5.732405, mae: 59.686076, mean_q: 81.072502, mean_eps: 0.100000\n",
      " 286370/1000000: episode: 822, duration: 1.020s, episode steps: 212, steps per second: 208, episode reward: 266.493, mean reward:  1.257 [-9.345, 100.000], mean action: 1.151 [0.000, 3.000],  loss: 5.130163, mae: 58.675323, mean_q: 80.045120, mean_eps: 0.100000\n",
      " 286996/1000000: episode: 823, duration: 3.169s, episode steps: 626, steps per second: 198, episode reward: 310.195, mean reward:  0.496 [-18.999, 100.000], mean action: 0.919 [0.000, 3.000],  loss: 4.349494, mae: 57.290129, mean_q: 77.455584, mean_eps: 0.100000\n",
      " 287231/1000000: episode: 824, duration: 1.135s, episode steps: 235, steps per second: 207, episode reward: 291.092, mean reward:  1.239 [-11.970, 100.000], mean action: 1.294 [0.000, 3.000],  loss: 4.293454, mae: 54.055103, mean_q: 73.141166, mean_eps: 0.100000\n",
      " 287333/1000000: episode: 825, duration: 0.485s, episode steps: 102, steps per second: 210, episode reward: -46.611, mean reward: -0.457 [-100.000, 10.455], mean action: 1.490 [0.000, 3.000],  loss: 4.528679, mae: 54.066297, mean_q: 73.381300, mean_eps: 0.100000\n",
      " 287586/1000000: episode: 826, duration: 1.231s, episode steps: 253, steps per second: 206, episode reward: 304.471, mean reward:  1.203 [-8.898, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 4.946214, mae: 55.918702, mean_q: 75.697820, mean_eps: 0.100000\n",
      " 287713/1000000: episode: 827, duration: 0.612s, episode steps: 127, steps per second: 207, episode reward: 14.011, mean reward:  0.110 [-100.000, 19.978], mean action: 1.764 [0.000, 3.000],  loss: 6.493430, mae: 58.704133, mean_q: 79.079207, mean_eps: 0.100000\n",
      " 287809/1000000: episode: 828, duration: 0.464s, episode steps:  96, steps per second: 207, episode reward: -84.072, mean reward: -0.876 [-100.000, 45.427], mean action: 1.729 [0.000, 3.000],  loss: 8.852730, mae: 61.747935, mean_q: 82.682224, mean_eps: 0.100000\n",
      " 288090/1000000: episode: 829, duration: 1.381s, episode steps: 281, steps per second: 203, episode reward: 223.894, mean reward:  0.797 [-18.298, 100.000], mean action: 1.520 [0.000, 3.000],  loss: 12.567515, mae: 65.913559, mean_q: 88.207907, mean_eps: 0.100000\n",
      " 288423/1000000: episode: 830, duration: 1.650s, episode steps: 333, steps per second: 202, episode reward: 278.954, mean reward:  0.838 [-20.440, 100.000], mean action: 1.024 [0.000, 3.000],  loss: 10.749218, mae: 64.099246, mean_q: 85.686551, mean_eps: 0.100000\n",
      " 288655/1000000: episode: 831, duration: 1.122s, episode steps: 232, steps per second: 207, episode reward: 278.360, mean reward:  1.200 [-9.959, 100.000], mean action: 1.246 [0.000, 3.000],  loss: 8.389196, mae: 61.646380, mean_q: 82.258190, mean_eps: 0.100000\n",
      " 288935/1000000: episode: 832, duration: 1.371s, episode steps: 280, steps per second: 204, episode reward: 274.629, mean reward:  0.981 [-11.458, 100.000], mean action: 1.107 [0.000, 3.000],  loss: 5.668977, mae: 63.550145, mean_q: 85.291155, mean_eps: 0.100000\n",
      " 289231/1000000: episode: 833, duration: 1.468s, episode steps: 296, steps per second: 202, episode reward: 269.994, mean reward:  0.912 [-8.614, 100.000], mean action: 1.841 [0.000, 3.000],  loss: 3.973368, mae: 65.105865, mean_q: 88.396258, mean_eps: 0.100000\n",
      " 289433/1000000: episode: 834, duration: 0.993s, episode steps: 202, steps per second: 203, episode reward: 192.970, mean reward:  0.955 [-14.873, 100.000], mean action: 1.589 [0.000, 3.000],  loss: 6.218195, mae: 64.633413, mean_q: 87.820405, mean_eps: 0.100000\n",
      " 289711/1000000: episode: 835, duration: 1.345s, episode steps: 278, steps per second: 207, episode reward: 240.113, mean reward:  0.864 [-10.347, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 5.008529, mae: 64.362446, mean_q: 87.813912, mean_eps: 0.100000\n",
      " 289887/1000000: episode: 836, duration: 0.843s, episode steps: 176, steps per second: 209, episode reward: 53.534, mean reward:  0.304 [-100.000, 14.711], mean action: 1.602 [0.000, 3.000],  loss: 7.019125, mae: 58.679606, mean_q: 80.326237, mean_eps: 0.100000\n",
      " 290271/1000000: episode: 837, duration: 1.912s, episode steps: 384, steps per second: 201, episode reward: 248.275, mean reward:  0.647 [-17.788, 100.000], mean action: 1.289 [0.000, 3.000],  loss: 20.633301, mae: 61.292937, mean_q: 83.853565, mean_eps: 0.100000\n",
      " 290348/1000000: episode: 838, duration: 0.375s, episode steps:  77, steps per second: 205, episode reward: 24.219, mean reward:  0.315 [-100.000, 18.858], mean action: 2.013 [0.000, 3.000],  loss: 20.323643, mae: 58.052647, mean_q: 79.368249, mean_eps: 0.100000\n",
      " 290432/1000000: episode: 839, duration: 0.403s, episode steps:  84, steps per second: 208, episode reward: 17.234, mean reward:  0.205 [-100.000, 14.800], mean action: 1.857 [0.000, 3.000],  loss: 21.820356, mae: 62.026944, mean_q: 84.653622, mean_eps: 0.100000\n",
      " 290565/1000000: episode: 840, duration: 0.633s, episode steps: 133, steps per second: 210, episode reward: -119.272, mean reward: -0.897 [-100.000, 67.110], mean action: 1.767 [0.000, 3.000],  loss: 37.087561, mae: 62.975109, mean_q: 85.966730, mean_eps: 0.100000\n",
      " 290638/1000000: episode: 841, duration: 0.356s, episode steps:  73, steps per second: 205, episode reward: -48.365, mean reward: -0.663 [-100.000, 19.003], mean action: 1.986 [0.000, 3.000],  loss: 35.403881, mae: 63.591976, mean_q: 86.560953, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 290782/1000000: episode: 842, duration: 0.687s, episode steps: 144, steps per second: 210, episode reward: -132.120, mean reward: -0.917 [-100.000, 51.566], mean action: 1.215 [0.000, 3.000],  loss: 31.100093, mae: 67.299989, mean_q: 91.258233, mean_eps: 0.100000\n",
      " 290900/1000000: episode: 843, duration: 0.566s, episode steps: 118, steps per second: 208, episode reward: -16.454, mean reward: -0.139 [-100.000, 10.445], mean action: 1.703 [0.000, 3.000],  loss: 26.251449, mae: 64.178566, mean_q: 85.977237, mean_eps: 0.100000\n",
      " 290986/1000000: episode: 844, duration: 0.413s, episode steps:  86, steps per second: 208, episode reward: -89.364, mean reward: -1.039 [-100.000, 15.076], mean action: 1.884 [0.000, 3.000],  loss: 30.912980, mae: 65.246644, mean_q: 86.594667, mean_eps: 0.100000\n",
      " 291061/1000000: episode: 845, duration: 0.357s, episode steps:  75, steps per second: 210, episode reward: -9.975, mean reward: -0.133 [-100.000, 14.553], mean action: 1.267 [0.000, 3.000],  loss: 22.505591, mae: 66.382971, mean_q: 87.394661, mean_eps: 0.100000\n",
      " 291197/1000000: episode: 846, duration: 0.649s, episode steps: 136, steps per second: 210, episode reward: 21.876, mean reward:  0.161 [-100.000, 14.136], mean action: 1.331 [0.000, 3.000],  loss: 19.687169, mae: 69.167781, mean_q: 89.390438, mean_eps: 0.100000\n",
      " 291377/1000000: episode: 847, duration: 0.864s, episode steps: 180, steps per second: 208, episode reward: 72.638, mean reward:  0.404 [-100.000, 22.383], mean action: 1.528 [0.000, 3.000],  loss: 22.905522, mae: 72.652240, mean_q: 92.594160, mean_eps: 0.100000\n",
      " 291600/1000000: episode: 848, duration: 1.092s, episode steps: 223, steps per second: 204, episode reward: 272.588, mean reward:  1.222 [-2.516, 100.000], mean action: 1.094 [0.000, 3.000],  loss: 23.307622, mae: 70.385586, mean_q: 88.932375, mean_eps: 0.100000\n",
      " 292251/1000000: episode: 849, duration: 3.435s, episode steps: 651, steps per second: 190, episode reward: 253.300, mean reward:  0.389 [-18.715, 100.000], mean action: 1.759 [0.000, 3.000],  loss: 17.699876, mae: 57.390756, mean_q: 75.632978, mean_eps: 0.100000\n",
      " 292645/1000000: episode: 850, duration: 1.947s, episode steps: 394, steps per second: 202, episode reward: 244.479, mean reward:  0.621 [-12.145, 100.000], mean action: 1.772 [0.000, 3.000],  loss: 8.093008, mae: 39.539716, mean_q: 53.703463, mean_eps: 0.100000\n",
      " 292974/1000000: episode: 851, duration: 1.607s, episode steps: 329, steps per second: 205, episode reward: 269.340, mean reward:  0.819 [-18.298, 100.000], mean action: 0.939 [0.000, 3.000],  loss: 5.658275, mae: 36.522150, mean_q: 49.408621, mean_eps: 0.100000\n",
      " 293079/1000000: episode: 852, duration: 0.497s, episode steps: 105, steps per second: 211, episode reward: -47.456, mean reward: -0.452 [-100.000, 10.410], mean action: 1.305 [0.000, 3.000],  loss: 9.119346, mae: 40.794727, mean_q: 55.239451, mean_eps: 0.100000\n",
      " 293291/1000000: episode: 853, duration: 1.033s, episode steps: 212, steps per second: 205, episode reward: 277.212, mean reward:  1.308 [-7.130, 100.000], mean action: 1.429 [0.000, 3.000],  loss: 11.143461, mae: 47.428747, mean_q: 63.722827, mean_eps: 0.100000\n",
      " 293645/1000000: episode: 854, duration: 1.764s, episode steps: 354, steps per second: 201, episode reward: 257.219, mean reward:  0.727 [-10.476, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 8.082478, mae: 48.465788, mean_q: 64.781468, mean_eps: 0.100000\n",
      " 293756/1000000: episode: 855, duration: 0.532s, episode steps: 111, steps per second: 208, episode reward: -18.849, mean reward: -0.170 [-100.000, 13.679], mean action: 1.640 [0.000, 3.000],  loss: 8.010251, mae: 50.483766, mean_q: 67.519526, mean_eps: 0.100000\n",
      " 294258/1000000: episode: 856, duration: 2.618s, episode steps: 502, steps per second: 192, episode reward: 241.699, mean reward:  0.481 [-21.945, 100.000], mean action: 1.323 [0.000, 3.000],  loss: 6.494058, mae: 49.989015, mean_q: 66.175014, mean_eps: 0.100000\n",
      " 294394/1000000: episode: 857, duration: 0.656s, episode steps: 136, steps per second: 207, episode reward: 21.704, mean reward:  0.160 [-100.000, 15.725], mean action: 1.544 [0.000, 3.000],  loss: 8.415634, mae: 45.205574, mean_q: 60.475541, mean_eps: 0.100000\n",
      " 294766/1000000: episode: 858, duration: 1.863s, episode steps: 372, steps per second: 200, episode reward: 228.157, mean reward:  0.613 [-17.745, 100.000], mean action: 1.043 [0.000, 3.000],  loss: 6.534415, mae: 48.232195, mean_q: 63.807763, mean_eps: 0.100000\n",
      " 295421/1000000: episode: 859, duration: 3.476s, episode steps: 655, steps per second: 188, episode reward: 183.127, mean reward:  0.280 [-19.147, 100.000], mean action: 1.998 [0.000, 3.000],  loss: 5.767405, mae: 40.217485, mean_q: 53.946378, mean_eps: 0.100000\n",
      " 295878/1000000: episode: 860, duration: 2.252s, episode steps: 457, steps per second: 203, episode reward: 280.030, mean reward:  0.613 [-18.512, 100.000], mean action: 0.834 [0.000, 3.000],  loss: 6.798000, mae: 29.168303, mean_q: 39.979929, mean_eps: 0.100000\n",
      " 296101/1000000: episode: 861, duration: 1.076s, episode steps: 223, steps per second: 207, episode reward: 306.029, mean reward:  1.372 [-7.849, 100.000], mean action: 1.457 [0.000, 3.000],  loss: 6.435030, mae: 32.513881, mean_q: 44.789667, mean_eps: 0.100000\n",
      " 296457/1000000: episode: 862, duration: 1.795s, episode steps: 356, steps per second: 198, episode reward: 241.651, mean reward:  0.679 [-18.504, 100.000], mean action: 1.275 [0.000, 3.000],  loss: 8.831765, mae: 41.642421, mean_q: 56.831862, mean_eps: 0.100000\n",
      " 296891/1000000: episode: 863, duration: 2.161s, episode steps: 434, steps per second: 201, episode reward: 268.884, mean reward:  0.620 [-10.943, 100.000], mean action: 1.348 [0.000, 3.000],  loss: 7.139141, mae: 45.945910, mean_q: 62.302512, mean_eps: 0.100000\n",
      " 297159/1000000: episode: 864, duration: 1.333s, episode steps: 268, steps per second: 201, episode reward: 294.834, mean reward:  1.100 [-10.933, 100.000], mean action: 1.459 [0.000, 3.000],  loss: 5.539565, mae: 47.489485, mean_q: 64.204468, mean_eps: 0.100000\n",
      " 297945/1000000: episode: 865, duration: 4.242s, episode steps: 786, steps per second: 185, episode reward: 184.122, mean reward:  0.234 [-24.200, 100.000], mean action: 1.031 [0.000, 3.000],  loss: 5.097829, mae: 45.529043, mean_q: 61.703187, mean_eps: 0.100000\n",
      " 298363/1000000: episode: 866, duration: 2.140s, episode steps: 418, steps per second: 195, episode reward: 248.760, mean reward:  0.595 [-11.288, 100.000], mean action: 1.184 [0.000, 3.000],  loss: 3.821330, mae: 41.510640, mean_q: 56.417376, mean_eps: 0.100000\n",
      " 298779/1000000: episode: 867, duration: 2.081s, episode steps: 416, steps per second: 200, episode reward: 272.542, mean reward:  0.655 [-17.691, 100.000], mean action: 1.137 [0.000, 3.000],  loss: 3.640116, mae: 43.053436, mean_q: 58.348515, mean_eps: 0.100000\n",
      " 299053/1000000: episode: 868, duration: 1.371s, episode steps: 274, steps per second: 200, episode reward: 264.440, mean reward:  0.965 [-17.729, 100.000], mean action: 1.529 [0.000, 3.000],  loss: 4.465689, mae: 46.815407, mean_q: 63.354795, mean_eps: 0.100000\n",
      " 299375/1000000: episode: 869, duration: 1.597s, episode steps: 322, steps per second: 202, episode reward: 282.924, mean reward:  0.879 [-9.852, 100.000], mean action: 1.441 [0.000, 3.000],  loss: 3.643590, mae: 47.994194, mean_q: 64.971953, mean_eps: 0.100000\n",
      " 299706/1000000: episode: 870, duration: 1.678s, episode steps: 331, steps per second: 197, episode reward: 261.806, mean reward:  0.791 [-17.829, 100.000], mean action: 1.287 [0.000, 3.000],  loss: 5.363965, mae: 50.205008, mean_q: 67.859962, mean_eps: 0.100000\n",
      " 300186/1000000: episode: 871, duration: 2.501s, episode steps: 480, steps per second: 192, episode reward: 242.212, mean reward:  0.505 [-17.887, 100.000], mean action: 1.123 [0.000, 3.000],  loss: 4.948696, mae: 50.581351, mean_q: 68.342936, mean_eps: 0.100000\n",
      " 300663/1000000: episode: 872, duration: 2.429s, episode steps: 477, steps per second: 196, episode reward: 230.062, mean reward:  0.482 [-18.684, 100.000], mean action: 1.130 [0.000, 3.000],  loss: 5.038556, mae: 47.254609, mean_q: 63.922585, mean_eps: 0.100000\n",
      " 301142/1000000: episode: 873, duration: 2.520s, episode steps: 479, steps per second: 190, episode reward: 249.599, mean reward:  0.521 [-18.926, 100.000], mean action: 1.117 [0.000, 3.000],  loss: 3.152645, mae: 46.308474, mean_q: 62.758693, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 301494/1000000: episode: 874, duration: 1.759s, episode steps: 352, steps per second: 200, episode reward: 286.061, mean reward:  0.813 [-18.011, 100.000], mean action: 1.571 [0.000, 3.000],  loss: 3.663547, mae: 46.456323, mean_q: 63.108806, mean_eps: 0.100000\n",
      " 301894/1000000: episode: 875, duration: 2.018s, episode steps: 400, steps per second: 198, episode reward: 226.845, mean reward:  0.567 [-10.917, 100.000], mean action: 1.262 [0.000, 3.000],  loss: 3.013542, mae: 48.831351, mean_q: 66.175519, mean_eps: 0.100000\n",
      " 302341/1000000: episode: 876, duration: 2.313s, episode steps: 447, steps per second: 193, episode reward: 277.648, mean reward:  0.621 [-19.024, 100.000], mean action: 1.230 [0.000, 3.000],  loss: 2.072605, mae: 49.750056, mean_q: 67.196490, mean_eps: 0.100000\n",
      " 302668/1000000: episode: 877, duration: 1.634s, episode steps: 327, steps per second: 200, episode reward: 297.571, mean reward:  0.910 [-10.402, 100.000], mean action: 1.621 [0.000, 3.000],  loss: 2.473925, mae: 47.029820, mean_q: 63.538632, mean_eps: 0.100000\n",
      " 303025/1000000: episode: 878, duration: 1.782s, episode steps: 357, steps per second: 200, episode reward: 251.726, mean reward:  0.705 [-18.228, 100.000], mean action: 0.972 [0.000, 3.000],  loss: 4.048104, mae: 48.499287, mean_q: 65.473798, mean_eps: 0.100000\n",
      " 303335/1000000: episode: 879, duration: 1.560s, episode steps: 310, steps per second: 199, episode reward: 285.558, mean reward:  0.921 [-17.653, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 3.783544, mae: 48.387315, mean_q: 65.411858, mean_eps: 0.100000\n",
      " 303583/1000000: episode: 880, duration: 1.211s, episode steps: 248, steps per second: 205, episode reward: 236.975, mean reward:  0.956 [-9.738, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 5.398949, mae: 49.856318, mean_q: 67.234145, mean_eps: 0.100000\n",
      " 303846/1000000: episode: 881, duration: 1.292s, episode steps: 263, steps per second: 204, episode reward: 276.073, mean reward:  1.050 [-11.895, 100.000], mean action: 1.662 [0.000, 3.000],  loss: 5.726180, mae: 49.668933, mean_q: 67.172150, mean_eps: 0.100000\n",
      " 304066/1000000: episode: 882, duration: 1.075s, episode steps: 220, steps per second: 205, episode reward: 284.972, mean reward:  1.295 [-3.195, 100.000], mean action: 1.523 [0.000, 3.000],  loss: 5.241247, mae: 50.985466, mean_q: 68.934721, mean_eps: 0.100000\n",
      " 304393/1000000: episode: 883, duration: 1.605s, episode steps: 327, steps per second: 204, episode reward: 276.767, mean reward:  0.846 [-11.775, 100.000], mean action: 1.624 [0.000, 3.000],  loss: 5.618204, mae: 52.124059, mean_q: 70.439252, mean_eps: 0.100000\n",
      " 304656/1000000: episode: 884, duration: 1.284s, episode steps: 263, steps per second: 205, episode reward: 239.196, mean reward:  0.909 [-11.119, 100.000], mean action: 1.266 [0.000, 3.000],  loss: 6.538551, mae: 53.379477, mean_q: 71.976116, mean_eps: 0.100000\n",
      " 305327/1000000: episode: 885, duration: 3.330s, episode steps: 671, steps per second: 202, episode reward: 233.986, mean reward:  0.349 [-20.354, 100.000], mean action: 0.852 [0.000, 3.000],  loss: 3.541270, mae: 46.999639, mean_q: 63.583511, mean_eps: 0.100000\n",
      " 306218/1000000: episode: 886, duration: 4.983s, episode steps: 891, steps per second: 179, episode reward: 229.238, mean reward:  0.257 [-19.543, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 4.477748, mae: 39.444914, mean_q: 53.910453, mean_eps: 0.100000\n",
      " 306639/1000000: episode: 887, duration: 2.151s, episode steps: 421, steps per second: 196, episode reward: 266.214, mean reward:  0.632 [-17.455, 100.000], mean action: 0.753 [0.000, 3.000],  loss: 1.760573, mae: 47.508703, mean_q: 64.497934, mean_eps: 0.100000\n",
      " 306928/1000000: episode: 888, duration: 1.388s, episode steps: 289, steps per second: 208, episode reward: 293.191, mean reward:  1.015 [-18.795, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 2.106909, mae: 49.701665, mean_q: 67.584789, mean_eps: 0.100000\n",
      " 307344/1000000: episode: 889, duration: 2.135s, episode steps: 416, steps per second: 195, episode reward: 307.148, mean reward:  0.738 [-9.222, 100.000], mean action: 1.325 [0.000, 3.000],  loss: 2.483212, mae: 54.528154, mean_q: 73.874186, mean_eps: 0.100000\n",
      " 307814/1000000: episode: 890, duration: 2.367s, episode steps: 470, steps per second: 199, episode reward: 263.808, mean reward:  0.561 [-21.184, 100.000], mean action: 1.253 [0.000, 3.000],  loss: 2.397143, mae: 54.326373, mean_q: 73.565109, mean_eps: 0.100000\n",
      " 308510/1000000: episode: 891, duration: 3.639s, episode steps: 696, steps per second: 191, episode reward: 242.455, mean reward:  0.348 [-18.045, 100.000], mean action: 1.069 [0.000, 3.000],  loss: 2.608649, mae: 48.691283, mean_q: 66.168038, mean_eps: 0.100000\n",
      " 308739/1000000: episode: 892, duration: 1.110s, episode steps: 229, steps per second: 206, episode reward: 258.684, mean reward:  1.130 [-9.750, 100.000], mean action: 1.201 [0.000, 3.000],  loss: 3.392713, mae: 44.492472, mean_q: 60.656881, mean_eps: 0.100000\n",
      " 308921/1000000: episode: 893, duration: 0.869s, episode steps: 182, steps per second: 209, episode reward: 276.179, mean reward:  1.517 [-10.203, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 3.265778, mae: 47.623554, mean_q: 64.823846, mean_eps: 0.100000\n",
      " 309119/1000000: episode: 894, duration: 0.953s, episode steps: 198, steps per second: 208, episode reward: 275.284, mean reward:  1.390 [-7.308, 100.000], mean action: 1.530 [0.000, 3.000],  loss: 3.727923, mae: 48.661475, mean_q: 66.015013, mean_eps: 0.100000\n",
      " 309297/1000000: episode: 895, duration: 0.851s, episode steps: 178, steps per second: 209, episode reward: 268.196, mean reward:  1.507 [-12.646, 100.000], mean action: 1.264 [0.000, 3.000],  loss: 4.736903, mae: 54.657649, mean_q: 74.020119, mean_eps: 0.100000\n",
      " 309514/1000000: episode: 896, duration: 1.050s, episode steps: 217, steps per second: 207, episode reward: 299.951, mean reward:  1.382 [-8.755, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 6.043473, mae: 61.790938, mean_q: 83.430369, mean_eps: 0.100000\n",
      " 309622/1000000: episode: 897, duration: 0.515s, episode steps: 108, steps per second: 210, episode reward:  3.304, mean reward:  0.031 [-100.000, 22.409], mean action: 1.574 [0.000, 3.000],  loss: 3.386833, mae: 63.635999, mean_q: 85.877371, mean_eps: 0.100000\n",
      " 309804/1000000: episode: 898, duration: 0.885s, episode steps: 182, steps per second: 206, episode reward: 270.208, mean reward:  1.485 [-8.524, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 11.500958, mae: 66.302775, mean_q: 89.612454, mean_eps: 0.100000\n",
      " 310074/1000000: episode: 899, duration: 1.313s, episode steps: 270, steps per second: 206, episode reward: 292.185, mean reward:  1.082 [-17.194, 100.000], mean action: 1.122 [0.000, 3.000],  loss: 6.293636, mae: 66.982373, mean_q: 90.798279, mean_eps: 0.100000\n",
      " 310292/1000000: episode: 900, duration: 1.047s, episode steps: 218, steps per second: 208, episode reward: 254.864, mean reward:  1.169 [-8.877, 100.000], mean action: 1.257 [0.000, 3.000],  loss: 5.930631, mae: 65.912236, mean_q: 89.442299, mean_eps: 0.100000\n",
      " 310389/1000000: episode: 901, duration: 0.467s, episode steps:  97, steps per second: 208, episode reward: -33.102, mean reward: -0.341 [-100.000,  9.384], mean action: 1.639 [0.000, 3.000],  loss: 7.080955, mae: 65.882249, mean_q: 89.189828, mean_eps: 0.100000\n",
      " 310571/1000000: episode: 902, duration: 0.874s, episode steps: 182, steps per second: 208, episode reward: 299.501, mean reward:  1.646 [-4.404, 100.000], mean action: 1.192 [0.000, 3.000],  loss: 8.701397, mae: 68.431411, mean_q: 91.985423, mean_eps: 0.100000\n",
      " 311571/1000000: episode: 903, duration: 5.081s, episode steps: 1000, steps per second: 197, episode reward: 133.717, mean reward:  0.134 [-19.519, 12.871], mean action: 1.340 [0.000, 3.000],  loss: 3.053060, mae: 59.244996, mean_q: 79.745672, mean_eps: 0.100000\n",
      " 312494/1000000: episode: 904, duration: 4.767s, episode steps: 923, steps per second: 194, episode reward: 265.717, mean reward:  0.288 [-18.887, 100.000], mean action: 1.362 [0.000, 3.000],  loss: 2.185181, mae: 47.409470, mean_q: 64.228256, mean_eps: 0.100000\n",
      " 313242/1000000: episode: 905, duration: 3.832s, episode steps: 748, steps per second: 195, episode reward: 259.438, mean reward:  0.347 [-20.342, 100.000], mean action: 0.715 [0.000, 3.000],  loss: 1.352231, mae: 46.470848, mean_q: 63.036893, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 313774/1000000: episode: 906, duration: 2.679s, episode steps: 532, steps per second: 199, episode reward: 260.183, mean reward:  0.489 [-11.453, 100.000], mean action: 1.002 [0.000, 3.000],  loss: 1.803323, mae: 47.351245, mean_q: 64.055878, mean_eps: 0.100000\n",
      " 314203/1000000: episode: 907, duration: 2.203s, episode steps: 429, steps per second: 195, episode reward: 249.137, mean reward:  0.581 [-19.387, 100.000], mean action: 1.380 [0.000, 3.000],  loss: 2.721760, mae: 48.285666, mean_q: 65.191825, mean_eps: 0.100000\n",
      " 314616/1000000: episode: 908, duration: 2.114s, episode steps: 413, steps per second: 195, episode reward: 270.454, mean reward:  0.655 [-18.620, 100.000], mean action: 2.310 [0.000, 3.000],  loss: 2.996152, mae: 49.269288, mean_q: 66.689595, mean_eps: 0.100000\n",
      " 314782/1000000: episode: 909, duration: 0.792s, episode steps: 166, steps per second: 209, episode reward: 10.350, mean reward:  0.062 [-100.000, 11.350], mean action: 1.687 [0.000, 3.000],  loss: 2.697266, mae: 51.333571, mean_q: 69.588894, mean_eps: 0.100000\n",
      " 315782/1000000: episode: 910, duration: 5.073s, episode steps: 1000, steps per second: 197, episode reward: 150.811, mean reward:  0.151 [-18.924, 23.041], mean action: 0.961 [0.000, 3.000],  loss: 7.806400, mae: 48.921428, mean_q: 66.607647, mean_eps: 0.100000\n",
      " 316079/1000000: episode: 911, duration: 1.474s, episode steps: 297, steps per second: 201, episode reward: 247.598, mean reward:  0.834 [-18.878, 100.000], mean action: 1.340 [0.000, 3.000],  loss: 1.616465, mae: 40.264542, mean_q: 54.763490, mean_eps: 0.100000\n",
      " 316735/1000000: episode: 912, duration: 3.413s, episode steps: 656, steps per second: 192, episode reward: 237.505, mean reward:  0.362 [-18.879, 100.000], mean action: 0.960 [0.000, 3.000],  loss: 2.585373, mae: 45.350271, mean_q: 61.319010, mean_eps: 0.100000\n",
      " 317165/1000000: episode: 913, duration: 2.122s, episode steps: 430, steps per second: 203, episode reward: 298.181, mean reward:  0.693 [-18.926, 100.000], mean action: 1.302 [0.000, 3.000],  loss: 4.329318, mae: 47.064914, mean_q: 63.912963, mean_eps: 0.100000\n",
      " 318165/1000000: episode: 914, duration: 5.088s, episode steps: 1000, steps per second: 197, episode reward: 155.529, mean reward:  0.156 [-20.697, 22.181], mean action: 0.611 [0.000, 3.000],  loss: 2.593744, mae: 45.745072, mean_q: 62.139840, mean_eps: 0.100000\n",
      " 319165/1000000: episode: 915, duration: 5.547s, episode steps: 1000, steps per second: 180, episode reward: 151.511, mean reward:  0.152 [-18.454, 22.743], mean action: 1.377 [0.000, 3.000],  loss: 1.374410, mae: 39.523432, mean_q: 53.412061, mean_eps: 0.100000\n",
      " 319436/1000000: episode: 916, duration: 1.330s, episode steps: 271, steps per second: 204, episode reward: 276.256, mean reward:  1.019 [-9.216, 100.000], mean action: 1.207 [0.000, 3.000],  loss: 1.336986, mae: 38.381994, mean_q: 51.764101, mean_eps: 0.100000\n",
      " 319635/1000000: episode: 917, duration: 0.961s, episode steps: 199, steps per second: 207, episode reward: 292.071, mean reward:  1.468 [-2.651, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 3.209978, mae: 41.594553, mean_q: 56.116317, mean_eps: 0.100000\n",
      " 319822/1000000: episode: 918, duration: 0.897s, episode steps: 187, steps per second: 208, episode reward: 268.580, mean reward:  1.436 [-2.421, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 5.388455, mae: 47.447183, mean_q: 63.865692, mean_eps: 0.100000\n",
      " 319968/1000000: episode: 919, duration: 0.700s, episode steps: 146, steps per second: 209, episode reward: -134.725, mean reward: -0.923 [-100.000, 17.383], mean action: 1.562 [0.000, 3.000],  loss: 5.941916, mae: 49.847287, mean_q: 67.153657, mean_eps: 0.100000\n",
      " 320623/1000000: episode: 920, duration: 3.301s, episode steps: 655, steps per second: 198, episode reward: 247.892, mean reward:  0.378 [-21.635, 100.000], mean action: 0.985 [0.000, 3.000],  loss: 8.263677, mae: 48.290151, mean_q: 65.129628, mean_eps: 0.100000\n",
      " 321623/1000000: episode: 921, duration: 5.317s, episode steps: 1000, steps per second: 188, episode reward: 131.191, mean reward:  0.131 [-17.431, 23.203], mean action: 1.298 [0.000, 3.000],  loss: 4.603490, mae: 34.806999, mean_q: 47.287452, mean_eps: 0.100000\n",
      " 322623/1000000: episode: 922, duration: 5.578s, episode steps: 1000, steps per second: 179, episode reward: 110.515, mean reward:  0.111 [-20.613, 25.351], mean action: 1.602 [0.000, 3.000],  loss: 1.286867, mae: 36.053561, mean_q: 48.857542, mean_eps: 0.100000\n",
      " 323491/1000000: episode: 923, duration: 4.434s, episode steps: 868, steps per second: 196, episode reward: 229.795, mean reward:  0.265 [-18.322, 100.000], mean action: 0.738 [0.000, 3.000],  loss: 0.965823, mae: 32.307155, mean_q: 43.742260, mean_eps: 0.100000\n",
      " 323773/1000000: episode: 924, duration: 1.384s, episode steps: 282, steps per second: 204, episode reward: 267.046, mean reward:  0.947 [-17.012, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 3.366101, mae: 30.463006, mean_q: 41.454800, mean_eps: 0.100000\n",
      " 324078/1000000: episode: 925, duration: 1.496s, episode steps: 305, steps per second: 204, episode reward: 308.320, mean reward:  1.011 [-13.746, 100.000], mean action: 1.082 [0.000, 3.000],  loss: 4.181461, mae: 33.768429, mean_q: 46.167168, mean_eps: 0.100000\n",
      " 324434/1000000: episode: 926, duration: 1.719s, episode steps: 356, steps per second: 207, episode reward: 299.359, mean reward:  0.841 [-18.140, 100.000], mean action: 1.048 [0.000, 3.000],  loss: 8.869052, mae: 44.091783, mean_q: 60.285974, mean_eps: 0.100000\n",
      " 324634/1000000: episode: 927, duration: 0.950s, episode steps: 200, steps per second: 211, episode reward: 262.959, mean reward:  1.315 [-11.130, 100.000], mean action: 1.045 [0.000, 3.000],  loss: 9.085128, mae: 49.784977, mean_q: 67.915648, mean_eps: 0.100000\n",
      " 324961/1000000: episode: 928, duration: 1.629s, episode steps: 327, steps per second: 201, episode reward: 247.675, mean reward:  0.757 [-17.491, 100.000], mean action: 1.193 [0.000, 3.000],  loss: 7.464440, mae: 51.291281, mean_q: 69.812060, mean_eps: 0.100000\n",
      " 325086/1000000: episode: 929, duration: 0.596s, episode steps: 125, steps per second: 210, episode reward: 10.289, mean reward:  0.082 [-100.000, 14.597], mean action: 1.576 [0.000, 3.000],  loss: 8.807909, mae: 49.818492, mean_q: 67.594289, mean_eps: 0.100000\n",
      " 325879/1000000: episode: 930, duration: 4.330s, episode steps: 793, steps per second: 183, episode reward: 247.649, mean reward:  0.312 [-20.289, 100.000], mean action: 0.919 [0.000, 3.000],  loss: 4.227156, mae: 47.074680, mean_q: 63.643084, mean_eps: 0.100000\n",
      " 326289/1000000: episode: 931, duration: 2.085s, episode steps: 410, steps per second: 197, episode reward: 303.764, mean reward:  0.741 [-18.453, 100.000], mean action: 1.007 [0.000, 3.000],  loss: 2.910870, mae: 40.483818, mean_q: 54.845049, mean_eps: 0.100000\n",
      " 326377/1000000: episode: 932, duration: 0.424s, episode steps:  88, steps per second: 208, episode reward: -66.310, mean reward: -0.754 [-100.000,  7.108], mean action: 1.398 [0.000, 3.000],  loss: 2.243503, mae: 39.641459, mean_q: 53.983030, mean_eps: 0.100000\n",
      " 326680/1000000: episode: 933, duration: 1.475s, episode steps: 303, steps per second: 205, episode reward: 248.441, mean reward:  0.820 [-17.615, 100.000], mean action: 1.228 [0.000, 3.000],  loss: 3.001806, mae: 44.781808, mean_q: 60.228839, mean_eps: 0.100000\n",
      " 326882/1000000: episode: 934, duration: 0.987s, episode steps: 202, steps per second: 205, episode reward: 254.176, mean reward:  1.258 [-10.181, 100.000], mean action: 1.733 [0.000, 3.000],  loss: 5.252990, mae: 51.036691, mean_q: 68.344579, mean_eps: 0.100000\n",
      " 327203/1000000: episode: 935, duration: 1.571s, episode steps: 321, steps per second: 204, episode reward: 289.337, mean reward:  0.901 [-19.333, 100.000], mean action: 1.209 [0.000, 3.000],  loss: 5.842567, mae: 51.148614, mean_q: 68.603104, mean_eps: 0.100000\n",
      " 327465/1000000: episode: 936, duration: 1.279s, episode steps: 262, steps per second: 205, episode reward: 288.459, mean reward:  1.101 [-17.481, 100.000], mean action: 1.073 [0.000, 3.000],  loss: 8.613218, mae: 52.268060, mean_q: 70.873419, mean_eps: 0.100000\n",
      " 327661/1000000: episode: 937, duration: 0.938s, episode steps: 196, steps per second: 209, episode reward: 240.722, mean reward:  1.228 [-8.625, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 7.872029, mae: 51.731053, mean_q: 70.841069, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 327927/1000000: episode: 938, duration: 1.300s, episode steps: 266, steps per second: 205, episode reward: 272.532, mean reward:  1.025 [-18.006, 100.000], mean action: 0.823 [0.000, 3.000],  loss: 8.728278, mae: 54.355667, mean_q: 74.268080, mean_eps: 0.100000\n",
      " 328187/1000000: episode: 939, duration: 1.263s, episode steps: 260, steps per second: 206, episode reward: 257.095, mean reward:  0.989 [-17.515, 100.000], mean action: 0.965 [0.000, 3.000],  loss: 6.493326, mae: 54.225107, mean_q: 74.081605, mean_eps: 0.100000\n",
      " 328419/1000000: episode: 940, duration: 1.119s, episode steps: 232, steps per second: 207, episode reward: 249.075, mean reward:  1.074 [-7.731, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 7.906148, mae: 56.284828, mean_q: 76.698457, mean_eps: 0.100000\n",
      " 328626/1000000: episode: 941, duration: 0.999s, episode steps: 207, steps per second: 207, episode reward: 266.590, mean reward:  1.288 [-9.442, 100.000], mean action: 1.213 [0.000, 3.000],  loss: 5.746250, mae: 57.883606, mean_q: 78.735534, mean_eps: 0.100000\n",
      " 328967/1000000: episode: 942, duration: 1.717s, episode steps: 341, steps per second: 199, episode reward: 302.808, mean reward:  0.888 [-20.416, 100.000], mean action: 1.062 [0.000, 3.000],  loss: 6.032504, mae: 57.706275, mean_q: 78.203818, mean_eps: 0.100000\n",
      " 329388/1000000: episode: 943, duration: 2.107s, episode steps: 421, steps per second: 200, episode reward: 286.967, mean reward:  0.682 [-17.777, 100.000], mean action: 1.031 [0.000, 3.000],  loss: 5.779529, mae: 58.094027, mean_q: 78.625903, mean_eps: 0.100000\n",
      " 329593/1000000: episode: 944, duration: 0.985s, episode steps: 205, steps per second: 208, episode reward: 235.180, mean reward:  1.147 [-9.925, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 5.293750, mae: 56.816561, mean_q: 76.891056, mean_eps: 0.100000\n",
      " 329971/1000000: episode: 945, duration: 1.928s, episode steps: 378, steps per second: 196, episode reward: 248.172, mean reward:  0.657 [-16.894, 100.000], mean action: 0.952 [0.000, 3.000],  loss: 4.287767, mae: 55.138920, mean_q: 74.509209, mean_eps: 0.100000\n",
      " 330393/1000000: episode: 946, duration: 2.160s, episode steps: 422, steps per second: 195, episode reward: 273.821, mean reward:  0.649 [-17.437, 100.000], mean action: 1.431 [0.000, 3.000],  loss: 4.026933, mae: 53.071867, mean_q: 71.721880, mean_eps: 0.100000\n",
      " 330835/1000000: episode: 947, duration: 2.229s, episode steps: 442, steps per second: 198, episode reward: 290.239, mean reward:  0.657 [-20.543, 100.000], mean action: 2.471 [0.000, 3.000],  loss: 3.961339, mae: 51.310697, mean_q: 69.567680, mean_eps: 0.100000\n",
      " 331430/1000000: episode: 948, duration: 3.029s, episode steps: 595, steps per second: 196, episode reward: 296.944, mean reward:  0.499 [-19.113, 100.000], mean action: 1.121 [0.000, 3.000],  loss: 2.618786, mae: 50.861775, mean_q: 68.916400, mean_eps: 0.100000\n",
      " 331757/1000000: episode: 949, duration: 1.612s, episode steps: 327, steps per second: 203, episode reward: 235.463, mean reward:  0.720 [-7.454, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 3.180937, mae: 48.609806, mean_q: 65.735602, mean_eps: 0.100000\n",
      " 332103/1000000: episode: 950, duration: 1.734s, episode steps: 346, steps per second: 200, episode reward: 244.680, mean reward:  0.707 [-20.798, 100.000], mean action: 1.052 [0.000, 3.000],  loss: 2.032100, mae: 50.212326, mean_q: 67.761048, mean_eps: 0.100000\n",
      " 332901/1000000: episode: 951, duration: 4.054s, episode steps: 798, steps per second: 197, episode reward: 254.856, mean reward:  0.319 [-18.344, 100.000], mean action: 0.561 [0.000, 3.000],  loss: 2.826473, mae: 49.648092, mean_q: 67.112982, mean_eps: 0.100000\n",
      " 332978/1000000: episode: 952, duration: 0.374s, episode steps:  77, steps per second: 206, episode reward: -22.071, mean reward: -0.287 [-100.000, 12.589], mean action: 1.922 [0.000, 3.000],  loss: 2.276488, mae: 44.968896, mean_q: 60.726899, mean_eps: 0.100000\n",
      " 333092/1000000: episode: 953, duration: 0.545s, episode steps: 114, steps per second: 209, episode reward: -13.064, mean reward: -0.115 [-100.000, 11.431], mean action: 1.588 [0.000, 3.000],  loss: 29.074805, mae: 48.725058, mean_q: 65.959025, mean_eps: 0.100000\n",
      " 333208/1000000: episode: 954, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -39.626, mean reward: -0.342 [-100.000, 13.944], mean action: 1.543 [0.000, 3.000],  loss: 26.309115, mae: 50.686385, mean_q: 68.787068, mean_eps: 0.100000\n",
      " 333326/1000000: episode: 955, duration: 0.566s, episode steps: 118, steps per second: 209, episode reward:  9.897, mean reward:  0.084 [-100.000, 28.331], mean action: 1.636 [0.000, 3.000],  loss: 21.907553, mae: 51.908556, mean_q: 70.340050, mean_eps: 0.100000\n",
      " 333429/1000000: episode: 956, duration: 0.494s, episode steps: 103, steps per second: 209, episode reward: -35.199, mean reward: -0.342 [-100.000, 10.114], mean action: 1.553 [0.000, 3.000],  loss: 22.153779, mae: 55.939456, mean_q: 75.052543, mean_eps: 0.100000\n",
      " 333548/1000000: episode: 957, duration: 0.571s, episode steps: 119, steps per second: 209, episode reward: -38.356, mean reward: -0.322 [-100.000, 15.285], mean action: 2.092 [0.000, 3.000],  loss: 26.346749, mae: 59.276464, mean_q: 78.218881, mean_eps: 0.100000\n",
      " 333775/1000000: episode: 958, duration: 1.107s, episode steps: 227, steps per second: 205, episode reward: 220.599, mean reward:  0.972 [-10.335, 100.000], mean action: 2.101 [0.000, 3.000],  loss: 14.974311, mae: 62.662696, mean_q: 77.681126, mean_eps: 0.100000\n",
      " 333865/1000000: episode: 959, duration: 0.434s, episode steps:  90, steps per second: 207, episode reward: -46.061, mean reward: -0.512 [-100.000, 44.355], mean action: 1.667 [0.000, 3.000],  loss: 20.604241, mae: 61.751913, mean_q: 75.037323, mean_eps: 0.100000\n",
      " 333986/1000000: episode: 960, duration: 0.581s, episode steps: 121, steps per second: 208, episode reward: -133.619, mean reward: -1.104 [-100.000, 33.387], mean action: 2.165 [0.000, 3.000],  loss: 26.459882, mae: 63.563404, mean_q: 78.501635, mean_eps: 0.100000\n",
      " 334648/1000000: episode: 961, duration: 3.455s, episode steps: 662, steps per second: 192, episode reward: 255.888, mean reward:  0.387 [-18.502, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 18.463242, mae: 47.722959, mean_q: 56.129845, mean_eps: 0.100000\n",
      " 334758/1000000: episode: 962, duration: 0.524s, episode steps: 110, steps per second: 210, episode reward: -61.282, mean reward: -0.557 [-100.000, 10.808], mean action: 1.682 [0.000, 3.000],  loss: 17.151911, mae: 37.453095, mean_q: 47.798024, mean_eps: 0.100000\n",
      " 335404/1000000: episode: 963, duration: 3.326s, episode steps: 646, steps per second: 194, episode reward: 166.562, mean reward:  0.258 [-19.003, 100.000], mean action: 0.895 [0.000, 3.000],  loss: 6.991536, mae: 35.124104, mean_q: 44.171473, mean_eps: 0.100000\n",
      " 335714/1000000: episode: 964, duration: 1.567s, episode steps: 310, steps per second: 198, episode reward: 233.030, mean reward:  0.752 [-19.573, 100.000], mean action: 1.416 [0.000, 3.000],  loss: 6.989282, mae: 37.051362, mean_q: 47.513339, mean_eps: 0.100000\n",
      " 336454/1000000: episode: 965, duration: 3.913s, episode steps: 740, steps per second: 189, episode reward: 186.652, mean reward:  0.252 [-22.117, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 6.292106, mae: 38.788303, mean_q: 52.869421, mean_eps: 0.100000\n",
      " 336566/1000000: episode: 966, duration: 0.542s, episode steps: 112, steps per second: 207, episode reward: -21.737, mean reward: -0.194 [-100.000, 10.153], mean action: 2.045 [0.000, 3.000],  loss: 3.645356, mae: 42.103881, mean_q: 58.113744, mean_eps: 0.100000\n",
      " 336661/1000000: episode: 967, duration: 0.457s, episode steps:  95, steps per second: 208, episode reward: -23.045, mean reward: -0.243 [-100.000,  9.801], mean action: 2.021 [1.000, 3.000],  loss: 10.343765, mae: 43.980064, mean_q: 60.705710, mean_eps: 0.100000\n",
      " 336874/1000000: episode: 968, duration: 1.023s, episode steps: 213, steps per second: 208, episode reward: -44.824, mean reward: -0.210 [-100.000, 37.317], mean action: 1.732 [0.000, 3.000],  loss: 16.426374, mae: 44.145423, mean_q: 60.815205, mean_eps: 0.100000\n",
      " 337074/1000000: episode: 969, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: 233.360, mean reward:  1.167 [-12.466, 100.000], mean action: 1.325 [0.000, 3.000],  loss: 30.299346, mae: 44.978113, mean_q: 60.841801, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 338074/1000000: episode: 970, duration: 5.627s, episode steps: 1000, steps per second: 178, episode reward: -57.678, mean reward: -0.058 [-20.918, 10.894], mean action: 1.921 [0.000, 3.000],  loss: 21.476505, mae: 38.764527, mean_q: 41.888531, mean_eps: 0.100000\n",
      " 338561/1000000: episode: 971, duration: 2.520s, episode steps: 487, steps per second: 193, episode reward: 236.987, mean reward:  0.487 [-20.955, 100.000], mean action: 0.649 [0.000, 3.000],  loss: 1.927235, mae: 21.679892, mean_q: 23.853182, mean_eps: 0.100000\n",
      " 338889/1000000: episode: 972, duration: 1.621s, episode steps: 328, steps per second: 202, episode reward: 227.092, mean reward:  0.692 [-17.839, 100.000], mean action: 1.119 [0.000, 3.000],  loss: 2.813269, mae: 32.271567, mean_q: 44.694823, mean_eps: 0.100000\n",
      " 339292/1000000: episode: 973, duration: 1.991s, episode steps: 403, steps per second: 202, episode reward: 303.370, mean reward:  0.753 [-18.178, 100.000], mean action: 0.789 [0.000, 3.000],  loss: 4.978358, mae: 39.696352, mean_q: 54.623187, mean_eps: 0.100000\n",
      " 339703/1000000: episode: 974, duration: 2.012s, episode steps: 411, steps per second: 204, episode reward: 233.649, mean reward:  0.568 [-18.595, 100.000], mean action: 1.036 [0.000, 3.000],  loss: 4.535774, mae: 40.970018, mean_q: 56.279609, mean_eps: 0.100000\n",
      " 340049/1000000: episode: 975, duration: 1.716s, episode steps: 346, steps per second: 202, episode reward: 254.327, mean reward:  0.735 [-19.849, 100.000], mean action: 0.948 [0.000, 3.000],  loss: 4.732067, mae: 41.951685, mean_q: 57.447014, mean_eps: 0.100000\n",
      " 340198/1000000: episode: 976, duration: 0.716s, episode steps: 149, steps per second: 208, episode reward: -8.444, mean reward: -0.057 [-100.000, 13.221], mean action: 1.832 [0.000, 3.000],  loss: 4.663418, mae: 42.080150, mean_q: 57.227464, mean_eps: 0.100000\n",
      " 340499/1000000: episode: 977, duration: 1.486s, episode steps: 301, steps per second: 202, episode reward: 279.862, mean reward:  0.930 [-10.885, 100.000], mean action: 1.076 [0.000, 3.000],  loss: 10.316981, mae: 43.330082, mean_q: 59.027626, mean_eps: 0.100000\n",
      " 340706/1000000: episode: 978, duration: 1.006s, episode steps: 207, steps per second: 206, episode reward: 303.231, mean reward:  1.465 [-9.376, 100.000], mean action: 1.435 [0.000, 3.000],  loss: 9.085825, mae: 47.811986, mean_q: 65.159850, mean_eps: 0.100000\n",
      " 340968/1000000: episode: 979, duration: 1.275s, episode steps: 262, steps per second: 206, episode reward: 255.888, mean reward:  0.977 [-17.726, 100.000], mean action: 1.065 [0.000, 3.000],  loss: 12.027322, mae: 50.064428, mean_q: 68.071166, mean_eps: 0.100000\n",
      " 341291/1000000: episode: 980, duration: 1.604s, episode steps: 323, steps per second: 201, episode reward: 276.360, mean reward:  0.856 [-12.819, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 8.089444, mae: 54.018142, mean_q: 73.353658, mean_eps: 0.100000\n",
      " 341669/1000000: episode: 981, duration: 1.904s, episode steps: 378, steps per second: 199, episode reward: 263.966, mean reward:  0.698 [-17.378, 100.000], mean action: 1.378 [0.000, 3.000],  loss: 6.122509, mae: 54.891126, mean_q: 74.354442, mean_eps: 0.100000\n",
      " 341876/1000000: episode: 982, duration: 1.007s, episode steps: 207, steps per second: 206, episode reward: 244.941, mean reward:  1.183 [-9.684, 100.000], mean action: 1.353 [0.000, 3.000],  loss: 5.264339, mae: 53.306459, mean_q: 72.149333, mean_eps: 0.100000\n",
      " 342150/1000000: episode: 983, duration: 1.334s, episode steps: 274, steps per second: 205, episode reward: 246.532, mean reward:  0.900 [-21.172, 100.000], mean action: 1.113 [0.000, 3.000],  loss: 3.702094, mae: 53.634858, mean_q: 72.515159, mean_eps: 0.100000\n",
      " 342358/1000000: episode: 984, duration: 1.009s, episode steps: 208, steps per second: 206, episode reward: 274.710, mean reward:  1.321 [-9.390, 100.000], mean action: 1.260 [0.000, 3.000],  loss: 4.340754, mae: 54.513468, mean_q: 73.588252, mean_eps: 0.100000\n",
      " 342751/1000000: episode: 985, duration: 1.925s, episode steps: 393, steps per second: 204, episode reward: 305.877, mean reward:  0.778 [-18.378, 100.000], mean action: 1.193 [0.000, 3.000],  loss: 4.816572, mae: 54.937121, mean_q: 74.267567, mean_eps: 0.100000\n",
      " 343301/1000000: episode: 986, duration: 2.931s, episode steps: 550, steps per second: 188, episode reward: 232.124, mean reward:  0.422 [-19.750, 100.000], mean action: 1.005 [0.000, 3.000],  loss: 3.692151, mae: 50.920058, mean_q: 68.879639, mean_eps: 0.100000\n",
      " 344301/1000000: episode: 987, duration: 5.654s, episode steps: 1000, steps per second: 177, episode reward: 145.221, mean reward:  0.145 [-18.811, 13.678], mean action: 1.344 [0.000, 3.000],  loss: 3.670374, mae: 42.202030, mean_q: 57.080682, mean_eps: 0.100000\n",
      " 344542/1000000: episode: 988, duration: 1.187s, episode steps: 241, steps per second: 203, episode reward: 251.050, mean reward:  1.042 [-3.050, 100.000], mean action: 1.689 [0.000, 3.000],  loss: 1.921722, mae: 42.652836, mean_q: 57.632888, mean_eps: 0.100000\n",
      " 344841/1000000: episode: 989, duration: 1.460s, episode steps: 299, steps per second: 205, episode reward: 289.893, mean reward:  0.970 [-17.948, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 4.321283, mae: 46.900262, mean_q: 63.309735, mean_eps: 0.100000\n",
      " 345087/1000000: episode: 990, duration: 1.192s, episode steps: 246, steps per second: 206, episode reward: 287.274, mean reward:  1.168 [-2.174, 100.000], mean action: 1.382 [0.000, 3.000],  loss: 3.767316, mae: 51.303109, mean_q: 69.259753, mean_eps: 0.100000\n",
      " 345385/1000000: episode: 991, duration: 1.470s, episode steps: 298, steps per second: 203, episode reward: 270.969, mean reward:  0.909 [-17.394, 100.000], mean action: 1.087 [0.000, 3.000],  loss: 4.641314, mae: 55.579952, mean_q: 75.002929, mean_eps: 0.100000\n",
      " 345686/1000000: episode: 992, duration: 1.469s, episode steps: 301, steps per second: 205, episode reward: 301.943, mean reward:  1.003 [-7.490, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 4.081596, mae: 56.756346, mean_q: 76.694349, mean_eps: 0.100000\n",
      " 345934/1000000: episode: 993, duration: 1.200s, episode steps: 248, steps per second: 207, episode reward: 295.167, mean reward:  1.190 [-9.384, 100.000], mean action: 1.460 [0.000, 3.000],  loss: 3.630869, mae: 53.576505, mean_q: 72.504890, mean_eps: 0.100000\n",
      " 346227/1000000: episode: 994, duration: 1.428s, episode steps: 293, steps per second: 205, episode reward: 280.287, mean reward:  0.957 [-21.004, 100.000], mean action: 1.164 [0.000, 3.000],  loss: 5.294229, mae: 55.535302, mean_q: 74.942492, mean_eps: 0.100000\n",
      " 346422/1000000: episode: 995, duration: 0.947s, episode steps: 195, steps per second: 206, episode reward: 243.845, mean reward:  1.250 [-2.671, 100.000], mean action: 1.441 [0.000, 3.000],  loss: 2.664641, mae: 55.265980, mean_q: 74.540610, mean_eps: 0.100000\n",
      " 346944/1000000: episode: 996, duration: 2.727s, episode steps: 522, steps per second: 191, episode reward: 257.370, mean reward:  0.493 [-18.732, 100.000], mean action: 1.264 [0.000, 3.000],  loss: 3.574189, mae: 56.222483, mean_q: 75.859676, mean_eps: 0.100000\n",
      " 347185/1000000: episode: 997, duration: 1.162s, episode steps: 241, steps per second: 207, episode reward: 239.605, mean reward:  0.994 [-3.352, 100.000], mean action: 0.979 [0.000, 3.000],  loss: 2.881220, mae: 53.915419, mean_q: 72.710260, mean_eps: 0.100000\n",
      " 347502/1000000: episode: 998, duration: 1.575s, episode steps: 317, steps per second: 201, episode reward: 301.216, mean reward:  0.950 [-8.210, 100.000], mean action: 0.950 [0.000, 3.000],  loss: 3.824416, mae: 52.657052, mean_q: 71.215895, mean_eps: 0.100000\n",
      " 347765/1000000: episode: 999, duration: 1.288s, episode steps: 263, steps per second: 204, episode reward: 294.204, mean reward:  1.119 [-16.530, 100.000], mean action: 1.802 [0.000, 3.000],  loss: 3.910871, mae: 51.668932, mean_q: 69.960471, mean_eps: 0.100000\n",
      " 348068/1000000: episode: 1000, duration: 1.494s, episode steps: 303, steps per second: 203, episode reward: 282.060, mean reward:  0.931 [-20.811, 100.000], mean action: 0.921 [0.000, 3.000],  loss: 4.653448, mae: 55.353727, mean_q: 74.713259, mean_eps: 0.100000\n",
      " 348301/1000000: episode: 1001, duration: 1.114s, episode steps: 233, steps per second: 209, episode reward: 277.091, mean reward:  1.189 [-9.027, 100.000], mean action: 1.322 [0.000, 3.000],  loss: 3.485076, mae: 54.252699, mean_q: 73.233629, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 348620/1000000: episode: 1002, duration: 1.559s, episode steps: 319, steps per second: 205, episode reward: 302.353, mean reward:  0.948 [-20.107, 100.000], mean action: 0.994 [0.000, 3.000],  loss: 3.277513, mae: 55.694310, mean_q: 75.271074, mean_eps: 0.100000\n",
      " 348957/1000000: episode: 1003, duration: 1.633s, episode steps: 337, steps per second: 206, episode reward: 301.706, mean reward:  0.895 [-10.398, 100.000], mean action: 0.920 [0.000, 3.000],  loss: 2.299177, mae: 54.692834, mean_q: 74.219234, mean_eps: 0.100000\n",
      " 349101/1000000: episode: 1004, duration: 0.695s, episode steps: 144, steps per second: 207, episode reward: 38.354, mean reward:  0.266 [-100.000, 14.954], mean action: 1.806 [0.000, 3.000],  loss: 3.846889, mae: 56.152313, mean_q: 76.159316, mean_eps: 0.100000\n",
      " 349404/1000000: episode: 1005, duration: 1.468s, episode steps: 303, steps per second: 206, episode reward: 281.251, mean reward:  0.928 [-18.366, 100.000], mean action: 0.851 [0.000, 3.000],  loss: 13.743521, mae: 57.305727, mean_q: 77.963132, mean_eps: 0.100000\n",
      " 349763/1000000: episode: 1006, duration: 1.760s, episode steps: 359, steps per second: 204, episode reward: 272.055, mean reward:  0.758 [-18.094, 100.000], mean action: 0.805 [0.000, 3.000],  loss: 15.239565, mae: 57.099526, mean_q: 78.281763, mean_eps: 0.100000\n",
      " 349948/1000000: episode: 1007, duration: 0.945s, episode steps: 185, steps per second: 196, episode reward: 248.520, mean reward:  1.343 [-3.011, 100.000], mean action: 0.914 [0.000, 3.000],  loss: 10.893531, mae: 57.270390, mean_q: 78.855544, mean_eps: 0.100000\n",
      " 350173/1000000: episode: 1008, duration: 1.224s, episode steps: 225, steps per second: 184, episode reward: 228.100, mean reward:  1.014 [-16.182, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 5.499998, mae: 57.325437, mean_q: 78.997411, mean_eps: 0.100000\n",
      " 350400/1000000: episode: 1009, duration: 1.306s, episode steps: 227, steps per second: 174, episode reward: 282.791, mean reward:  1.246 [-11.123, 100.000], mean action: 1.009 [0.000, 3.000],  loss: 5.465169, mae: 57.160943, mean_q: 78.857370, mean_eps: 0.100000\n",
      " 350778/1000000: episode: 1010, duration: 1.978s, episode steps: 378, steps per second: 191, episode reward: 289.972, mean reward:  0.767 [-17.918, 100.000], mean action: 0.934 [0.000, 3.000],  loss: 6.794896, mae: 57.995564, mean_q: 79.605554, mean_eps: 0.100000\n",
      " 351021/1000000: episode: 1011, duration: 1.266s, episode steps: 243, steps per second: 192, episode reward: -209.046, mean reward: -0.860 [-100.000, 31.147], mean action: 1.852 [0.000, 3.000],  loss: 7.439378, mae: 57.637608, mean_q: 78.914095, mean_eps: 0.100000\n",
      " 351153/1000000: episode: 1012, duration: 0.645s, episode steps: 132, steps per second: 205, episode reward:  9.560, mean reward:  0.072 [-100.000, 10.045], mean action: 1.871 [0.000, 3.000],  loss: 12.101872, mae: 56.330534, mean_q: 77.103913, mean_eps: 0.100000\n",
      " 351522/1000000: episode: 1013, duration: 1.870s, episode steps: 369, steps per second: 197, episode reward: 284.531, mean reward:  0.771 [-17.159, 100.000], mean action: 0.840 [0.000, 3.000],  loss: 19.343619, mae: 58.315293, mean_q: 79.582521, mean_eps: 0.100000\n",
      " 351624/1000000: episode: 1014, duration: 0.534s, episode steps: 102, steps per second: 191, episode reward: 33.151, mean reward:  0.325 [-100.000, 12.789], mean action: 1.961 [0.000, 3.000],  loss: 8.114402, mae: 56.671725, mean_q: 77.442989, mean_eps: 0.100000\n",
      " 351828/1000000: episode: 1015, duration: 1.016s, episode steps: 204, steps per second: 201, episode reward: 254.824, mean reward:  1.249 [-2.843, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 12.936869, mae: 61.364325, mean_q: 83.401191, mean_eps: 0.100000\n",
      " 351948/1000000: episode: 1016, duration: 0.640s, episode steps: 120, steps per second: 187, episode reward: 51.331, mean reward:  0.428 [-100.000, 14.585], mean action: 1.625 [0.000, 3.000],  loss: 11.641016, mae: 61.431126, mean_q: 83.025281, mean_eps: 0.100000\n",
      " 352072/1000000: episode: 1017, duration: 0.661s, episode steps: 124, steps per second: 188, episode reward: 30.754, mean reward:  0.248 [-100.000, 16.630], mean action: 1.863 [0.000, 3.000],  loss: 28.908157, mae: 65.695739, mean_q: 88.281628, mean_eps: 0.100000\n",
      " 352203/1000000: episode: 1018, duration: 0.697s, episode steps: 131, steps per second: 188, episode reward: 31.791, mean reward:  0.243 [-100.000, 16.136], mean action: 1.679 [0.000, 3.000],  loss: 11.096935, mae: 66.380503, mean_q: 89.146247, mean_eps: 0.100000\n",
      " 352335/1000000: episode: 1019, duration: 0.673s, episode steps: 132, steps per second: 196, episode reward: 45.278, mean reward:  0.343 [-100.000, 11.101], mean action: 1.644 [0.000, 3.000],  loss: 15.653011, mae: 66.926979, mean_q: 89.750061, mean_eps: 0.100000\n",
      " 353205/1000000: episode: 1020, duration: 5.467s, episode steps: 870, steps per second: 159, episode reward: -253.959, mean reward: -0.292 [-100.000, 27.828], mean action: 1.799 [0.000, 3.000],  loss: 21.153377, mae: 59.405273, mean_q: 79.253253, mean_eps: 0.100000\n",
      " 354205/1000000: episode: 1021, duration: 6.539s, episode steps: 1000, steps per second: 153, episode reward: -23.540, mean reward: -0.024 [-24.857, 22.769], mean action: 2.158 [0.000, 3.000],  loss: 6.561877, mae: 35.088604, mean_q: 45.522161, mean_eps: 0.100000\n",
      " 354474/1000000: episode: 1022, duration: 1.405s, episode steps: 269, steps per second: 192, episode reward: 281.817, mean reward:  1.048 [-11.267, 100.000], mean action: 1.569 [0.000, 3.000],  loss: 2.735304, mae: 31.869516, mean_q: 44.043465, mean_eps: 0.100000\n",
      " 355474/1000000: episode: 1023, duration: 7.033s, episode steps: 1000, steps per second: 142, episode reward: -24.243, mean reward: -0.024 [-21.283, 16.042], mean action: 1.986 [0.000, 3.000],  loss: 5.051472, mae: 34.587622, mean_q: 46.774827, mean_eps: 0.100000\n",
      " 356176/1000000: episode: 1024, duration: 4.222s, episode steps: 702, steps per second: 166, episode reward: 192.583, mean reward:  0.274 [-21.961, 100.000], mean action: 1.556 [0.000, 3.000],  loss: 3.018936, mae: 28.292229, mean_q: 38.791159, mean_eps: 0.100000\n",
      " 356346/1000000: episode: 1025, duration: 0.974s, episode steps: 170, steps per second: 175, episode reward: 293.914, mean reward:  1.729 [-9.047, 100.000], mean action: 1.218 [0.000, 3.000],  loss: 2.860628, mae: 37.570106, mean_q: 51.264835, mean_eps: 0.100000\n",
      " 356983/1000000: episode: 1026, duration: 3.526s, episode steps: 637, steps per second: 181, episode reward: 278.509, mean reward:  0.437 [-19.911, 100.000], mean action: 0.721 [0.000, 3.000],  loss: 3.887791, mae: 43.225691, mean_q: 58.678030, mean_eps: 0.100000\n",
      " 357385/1000000: episode: 1027, duration: 1.988s, episode steps: 402, steps per second: 202, episode reward: 286.303, mean reward:  0.712 [-19.133, 100.000], mean action: 0.968 [0.000, 3.000],  loss: 3.507949, mae: 48.318682, mean_q: 65.213371, mean_eps: 0.100000\n",
      " 357715/1000000: episode: 1028, duration: 1.660s, episode steps: 330, steps per second: 199, episode reward: 241.143, mean reward:  0.731 [-9.709, 100.000], mean action: 1.512 [0.000, 3.000],  loss: 3.517667, mae: 45.334244, mean_q: 61.200929, mean_eps: 0.100000\n",
      " 357938/1000000: episode: 1029, duration: 1.081s, episode steps: 223, steps per second: 206, episode reward: 260.437, mean reward:  1.168 [-10.261, 100.000], mean action: 1.090 [0.000, 3.000],  loss: 5.729302, mae: 45.926028, mean_q: 62.245080, mean_eps: 0.100000\n",
      " 358167/1000000: episode: 1030, duration: 1.109s, episode steps: 229, steps per second: 207, episode reward: 251.726, mean reward:  1.099 [-17.395, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 7.552027, mae: 46.488302, mean_q: 63.115098, mean_eps: 0.100000\n",
      " 358742/1000000: episode: 1031, duration: 3.023s, episode steps: 575, steps per second: 190, episode reward: 151.736, mean reward:  0.264 [-19.823, 100.000], mean action: 1.497 [0.000, 3.000],  loss: 8.426767, mae: 43.729265, mean_q: 59.694610, mean_eps: 0.100000\n",
      " 359255/1000000: episode: 1032, duration: 2.613s, episode steps: 513, steps per second: 196, episode reward: 277.254, mean reward:  0.540 [-18.887, 100.000], mean action: 0.850 [0.000, 3.000],  loss: 9.980111, mae: 37.344457, mean_q: 51.478500, mean_eps: 0.100000\n",
      " 359425/1000000: episode: 1033, duration: 0.824s, episode steps: 170, steps per second: 206, episode reward: 31.889, mean reward:  0.188 [-100.000, 17.737], mean action: 1.847 [0.000, 3.000],  loss: 9.489503, mae: 37.667152, mean_q: 52.114269, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 359741/1000000: episode: 1034, duration: 1.569s, episode steps: 316, steps per second: 201, episode reward: 255.422, mean reward:  0.808 [-13.983, 100.000], mean action: 0.813 [0.000, 3.000],  loss: 25.345198, mae: 50.598294, mean_q: 69.364751, mean_eps: 0.100000\n",
      " 360304/1000000: episode: 1035, duration: 2.903s, episode steps: 563, steps per second: 194, episode reward: 156.165, mean reward:  0.277 [-19.751, 100.000], mean action: 1.849 [0.000, 3.000],  loss: 12.068429, mae: 48.480913, mean_q: 64.274188, mean_eps: 0.100000\n",
      " 361276/1000000: episode: 1036, duration: 5.348s, episode steps: 972, steps per second: 182, episode reward: 256.542, mean reward:  0.264 [-20.833, 100.000], mean action: 1.718 [0.000, 3.000],  loss: 10.682830, mae: 36.658955, mean_q: 41.366155, mean_eps: 0.100000\n",
      " 362276/1000000: episode: 1037, duration: 5.613s, episode steps: 1000, steps per second: 178, episode reward: -2.267, mean reward: -0.002 [-20.482, 21.361], mean action: 1.580 [0.000, 3.000],  loss: 1.974191, mae: 38.257207, mean_q: 52.240098, mean_eps: 0.100000\n",
      " 362391/1000000: episode: 1038, duration: 0.554s, episode steps: 115, steps per second: 208, episode reward: 14.102, mean reward:  0.123 [-100.000, 18.426], mean action: 1.617 [0.000, 3.000],  loss: 1.869327, mae: 25.734176, mean_q: 35.731867, mean_eps: 0.100000\n",
      " 362672/1000000: episode: 1039, duration: 1.360s, episode steps: 281, steps per second: 207, episode reward: 210.213, mean reward:  0.748 [-17.886, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 17.477926, mae: 29.666342, mean_q: 40.976255, mean_eps: 0.100000\n",
      " 363149/1000000: episode: 1040, duration: 2.442s, episode steps: 477, steps per second: 195, episode reward: 171.390, mean reward:  0.359 [-19.527, 100.000], mean action: 1.857 [0.000, 3.000],  loss: 14.396278, mae: 35.502178, mean_q: 49.056154, mean_eps: 0.100000\n",
      " 363313/1000000: episode: 1041, duration: 0.785s, episode steps: 164, steps per second: 209, episode reward: 25.449, mean reward:  0.155 [-100.000, 18.074], mean action: 1.695 [0.000, 3.000],  loss: 16.795487, mae: 38.850662, mean_q: 53.272153, mean_eps: 0.100000\n",
      " 363445/1000000: episode: 1042, duration: 0.631s, episode steps: 132, steps per second: 209, episode reward: -1.549, mean reward: -0.012 [-100.000, 11.990], mean action: 1.682 [0.000, 3.000],  loss: 13.947559, mae: 42.401561, mean_q: 58.087120, mean_eps: 0.100000\n",
      " 363654/1000000: episode: 1043, duration: 1.004s, episode steps: 209, steps per second: 208, episode reward: 258.569, mean reward:  1.237 [-2.697, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 30.559694, mae: 43.899729, mean_q: 60.026916, mean_eps: 0.100000\n",
      " 363882/1000000: episode: 1044, duration: 1.109s, episode steps: 228, steps per second: 206, episode reward: 276.250, mean reward:  1.212 [-2.885, 100.000], mean action: 1.338 [0.000, 3.000],  loss: 14.778045, mae: 46.709749, mean_q: 63.893710, mean_eps: 0.100000\n",
      " 364164/1000000: episode: 1045, duration: 1.367s, episode steps: 282, steps per second: 206, episode reward: 277.495, mean reward:  0.984 [-9.719, 100.000], mean action: 0.993 [0.000, 3.000],  loss: 15.105063, mae: 55.773513, mean_q: 75.410719, mean_eps: 0.100000\n",
      " 364454/1000000: episode: 1046, duration: 1.413s, episode steps: 290, steps per second: 205, episode reward: 278.899, mean reward:  0.962 [-17.411, 100.000], mean action: 0.990 [0.000, 3.000],  loss: 11.450224, mae: 57.573018, mean_q: 77.622775, mean_eps: 0.100000\n",
      " 365258/1000000: episode: 1047, duration: 4.186s, episode steps: 804, steps per second: 192, episode reward: 202.665, mean reward:  0.252 [-19.555, 100.000], mean action: 1.874 [0.000, 3.000],  loss: 6.239019, mae: 42.957389, mean_q: 58.324248, mean_eps: 0.100000\n",
      " 365407/1000000: episode: 1048, duration: 0.718s, episode steps: 149, steps per second: 208, episode reward: 29.242, mean reward:  0.196 [-100.000, 15.135], mean action: 1.631 [0.000, 3.000],  loss: 5.160136, mae: 27.143083, mean_q: 37.194349, mean_eps: 0.100000\n",
      " 365627/1000000: episode: 1049, duration: 1.076s, episode steps: 220, steps per second: 204, episode reward: -15.736, mean reward: -0.072 [-100.000, 27.631], mean action: 1.723 [0.000, 3.000],  loss: 15.694709, mae: 32.700528, mean_q: 44.647751, mean_eps: 0.100000\n",
      " 365858/1000000: episode: 1050, duration: 1.120s, episode steps: 231, steps per second: 206, episode reward: 308.930, mean reward:  1.337 [-10.715, 100.000], mean action: 1.554 [0.000, 3.000],  loss: 16.379946, mae: 34.046080, mean_q: 46.644554, mean_eps: 0.100000\n",
      " 366185/1000000: episode: 1051, duration: 1.609s, episode steps: 327, steps per second: 203, episode reward: 230.127, mean reward:  0.704 [-17.819, 100.000], mean action: 1.694 [0.000, 3.000],  loss: 17.756363, mae: 42.556848, mean_q: 57.878577, mean_eps: 0.100000\n",
      " 366359/1000000: episode: 1052, duration: 0.838s, episode steps: 174, steps per second: 208, episode reward: 289.887, mean reward:  1.666 [-9.517, 100.000], mean action: 1.466 [0.000, 3.000],  loss: 13.343563, mae: 49.931492, mean_q: 67.931203, mean_eps: 0.100000\n",
      " 366447/1000000: episode: 1053, duration: 0.422s, episode steps:  88, steps per second: 208, episode reward: -20.947, mean reward: -0.238 [-100.000, 19.819], mean action: 1.557 [0.000, 3.000],  loss: 20.430111, mae: 48.974183, mean_q: 66.597650, mean_eps: 0.100000\n",
      " 366932/1000000: episode: 1054, duration: 2.464s, episode steps: 485, steps per second: 197, episode reward: 269.836, mean reward:  0.556 [-20.693, 100.000], mean action: 1.388 [0.000, 3.000],  loss: 22.872891, mae: 49.180259, mean_q: 67.072239, mean_eps: 0.100000\n",
      " 367395/1000000: episode: 1055, duration: 2.390s, episode steps: 463, steps per second: 194, episode reward: 212.328, mean reward:  0.459 [-18.965, 100.000], mean action: 1.853 [0.000, 3.000],  loss: 11.465675, mae: 50.351373, mean_q: 68.235265, mean_eps: 0.100000\n",
      " 367913/1000000: episode: 1056, duration: 2.630s, episode steps: 518, steps per second: 197, episode reward: 262.438, mean reward:  0.507 [-18.723, 100.000], mean action: 1.216 [0.000, 3.000],  loss: 6.283251, mae: 46.391335, mean_q: 62.842289, mean_eps: 0.100000\n",
      " 368163/1000000: episode: 1057, duration: 1.213s, episode steps: 250, steps per second: 206, episode reward: -37.687, mean reward: -0.151 [-100.000, 18.296], mean action: 1.844 [0.000, 3.000],  loss: 3.616053, mae: 48.436917, mean_q: 65.700246, mean_eps: 0.100000\n",
      " 368544/1000000: episode: 1058, duration: 1.897s, episode steps: 381, steps per second: 201, episode reward: 261.503, mean reward:  0.686 [-17.829, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 4.006745, mae: 57.769339, mean_q: 77.928788, mean_eps: 0.100000\n",
      " 368738/1000000: episode: 1059, duration: 0.939s, episode steps: 194, steps per second: 207, episode reward: 269.279, mean reward:  1.388 [-2.960, 100.000], mean action: 1.572 [0.000, 3.000],  loss: 2.890894, mae: 59.511541, mean_q: 80.088454, mean_eps: 0.100000\n",
      " 369330/1000000: episode: 1060, duration: 3.031s, episode steps: 592, steps per second: 195, episode reward: 232.571, mean reward:  0.393 [-19.213, 100.000], mean action: 2.221 [0.000, 3.000],  loss: 3.616689, mae: 57.023108, mean_q: 76.454058, mean_eps: 0.100000\n",
      " 369529/1000000: episode: 1061, duration: 0.969s, episode steps: 199, steps per second: 205, episode reward: 277.043, mean reward:  1.392 [-11.460, 100.000], mean action: 1.437 [0.000, 3.000],  loss: 5.101060, mae: 48.037973, mean_q: 65.202520, mean_eps: 0.100000\n",
      " 369821/1000000: episode: 1062, duration: 1.436s, episode steps: 292, steps per second: 203, episode reward: 255.861, mean reward:  0.876 [-9.446, 100.000], mean action: 1.147 [0.000, 3.000],  loss: 4.092521, mae: 47.972112, mean_q: 65.126623, mean_eps: 0.100000\n",
      " 370185/1000000: episode: 1063, duration: 1.821s, episode steps: 364, steps per second: 200, episode reward: 218.353, mean reward:  0.600 [-11.146, 100.000], mean action: 1.255 [0.000, 3.000],  loss: 4.882202, mae: 47.910161, mean_q: 64.965258, mean_eps: 0.100000\n",
      " 371185/1000000: episode: 1064, duration: 5.280s, episode steps: 1000, steps per second: 189, episode reward: 83.791, mean reward:  0.084 [-18.637, 22.148], mean action: 1.401 [0.000, 3.000],  loss: 3.260511, mae: 44.611991, mean_q: 60.419815, mean_eps: 0.100000\n",
      " 371431/1000000: episode: 1065, duration: 1.215s, episode steps: 246, steps per second: 203, episode reward: 30.079, mean reward:  0.122 [-100.000, 12.640], mean action: 1.976 [0.000, 3.000],  loss: 0.998811, mae: 38.261476, mean_q: 52.111575, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 371677/1000000: episode: 1066, duration: 1.216s, episode steps: 246, steps per second: 202, episode reward: 245.887, mean reward:  1.000 [-9.130, 100.000], mean action: 1.504 [0.000, 3.000],  loss: 10.485647, mae: 43.433453, mean_q: 58.906519, mean_eps: 0.100000\n",
      " 372007/1000000: episode: 1067, duration: 1.671s, episode steps: 330, steps per second: 197, episode reward: 285.105, mean reward:  0.864 [-17.407, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 7.244094, mae: 47.631912, mean_q: 64.311901, mean_eps: 0.100000\n",
      " 372296/1000000: episode: 1068, duration: 1.414s, episode steps: 289, steps per second: 204, episode reward: 306.777, mean reward:  1.062 [-9.196, 100.000], mean action: 1.394 [0.000, 3.000],  loss: 7.670730, mae: 52.198240, mean_q: 70.358883, mean_eps: 0.100000\n",
      " 372576/1000000: episode: 1069, duration: 1.374s, episode steps: 280, steps per second: 204, episode reward: 250.406, mean reward:  0.894 [-17.728, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 5.272234, mae: 51.257295, mean_q: 69.499406, mean_eps: 0.100000\n",
      " 372874/1000000: episode: 1070, duration: 1.447s, episode steps: 298, steps per second: 206, episode reward: 262.680, mean reward:  0.881 [-17.557, 100.000], mean action: 0.997 [0.000, 3.000],  loss: 5.255665, mae: 52.544032, mean_q: 71.401829, mean_eps: 0.100000\n",
      " 373170/1000000: episode: 1071, duration: 1.466s, episode steps: 296, steps per second: 202, episode reward: 245.448, mean reward:  0.829 [-17.851, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 4.696374, mae: 55.892244, mean_q: 75.649588, mean_eps: 0.100000\n",
      " 373435/1000000: episode: 1072, duration: 1.303s, episode steps: 265, steps per second: 203, episode reward: 262.506, mean reward:  0.991 [-10.951, 100.000], mean action: 1.313 [0.000, 3.000],  loss: 4.870973, mae: 57.056606, mean_q: 77.141313, mean_eps: 0.100000\n",
      " 373712/1000000: episode: 1073, duration: 1.337s, episode steps: 277, steps per second: 207, episode reward: 264.336, mean reward:  0.954 [-9.765, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 3.385881, mae: 56.268827, mean_q: 76.041112, mean_eps: 0.100000\n",
      " 373999/1000000: episode: 1074, duration: 1.425s, episode steps: 287, steps per second: 201, episode reward: 260.260, mean reward:  0.907 [-8.961, 100.000], mean action: 1.362 [0.000, 3.000],  loss: 3.075409, mae: 55.882001, mean_q: 75.276221, mean_eps: 0.100000\n",
      " 374239/1000000: episode: 1075, duration: 1.177s, episode steps: 240, steps per second: 204, episode reward: 294.844, mean reward:  1.229 [-20.469, 100.000], mean action: 1.746 [0.000, 3.000],  loss: 5.273709, mae: 55.715833, mean_q: 75.076049, mean_eps: 0.100000\n",
      " 374566/1000000: episode: 1076, duration: 1.619s, episode steps: 327, steps per second: 202, episode reward: 259.367, mean reward:  0.793 [-3.065, 100.000], mean action: 1.434 [0.000, 3.000],  loss: 4.667313, mae: 57.080503, mean_q: 76.855745, mean_eps: 0.100000\n",
      " 375042/1000000: episode: 1077, duration: 2.497s, episode steps: 476, steps per second: 191, episode reward: -258.009, mean reward: -0.542 [-100.000, 11.287], mean action: 1.601 [0.000, 3.000],  loss: 4.797052, mae: 57.367732, mean_q: 77.429231, mean_eps: 0.100000\n",
      " 375353/1000000: episode: 1078, duration: 1.521s, episode steps: 311, steps per second: 204, episode reward: 268.919, mean reward:  0.865 [-18.294, 100.000], mean action: 1.125 [0.000, 3.000],  loss: 2.753561, mae: 51.782350, mean_q: 68.607829, mean_eps: 0.100000\n",
      " 375714/1000000: episode: 1079, duration: 1.778s, episode steps: 361, steps per second: 203, episode reward: 288.696, mean reward:  0.800 [-19.598, 100.000], mean action: 0.956 [0.000, 3.000],  loss: 2.460061, mae: 51.086506, mean_q: 67.379675, mean_eps: 0.100000\n",
      " 375987/1000000: episode: 1080, duration: 1.381s, episode steps: 273, steps per second: 198, episode reward: 278.765, mean reward:  1.021 [-10.349, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 3.761290, mae: 55.769487, mean_q: 72.561461, mean_eps: 0.100000\n",
      " 376527/1000000: episode: 1081, duration: 2.840s, episode steps: 540, steps per second: 190, episode reward: 269.206, mean reward:  0.499 [-18.679, 100.000], mean action: 0.713 [0.000, 3.000],  loss: 4.148267, mae: 59.063038, mean_q: 79.357864, mean_eps: 0.100000\n",
      " 376865/1000000: episode: 1082, duration: 1.677s, episode steps: 338, steps per second: 202, episode reward: 307.122, mean reward:  0.909 [-17.545, 100.000], mean action: 1.018 [0.000, 3.000],  loss: 2.101912, mae: 55.616562, mean_q: 75.311861, mean_eps: 0.100000\n",
      " 377104/1000000: episode: 1083, duration: 1.151s, episode steps: 239, steps per second: 208, episode reward: 259.097, mean reward:  1.084 [-17.439, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 3.409707, mae: 54.486964, mean_q: 73.915305, mean_eps: 0.100000\n",
      " 377651/1000000: episode: 1084, duration: 2.780s, episode steps: 547, steps per second: 197, episode reward: 295.468, mean reward:  0.540 [-18.649, 100.000], mean action: 1.053 [0.000, 3.000],  loss: 3.242643, mae: 55.384363, mean_q: 75.176987, mean_eps: 0.100000\n",
      " 377937/1000000: episode: 1085, duration: 1.410s, episode steps: 286, steps per second: 203, episode reward: 266.839, mean reward:  0.933 [-18.097, 100.000], mean action: 1.063 [0.000, 3.000],  loss: 2.531993, mae: 54.305241, mean_q: 73.617713, mean_eps: 0.100000\n",
      " 378937/1000000: episode: 1086, duration: 5.181s, episode steps: 1000, steps per second: 193, episode reward: 157.168, mean reward:  0.157 [-19.182, 22.684], mean action: 0.929 [0.000, 3.000],  loss: 2.487582, mae: 47.379046, mean_q: 64.434363, mean_eps: 0.100000\n",
      " 379202/1000000: episode: 1087, duration: 1.287s, episode steps: 265, steps per second: 206, episode reward: 275.480, mean reward:  1.040 [-4.551, 100.000], mean action: 1.045 [0.000, 3.000],  loss: 1.244469, mae: 39.613090, mean_q: 53.861057, mean_eps: 0.100000\n",
      " 379474/1000000: episode: 1088, duration: 1.317s, episode steps: 272, steps per second: 207, episode reward: 307.157, mean reward:  1.129 [-9.859, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 3.161858, mae: 46.142827, mean_q: 62.510994, mean_eps: 0.100000\n",
      " 380219/1000000: episode: 1089, duration: 3.871s, episode steps: 745, steps per second: 192, episode reward: 263.813, mean reward:  0.354 [-17.653, 100.000], mean action: 1.709 [0.000, 3.000],  loss: 3.503606, mae: 52.375277, mean_q: 70.737835, mean_eps: 0.100000\n",
      " 380389/1000000: episode: 1090, duration: 0.817s, episode steps: 170, steps per second: 208, episode reward: 250.075, mean reward:  1.471 [-2.060, 100.000], mean action: 1.253 [0.000, 3.000],  loss: 1.360519, mae: 47.157224, mean_q: 63.641263, mean_eps: 0.100000\n",
      " 380705/1000000: episode: 1091, duration: 1.549s, episode steps: 316, steps per second: 204, episode reward: 303.262, mean reward:  0.960 [-6.597, 100.000], mean action: 1.076 [0.000, 3.000],  loss: 2.358687, mae: 49.855741, mean_q: 67.553816, mean_eps: 0.100000\n",
      " 380958/1000000: episode: 1092, duration: 1.216s, episode steps: 253, steps per second: 208, episode reward: 308.147, mean reward:  1.218 [-10.029, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 2.813335, mae: 57.070579, mean_q: 77.279493, mean_eps: 0.100000\n",
      " 381228/1000000: episode: 1093, duration: 1.347s, episode steps: 270, steps per second: 201, episode reward: 242.983, mean reward:  0.900 [-5.808, 100.000], mean action: 2.496 [0.000, 3.000],  loss: 6.183547, mae: 65.337413, mean_q: 88.490696, mean_eps: 0.100000\n",
      " 381754/1000000: episode: 1094, duration: 2.701s, episode steps: 526, steps per second: 195, episode reward: 268.223, mean reward:  0.510 [-20.458, 100.000], mean action: 0.876 [0.000, 3.000],  loss: 3.827906, mae: 62.240374, mean_q: 84.684740, mean_eps: 0.100000\n",
      " 382039/1000000: episode: 1095, duration: 1.384s, episode steps: 285, steps per second: 206, episode reward: 248.775, mean reward:  0.873 [-17.556, 100.000], mean action: 0.965 [0.000, 3.000],  loss: 2.740955, mae: 55.607783, mean_q: 75.687529, mean_eps: 0.100000\n",
      " 382234/1000000: episode: 1096, duration: 0.941s, episode steps: 195, steps per second: 207, episode reward: 293.987, mean reward:  1.508 [-17.093, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 2.912431, mae: 56.938851, mean_q: 77.087074, mean_eps: 0.100000\n",
      " 382432/1000000: episode: 1097, duration: 0.949s, episode steps: 198, steps per second: 209, episode reward: 244.636, mean reward:  1.236 [-11.128, 100.000], mean action: 1.091 [0.000, 3.000],  loss: 1.363036, mae: 58.201656, mean_q: 78.495629, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 382657/1000000: episode: 1098, duration: 1.087s, episode steps: 225, steps per second: 207, episode reward: 301.899, mean reward:  1.342 [-9.260, 100.000], mean action: 1.436 [0.000, 3.000],  loss: 3.321635, mae: 62.619747, mean_q: 84.374754, mean_eps: 0.100000\n",
      " 383055/1000000: episode: 1099, duration: 1.945s, episode steps: 398, steps per second: 205, episode reward: 275.869, mean reward:  0.693 [-18.424, 100.000], mean action: 0.897 [0.000, 3.000],  loss: 4.492318, mae: 66.326195, mean_q: 89.457513, mean_eps: 0.100000\n",
      " 383214/1000000: episode: 1100, duration: 0.758s, episode steps: 159, steps per second: 210, episode reward: 256.035, mean reward:  1.610 [-7.414, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 4.750833, mae: 64.340017, mean_q: 86.826908, mean_eps: 0.100000\n",
      " 383438/1000000: episode: 1101, duration: 1.073s, episode steps: 224, steps per second: 209, episode reward: 298.818, mean reward:  1.334 [-19.807, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 4.234801, mae: 66.212662, mean_q: 89.333478, mean_eps: 0.100000\n",
      " 383554/1000000: episode: 1102, duration: 0.557s, episode steps: 116, steps per second: 208, episode reward: 11.430, mean reward:  0.099 [-100.000, 13.539], mean action: 1.552 [0.000, 3.000],  loss: 5.419674, mae: 64.548876, mean_q: 87.185020, mean_eps: 0.100000\n",
      " 383751/1000000: episode: 1103, duration: 0.941s, episode steps: 197, steps per second: 209, episode reward: 283.132, mean reward:  1.437 [-8.235, 100.000], mean action: 1.442 [0.000, 3.000],  loss: 13.744654, mae: 65.874946, mean_q: 89.205757, mean_eps: 0.100000\n",
      " 384040/1000000: episode: 1104, duration: 1.439s, episode steps: 289, steps per second: 201, episode reward: 297.929, mean reward:  1.031 [-18.210, 100.000], mean action: 0.958 [0.000, 3.000],  loss: 5.498708, mae: 67.208001, mean_q: 90.926944, mean_eps: 0.100000\n",
      " 384139/1000000: episode: 1105, duration: 0.468s, episode steps:  99, steps per second: 212, episode reward: 51.382, mean reward:  0.519 [-100.000, 16.720], mean action: 1.616 [0.000, 3.000],  loss: 3.969100, mae: 69.991081, mean_q: 94.274248, mean_eps: 0.100000\n",
      " 385139/1000000: episode: 1106, duration: 5.440s, episode steps: 1000, steps per second: 184, episode reward: 129.699, mean reward:  0.130 [-18.500, 16.511], mean action: 1.350 [0.000, 3.000],  loss: 7.043948, mae: 60.663816, mean_q: 81.349995, mean_eps: 0.100000\n",
      " 386139/1000000: episode: 1107, duration: 5.317s, episode steps: 1000, steps per second: 188, episode reward: 159.378, mean reward:  0.159 [-19.156, 22.810], mean action: 1.525 [0.000, 3.000],  loss: 2.204850, mae: 45.267519, mean_q: 60.966345, mean_eps: 0.100000\n",
      " 386234/1000000: episode: 1108, duration: 0.460s, episode steps:  95, steps per second: 207, episode reward: -30.256, mean reward: -0.318 [-100.000, 16.779], mean action: 1.305 [0.000, 3.000],  loss: 1.278840, mae: 42.995887, mean_q: 57.779206, mean_eps: 0.100000\n",
      " 386333/1000000: episode: 1109, duration: 0.466s, episode steps:  99, steps per second: 212, episode reward: -44.100, mean reward: -0.445 [-100.000,  9.770], mean action: 1.152 [0.000, 3.000],  loss: 2.440187, mae: 44.889978, mean_q: 60.508134, mean_eps: 0.100000\n",
      " 386401/1000000: episode: 1110, duration: 0.327s, episode steps:  68, steps per second: 208, episode reward: 30.960, mean reward:  0.455 [-100.000, 14.874], mean action: 1.471 [0.000, 3.000],  loss: 30.123754, mae: 49.086511, mean_q: 66.404570, mean_eps: 0.100000\n",
      " 386502/1000000: episode: 1111, duration: 0.477s, episode steps: 101, steps per second: 212, episode reward: -71.509, mean reward: -0.708 [-100.000,  7.252], mean action: 1.248 [0.000, 3.000],  loss: 20.665844, mae: 51.326293, mean_q: 69.657715, mean_eps: 0.100000\n",
      " 386683/1000000: episode: 1112, duration: 0.880s, episode steps: 181, steps per second: 206, episode reward: 266.107, mean reward:  1.470 [-3.398, 100.000], mean action: 1.519 [0.000, 3.000],  loss: 16.206152, mae: 57.598646, mean_q: 77.543408, mean_eps: 0.100000\n",
      " 386803/1000000: episode: 1113, duration: 0.573s, episode steps: 120, steps per second: 210, episode reward: -28.334, mean reward: -0.236 [-100.000, 10.180], mean action: 1.308 [0.000, 3.000],  loss: 13.127928, mae: 60.359180, mean_q: 81.218329, mean_eps: 0.100000\n",
      " 386920/1000000: episode: 1114, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: 21.035, mean reward:  0.180 [-100.000, 16.888], mean action: 1.598 [0.000, 3.000],  loss: 12.144586, mae: 66.172273, mean_q: 88.247370, mean_eps: 0.100000\n",
      " 387151/1000000: episode: 1115, duration: 1.118s, episode steps: 231, steps per second: 207, episode reward: 255.244, mean reward:  1.105 [-9.349, 100.000], mean action: 1.173 [0.000, 3.000],  loss: 13.666793, mae: 71.810718, mean_q: 95.325358, mean_eps: 0.100000\n",
      " 387245/1000000: episode: 1116, duration: 0.449s, episode steps:  94, steps per second: 209, episode reward: 48.878, mean reward:  0.520 [-100.000, 13.911], mean action: 1.553 [0.000, 3.000],  loss: 11.126795, mae: 70.214531, mean_q: 92.963585, mean_eps: 0.100000\n",
      " 387395/1000000: episode: 1117, duration: 0.719s, episode steps: 150, steps per second: 209, episode reward: -2.361, mean reward: -0.016 [-100.000, 18.050], mean action: 1.780 [0.000, 3.000],  loss: 10.541016, mae: 68.827876, mean_q: 90.131038, mean_eps: 0.100000\n",
      " 387953/1000000: episode: 1118, duration: 2.775s, episode steps: 558, steps per second: 201, episode reward: 257.351, mean reward:  0.461 [-17.409, 100.000], mean action: 0.586 [0.000, 3.000],  loss: 11.714755, mae: 61.950973, mean_q: 80.725314, mean_eps: 0.100000\n",
      " 388047/1000000: episode: 1119, duration: 0.451s, episode steps:  94, steps per second: 208, episode reward: -49.498, mean reward: -0.527 [-100.000, 10.844], mean action: 1.255 [0.000, 3.000],  loss: 8.213040, mae: 50.897401, mean_q: 66.386186, mean_eps: 0.100000\n",
      " 388359/1000000: episode: 1120, duration: 1.537s, episode steps: 312, steps per second: 203, episode reward: -78.002, mean reward: -0.250 [-100.000, 13.612], mean action: 1.577 [0.000, 3.000],  loss: 18.401792, mae: 54.065842, mean_q: 66.182572, mean_eps: 0.100000\n",
      " 388505/1000000: episode: 1121, duration: 0.695s, episode steps: 146, steps per second: 210, episode reward: -68.566, mean reward: -0.470 [-100.000, 16.538], mean action: 1.623 [0.000, 3.000],  loss: 25.873556, mae: 50.382166, mean_q: 58.895124, mean_eps: 0.100000\n",
      " 389505/1000000: episode: 1122, duration: 5.977s, episode steps: 1000, steps per second: 167, episode reward: -58.371, mean reward: -0.058 [-4.738,  5.174], mean action: 1.563 [0.000, 3.000],  loss: 6.948094, mae: 40.087778, mean_q: 43.769878, mean_eps: 0.100000\n",
      " 390505/1000000: episode: 1123, duration: 5.866s, episode steps: 1000, steps per second: 170, episode reward: -29.866, mean reward: -0.030 [-4.139,  5.370], mean action: 1.863 [0.000, 3.000],  loss: 1.101454, mae: 26.634890, mean_q: 34.699570, mean_eps: 0.100000\n",
      " 391493/1000000: episode: 1124, duration: 5.535s, episode steps: 988, steps per second: 179, episode reward: 141.696, mean reward:  0.143 [-19.702, 100.000], mean action: 1.528 [0.000, 3.000],  loss: 3.416102, mae: 42.588248, mean_q: 57.509501, mean_eps: 0.100000\n",
      " 391642/1000000: episode: 1125, duration: 0.718s, episode steps: 149, steps per second: 208, episode reward: -14.526, mean reward: -0.097 [-100.000,  9.562], mean action: 1.570 [0.000, 3.000],  loss: 3.239678, mae: 42.633620, mean_q: 57.665137, mean_eps: 0.100000\n",
      " 391863/1000000: episode: 1126, duration: 1.063s, episode steps: 221, steps per second: 208, episode reward: -100.013, mean reward: -0.453 [-100.000,  5.480], mean action: 1.543 [0.000, 3.000],  loss: 5.787800, mae: 42.409413, mean_q: 49.161895, mean_eps: 0.100000\n",
      " 392692/1000000: episode: 1127, duration: 4.392s, episode steps: 829, steps per second: 189, episode reward: -267.540, mean reward: -0.323 [-100.000, 21.260], mean action: 1.362 [0.000, 3.000],  loss: 12.272275, mae: 43.197911, mean_q: 18.291656, mean_eps: 0.100000\n",
      " 392779/1000000: episode: 1128, duration: 0.423s, episode steps:  87, steps per second: 205, episode reward: -99.406, mean reward: -1.143 [-100.000, 13.023], mean action: 1.828 [0.000, 3.000],  loss: 9.288858, mae: 36.426863, mean_q: -8.452523, mean_eps: 0.100000\n",
      " 393256/1000000: episode: 1129, duration: 2.419s, episode steps: 477, steps per second: 197, episode reward: 195.808, mean reward:  0.410 [-18.062, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 9.443808, mae: 34.189829, mean_q: 7.093807, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 393359/1000000: episode: 1130, duration: 0.496s, episode steps: 103, steps per second: 208, episode reward: -6.664, mean reward: -0.065 [-100.000, 18.975], mean action: 1.505 [0.000, 3.000],  loss: 9.435954, mae: 35.722897, mean_q: 21.834626, mean_eps: 0.100000\n",
      " 394359/1000000: episode: 1131, duration: 5.319s, episode steps: 1000, steps per second: 188, episode reward: 89.497, mean reward:  0.089 [-19.206, 21.715], mean action: 1.005 [0.000, 3.000],  loss: 11.314097, mae: 37.927577, mean_q: 47.811316, mean_eps: 0.100000\n",
      " 394519/1000000: episode: 1132, duration: 0.766s, episode steps: 160, steps per second: 209, episode reward: -238.055, mean reward: -1.488 [-100.000,  4.244], mean action: 1.038 [0.000, 2.000],  loss: 1.189346, mae: 32.395903, mean_q: 44.860846, mean_eps: 0.100000\n",
      " 394607/1000000: episode: 1133, duration: 0.418s, episode steps:  88, steps per second: 211, episode reward: -4.493, mean reward: -0.051 [-100.000, 23.346], mean action: 1.239 [0.000, 3.000],  loss: 9.885989, mae: 34.661812, mean_q: 47.358283, mean_eps: 0.100000\n",
      " 394879/1000000: episode: 1134, duration: 1.319s, episode steps: 272, steps per second: 206, episode reward: 231.721, mean reward:  0.852 [-9.930, 100.000], mean action: 0.831 [0.000, 3.000],  loss: 15.745100, mae: 36.928457, mean_q: 49.558755, mean_eps: 0.100000\n",
      " 395033/1000000: episode: 1135, duration: 0.734s, episode steps: 154, steps per second: 210, episode reward: -142.921, mean reward: -0.928 [-100.000,  7.758], mean action: 1.182 [0.000, 3.000],  loss: 5.773443, mae: 39.406495, mean_q: 52.775376, mean_eps: 0.100000\n",
      " 395248/1000000: episode: 1136, duration: 1.039s, episode steps: 215, steps per second: 207, episode reward: 265.767, mean reward:  1.236 [-18.595, 100.000], mean action: 1.265 [0.000, 3.000],  loss: 12.396714, mae: 42.939657, mean_q: 55.984052, mean_eps: 0.100000\n",
      " 395483/1000000: episode: 1137, duration: 1.149s, episode steps: 235, steps per second: 205, episode reward: 251.410, mean reward:  1.070 [-8.478, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 11.843503, mae: 45.999563, mean_q: 59.886737, mean_eps: 0.100000\n",
      " 395819/1000000: episode: 1138, duration: 1.655s, episode steps: 336, steps per second: 203, episode reward: 260.493, mean reward:  0.775 [-18.788, 100.000], mean action: 1.092 [0.000, 3.000],  loss: 9.011274, mae: 45.034571, mean_q: 60.176352, mean_eps: 0.100000\n",
      " 396148/1000000: episode: 1139, duration: 1.653s, episode steps: 329, steps per second: 199, episode reward: 208.558, mean reward:  0.634 [-9.634, 100.000], mean action: 1.447 [0.000, 3.000],  loss: 6.817173, mae: 43.372590, mean_q: 58.425705, mean_eps: 0.100000\n",
      " 396415/1000000: episode: 1140, duration: 1.305s, episode steps: 267, steps per second: 205, episode reward: -220.037, mean reward: -0.824 [-100.000,  6.574], mean action: 1.685 [0.000, 3.000],  loss: 8.998078, mae: 42.575212, mean_q: 58.085723, mean_eps: 0.100000\n",
      " 396562/1000000: episode: 1141, duration: 0.702s, episode steps: 147, steps per second: 209, episode reward: -65.514, mean reward: -0.446 [-100.000, 17.811], mean action: 1.891 [0.000, 3.000],  loss: 6.376462, mae: 41.388056, mean_q: 55.837654, mean_eps: 0.100000\n",
      " 396983/1000000: episode: 1142, duration: 2.205s, episode steps: 421, steps per second: 191, episode reward: 231.801, mean reward:  0.551 [-10.890, 100.000], mean action: 2.190 [0.000, 3.000],  loss: 5.642199, mae: 44.008783, mean_q: 55.904375, mean_eps: 0.100000\n",
      " 397187/1000000: episode: 1143, duration: 0.980s, episode steps: 204, steps per second: 208, episode reward: -89.105, mean reward: -0.437 [-100.000,  8.682], mean action: 1.691 [0.000, 3.000],  loss: 4.404224, mae: 44.002827, mean_q: 55.098526, mean_eps: 0.100000\n",
      " 397519/1000000: episode: 1144, duration: 1.663s, episode steps: 332, steps per second: 200, episode reward: 268.074, mean reward:  0.807 [-5.866, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 5.274080, mae: 44.250876, mean_q: 53.496803, mean_eps: 0.100000\n",
      " 398166/1000000: episode: 1145, duration: 3.440s, episode steps: 647, steps per second: 188, episode reward: 188.255, mean reward:  0.291 [-24.483, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 5.428756, mae: 43.386891, mean_q: 55.944683, mean_eps: 0.100000\n",
      " 399088/1000000: episode: 1146, duration: 5.288s, episode steps: 922, steps per second: 174, episode reward: 191.832, mean reward:  0.208 [-18.913, 100.000], mean action: 0.862 [0.000, 3.000],  loss: 2.573214, mae: 41.210483, mean_q: 56.190716, mean_eps: 0.100000\n",
      " 399283/1000000: episode: 1147, duration: 0.974s, episode steps: 195, steps per second: 200, episode reward: -13.428, mean reward: -0.069 [-100.000, 18.638], mean action: 1.641 [0.000, 3.000],  loss: 2.364931, mae: 41.028514, mean_q: 55.508660, mean_eps: 0.100000\n",
      " 399952/1000000: episode: 1148, duration: 3.641s, episode steps: 669, steps per second: 184, episode reward: 261.090, mean reward:  0.390 [-17.205, 100.000], mean action: 1.398 [0.000, 3.000],  loss: 3.436088, mae: 38.037309, mean_q: 51.031563, mean_eps: 0.100000\n",
      " 400370/1000000: episode: 1149, duration: 2.109s, episode steps: 418, steps per second: 198, episode reward: 229.381, mean reward:  0.549 [-2.763, 100.000], mean action: 1.014 [0.000, 3.000],  loss: 3.310438, mae: 35.810366, mean_q: 47.947832, mean_eps: 0.100000\n",
      " 400951/1000000: episode: 1150, duration: 3.035s, episode steps: 581, steps per second: 191, episode reward: 240.485, mean reward:  0.414 [-18.469, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 2.442588, mae: 38.211343, mean_q: 51.624253, mean_eps: 0.100000\n",
      " 401805/1000000: episode: 1151, duration: 4.560s, episode steps: 854, steps per second: 187, episode reward: 236.973, mean reward:  0.277 [-18.093, 100.000], mean action: 0.967 [0.000, 3.000],  loss: 1.692598, mae: 41.939787, mean_q: 56.547630, mean_eps: 0.100000\n",
      " 402102/1000000: episode: 1152, duration: 1.461s, episode steps: 297, steps per second: 203, episode reward: 271.303, mean reward:  0.913 [-17.590, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 1.687997, mae: 42.921325, mean_q: 58.098836, mean_eps: 0.100000\n",
      " 402356/1000000: episode: 1153, duration: 1.238s, episode steps: 254, steps per second: 205, episode reward: 267.692, mean reward:  1.054 [-9.724, 100.000], mean action: 1.299 [0.000, 3.000],  loss: 2.629145, mae: 44.225601, mean_q: 59.929496, mean_eps: 0.100000\n",
      " 402701/1000000: episode: 1154, duration: 1.671s, episode steps: 345, steps per second: 207, episode reward: 277.390, mean reward:  0.804 [-14.069, 100.000], mean action: 1.380 [0.000, 3.000],  loss: 3.984118, mae: 46.265588, mean_q: 62.650650, mean_eps: 0.100000\n",
      " 403701/1000000: episode: 1155, duration: 5.214s, episode steps: 1000, steps per second: 192, episode reward: 156.231, mean reward:  0.156 [-18.983, 23.087], mean action: 0.897 [0.000, 3.000],  loss: 2.856906, mae: 45.221777, mean_q: 61.245878, mean_eps: 0.100000\n",
      " 404022/1000000: episode: 1156, duration: 1.592s, episode steps: 321, steps per second: 202, episode reward: 283.199, mean reward:  0.882 [-9.931, 100.000], mean action: 1.153 [0.000, 3.000],  loss: 1.590701, mae: 38.478349, mean_q: 52.175388, mean_eps: 0.100000\n",
      " 404368/1000000: episode: 1157, duration: 1.748s, episode steps: 346, steps per second: 198, episode reward: 302.352, mean reward:  0.874 [-18.789, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 2.715984, mae: 41.775524, mean_q: 56.700689, mean_eps: 0.100000\n",
      " 404609/1000000: episode: 1158, duration: 1.173s, episode steps: 241, steps per second: 206, episode reward: 256.822, mean reward:  1.066 [-12.457, 100.000], mean action: 1.249 [0.000, 3.000],  loss: 4.981362, mae: 45.768352, mean_q: 61.964741, mean_eps: 0.100000\n",
      " 404784/1000000: episode: 1159, duration: 0.844s, episode steps: 175, steps per second: 207, episode reward: 261.974, mean reward:  1.497 [-4.237, 100.000], mean action: 1.509 [0.000, 3.000],  loss: 3.930225, mae: 49.642232, mean_q: 67.298010, mean_eps: 0.100000\n",
      " 404995/1000000: episode: 1160, duration: 1.017s, episode steps: 211, steps per second: 207, episode reward: 243.472, mean reward:  1.154 [-11.582, 100.000], mean action: 1.265 [0.000, 3.000],  loss: 5.913468, mae: 50.441949, mean_q: 68.240322, mean_eps: 0.100000\n",
      " 405120/1000000: episode: 1161, duration: 0.605s, episode steps: 125, steps per second: 207, episode reward: 24.163, mean reward:  0.193 [-100.000, 11.188], mean action: 1.840 [0.000, 3.000],  loss: 8.041516, mae: 52.117680, mean_q: 70.389588, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 405248/1000000: episode: 1162, duration: 0.618s, episode steps: 128, steps per second: 207, episode reward:  7.873, mean reward:  0.062 [-100.000, 15.551], mean action: 1.742 [0.000, 3.000],  loss: 8.542875, mae: 54.024425, mean_q: 73.185967, mean_eps: 0.100000\n",
      " 405731/1000000: episode: 1163, duration: 2.437s, episode steps: 483, steps per second: 198, episode reward: 313.582, mean reward:  0.649 [-17.705, 100.000], mean action: 1.573 [0.000, 3.000],  loss: 11.471805, mae: 56.787670, mean_q: 76.480452, mean_eps: 0.100000\n",
      " 405977/1000000: episode: 1164, duration: 1.198s, episode steps: 246, steps per second: 205, episode reward: 264.281, mean reward:  1.074 [-10.163, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 6.098580, mae: 54.569168, mean_q: 73.041765, mean_eps: 0.100000\n",
      " 406163/1000000: episode: 1165, duration: 0.903s, episode steps: 186, steps per second: 206, episode reward: 35.724, mean reward:  0.192 [-100.000, 16.369], mean action: 1.919 [0.000, 3.000],  loss: 5.932064, mae: 51.854325, mean_q: 69.207597, mean_eps: 0.100000\n",
      " 406611/1000000: episode: 1166, duration: 2.350s, episode steps: 448, steps per second: 191, episode reward: 244.232, mean reward:  0.545 [-19.456, 100.000], mean action: 1.489 [0.000, 3.000],  loss: 18.935107, mae: 46.537933, mean_q: 63.007656, mean_eps: 0.100000\n",
      " 407081/1000000: episode: 1167, duration: 2.417s, episode steps: 470, steps per second: 194, episode reward: -140.409, mean reward: -0.299 [-100.000, 21.922], mean action: 1.719 [0.000, 3.000],  loss: 13.086381, mae: 44.807387, mean_q: 60.434945, mean_eps: 0.100000\n",
      " 407302/1000000: episode: 1168, duration: 1.066s, episode steps: 221, steps per second: 207, episode reward: 267.236, mean reward:  1.209 [-12.560, 100.000], mean action: 1.222 [0.000, 3.000],  loss: 10.651494, mae: 42.843324, mean_q: 53.903196, mean_eps: 0.100000\n",
      " 407688/1000000: episode: 1169, duration: 1.936s, episode steps: 386, steps per second: 199, episode reward: 313.518, mean reward:  0.812 [-17.967, 100.000], mean action: 0.896 [0.000, 3.000],  loss: 6.688292, mae: 46.148663, mean_q: 58.142210, mean_eps: 0.100000\n",
      " 407910/1000000: episode: 1170, duration: 1.068s, episode steps: 222, steps per second: 208, episode reward: 278.238, mean reward:  1.253 [-19.172, 100.000], mean action: 1.270 [0.000, 3.000],  loss: 6.534155, mae: 45.120438, mean_q: 56.613868, mean_eps: 0.100000\n",
      " 408060/1000000: episode: 1171, duration: 0.708s, episode steps: 150, steps per second: 212, episode reward: 255.844, mean reward:  1.706 [-2.179, 100.000], mean action: 1.013 [0.000, 3.000],  loss: 6.223007, mae: 52.669663, mean_q: 68.197654, mean_eps: 0.100000\n",
      " 408143/1000000: episode: 1172, duration: 0.397s, episode steps:  83, steps per second: 209, episode reward: 58.695, mean reward:  0.707 [-100.000, 17.439], mean action: 1.398 [0.000, 3.000],  loss: 6.202460, mae: 57.166060, mean_q: 77.712984, mean_eps: 0.100000\n",
      " 408342/1000000: episode: 1173, duration: 0.956s, episode steps: 199, steps per second: 208, episode reward: 258.399, mean reward:  1.298 [-17.657, 100.000], mean action: 0.920 [0.000, 3.000],  loss: 14.156061, mae: 57.334885, mean_q: 78.429893, mean_eps: 0.100000\n",
      " 408456/1000000: episode: 1174, duration: 0.545s, episode steps: 114, steps per second: 209, episode reward:  3.512, mean reward:  0.031 [-100.000, 18.708], mean action: 1.798 [0.000, 3.000],  loss: 19.314711, mae: 56.453062, mean_q: 77.340119, mean_eps: 0.100000\n",
      " 408573/1000000: episode: 1175, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -43.480, mean reward: -0.372 [-100.000, 14.974], mean action: 1.325 [0.000, 3.000],  loss: 28.047349, mae: 58.667353, mean_q: 80.600422, mean_eps: 0.100000\n",
      " 408711/1000000: episode: 1176, duration: 0.654s, episode steps: 138, steps per second: 211, episode reward: 18.131, mean reward:  0.131 [-100.000, 17.878], mean action: 1.319 [0.000, 3.000],  loss: 23.114804, mae: 61.061964, mean_q: 83.804994, mean_eps: 0.100000\n",
      " 408801/1000000: episode: 1177, duration: 0.430s, episode steps:  90, steps per second: 209, episode reward: 35.480, mean reward:  0.394 [-100.000, 16.321], mean action: 1.544 [0.000, 3.000],  loss: 47.258227, mae: 62.972038, mean_q: 86.244219, mean_eps: 0.100000\n",
      " 409801/1000000: episode: 1178, duration: 5.189s, episode steps: 1000, steps per second: 193, episode reward: -16.994, mean reward: -0.017 [-20.050, 18.647], mean action: 1.301 [0.000, 3.000],  loss: 28.174505, mae: 44.216159, mean_q: 57.763980, mean_eps: 0.100000\n",
      " 410230/1000000: episode: 1179, duration: 2.109s, episode steps: 429, steps per second: 203, episode reward: 254.972, mean reward:  0.594 [-19.377, 100.000], mean action: 0.876 [0.000, 3.000],  loss: 3.549892, mae: 17.446485, mean_q: 22.594569, mean_eps: 0.100000\n",
      " 410377/1000000: episode: 1180, duration: 0.707s, episode steps: 147, steps per second: 208, episode reward: 14.079, mean reward:  0.096 [-100.000, 15.635], mean action: 1.898 [0.000, 3.000],  loss: 3.999493, mae: 20.336093, mean_q: 28.447337, mean_eps: 0.100000\n",
      " 410604/1000000: episode: 1181, duration: 1.103s, episode steps: 227, steps per second: 206, episode reward: 282.578, mean reward:  1.245 [-9.527, 100.000], mean action: 1.348 [0.000, 3.000],  loss: 11.584078, mae: 29.979493, mean_q: 41.142136, mean_eps: 0.100000\n",
      " 410759/1000000: episode: 1182, duration: 0.742s, episode steps: 155, steps per second: 209, episode reward: 271.299, mean reward:  1.750 [-3.461, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 16.360014, mae: 39.542886, mean_q: 54.154493, mean_eps: 0.100000\n",
      " 410927/1000000: episode: 1183, duration: 0.805s, episode steps: 168, steps per second: 209, episode reward: -13.412, mean reward: -0.080 [-100.000,  9.356], mean action: 1.679 [0.000, 3.000],  loss: 22.242437, mae: 41.464929, mean_q: 56.717797, mean_eps: 0.100000\n",
      " 411096/1000000: episode: 1184, duration: 0.807s, episode steps: 169, steps per second: 209, episode reward: 289.101, mean reward:  1.711 [-10.818, 100.000], mean action: 1.166 [0.000, 3.000],  loss: 14.367706, mae: 45.332818, mean_q: 61.549154, mean_eps: 0.100000\n",
      " 411232/1000000: episode: 1185, duration: 0.653s, episode steps: 136, steps per second: 208, episode reward: -22.565, mean reward: -0.166 [-100.000, 10.751], mean action: 1.875 [0.000, 3.000],  loss: 24.652430, mae: 50.170024, mean_q: 67.626053, mean_eps: 0.100000\n",
      " 411347/1000000: episode: 1186, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward:  6.566, mean reward:  0.057 [-100.000, 10.420], mean action: 1.574 [0.000, 3.000],  loss: 21.620147, mae: 51.068287, mean_q: 69.035909, mean_eps: 0.100000\n",
      " 411760/1000000: episode: 1187, duration: 2.115s, episode steps: 413, steps per second: 195, episode reward: 276.193, mean reward:  0.669 [-17.750, 100.000], mean action: 0.896 [0.000, 3.000],  loss: 18.052919, mae: 49.531361, mean_q: 65.973094, mean_eps: 0.100000\n",
      " 412073/1000000: episode: 1188, duration: 1.560s, episode steps: 313, steps per second: 201, episode reward: 246.132, mean reward:  0.786 [-19.466, 100.000], mean action: 1.460 [0.000, 3.000],  loss: 19.593289, mae: 47.601445, mean_q: 63.654470, mean_eps: 0.100000\n",
      " 412226/1000000: episode: 1189, duration: 0.740s, episode steps: 153, steps per second: 207, episode reward: -115.117, mean reward: -0.752 [-100.000, 24.779], mean action: 1.693 [0.000, 3.000],  loss: 14.357638, mae: 47.903975, mean_q: 64.138424, mean_eps: 0.100000\n",
      " 412783/1000000: episode: 1190, duration: 2.982s, episode steps: 557, steps per second: 187, episode reward: 208.532, mean reward:  0.374 [-19.700, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 7.259561, mae: 43.001487, mean_q: 56.685779, mean_eps: 0.100000\n",
      " 412869/1000000: episode: 1191, duration: 0.412s, episode steps:  86, steps per second: 209, episode reward: 41.400, mean reward:  0.481 [-100.000, 17.646], mean action: 1.570 [0.000, 3.000],  loss: 9.814415, mae: 37.130965, mean_q: 48.999980, mean_eps: 0.100000\n",
      " 413869/1000000: episode: 1192, duration: 5.553s, episode steps: 1000, steps per second: 180, episode reward: 110.234, mean reward:  0.110 [-19.566, 14.428], mean action: 1.207 [0.000, 3.000],  loss: 9.346713, mae: 33.079800, mean_q: 45.050220, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 414138/1000000: episode: 1193, duration: 1.338s, episode steps: 269, steps per second: 201, episode reward: -76.019, mean reward: -0.283 [-100.000, 15.541], mean action: 1.472 [0.000, 3.000],  loss: 0.857631, mae: 26.761435, mean_q: 37.324745, mean_eps: 0.100000\n",
      " 414408/1000000: episode: 1194, duration: 1.299s, episode steps: 270, steps per second: 208, episode reward: 264.318, mean reward:  0.979 [-10.362, 100.000], mean action: 0.841 [0.000, 3.000],  loss: 1.636758, mae: 37.530195, mean_q: 50.777032, mean_eps: 0.100000\n",
      " 414516/1000000: episode: 1195, duration: 0.517s, episode steps: 108, steps per second: 209, episode reward:  9.684, mean reward:  0.090 [-100.000, 10.529], mean action: 1.259 [0.000, 3.000],  loss: 4.108835, mae: 43.011916, mean_q: 57.488139, mean_eps: 0.100000\n",
      " 414747/1000000: episode: 1196, duration: 1.112s, episode steps: 231, steps per second: 208, episode reward: 298.241, mean reward:  1.291 [-17.809, 100.000], mean action: 1.442 [0.000, 3.000],  loss: 16.193568, mae: 51.074710, mean_q: 68.241399, mean_eps: 0.100000\n",
      " 414983/1000000: episode: 1197, duration: 1.167s, episode steps: 236, steps per second: 202, episode reward: 229.991, mean reward:  0.975 [-7.775, 100.000], mean action: 2.314 [0.000, 3.000],  loss: 12.827509, mae: 58.870919, mean_q: 77.617200, mean_eps: 0.100000\n",
      " 415222/1000000: episode: 1198, duration: 1.165s, episode steps: 239, steps per second: 205, episode reward: 246.640, mean reward:  1.032 [-19.950, 100.000], mean action: 2.046 [0.000, 3.000],  loss: 16.492255, mae: 57.246392, mean_q: 75.502952, mean_eps: 0.100000\n",
      " 415316/1000000: episode: 1199, duration: 0.452s, episode steps:  94, steps per second: 208, episode reward: -15.829, mean reward: -0.168 [-100.000,  9.917], mean action: 1.915 [0.000, 3.000],  loss: 11.995421, mae: 53.324859, mean_q: 71.334342, mean_eps: 0.100000\n",
      " 415521/1000000: episode: 1200, duration: 1.000s, episode steps: 205, steps per second: 205, episode reward: 234.022, mean reward:  1.142 [-17.948, 100.000], mean action: 2.107 [0.000, 3.000],  loss: 13.735932, mae: 54.060999, mean_q: 70.993920, mean_eps: 0.100000\n",
      " 415836/1000000: episode: 1201, duration: 1.583s, episode steps: 315, steps per second: 199, episode reward: 211.338, mean reward:  0.671 [-12.271, 100.000], mean action: 1.527 [0.000, 3.000],  loss: 16.899930, mae: 48.066270, mean_q: 62.767471, mean_eps: 0.100000\n",
      " 415983/1000000: episode: 1202, duration: 0.709s, episode steps: 147, steps per second: 207, episode reward: 25.271, mean reward:  0.172 [-100.000, 11.016], mean action: 2.020 [0.000, 3.000],  loss: 14.821173, mae: 43.599232, mean_q: 57.569722, mean_eps: 0.100000\n",
      " 416362/1000000: episode: 1203, duration: 1.878s, episode steps: 379, steps per second: 202, episode reward: 301.523, mean reward:  0.796 [-17.746, 100.000], mean action: 1.211 [0.000, 3.000],  loss: 15.465484, mae: 46.919570, mean_q: 62.450495, mean_eps: 0.100000\n",
      " 416741/1000000: episode: 1204, duration: 1.900s, episode steps: 379, steps per second: 199, episode reward: 284.308, mean reward:  0.750 [-19.654, 100.000], mean action: 1.298 [0.000, 3.000],  loss: 13.712092, mae: 49.251013, mean_q: 66.981767, mean_eps: 0.100000\n",
      " 417741/1000000: episode: 1205, duration: 5.305s, episode steps: 1000, steps per second: 189, episode reward: 125.891, mean reward:  0.126 [-20.439, 21.798], mean action: 0.820 [0.000, 3.000],  loss: 5.333366, mae: 48.597108, mean_q: 66.409762, mean_eps: 0.100000\n",
      " 418178/1000000: episode: 1206, duration: 2.147s, episode steps: 437, steps per second: 203, episode reward: 278.014, mean reward:  0.636 [-19.069, 100.000], mean action: 0.822 [0.000, 3.000],  loss: 2.956109, mae: 42.010045, mean_q: 57.764571, mean_eps: 0.100000\n",
      " 418503/1000000: episode: 1207, duration: 1.622s, episode steps: 325, steps per second: 200, episode reward: 235.683, mean reward:  0.725 [-8.364, 100.000], mean action: 1.215 [0.000, 3.000],  loss: 5.927540, mae: 45.582863, mean_q: 62.210421, mean_eps: 0.100000\n",
      " 418598/1000000: episode: 1208, duration: 0.451s, episode steps:  95, steps per second: 211, episode reward: 30.392, mean reward:  0.320 [-100.000, 20.803], mean action: 1.453 [0.000, 3.000],  loss: 6.290843, mae: 48.468166, mean_q: 66.027862, mean_eps: 0.100000\n",
      " 418903/1000000: episode: 1209, duration: 1.492s, episode steps: 305, steps per second: 204, episode reward: 38.171, mean reward:  0.125 [-100.000, 10.907], mean action: 1.895 [0.000, 3.000],  loss: 10.251077, mae: 52.402870, mean_q: 71.335139, mean_eps: 0.100000\n",
      " 419132/1000000: episode: 1210, duration: 1.120s, episode steps: 229, steps per second: 205, episode reward: 236.081, mean reward:  1.031 [-12.882, 100.000], mean action: 1.659 [0.000, 3.000],  loss: 12.354247, mae: 53.491697, mean_q: 72.372370, mean_eps: 0.100000\n",
      " 419450/1000000: episode: 1211, duration: 1.584s, episode steps: 318, steps per second: 201, episode reward: 261.707, mean reward:  0.823 [-8.625, 100.000], mean action: 1.673 [0.000, 3.000],  loss: 9.221001, mae: 52.765276, mean_q: 70.993339, mean_eps: 0.100000\n",
      " 419731/1000000: episode: 1212, duration: 1.399s, episode steps: 281, steps per second: 201, episode reward: 265.118, mean reward:  0.943 [-12.998, 100.000], mean action: 1.569 [0.000, 3.000],  loss: 9.471487, mae: 50.261335, mean_q: 67.714466, mean_eps: 0.100000\n",
      " 420235/1000000: episode: 1213, duration: 2.528s, episode steps: 504, steps per second: 199, episode reward: 279.467, mean reward:  0.554 [-20.722, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 7.836020, mae: 45.018812, mean_q: 60.947519, mean_eps: 0.100000\n",
      " 420707/1000000: episode: 1214, duration: 2.463s, episode steps: 472, steps per second: 192, episode reward: 239.110, mean reward:  0.507 [-19.633, 100.000], mean action: 1.350 [0.000, 3.000],  loss: 4.919101, mae: 46.216059, mean_q: 62.608134, mean_eps: 0.100000\n",
      " 421241/1000000: episode: 1215, duration: 2.632s, episode steps: 534, steps per second: 203, episode reward: 302.043, mean reward:  0.566 [-20.125, 100.000], mean action: 0.867 [0.000, 3.000],  loss: 3.279963, mae: 47.686521, mean_q: 64.697946, mean_eps: 0.100000\n",
      " 421577/1000000: episode: 1216, duration: 1.698s, episode steps: 336, steps per second: 198, episode reward: 262.848, mean reward:  0.782 [-17.311, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 2.967347, mae: 46.369820, mean_q: 62.865824, mean_eps: 0.100000\n",
      " 421880/1000000: episode: 1217, duration: 1.496s, episode steps: 303, steps per second: 203, episode reward: 268.306, mean reward:  0.885 [-9.014, 100.000], mean action: 1.779 [0.000, 3.000],  loss: 3.828699, mae: 45.820479, mean_q: 62.017723, mean_eps: 0.100000\n",
      " 422116/1000000: episode: 1218, duration: 1.134s, episode steps: 236, steps per second: 208, episode reward: 235.525, mean reward:  0.998 [-8.302, 100.000], mean action: 0.877 [0.000, 3.000],  loss: 5.497482, mae: 46.393988, mean_q: 62.734942, mean_eps: 0.100000\n",
      " 422700/1000000: episode: 1219, duration: 2.967s, episode steps: 584, steps per second: 197, episode reward: 210.115, mean reward:  0.360 [-21.042, 100.000], mean action: 0.985 [0.000, 3.000],  loss: 5.518992, mae: 47.826157, mean_q: 64.839302, mean_eps: 0.100000\n",
      " 422919/1000000: episode: 1220, duration: 1.059s, episode steps: 219, steps per second: 207, episode reward: 252.247, mean reward:  1.152 [-21.153, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 4.346285, mae: 45.524130, mean_q: 62.024743, mean_eps: 0.100000\n",
      " 423109/1000000: episode: 1221, duration: 0.950s, episode steps: 190, steps per second: 200, episode reward: 272.506, mean reward:  1.434 [-9.168, 100.000], mean action: 1.474 [0.000, 3.000],  loss: 2.524603, mae: 45.489624, mean_q: 61.998234, mean_eps: 0.100000\n",
      " 423313/1000000: episode: 1222, duration: 1.010s, episode steps: 204, steps per second: 202, episode reward: 246.232, mean reward:  1.207 [-9.568, 100.000], mean action: 1.333 [0.000, 3.000],  loss: 5.127290, mae: 46.588014, mean_q: 63.321314, mean_eps: 0.100000\n",
      " 423491/1000000: episode: 1223, duration: 0.886s, episode steps: 178, steps per second: 201, episode reward: 263.582, mean reward:  1.481 [-10.684, 100.000], mean action: 1.433 [0.000, 3.000],  loss: 6.850108, mae: 51.597523, mean_q: 70.045358, mean_eps: 0.100000\n",
      " 423677/1000000: episode: 1224, duration: 0.897s, episode steps: 186, steps per second: 207, episode reward: 287.690, mean reward:  1.547 [-8.762, 100.000], mean action: 1.344 [0.000, 3.000],  loss: 6.811460, mae: 58.560880, mean_q: 79.412837, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 424167/1000000: episode: 1225, duration: 2.460s, episode steps: 490, steps per second: 199, episode reward: 269.651, mean reward:  0.550 [-18.120, 100.000], mean action: 0.818 [0.000, 3.000],  loss: 5.020676, mae: 59.664080, mean_q: 80.955904, mean_eps: 0.100000\n",
      " 424388/1000000: episode: 1226, duration: 1.081s, episode steps: 221, steps per second: 204, episode reward: 275.221, mean reward:  1.245 [-11.120, 100.000], mean action: 1.452 [0.000, 3.000],  loss: 3.893179, mae: 55.511979, mean_q: 75.376836, mean_eps: 0.100000\n",
      " 424614/1000000: episode: 1227, duration: 1.098s, episode steps: 226, steps per second: 206, episode reward: 251.686, mean reward:  1.114 [-18.354, 100.000], mean action: 1.248 [0.000, 3.000],  loss: 3.612307, mae: 54.905341, mean_q: 74.494982, mean_eps: 0.100000\n",
      " 424919/1000000: episode: 1228, duration: 1.492s, episode steps: 305, steps per second: 204, episode reward: 282.399, mean reward:  0.926 [-18.253, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 3.429917, mae: 51.584772, mean_q: 69.941095, mean_eps: 0.100000\n",
      " 425366/1000000: episode: 1229, duration: 2.251s, episode steps: 447, steps per second: 199, episode reward: 264.767, mean reward:  0.592 [-19.518, 100.000], mean action: 1.723 [0.000, 3.000],  loss: 3.505044, mae: 54.046279, mean_q: 73.188099, mean_eps: 0.100000\n",
      " 426366/1000000: episode: 1230, duration: 5.276s, episode steps: 1000, steps per second: 190, episode reward: 131.740, mean reward:  0.132 [-19.699, 21.986], mean action: 1.157 [0.000, 3.000],  loss: 2.682771, mae: 46.852446, mean_q: 63.792971, mean_eps: 0.100000\n",
      " 426675/1000000: episode: 1231, duration: 1.531s, episode steps: 309, steps per second: 202, episode reward: 270.374, mean reward:  0.875 [-18.766, 100.000], mean action: 1.602 [0.000, 3.000],  loss: 1.762521, mae: 38.746082, mean_q: 52.657859, mean_eps: 0.100000\n",
      " 427281/1000000: episode: 1232, duration: 3.233s, episode steps: 606, steps per second: 187, episode reward: 266.547, mean reward:  0.440 [-18.272, 100.000], mean action: 1.439 [0.000, 3.000],  loss: 3.063910, mae: 42.269269, mean_q: 57.486737, mean_eps: 0.100000\n",
      " 427473/1000000: episode: 1233, duration: 0.918s, episode steps: 192, steps per second: 209, episode reward: 258.773, mean reward:  1.348 [-9.429, 100.000], mean action: 0.995 [0.000, 3.000],  loss: 3.812384, mae: 46.345218, mean_q: 62.995624, mean_eps: 0.100000\n",
      " 427964/1000000: episode: 1234, duration: 2.427s, episode steps: 491, steps per second: 202, episode reward: 271.074, mean reward:  0.552 [-18.446, 100.000], mean action: 0.800 [0.000, 3.000],  loss: 3.311603, mae: 50.535653, mean_q: 68.833563, mean_eps: 0.100000\n",
      " 428207/1000000: episode: 1235, duration: 1.183s, episode steps: 243, steps per second: 205, episode reward: 294.308, mean reward:  1.211 [-9.240, 100.000], mean action: 1.218 [0.000, 3.000],  loss: 3.550099, mae: 55.330111, mean_q: 75.524706, mean_eps: 0.100000\n",
      " 428522/1000000: episode: 1236, duration: 1.509s, episode steps: 315, steps per second: 209, episode reward: 282.902, mean reward:  0.898 [-17.615, 100.000], mean action: 0.648 [0.000, 3.000],  loss: 3.001671, mae: 59.093714, mean_q: 80.434621, mean_eps: 0.100000\n",
      " 428761/1000000: episode: 1237, duration: 1.145s, episode steps: 239, steps per second: 209, episode reward: 272.846, mean reward:  1.142 [-17.867, 100.000], mean action: 0.975 [0.000, 3.000],  loss: 3.249249, mae: 57.782589, mean_q: 78.472068, mean_eps: 0.100000\n",
      " 429337/1000000: episode: 1238, duration: 3.029s, episode steps: 576, steps per second: 190, episode reward: 291.336, mean reward:  0.506 [-22.524, 100.000], mean action: 1.012 [0.000, 3.000],  loss: 2.957264, mae: 55.899771, mean_q: 75.707673, mean_eps: 0.100000\n",
      " 430125/1000000: episode: 1239, duration: 4.058s, episode steps: 788, steps per second: 194, episode reward: 287.913, mean reward:  0.365 [-22.237, 100.000], mean action: 1.180 [0.000, 3.000],  loss: 1.821427, mae: 50.444027, mean_q: 68.397709, mean_eps: 0.100000\n",
      " 430965/1000000: episode: 1240, duration: 4.478s, episode steps: 840, steps per second: 188, episode reward: 260.533, mean reward:  0.310 [-18.528, 100.000], mean action: 1.037 [0.000, 3.000],  loss: 1.688366, mae: 42.118184, mean_q: 56.994530, mean_eps: 0.100000\n",
      " 431200/1000000: episode: 1241, duration: 1.126s, episode steps: 235, steps per second: 209, episode reward: 265.149, mean reward:  1.128 [-9.576, 100.000], mean action: 1.077 [0.000, 3.000],  loss: 4.087210, mae: 41.003107, mean_q: 55.620683, mean_eps: 0.100000\n",
      " 431461/1000000: episode: 1242, duration: 1.279s, episode steps: 261, steps per second: 204, episode reward: 304.918, mean reward:  1.168 [-18.044, 100.000], mean action: 1.280 [0.000, 3.000],  loss: 3.947382, mae: 45.189189, mean_q: 61.512966, mean_eps: 0.100000\n",
      " 431657/1000000: episode: 1243, duration: 0.949s, episode steps: 196, steps per second: 207, episode reward: 315.634, mean reward:  1.610 [-8.069, 100.000], mean action: 1.357 [0.000, 3.000],  loss: 3.766969, mae: 52.605130, mean_q: 71.606868, mean_eps: 0.100000\n",
      " 431970/1000000: episode: 1244, duration: 1.515s, episode steps: 313, steps per second: 207, episode reward: 309.727, mean reward:  0.990 [-13.629, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 5.590637, mae: 60.806668, mean_q: 82.636254, mean_eps: 0.100000\n",
      " 432224/1000000: episode: 1245, duration: 1.215s, episode steps: 254, steps per second: 209, episode reward: 284.347, mean reward:  1.119 [-18.457, 100.000], mean action: 0.941 [0.000, 3.000],  loss: 3.241757, mae: 63.203142, mean_q: 85.687468, mean_eps: 0.100000\n",
      " 432408/1000000: episode: 1246, duration: 0.873s, episode steps: 184, steps per second: 211, episode reward: 266.601, mean reward:  1.449 [-11.515, 100.000], mean action: 1.005 [0.000, 3.000],  loss: 4.313279, mae: 63.840052, mean_q: 86.325017, mean_eps: 0.100000\n",
      " 432705/1000000: episode: 1247, duration: 1.489s, episode steps: 297, steps per second: 199, episode reward: 287.392, mean reward:  0.968 [-18.093, 100.000], mean action: 1.306 [0.000, 3.000],  loss: 4.265220, mae: 63.981109, mean_q: 86.460786, mean_eps: 0.100000\n",
      " 432965/1000000: episode: 1248, duration: 1.272s, episode steps: 260, steps per second: 204, episode reward: 249.894, mean reward:  0.961 [-9.808, 100.000], mean action: 1.177 [0.000, 3.000],  loss: 4.463139, mae: 60.375554, mean_q: 81.589237, mean_eps: 0.100000\n",
      " 433240/1000000: episode: 1249, duration: 1.358s, episode steps: 275, steps per second: 202, episode reward: 297.384, mean reward:  1.081 [-10.821, 100.000], mean action: 1.127 [0.000, 3.000],  loss: 5.131709, mae: 57.113209, mean_q: 77.153078, mean_eps: 0.100000\n",
      " 433664/1000000: episode: 1250, duration: 2.152s, episode steps: 424, steps per second: 197, episode reward: 247.067, mean reward:  0.583 [-18.947, 100.000], mean action: 1.337 [0.000, 3.000],  loss: 6.194690, mae: 53.358163, mean_q: 72.272562, mean_eps: 0.100000\n",
      " 433754/1000000: episode: 1251, duration: 0.434s, episode steps:  90, steps per second: 208, episode reward: 33.705, mean reward:  0.375 [-100.000, 17.204], mean action: 1.833 [0.000, 3.000],  loss: 3.926506, mae: 50.856809, mean_q: 69.171909, mean_eps: 0.100000\n",
      " 434056/1000000: episode: 1252, duration: 1.464s, episode steps: 302, steps per second: 206, episode reward: 292.007, mean reward:  0.967 [-17.670, 100.000], mean action: 1.043 [0.000, 3.000],  loss: 16.280553, mae: 54.655305, mean_q: 74.293798, mean_eps: 0.100000\n",
      " 434231/1000000: episode: 1253, duration: 0.829s, episode steps: 175, steps per second: 211, episode reward: 18.916, mean reward:  0.108 [-100.000, 17.811], mean action: 1.440 [0.000, 3.000],  loss: 16.154780, mae: 56.735810, mean_q: 77.249138, mean_eps: 0.100000\n",
      " 434664/1000000: episode: 1254, duration: 2.176s, episode steps: 433, steps per second: 199, episode reward: 199.666, mean reward:  0.461 [-10.362, 100.000], mean action: 1.861 [0.000, 3.000],  loss: 14.610482, mae: 59.038041, mean_q: 80.504719, mean_eps: 0.100000\n",
      " 435199/1000000: episode: 1255, duration: 2.655s, episode steps: 535, steps per second: 202, episode reward: 189.218, mean reward:  0.354 [-20.765, 100.000], mean action: 1.454 [0.000, 3.000],  loss: 8.494651, mae: 53.947234, mean_q: 74.321328, mean_eps: 0.100000\n",
      " 436199/1000000: episode: 1256, duration: 5.296s, episode steps: 1000, steps per second: 189, episode reward: 160.941, mean reward:  0.161 [-21.554, 22.744], mean action: 1.031 [0.000, 3.000],  loss: 3.791735, mae: 48.760743, mean_q: 66.332002, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 436494/1000000: episode: 1257, duration: 1.436s, episode steps: 295, steps per second: 205, episode reward: 274.510, mean reward:  0.931 [-12.078, 100.000], mean action: 0.915 [0.000, 3.000],  loss: 1.666148, mae: 49.589516, mean_q: 66.963558, mean_eps: 0.100000\n",
      " 436946/1000000: episode: 1258, duration: 2.253s, episode steps: 452, steps per second: 201, episode reward: 271.496, mean reward:  0.601 [-18.429, 100.000], mean action: 0.834 [0.000, 3.000],  loss: 2.631103, mae: 51.665028, mean_q: 69.970249, mean_eps: 0.100000\n",
      " 437138/1000000: episode: 1259, duration: 0.923s, episode steps: 192, steps per second: 208, episode reward: 286.191, mean reward:  1.491 [-3.672, 100.000], mean action: 1.229 [0.000, 3.000],  loss: 4.045168, mae: 55.816902, mean_q: 75.703618, mean_eps: 0.100000\n",
      " 437411/1000000: episode: 1260, duration: 1.326s, episode steps: 273, steps per second: 206, episode reward: 280.423, mean reward:  1.027 [-9.734, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 4.200535, mae: 56.950622, mean_q: 77.109944, mean_eps: 0.100000\n",
      " 437508/1000000: episode: 1261, duration: 0.466s, episode steps:  97, steps per second: 208, episode reward: 29.870, mean reward:  0.308 [-100.000, 12.534], mean action: 1.773 [0.000, 3.000],  loss: 7.206083, mae: 59.888217, mean_q: 80.967822, mean_eps: 0.100000\n",
      " 437596/1000000: episode: 1262, duration: 0.417s, episode steps:  88, steps per second: 211, episode reward: -12.872, mean reward: -0.146 [-100.000, 17.486], mean action: 1.432 [0.000, 3.000],  loss: 14.427232, mae: 60.451320, mean_q: 82.032271, mean_eps: 0.100000\n",
      " 437704/1000000: episode: 1263, duration: 0.516s, episode steps: 108, steps per second: 209, episode reward: 23.380, mean reward:  0.216 [-100.000, 13.864], mean action: 1.676 [0.000, 3.000],  loss: 20.229931, mae: 61.759080, mean_q: 84.168498, mean_eps: 0.100000\n",
      " 437798/1000000: episode: 1264, duration: 0.450s, episode steps:  94, steps per second: 209, episode reward: -21.841, mean reward: -0.232 [-100.000, 10.782], mean action: 1.436 [0.000, 3.000],  loss: 21.559747, mae: 65.051629, mean_q: 88.578355, mean_eps: 0.100000\n",
      " 437910/1000000: episode: 1265, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward:  9.495, mean reward:  0.085 [-100.000, 16.143], mean action: 1.509 [0.000, 3.000],  loss: 40.291973, mae: 70.356873, mean_q: 94.855357, mean_eps: 0.100000\n",
      " 438034/1000000: episode: 1266, duration: 0.591s, episode steps: 124, steps per second: 210, episode reward: 26.783, mean reward:  0.216 [-100.000, 23.410], mean action: 1.427 [0.000, 3.000],  loss: 28.622747, mae: 73.238508, mean_q: 98.486204, mean_eps: 0.100000\n",
      " 438134/1000000: episode: 1267, duration: 0.476s, episode steps: 100, steps per second: 210, episode reward: -103.426, mean reward: -1.034 [-100.000, 10.241], mean action: 1.410 [0.000, 3.000],  loss: 50.902913, mae: 73.361901, mean_q: 97.943033, mean_eps: 0.100000\n",
      " 438278/1000000: episode: 1268, duration: 0.682s, episode steps: 144, steps per second: 211, episode reward: 52.183, mean reward:  0.362 [-100.000, 17.736], mean action: 1.264 [0.000, 3.000],  loss: 27.601834, mae: 77.424975, mean_q: 103.185141, mean_eps: 0.100000\n",
      " 438372/1000000: episode: 1269, duration: 0.455s, episode steps:  94, steps per second: 206, episode reward:  3.223, mean reward:  0.034 [-100.000, 21.348], mean action: 1.745 [0.000, 3.000],  loss: 31.956387, mae: 78.780265, mean_q: 104.249656, mean_eps: 0.100000\n",
      " 438801/1000000: episode: 1270, duration: 2.151s, episode steps: 429, steps per second: 199, episode reward: 183.376, mean reward:  0.427 [-16.612, 100.000], mean action: 2.068 [0.000, 3.000],  loss: 33.170521, mae: 76.830722, mean_q: 99.526346, mean_eps: 0.100000\n",
      " 438907/1000000: episode: 1271, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: -18.686, mean reward: -0.176 [-100.000,  7.910], mean action: 1.745 [0.000, 3.000],  loss: 35.734191, mae: 67.618310, mean_q: 75.551827, mean_eps: 0.100000\n",
      " 439012/1000000: episode: 1272, duration: 0.501s, episode steps: 105, steps per second: 210, episode reward: -44.440, mean reward: -0.423 [-100.000,  6.173], mean action: 1.152 [0.000, 3.000],  loss: 27.540411, mae: 67.810496, mean_q: 78.333622, mean_eps: 0.100000\n",
      " 440012/1000000: episode: 1273, duration: 5.458s, episode steps: 1000, steps per second: 183, episode reward: 30.654, mean reward:  0.031 [-22.790, 24.488], mean action: 1.709 [0.000, 3.000],  loss: 17.212386, mae: 49.093644, mean_q: 58.074646, mean_eps: 0.100000\n",
      " 440935/1000000: episode: 1274, duration: 4.728s, episode steps: 923, steps per second: 195, episode reward: 164.678, mean reward:  0.178 [-18.934, 100.000], mean action: 1.069 [0.000, 3.000],  loss: 3.807385, mae: 30.366085, mean_q: 41.322422, mean_eps: 0.100000\n",
      " 441935/1000000: episode: 1275, duration: 5.426s, episode steps: 1000, steps per second: 184, episode reward: -39.476, mean reward: -0.039 [-18.978, 18.998], mean action: 1.690 [0.000, 3.000],  loss: 4.373013, mae: 26.707375, mean_q: 36.701788, mean_eps: 0.100000\n",
      " 442299/1000000: episode: 1276, duration: 1.855s, episode steps: 364, steps per second: 196, episode reward: 216.369, mean reward:  0.594 [-20.015, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 0.903524, mae: 25.120422, mean_q: 34.818736, mean_eps: 0.100000\n",
      " 442536/1000000: episode: 1277, duration: 1.168s, episode steps: 237, steps per second: 203, episode reward: 201.430, mean reward:  0.850 [-9.754, 100.000], mean action: 1.658 [0.000, 3.000],  loss: 5.897896, mae: 29.461167, mean_q: 40.564791, mean_eps: 0.100000\n",
      " 442984/1000000: episode: 1278, duration: 2.319s, episode steps: 448, steps per second: 193, episode reward: 203.320, mean reward:  0.454 [-15.305, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 8.418100, mae: 42.280912, mean_q: 57.891457, mean_eps: 0.100000\n",
      " 443148/1000000: episode: 1279, duration: 0.790s, episode steps: 164, steps per second: 208, episode reward: -62.542, mean reward: -0.381 [-100.000,  8.614], mean action: 1.713 [0.000, 3.000],  loss: 10.627497, mae: 45.966569, mean_q: 62.967962, mean_eps: 0.100000\n",
      " 443401/1000000: episode: 1280, duration: 1.230s, episode steps: 253, steps per second: 206, episode reward: -54.285, mean reward: -0.215 [-100.000, 13.270], mean action: 1.759 [0.000, 3.000],  loss: 16.853846, mae: 53.548604, mean_q: 72.905686, mean_eps: 0.100000\n",
      " 443516/1000000: episode: 1281, duration: 0.556s, episode steps: 115, steps per second: 207, episode reward: -101.702, mean reward: -0.884 [-100.000,  4.461], mean action: 2.026 [0.000, 3.000],  loss: 18.912800, mae: 53.148425, mean_q: 70.900807, mean_eps: 0.100000\n",
      " 443834/1000000: episode: 1282, duration: 1.599s, episode steps: 318, steps per second: 199, episode reward: 204.177, mean reward:  0.642 [-17.450, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 16.026351, mae: 52.980483, mean_q: 66.097824, mean_eps: 0.100000\n",
      " 444003/1000000: episode: 1283, duration: 0.813s, episode steps: 169, steps per second: 208, episode reward: -74.595, mean reward: -0.441 [-100.000, 19.386], mean action: 1.740 [0.000, 3.000],  loss: 14.711116, mae: 50.616775, mean_q: 63.067578, mean_eps: 0.100000\n",
      " 444231/1000000: episode: 1284, duration: 1.110s, episode steps: 228, steps per second: 205, episode reward: 252.696, mean reward:  1.108 [-10.103, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 19.434584, mae: 48.894706, mean_q: 56.207650, mean_eps: 0.100000\n",
      " 444478/1000000: episode: 1285, duration: 1.197s, episode steps: 247, steps per second: 206, episode reward: 245.352, mean reward:  0.993 [-9.567, 100.000], mean action: 1.077 [0.000, 3.000],  loss: 12.247813, mae: 46.031105, mean_q: 52.745134, mean_eps: 0.100000\n",
      " 444686/1000000: episode: 1286, duration: 1.010s, episode steps: 208, steps per second: 206, episode reward: 279.910, mean reward:  1.346 [-10.007, 100.000], mean action: 1.476 [0.000, 3.000],  loss: 10.429247, mae: 47.206438, mean_q: 59.483989, mean_eps: 0.100000\n",
      " 444769/1000000: episode: 1287, duration: 0.403s, episode steps:  83, steps per second: 206, episode reward: -18.722, mean reward: -0.226 [-100.000, 25.709], mean action: 2.012 [0.000, 3.000],  loss: 12.796787, mae: 51.248453, mean_q: 65.200426, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 444879/1000000: episode: 1288, duration: 0.533s, episode steps: 110, steps per second: 207, episode reward: -30.013, mean reward: -0.273 [-100.000, 12.388], mean action: 1.791 [0.000, 3.000],  loss: 14.912733, mae: 54.665796, mean_q: 69.431568, mean_eps: 0.100000\n",
      " 445075/1000000: episode: 1289, duration: 0.943s, episode steps: 196, steps per second: 208, episode reward: 276.293, mean reward:  1.410 [-17.878, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 11.642072, mae: 58.546414, mean_q: 76.558734, mean_eps: 0.100000\n",
      " 445359/1000000: episode: 1290, duration: 1.391s, episode steps: 284, steps per second: 204, episode reward: 270.117, mean reward:  0.951 [-18.798, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 9.660045, mae: 60.593590, mean_q: 81.465835, mean_eps: 0.100000\n",
      " 445506/1000000: episode: 1291, duration: 0.711s, episode steps: 147, steps per second: 207, episode reward: 31.028, mean reward:  0.211 [-100.000, 12.449], mean action: 1.449 [0.000, 3.000],  loss: 11.069293, mae: 62.029927, mean_q: 83.142540, mean_eps: 0.100000\n",
      " 445578/1000000: episode: 1292, duration: 0.350s, episode steps:  72, steps per second: 206, episode reward: -149.407, mean reward: -2.075 [-100.000,  9.436], mean action: 1.986 [0.000, 3.000],  loss: 9.843554, mae: 61.983712, mean_q: 81.958403, mean_eps: 0.100000\n",
      " 445757/1000000: episode: 1293, duration: 0.863s, episode steps: 179, steps per second: 207, episode reward: 42.832, mean reward:  0.239 [-100.000, 13.756], mean action: 1.682 [0.000, 3.000],  loss: 11.382389, mae: 64.101832, mean_q: 83.929337, mean_eps: 0.100000\n",
      " 446062/1000000: episode: 1294, duration: 1.493s, episode steps: 305, steps per second: 204, episode reward: 249.806, mean reward:  0.819 [-17.335, 100.000], mean action: 0.934 [0.000, 3.000],  loss: 11.660826, mae: 60.667710, mean_q: 80.244726, mean_eps: 0.100000\n",
      " 446205/1000000: episode: 1295, duration: 0.690s, episode steps: 143, steps per second: 207, episode reward: -14.041, mean reward: -0.098 [-100.000, 10.279], mean action: 1.734 [0.000, 3.000],  loss: 8.418687, mae: 57.395348, mean_q: 75.709423, mean_eps: 0.100000\n",
      " 446318/1000000: episode: 1296, duration: 0.545s, episode steps: 113, steps per second: 207, episode reward: -10.024, mean reward: -0.089 [-100.000, 12.214], mean action: 1.973 [0.000, 3.000],  loss: 14.048475, mae: 56.617025, mean_q: 74.482196, mean_eps: 0.100000\n",
      " 446483/1000000: episode: 1297, duration: 0.795s, episode steps: 165, steps per second: 207, episode reward: 13.461, mean reward:  0.082 [-100.000, 19.722], mean action: 1.691 [0.000, 3.000],  loss: 9.921074, mae: 59.773863, mean_q: 77.033302, mean_eps: 0.100000\n",
      " 446876/1000000: episode: 1298, duration: 1.995s, episode steps: 393, steps per second: 197, episode reward: 280.152, mean reward:  0.713 [-9.721, 100.000], mean action: 1.153 [0.000, 3.000],  loss: 12.001309, mae: 56.164326, mean_q: 74.703375, mean_eps: 0.100000\n",
      " 447009/1000000: episode: 1299, duration: 0.642s, episode steps: 133, steps per second: 207, episode reward:  5.819, mean reward:  0.044 [-100.000, 11.357], mean action: 1.782 [0.000, 3.000],  loss: 11.228308, mae: 52.732241, mean_q: 70.612499, mean_eps: 0.100000\n",
      " 447282/1000000: episode: 1300, duration: 1.394s, episode steps: 273, steps per second: 196, episode reward: 277.914, mean reward:  1.018 [-9.596, 100.000], mean action: 0.945 [0.000, 3.000],  loss: 9.780670, mae: 53.876255, mean_q: 69.965127, mean_eps: 0.100000\n",
      " 447520/1000000: episode: 1301, duration: 1.160s, episode steps: 238, steps per second: 205, episode reward: 57.634, mean reward:  0.242 [-100.000, 18.031], mean action: 1.870 [0.000, 3.000],  loss: 10.437808, mae: 50.001601, mean_q: 66.097399, mean_eps: 0.100000\n",
      " 448520/1000000: episode: 1302, duration: 5.767s, episode steps: 1000, steps per second: 173, episode reward: 31.239, mean reward:  0.031 [-23.032, 21.466], mean action: 1.454 [0.000, 3.000],  loss: 10.828819, mae: 47.955952, mean_q: 62.612765, mean_eps: 0.100000\n",
      " 449111/1000000: episode: 1303, duration: 3.163s, episode steps: 591, steps per second: 187, episode reward: 234.684, mean reward:  0.397 [-22.286, 100.000], mean action: 1.220 [0.000, 3.000],  loss: 3.312763, mae: 35.396527, mean_q: 48.097189, mean_eps: 0.100000\n",
      " 450111/1000000: episode: 1304, duration: 5.558s, episode steps: 1000, steps per second: 180, episode reward: -30.987, mean reward: -0.031 [-11.295, 29.744], mean action: 1.900 [0.000, 3.000],  loss: 4.038915, mae: 33.915894, mean_q: 46.448195, mean_eps: 0.100000\n",
      " 450222/1000000: episode: 1305, duration: 0.534s, episode steps: 111, steps per second: 208, episode reward: -44.760, mean reward: -0.403 [-100.000, 11.706], mean action: 1.946 [0.000, 3.000],  loss: 1.286418, mae: 23.749081, mean_q: 32.287968, mean_eps: 0.100000\n",
      " 450492/1000000: episode: 1306, duration: 1.321s, episode steps: 270, steps per second: 204, episode reward: 217.790, mean reward:  0.807 [-9.778, 100.000], mean action: 1.348 [0.000, 3.000],  loss: 4.916863, mae: 23.263662, mean_q: 30.967762, mean_eps: 0.100000\n",
      " 450878/1000000: episode: 1307, duration: 1.945s, episode steps: 386, steps per second: 198, episode reward: 193.566, mean reward:  0.501 [-13.318, 100.000], mean action: 2.047 [0.000, 3.000],  loss: 3.268651, mae: 29.384187, mean_q: 35.861565, mean_eps: 0.100000\n",
      " 451412/1000000: episode: 1308, duration: 2.769s, episode steps: 534, steps per second: 193, episode reward: 277.525, mean reward:  0.520 [-18.557, 100.000], mean action: 1.212 [0.000, 3.000],  loss: 16.225632, mae: 38.789134, mean_q: 43.931474, mean_eps: 0.100000\n",
      " 451730/1000000: episode: 1309, duration: 1.573s, episode steps: 318, steps per second: 202, episode reward: 305.001, mean reward:  0.959 [-9.401, 100.000], mean action: 1.321 [0.000, 3.000],  loss: 8.420979, mae: 36.478099, mean_q: 45.057107, mean_eps: 0.100000\n",
      " 451836/1000000: episode: 1310, duration: 0.518s, episode steps: 106, steps per second: 205, episode reward: 27.391, mean reward:  0.258 [-100.000, 20.771], mean action: 1.925 [0.000, 3.000],  loss: 12.614393, mae: 42.320469, mean_q: 56.028274, mean_eps: 0.100000\n",
      " 452006/1000000: episode: 1311, duration: 0.814s, episode steps: 170, steps per second: 209, episode reward: -185.677, mean reward: -1.092 [-100.000, 15.869], mean action: 1.341 [0.000, 3.000],  loss: 7.921821, mae: 47.235274, mean_q: 64.209838, mean_eps: 0.100000\n",
      " 452379/1000000: episode: 1312, duration: 1.897s, episode steps: 373, steps per second: 197, episode reward: 243.744, mean reward:  0.653 [-18.796, 100.000], mean action: 1.531 [0.000, 3.000],  loss: 5.708927, mae: 52.320315, mean_q: 69.312810, mean_eps: 0.100000\n",
      " 452676/1000000: episode: 1313, duration: 1.488s, episode steps: 297, steps per second: 200, episode reward: 272.312, mean reward:  0.917 [-17.458, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 5.914384, mae: 54.776294, mean_q: 71.867604, mean_eps: 0.100000\n",
      " 453048/1000000: episode: 1314, duration: 1.877s, episode steps: 372, steps per second: 198, episode reward: 254.109, mean reward:  0.683 [-19.762, 100.000], mean action: 1.522 [0.000, 3.000],  loss: 5.872006, mae: 53.434268, mean_q: 70.021563, mean_eps: 0.100000\n",
      " 453406/1000000: episode: 1315, duration: 1.756s, episode steps: 358, steps per second: 204, episode reward: 277.826, mean reward:  0.776 [-18.586, 100.000], mean action: 1.268 [0.000, 3.000],  loss: 7.473196, mae: 49.292677, mean_q: 66.760566, mean_eps: 0.100000\n",
      " 453673/1000000: episode: 1316, duration: 1.314s, episode steps: 267, steps per second: 203, episode reward: 260.409, mean reward:  0.975 [-14.989, 100.000], mean action: 1.266 [0.000, 3.000],  loss: 5.033971, mae: 49.038850, mean_q: 66.358012, mean_eps: 0.100000\n",
      " 453972/1000000: episode: 1317, duration: 1.469s, episode steps: 299, steps per second: 204, episode reward: 278.974, mean reward:  0.933 [-17.384, 100.000], mean action: 1.599 [0.000, 3.000],  loss: 5.941489, mae: 49.059940, mean_q: 65.982555, mean_eps: 0.100000\n",
      " 454226/1000000: episode: 1318, duration: 1.263s, episode steps: 254, steps per second: 201, episode reward: 303.839, mean reward:  1.196 [-10.327, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 5.884830, mae: 50.453032, mean_q: 67.901007, mean_eps: 0.100000\n",
      " 454475/1000000: episode: 1319, duration: 1.229s, episode steps: 249, steps per second: 203, episode reward: 306.353, mean reward:  1.230 [-10.607, 100.000], mean action: 1.337 [0.000, 3.000],  loss: 6.844085, mae: 51.462656, mean_q: 69.380229, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 454792/1000000: episode: 1320, duration: 1.566s, episode steps: 317, steps per second: 202, episode reward: 266.069, mean reward:  0.839 [-17.366, 100.000], mean action: 1.303 [0.000, 3.000],  loss: 6.640857, mae: 53.469478, mean_q: 72.337551, mean_eps: 0.100000\n",
      " 455080/1000000: episode: 1321, duration: 1.401s, episode steps: 288, steps per second: 206, episode reward: 252.291, mean reward:  0.876 [-3.697, 100.000], mean action: 1.396 [0.000, 3.000],  loss: 5.794227, mae: 54.936225, mean_q: 74.403587, mean_eps: 0.100000\n",
      " 455246/1000000: episode: 1322, duration: 0.799s, episode steps: 166, steps per second: 208, episode reward: 32.931, mean reward:  0.198 [-100.000, 14.963], mean action: 1.988 [0.000, 3.000],  loss: 4.915382, mae: 55.444433, mean_q: 75.183710, mean_eps: 0.100000\n",
      " 455800/1000000: episode: 1323, duration: 2.813s, episode steps: 554, steps per second: 197, episode reward: 263.608, mean reward:  0.476 [-17.761, 100.000], mean action: 0.903 [0.000, 3.000],  loss: 7.360158, mae: 54.705050, mean_q: 74.073165, mean_eps: 0.100000\n",
      " 456607/1000000: episode: 1324, duration: 4.419s, episode steps: 807, steps per second: 183, episode reward: 106.839, mean reward:  0.132 [-20.892, 100.000], mean action: 1.995 [0.000, 3.000],  loss: 4.194804, mae: 45.429972, mean_q: 61.739335, mean_eps: 0.100000\n",
      " 457183/1000000: episode: 1325, duration: 3.022s, episode steps: 576, steps per second: 191, episode reward: 230.707, mean reward:  0.401 [-19.842, 100.000], mean action: 0.877 [0.000, 3.000],  loss: 6.374982, mae: 36.840061, mean_q: 50.204580, mean_eps: 0.100000\n",
      " 457659/1000000: episode: 1326, duration: 2.377s, episode steps: 476, steps per second: 200, episode reward: 265.088, mean reward:  0.557 [-17.963, 100.000], mean action: 1.019 [0.000, 3.000],  loss: 5.318960, mae: 42.581410, mean_q: 57.802322, mean_eps: 0.100000\n",
      " 457962/1000000: episode: 1327, duration: 1.501s, episode steps: 303, steps per second: 202, episode reward: 255.582, mean reward:  0.844 [-10.443, 100.000], mean action: 1.571 [0.000, 3.000],  loss: 3.506565, mae: 45.593798, mean_q: 61.721132, mean_eps: 0.100000\n",
      " 458533/1000000: episode: 1328, duration: 2.981s, episode steps: 571, steps per second: 192, episode reward: 228.575, mean reward:  0.400 [-18.071, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 4.789908, mae: 47.511065, mean_q: 64.197882, mean_eps: 0.100000\n",
      " 458769/1000000: episode: 1329, duration: 1.162s, episode steps: 236, steps per second: 203, episode reward: 268.453, mean reward:  1.138 [-11.015, 100.000], mean action: 1.441 [0.000, 3.000],  loss: 4.690387, mae: 47.449236, mean_q: 64.366015, mean_eps: 0.100000\n",
      " 458961/1000000: episode: 1330, duration: 0.925s, episode steps: 192, steps per second: 208, episode reward: 254.960, mean reward:  1.328 [-8.277, 100.000], mean action: 1.448 [0.000, 3.000],  loss: 4.574868, mae: 48.025617, mean_q: 65.062470, mean_eps: 0.100000\n",
      " 459263/1000000: episode: 1331, duration: 1.489s, episode steps: 302, steps per second: 203, episode reward: 294.942, mean reward:  0.977 [-17.399, 100.000], mean action: 1.334 [0.000, 3.000],  loss: 4.840041, mae: 49.467729, mean_q: 67.006486, mean_eps: 0.100000\n",
      " 459490/1000000: episode: 1332, duration: 1.112s, episode steps: 227, steps per second: 204, episode reward: 251.830, mean reward:  1.109 [-3.416, 100.000], mean action: 1.670 [0.000, 3.000],  loss: 5.051183, mae: 53.517115, mean_q: 72.353424, mean_eps: 0.100000\n",
      " 459786/1000000: episode: 1333, duration: 1.498s, episode steps: 296, steps per second: 198, episode reward: 274.532, mean reward:  0.927 [-18.613, 100.000], mean action: 1.034 [0.000, 3.000],  loss: 5.581434, mae: 55.827158, mean_q: 75.338809, mean_eps: 0.100000\n",
      " 459981/1000000: episode: 1334, duration: 0.945s, episode steps: 195, steps per second: 206, episode reward: 259.571, mean reward:  1.331 [-8.162, 100.000], mean action: 1.528 [0.000, 3.000],  loss: 5.154545, mae: 55.587453, mean_q: 74.978091, mean_eps: 0.100000\n",
      " 460364/1000000: episode: 1335, duration: 1.929s, episode steps: 383, steps per second: 199, episode reward: 262.139, mean reward:  0.684 [-17.304, 100.000], mean action: 1.198 [0.000, 3.000],  loss: 4.607431, mae: 52.929747, mean_q: 71.591459, mean_eps: 0.100000\n",
      " 460664/1000000: episode: 1336, duration: 1.477s, episode steps: 300, steps per second: 203, episode reward: 238.344, mean reward:  0.794 [-20.519, 100.000], mean action: 1.120 [0.000, 3.000],  loss: 4.919890, mae: 48.303511, mean_q: 65.437904, mean_eps: 0.100000\n",
      " 460971/1000000: episode: 1337, duration: 1.521s, episode steps: 307, steps per second: 202, episode reward: 256.665, mean reward:  0.836 [-9.630, 100.000], mean action: 0.984 [0.000, 3.000],  loss: 4.440245, mae: 47.583339, mean_q: 64.506688, mean_eps: 0.100000\n",
      " 461387/1000000: episode: 1338, duration: 2.086s, episode steps: 416, steps per second: 199, episode reward: 283.234, mean reward:  0.681 [-18.023, 100.000], mean action: 1.430 [0.000, 3.000],  loss: 4.570779, mae: 46.749238, mean_q: 63.198670, mean_eps: 0.100000\n",
      " 461720/1000000: episode: 1339, duration: 1.630s, episode steps: 333, steps per second: 204, episode reward: 260.743, mean reward:  0.783 [-17.239, 100.000], mean action: 1.177 [0.000, 3.000],  loss: 5.630092, mae: 47.399106, mean_q: 64.152154, mean_eps: 0.100000\n",
      " 461996/1000000: episode: 1340, duration: 1.343s, episode steps: 276, steps per second: 206, episode reward: 280.279, mean reward:  1.016 [-17.781, 100.000], mean action: 1.413 [0.000, 3.000],  loss: 4.420340, mae: 46.495594, mean_q: 62.975732, mean_eps: 0.100000\n",
      " 462305/1000000: episode: 1341, duration: 1.549s, episode steps: 309, steps per second: 199, episode reward: 271.146, mean reward:  0.877 [-17.617, 100.000], mean action: 1.333 [0.000, 3.000],  loss: 4.463634, mae: 46.711422, mean_q: 63.379463, mean_eps: 0.100000\n",
      " 462836/1000000: episode: 1342, duration: 2.669s, episode steps: 531, steps per second: 199, episode reward: 292.659, mean reward:  0.551 [-19.981, 100.000], mean action: 1.269 [0.000, 3.000],  loss: 6.421128, mae: 43.437850, mean_q: 58.958708, mean_eps: 0.100000\n",
      " 463426/1000000: episode: 1343, duration: 3.038s, episode steps: 590, steps per second: 194, episode reward: 213.909, mean reward:  0.363 [-19.489, 100.000], mean action: 1.300 [0.000, 3.000],  loss: 3.046366, mae: 43.370130, mean_q: 58.981487, mean_eps: 0.100000\n",
      " 463613/1000000: episode: 1344, duration: 0.904s, episode steps: 187, steps per second: 207, episode reward: 288.355, mean reward:  1.542 [-9.085, 100.000], mean action: 1.193 [0.000, 3.000],  loss: 5.041535, mae: 45.704317, mean_q: 62.020067, mean_eps: 0.100000\n",
      " 464613/1000000: episode: 1345, duration: 5.332s, episode steps: 1000, steps per second: 188, episode reward: 139.488, mean reward:  0.139 [-18.175, 15.361], mean action: 0.963 [0.000, 3.000],  loss: 3.311212, mae: 38.891953, mean_q: 52.984204, mean_eps: 0.100000\n",
      " 464993/1000000: episode: 1346, duration: 1.911s, episode steps: 380, steps per second: 199, episode reward: 252.540, mean reward:  0.665 [-17.924, 100.000], mean action: 0.861 [0.000, 3.000],  loss: 0.927262, mae: 31.754125, mean_q: 43.442398, mean_eps: 0.100000\n",
      " 465334/1000000: episode: 1347, duration: 1.694s, episode steps: 341, steps per second: 201, episode reward: -165.687, mean reward: -0.486 [-100.000, 18.127], mean action: 1.850 [0.000, 3.000],  loss: 1.709605, mae: 40.945177, mean_q: 55.537594, mean_eps: 0.100000\n",
      " 465681/1000000: episode: 1348, duration: 1.762s, episode steps: 347, steps per second: 197, episode reward: 218.225, mean reward:  0.629 [-11.370, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 3.477430, mae: 47.929986, mean_q: 60.747472, mean_eps: 0.100000\n",
      " 466079/1000000: episode: 1349, duration: 2.001s, episode steps: 398, steps per second: 199, episode reward: 210.649, mean reward:  0.529 [-18.098, 100.000], mean action: 1.613 [0.000, 3.000],  loss: 6.162555, mae: 45.279969, mean_q: 56.198261, mean_eps: 0.100000\n",
      " 466369/1000000: episode: 1350, duration: 1.453s, episode steps: 290, steps per second: 200, episode reward: 252.139, mean reward:  0.869 [-11.802, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 4.757669, mae: 43.242774, mean_q: 54.645433, mean_eps: 0.100000\n",
      " 466493/1000000: episode: 1351, duration: 0.592s, episode steps: 124, steps per second: 209, episode reward:  7.489, mean reward:  0.060 [-100.000, 15.569], mean action: 1.371 [0.000, 3.000],  loss: 5.336278, mae: 44.590970, mean_q: 60.665092, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 466660/1000000: episode: 1352, duration: 0.804s, episode steps: 167, steps per second: 208, episode reward: -195.797, mean reward: -1.172 [-100.000, 53.028], mean action: 1.545 [0.000, 3.000],  loss: 18.290550, mae: 48.536776, mean_q: 66.143249, mean_eps: 0.100000\n",
      " 467033/1000000: episode: 1353, duration: 1.813s, episode steps: 373, steps per second: 206, episode reward: 235.388, mean reward:  0.631 [-20.808, 100.000], mean action: 0.869 [0.000, 3.000],  loss: 14.222130, mae: 51.859505, mean_q: 70.485582, mean_eps: 0.100000\n",
      " 467255/1000000: episode: 1354, duration: 1.078s, episode steps: 222, steps per second: 206, episode reward: -35.311, mean reward: -0.159 [-100.000, 24.265], mean action: 1.671 [0.000, 3.000],  loss: 11.112571, mae: 51.251503, mean_q: 68.712969, mean_eps: 0.100000\n",
      " 467485/1000000: episode: 1355, duration: 1.123s, episode steps: 230, steps per second: 205, episode reward: -194.083, mean reward: -0.844 [-100.000, 28.539], mean action: 1.800 [0.000, 3.000],  loss: 9.233127, mae: 49.980657, mean_q: 66.646126, mean_eps: 0.100000\n",
      " 467761/1000000: episode: 1356, duration: 1.358s, episode steps: 276, steps per second: 203, episode reward: -128.020, mean reward: -0.464 [-100.000, 14.344], mean action: 1.623 [0.000, 3.000],  loss: 8.742442, mae: 47.487971, mean_q: 58.512302, mean_eps: 0.100000\n",
      " 468318/1000000: episode: 1357, duration: 2.875s, episode steps: 557, steps per second: 194, episode reward: 261.311, mean reward:  0.469 [-19.664, 100.000], mean action: 0.645 [0.000, 3.000],  loss: 11.496175, mae: 46.429270, mean_q: 50.288721, mean_eps: 0.100000\n",
      " 468639/1000000: episode: 1358, duration: 1.605s, episode steps: 321, steps per second: 200, episode reward: 288.008, mean reward:  0.897 [-8.926, 100.000], mean action: 1.221 [0.000, 3.000],  loss: 7.701099, mae: 47.103514, mean_q: 54.033381, mean_eps: 0.100000\n",
      " 468856/1000000: episode: 1359, duration: 1.063s, episode steps: 217, steps per second: 204, episode reward: 278.215, mean reward:  1.282 [-9.796, 100.000], mean action: 1.465 [0.000, 3.000],  loss: 4.574284, mae: 51.565888, mean_q: 67.494765, mean_eps: 0.100000\n",
      " 469077/1000000: episode: 1360, duration: 1.077s, episode steps: 221, steps per second: 205, episode reward: 245.043, mean reward:  1.109 [-10.552, 100.000], mean action: 1.602 [0.000, 3.000],  loss: 3.575389, mae: 54.725077, mean_q: 74.088351, mean_eps: 0.100000\n",
      " 469329/1000000: episode: 1361, duration: 1.231s, episode steps: 252, steps per second: 205, episode reward: 271.863, mean reward:  1.079 [-12.147, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 4.048013, mae: 61.066356, mean_q: 82.438891, mean_eps: 0.100000\n",
      " 469613/1000000: episode: 1362, duration: 1.408s, episode steps: 284, steps per second: 202, episode reward: 260.059, mean reward:  0.916 [-9.459, 100.000], mean action: 1.595 [0.000, 3.000],  loss: 4.058226, mae: 60.832179, mean_q: 82.016930, mean_eps: 0.100000\n",
      " 469788/1000000: episode: 1363, duration: 0.841s, episode steps: 175, steps per second: 208, episode reward: 242.152, mean reward:  1.384 [-2.920, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 4.382500, mae: 60.406126, mean_q: 81.419101, mean_eps: 0.100000\n",
      " 470110/1000000: episode: 1364, duration: 1.612s, episode steps: 322, steps per second: 200, episode reward: 272.675, mean reward:  0.847 [-12.522, 100.000], mean action: 0.894 [0.000, 3.000],  loss: 5.230775, mae: 60.234242, mean_q: 81.311303, mean_eps: 0.100000\n",
      " 470306/1000000: episode: 1365, duration: 0.947s, episode steps: 196, steps per second: 207, episode reward: 260.932, mean reward:  1.331 [-8.324, 100.000], mean action: 1.281 [0.000, 3.000],  loss: 5.927742, mae: 58.345525, mean_q: 78.802591, mean_eps: 0.100000\n",
      " 470642/1000000: episode: 1366, duration: 1.705s, episode steps: 336, steps per second: 197, episode reward: 295.084, mean reward:  0.878 [-18.056, 100.000], mean action: 1.060 [0.000, 3.000],  loss: 3.909428, mae: 61.656507, mean_q: 83.176231, mean_eps: 0.100000\n",
      " 471642/1000000: episode: 1367, duration: 5.418s, episode steps: 1000, steps per second: 185, episode reward: 118.662, mean reward:  0.119 [-18.671, 22.349], mean action: 2.030 [0.000, 3.000],  loss: 2.362042, mae: 54.684252, mean_q: 73.881084, mean_eps: 0.100000\n",
      " 471878/1000000: episode: 1368, duration: 1.144s, episode steps: 236, steps per second: 206, episode reward: 291.814, mean reward:  1.236 [-17.190, 100.000], mean action: 1.263 [0.000, 3.000],  loss: 1.669208, mae: 45.072721, mean_q: 61.055975, mean_eps: 0.100000\n",
      " 472120/1000000: episode: 1369, duration: 1.193s, episode steps: 242, steps per second: 203, episode reward: 286.688, mean reward:  1.185 [-10.105, 100.000], mean action: 1.207 [0.000, 3.000],  loss: 2.144823, mae: 49.459065, mean_q: 66.864824, mean_eps: 0.100000\n",
      " 472309/1000000: episode: 1370, duration: 0.907s, episode steps: 189, steps per second: 208, episode reward: 284.199, mean reward:  1.504 [-9.822, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 4.156810, mae: 55.144781, mean_q: 74.611604, mean_eps: 0.100000\n",
      " 472629/1000000: episode: 1371, duration: 1.595s, episode steps: 320, steps per second: 201, episode reward: 253.482, mean reward:  0.792 [-19.864, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 4.892444, mae: 62.706953, mean_q: 84.668714, mean_eps: 0.100000\n",
      " 472800/1000000: episode: 1372, duration: 0.812s, episode steps: 171, steps per second: 210, episode reward: 283.560, mean reward:  1.658 [-19.154, 100.000], mean action: 1.281 [0.000, 3.000],  loss: 4.372407, mae: 64.689191, mean_q: 87.284721, mean_eps: 0.100000\n",
      " 473000/1000000: episode: 1373, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: 273.698, mean reward:  1.368 [-6.646, 100.000], mean action: 1.170 [0.000, 3.000],  loss: 3.331141, mae: 65.871145, mean_q: 88.952493, mean_eps: 0.100000\n",
      " 473295/1000000: episode: 1374, duration: 1.445s, episode steps: 295, steps per second: 204, episode reward: 241.863, mean reward:  0.820 [-18.631, 100.000], mean action: 0.753 [0.000, 3.000],  loss: 4.199214, mae: 65.285474, mean_q: 88.207186, mean_eps: 0.100000\n",
      " 473677/1000000: episode: 1375, duration: 1.896s, episode steps: 382, steps per second: 202, episode reward: 269.109, mean reward:  0.704 [-10.759, 100.000], mean action: 0.607 [0.000, 3.000],  loss: 3.466751, mae: 62.821166, mean_q: 84.790934, mean_eps: 0.100000\n",
      " 473963/1000000: episode: 1376, duration: 1.396s, episode steps: 286, steps per second: 205, episode reward: 288.612, mean reward:  1.009 [-19.016, 100.000], mean action: 0.766 [0.000, 3.000],  loss: 3.739613, mae: 60.597870, mean_q: 81.778153, mean_eps: 0.100000\n",
      " 474142/1000000: episode: 1377, duration: 0.867s, episode steps: 179, steps per second: 206, episode reward: 275.551, mean reward:  1.539 [-2.298, 100.000], mean action: 1.240 [0.000, 3.000],  loss: 2.698196, mae: 59.314027, mean_q: 79.909491, mean_eps: 0.100000\n",
      " 474383/1000000: episode: 1378, duration: 1.170s, episode steps: 241, steps per second: 206, episode reward: 256.228, mean reward:  1.063 [-19.376, 100.000], mean action: 0.851 [0.000, 3.000],  loss: 4.012723, mae: 60.821827, mean_q: 81.791007, mean_eps: 0.100000\n",
      " 474760/1000000: episode: 1379, duration: 1.888s, episode steps: 377, steps per second: 200, episode reward: 303.791, mean reward:  0.806 [-18.526, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 3.832718, mae: 60.852111, mean_q: 81.941925, mean_eps: 0.100000\n",
      " 475011/1000000: episode: 1380, duration: 1.219s, episode steps: 251, steps per second: 206, episode reward: 307.105, mean reward:  1.224 [-18.620, 100.000], mean action: 1.474 [0.000, 3.000],  loss: 3.205502, mae: 60.177391, mean_q: 81.271255, mean_eps: 0.100000\n",
      " 475232/1000000: episode: 1381, duration: 1.068s, episode steps: 221, steps per second: 207, episode reward: 298.256, mean reward:  1.350 [-9.037, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 3.625734, mae: 60.719150, mean_q: 82.083744, mean_eps: 0.100000\n",
      " 475469/1000000: episode: 1382, duration: 1.154s, episode steps: 237, steps per second: 205, episode reward: 281.419, mean reward:  1.187 [-17.203, 100.000], mean action: 1.553 [0.000, 3.000],  loss: 3.132620, mae: 60.577891, mean_q: 81.996949, mean_eps: 0.100000\n",
      " 475826/1000000: episode: 1383, duration: 1.732s, episode steps: 357, steps per second: 206, episode reward: 272.320, mean reward:  0.763 [-17.224, 100.000], mean action: 0.849 [0.000, 3.000],  loss: 3.833711, mae: 62.095204, mean_q: 83.816559, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 475996/1000000: episode: 1384, duration: 0.812s, episode steps: 170, steps per second: 209, episode reward: 239.197, mean reward:  1.407 [-2.858, 100.000], mean action: 1.165 [0.000, 3.000],  loss: 5.572201, mae: 62.405670, mean_q: 84.347876, mean_eps: 0.100000\n",
      " 476099/1000000: episode: 1385, duration: 0.490s, episode steps: 103, steps per second: 210, episode reward: -0.209, mean reward: -0.002 [-100.000, 17.950], mean action: 1.369 [0.000, 3.000],  loss: 6.149998, mae: 62.310497, mean_q: 84.261897, mean_eps: 0.100000\n",
      " 476349/1000000: episode: 1386, duration: 1.217s, episode steps: 250, steps per second: 205, episode reward: 270.818, mean reward:  1.083 [-18.598, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 11.561277, mae: 63.996568, mean_q: 86.965406, mean_eps: 0.100000\n",
      " 476457/1000000: episode: 1387, duration: 0.520s, episode steps: 108, steps per second: 208, episode reward: 22.981, mean reward:  0.213 [-100.000,  7.920], mean action: 1.602 [0.000, 3.000],  loss: 6.794149, mae: 62.884041, mean_q: 85.537559, mean_eps: 0.100000\n",
      " 476648/1000000: episode: 1388, duration: 0.926s, episode steps: 191, steps per second: 206, episode reward: 288.679, mean reward:  1.511 [-9.608, 100.000], mean action: 1.361 [0.000, 3.000],  loss: 22.473162, mae: 66.129534, mean_q: 89.968583, mean_eps: 0.100000\n",
      " 476744/1000000: episode: 1389, duration: 0.459s, episode steps:  96, steps per second: 209, episode reward: 18.208, mean reward:  0.190 [-100.000, 19.554], mean action: 1.583 [0.000, 3.000],  loss: 13.254599, mae: 67.423145, mean_q: 91.479160, mean_eps: 0.100000\n",
      " 476837/1000000: episode: 1390, duration: 0.442s, episode steps:  93, steps per second: 211, episode reward: 30.039, mean reward:  0.323 [-100.000, 21.710], mean action: 1.763 [0.000, 3.000],  loss: 13.473517, mae: 70.719674, mean_q: 95.062627, mean_eps: 0.100000\n",
      " 476930/1000000: episode: 1391, duration: 0.442s, episode steps:  93, steps per second: 210, episode reward: 13.528, mean reward:  0.145 [-100.000, 11.583], mean action: 1.667 [0.000, 3.000],  loss: 17.213726, mae: 72.472422, mean_q: 96.407471, mean_eps: 0.100000\n",
      " 477204/1000000: episode: 1392, duration: 1.335s, episode steps: 274, steps per second: 205, episode reward: 245.091, mean reward:  0.894 [-10.166, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 14.331178, mae: 71.841448, mean_q: 94.920691, mean_eps: 0.100000\n",
      " 477443/1000000: episode: 1393, duration: 1.147s, episode steps: 239, steps per second: 208, episode reward: 280.823, mean reward:  1.175 [-17.492, 100.000], mean action: 1.038 [0.000, 3.000],  loss: 12.101458, mae: 70.901191, mean_q: 92.795438, mean_eps: 0.100000\n",
      " 477871/1000000: episode: 1394, duration: 2.172s, episode steps: 428, steps per second: 197, episode reward: 285.385, mean reward:  0.667 [-18.726, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 6.135502, mae: 67.686107, mean_q: 88.959846, mean_eps: 0.100000\n",
      " 477977/1000000: episode: 1395, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: 36.243, mean reward:  0.342 [-100.000, 42.177], mean action: 1.491 [0.000, 3.000],  loss: 5.683472, mae: 61.555890, mean_q: 82.910256, mean_eps: 0.100000\n",
      " 478197/1000000: episode: 1396, duration: 1.063s, episode steps: 220, steps per second: 207, episode reward: 263.162, mean reward:  1.196 [-17.433, 100.000], mean action: 1.091 [0.000, 3.000],  loss: 13.915052, mae: 62.705286, mean_q: 83.585872, mean_eps: 0.100000\n",
      " 478357/1000000: episode: 1397, duration: 0.790s, episode steps: 160, steps per second: 202, episode reward: 263.871, mean reward:  1.649 [-4.107, 100.000], mean action: 1.350 [0.000, 3.000],  loss: 14.482478, mae: 62.346213, mean_q: 83.672083, mean_eps: 0.100000\n",
      " 478735/1000000: episode: 1398, duration: 1.871s, episode steps: 378, steps per second: 202, episode reward: -102.202, mean reward: -0.270 [-100.000, 13.923], mean action: 1.362 [0.000, 3.000],  loss: 7.474456, mae: 62.256003, mean_q: 83.035980, mean_eps: 0.100000\n",
      " 478826/1000000: episode: 1399, duration: 0.436s, episode steps:  91, steps per second: 209, episode reward: 48.496, mean reward:  0.533 [-100.000, 17.768], mean action: 1.725 [0.000, 3.000],  loss: 8.277981, mae: 61.750131, mean_q: 69.845621, mean_eps: 0.100000\n",
      " 479007/1000000: episode: 1400, duration: 0.881s, episode steps: 181, steps per second: 206, episode reward: 304.198, mean reward:  1.681 [-10.655, 100.000], mean action: 1.387 [0.000, 3.000],  loss: 13.176516, mae: 64.585209, mean_q: 73.640177, mean_eps: 0.100000\n",
      " 479241/1000000: episode: 1401, duration: 1.128s, episode steps: 234, steps per second: 207, episode reward: 282.859, mean reward:  1.209 [-7.233, 100.000], mean action: 1.179 [0.000, 3.000],  loss: 9.476358, mae: 66.649801, mean_q: 76.715967, mean_eps: 0.100000\n",
      " 479565/1000000: episode: 1402, duration: 1.571s, episode steps: 324, steps per second: 206, episode reward: 167.738, mean reward:  0.518 [-16.230, 100.000], mean action: 0.796 [0.000, 3.000],  loss: 7.032147, mae: 62.927453, mean_q: 66.407871, mean_eps: 0.100000\n",
      " 479859/1000000: episode: 1403, duration: 1.446s, episode steps: 294, steps per second: 203, episode reward: 235.314, mean reward:  0.800 [-19.355, 100.000], mean action: 0.895 [0.000, 3.000],  loss: 19.523657, mae: 63.402055, mean_q: 61.249911, mean_eps: 0.100000\n",
      " 479938/1000000: episode: 1404, duration: 0.381s, episode steps:  79, steps per second: 207, episode reward: -11.785, mean reward: -0.149 [-100.000, 11.012], mean action: 1.937 [0.000, 3.000],  loss: 11.618522, mae: 59.998450, mean_q: 65.918487, mean_eps: 0.100000\n",
      " 480164/1000000: episode: 1405, duration: 1.094s, episode steps: 226, steps per second: 207, episode reward: 290.579, mean reward:  1.286 [-2.844, 100.000], mean action: 1.190 [0.000, 3.000],  loss: 18.261558, mae: 60.263340, mean_q: 66.152465, mean_eps: 0.100000\n",
      " 480253/1000000: episode: 1406, duration: 0.429s, episode steps:  89, steps per second: 207, episode reward: 44.210, mean reward:  0.497 [-100.000, 17.088], mean action: 1.719 [0.000, 3.000],  loss: 16.053470, mae: 59.334481, mean_q: 66.573857, mean_eps: 0.100000\n",
      " 480529/1000000: episode: 1407, duration: 1.337s, episode steps: 276, steps per second: 206, episode reward: 274.064, mean reward:  0.993 [-18.716, 100.000], mean action: 0.967 [0.000, 3.000],  loss: 17.930821, mae: 62.350551, mean_q: 72.750263, mean_eps: 0.100000\n",
      " 480788/1000000: episode: 1408, duration: 1.270s, episode steps: 259, steps per second: 204, episode reward: 301.711, mean reward:  1.165 [-10.466, 100.000], mean action: 1.112 [0.000, 3.000],  loss: 14.627576, mae: 67.493212, mean_q: 91.073749, mean_eps: 0.100000\n",
      " 481247/1000000: episode: 1409, duration: 2.307s, episode steps: 459, steps per second: 199, episode reward: 194.957, mean reward:  0.425 [-19.395, 100.000], mean action: 0.904 [0.000, 3.000],  loss: 8.909232, mae: 63.932742, mean_q: 84.942852, mean_eps: 0.100000\n",
      " 481748/1000000: episode: 1410, duration: 2.474s, episode steps: 501, steps per second: 202, episode reward: 202.342, mean reward:  0.404 [-19.905, 100.000], mean action: 0.762 [0.000, 3.000],  loss: 10.159724, mae: 42.691020, mean_q: 51.491047, mean_eps: 0.100000\n",
      " 481968/1000000: episode: 1411, duration: 1.060s, episode steps: 220, steps per second: 208, episode reward: 272.577, mean reward:  1.239 [-19.196, 100.000], mean action: 1.036 [0.000, 3.000],  loss: 19.016464, mae: 31.163326, mean_q: 27.325635, mean_eps: 0.100000\n",
      " 482184/1000000: episode: 1412, duration: 1.046s, episode steps: 216, steps per second: 207, episode reward: 275.440, mean reward:  1.275 [-10.972, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 12.914644, mae: 38.072253, mean_q: 39.033195, mean_eps: 0.100000\n",
      " 482347/1000000: episode: 1413, duration: 0.786s, episode steps: 163, steps per second: 207, episode reward: 269.418, mean reward:  1.653 [-7.772, 100.000], mean action: 1.160 [0.000, 3.000],  loss: 13.645782, mae: 47.117620, mean_q: 53.302940, mean_eps: 0.100000\n",
      " 482574/1000000: episode: 1414, duration: 1.116s, episode steps: 227, steps per second: 203, episode reward: 291.614, mean reward:  1.285 [-20.712, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 12.048839, mae: 51.590703, mean_q: 60.334554, mean_eps: 0.100000\n",
      " 482889/1000000: episode: 1415, duration: 1.549s, episode steps: 315, steps per second: 203, episode reward: -106.262, mean reward: -0.337 [-100.000, 18.681], mean action: 1.575 [0.000, 3.000],  loss: 9.084699, mae: 58.907014, mean_q: 77.253701, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 483296/1000000: episode: 1416, duration: 2.071s, episode steps: 407, steps per second: 196, episode reward: 245.750, mean reward:  0.604 [-20.386, 100.000], mean action: 0.921 [0.000, 3.000],  loss: 3.231780, mae: 55.004309, mean_q: 64.810936, mean_eps: 0.100000\n",
      " 483530/1000000: episode: 1417, duration: 1.132s, episode steps: 234, steps per second: 207, episode reward: 296.835, mean reward:  1.269 [-8.685, 100.000], mean action: 1.526 [0.000, 3.000],  loss: 3.115230, mae: 56.194587, mean_q: 66.138892, mean_eps: 0.100000\n",
      " 484082/1000000: episode: 1418, duration: 2.810s, episode steps: 552, steps per second: 196, episode reward: 301.041, mean reward:  0.545 [-19.990, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 1.827747, mae: 64.995033, mean_q: 82.418819, mean_eps: 0.100000\n",
      " 484472/1000000: episode: 1419, duration: 1.903s, episode steps: 390, steps per second: 205, episode reward: 307.485, mean reward:  0.788 [-17.738, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 1.499476, mae: 69.316760, mean_q: 93.773925, mean_eps: 0.100000\n",
      " 484815/1000000: episode: 1420, duration: 1.679s, episode steps: 343, steps per second: 204, episode reward: 274.163, mean reward:  0.799 [-18.494, 100.000], mean action: 0.880 [0.000, 3.000],  loss: 1.723619, mae: 64.153052, mean_q: 86.892624, mean_eps: 0.100000\n",
      " 485235/1000000: episode: 1421, duration: 2.068s, episode steps: 420, steps per second: 203, episode reward: 242.859, mean reward:  0.578 [-17.057, 100.000], mean action: 0.621 [0.000, 3.000],  loss: 2.130800, mae: 60.276955, mean_q: 81.635251, mean_eps: 0.100000\n",
      " 485756/1000000: episode: 1422, duration: 2.648s, episode steps: 521, steps per second: 197, episode reward: 284.142, mean reward:  0.545 [-19.852, 100.000], mean action: 1.228 [0.000, 3.000],  loss: 3.318991, mae: 55.498284, mean_q: 75.086121, mean_eps: 0.100000\n",
      " 486028/1000000: episode: 1423, duration: 1.341s, episode steps: 272, steps per second: 203, episode reward: 296.863, mean reward:  1.091 [-20.864, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 1.920244, mae: 56.896292, mean_q: 76.977983, mean_eps: 0.100000\n",
      " 486132/1000000: episode: 1424, duration: 0.494s, episode steps: 104, steps per second: 210, episode reward: 21.468, mean reward:  0.206 [-100.000, 21.254], mean action: 1.471 [0.000, 3.000],  loss: 4.322078, mae: 59.100725, mean_q: 79.880099, mean_eps: 0.100000\n",
      " 486898/1000000: episode: 1425, duration: 3.879s, episode steps: 766, steps per second: 197, episode reward: 281.031, mean reward:  0.367 [-19.655, 100.000], mean action: 0.751 [0.000, 3.000],  loss: 6.479943, mae: 56.817174, mean_q: 76.788894, mean_eps: 0.100000\n",
      " 487146/1000000: episode: 1426, duration: 1.227s, episode steps: 248, steps per second: 202, episode reward: 237.829, mean reward:  0.959 [-17.725, 100.000], mean action: 0.972 [0.000, 3.000],  loss: 3.333748, mae: 51.668125, mean_q: 69.358339, mean_eps: 0.100000\n",
      " 487445/1000000: episode: 1427, duration: 1.561s, episode steps: 299, steps per second: 192, episode reward: 261.142, mean reward:  0.873 [-16.926, 100.000], mean action: 1.518 [0.000, 3.000],  loss: 3.048952, mae: 47.731168, mean_q: 64.439391, mean_eps: 0.100000\n",
      " 487848/1000000: episode: 1428, duration: 2.131s, episode steps: 403, steps per second: 189, episode reward: 276.445, mean reward:  0.686 [-9.398, 100.000], mean action: 0.918 [0.000, 3.000],  loss: 5.175569, mae: 51.365393, mean_q: 69.404482, mean_eps: 0.100000\n",
      " 488144/1000000: episode: 1429, duration: 1.441s, episode steps: 296, steps per second: 205, episode reward: 272.773, mean reward:  0.922 [-24.715, 100.000], mean action: 0.990 [0.000, 3.000],  loss: 4.045657, mae: 55.083839, mean_q: 74.226698, mean_eps: 0.100000\n",
      " 488464/1000000: episode: 1430, duration: 1.600s, episode steps: 320, steps per second: 200, episode reward: 264.483, mean reward:  0.827 [-12.924, 100.000], mean action: 1.225 [0.000, 3.000],  loss: 4.806729, mae: 56.686783, mean_q: 76.392598, mean_eps: 0.100000\n",
      " 488868/1000000: episode: 1431, duration: 2.007s, episode steps: 404, steps per second: 201, episode reward: 292.862, mean reward:  0.725 [-19.827, 100.000], mean action: 0.856 [0.000, 3.000],  loss: 3.373677, mae: 58.761752, mean_q: 79.375800, mean_eps: 0.100000\n",
      " 489736/1000000: episode: 1432, duration: 4.486s, episode steps: 868, steps per second: 193, episode reward: 268.974, mean reward:  0.310 [-18.405, 100.000], mean action: 0.749 [0.000, 3.000],  loss: 2.270650, mae: 54.765503, mean_q: 74.137277, mean_eps: 0.100000\n",
      " 489917/1000000: episode: 1433, duration: 0.870s, episode steps: 181, steps per second: 208, episode reward: 255.914, mean reward:  1.414 [-8.911, 100.000], mean action: 1.066 [0.000, 3.000],  loss: 2.197389, mae: 51.365344, mean_q: 69.439474, mean_eps: 0.100000\n",
      " 490034/1000000: episode: 1434, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: 33.586, mean reward:  0.287 [-100.000, 21.246], mean action: 1.462 [0.000, 3.000],  loss: 3.582122, mae: 50.599280, mean_q: 68.447983, mean_eps: 0.100000\n",
      " 490135/1000000: episode: 1435, duration: 0.482s, episode steps: 101, steps per second: 210, episode reward: 36.634, mean reward:  0.363 [-100.000, 21.055], mean action: 1.545 [0.000, 3.000],  loss: 19.626659, mae: 55.025411, mean_q: 74.735107, mean_eps: 0.100000\n",
      " 490289/1000000: episode: 1436, duration: 0.742s, episode steps: 154, steps per second: 207, episode reward: 276.327, mean reward:  1.794 [-8.302, 100.000], mean action: 1.318 [0.000, 3.000],  loss: 31.865618, mae: 60.284633, mean_q: 81.865064, mean_eps: 0.100000\n",
      " 490436/1000000: episode: 1437, duration: 0.701s, episode steps: 147, steps per second: 210, episode reward: 35.821, mean reward:  0.244 [-100.000, 16.559], mean action: 1.422 [0.000, 3.000],  loss: 19.201315, mae: 63.955807, mean_q: 87.088548, mean_eps: 0.100000\n",
      " 490693/1000000: episode: 1438, duration: 1.260s, episode steps: 257, steps per second: 204, episode reward: 240.560, mean reward:  0.936 [-10.137, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 30.238749, mae: 72.043529, mean_q: 97.288471, mean_eps: 0.100000\n",
      " 490869/1000000: episode: 1439, duration: 0.845s, episode steps: 176, steps per second: 208, episode reward: -24.143, mean reward: -0.137 [-100.000,  7.602], mean action: 1.580 [0.000, 3.000],  loss: 25.103899, mae: 70.536352, mean_q: 95.151922, mean_eps: 0.100000\n",
      " 490979/1000000: episode: 1440, duration: 0.530s, episode steps: 110, steps per second: 208, episode reward: -16.581, mean reward: -0.151 [-100.000,  7.861], mean action: 1.927 [0.000, 3.000],  loss: 17.874758, mae: 69.488353, mean_q: 93.852840, mean_eps: 0.100000\n",
      " 491388/1000000: episode: 1441, duration: 2.020s, episode steps: 409, steps per second: 203, episode reward: 258.290, mean reward:  0.632 [-19.681, 100.000], mean action: 1.616 [0.000, 3.000],  loss: 16.709448, mae: 56.384991, mean_q: 75.016375, mean_eps: 0.100000\n",
      " 491695/1000000: episode: 1442, duration: 1.557s, episode steps: 307, steps per second: 197, episode reward: 244.591, mean reward:  0.797 [-23.473, 100.000], mean action: 2.231 [0.000, 3.000],  loss: 13.004780, mae: 36.341027, mean_q: 47.410221, mean_eps: 0.100000\n",
      " 491833/1000000: episode: 1443, duration: 0.660s, episode steps: 138, steps per second: 209, episode reward: 17.107, mean reward:  0.124 [-100.000, 10.058], mean action: 1.587 [0.000, 3.000],  loss: 14.281117, mae: 34.386228, mean_q: 46.240582, mean_eps: 0.100000\n",
      " 492395/1000000: episode: 1444, duration: 2.950s, episode steps: 562, steps per second: 191, episode reward: 264.175, mean reward:  0.470 [-20.816, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 18.793994, mae: 37.295692, mean_q: 50.820053, mean_eps: 0.100000\n",
      " 492696/1000000: episode: 1445, duration: 1.503s, episode steps: 301, steps per second: 200, episode reward: 282.364, mean reward:  0.938 [-15.322, 100.000], mean action: 2.186 [0.000, 3.000],  loss: 9.340569, mae: 45.497271, mean_q: 61.723395, mean_eps: 0.100000\n",
      " 492926/1000000: episode: 1446, duration: 1.150s, episode steps: 230, steps per second: 200, episode reward: 213.927, mean reward:  0.930 [-17.842, 100.000], mean action: 1.657 [0.000, 3.000],  loss: 7.961083, mae: 45.971774, mean_q: 61.574169, mean_eps: 0.100000\n",
      " 493723/1000000: episode: 1447, duration: 4.308s, episode steps: 797, steps per second: 185, episode reward: 195.669, mean reward:  0.246 [-22.120, 100.000], mean action: 1.265 [0.000, 3.000],  loss: 11.345427, mae: 47.498459, mean_q: 62.786782, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 494260/1000000: episode: 1448, duration: 2.774s, episode steps: 537, steps per second: 194, episode reward: 263.618, mean reward:  0.491 [-19.445, 100.000], mean action: 0.892 [0.000, 3.000],  loss: 4.783106, mae: 42.774439, mean_q: 57.770050, mean_eps: 0.100000\n",
      " 494396/1000000: episode: 1449, duration: 0.656s, episode steps: 136, steps per second: 207, episode reward: 54.431, mean reward:  0.400 [-100.000, 15.293], mean action: 1.596 [0.000, 3.000],  loss: 6.080901, mae: 43.482725, mean_q: 58.990428, mean_eps: 0.100000\n",
      " 494966/1000000: episode: 1450, duration: 3.150s, episode steps: 570, steps per second: 181, episode reward: 228.571, mean reward:  0.401 [-19.304, 100.000], mean action: 0.953 [0.000, 3.000],  loss: 12.917045, mae: 47.041484, mean_q: 64.090872, mean_eps: 0.100000\n",
      " 495541/1000000: episode: 1451, duration: 3.096s, episode steps: 575, steps per second: 186, episode reward: 201.804, mean reward:  0.351 [-19.975, 100.000], mean action: 1.163 [0.000, 3.000],  loss: 8.810593, mae: 46.226257, mean_q: 63.047790, mean_eps: 0.100000\n",
      " 495718/1000000: episode: 1452, duration: 0.851s, episode steps: 177, steps per second: 208, episode reward:  6.241, mean reward:  0.035 [-100.000, 18.881], mean action: 1.780 [0.000, 3.000],  loss: 4.573374, mae: 44.325105, mean_q: 60.233369, mean_eps: 0.100000\n",
      " 496235/1000000: episode: 1453, duration: 2.676s, episode steps: 517, steps per second: 193, episode reward: 252.349, mean reward:  0.488 [-21.165, 100.000], mean action: 0.998 [0.000, 3.000],  loss: 8.551694, mae: 49.060841, mean_q: 66.415693, mean_eps: 0.100000\n",
      " 496629/1000000: episode: 1454, duration: 2.042s, episode steps: 394, steps per second: 193, episode reward: 220.491, mean reward:  0.560 [-17.725, 100.000], mean action: 1.480 [0.000, 3.000],  loss: 5.000271, mae: 49.315172, mean_q: 66.480647, mean_eps: 0.100000\n",
      " 497629/1000000: episode: 1455, duration: 5.582s, episode steps: 1000, steps per second: 179, episode reward: -42.838, mean reward: -0.043 [-13.116, 29.737], mean action: 1.851 [0.000, 3.000],  loss: 5.152217, mae: 39.363757, mean_q: 53.380945, mean_eps: 0.100000\n",
      " 497752/1000000: episode: 1456, duration: 0.587s, episode steps: 123, steps per second: 209, episode reward: -16.873, mean reward: -0.137 [-100.000, 15.950], mean action: 1.187 [0.000, 3.000],  loss: 2.980478, mae: 27.469859, mean_q: 37.578441, mean_eps: 0.100000\n",
      " 498346/1000000: episode: 1457, duration: 2.918s, episode steps: 594, steps per second: 204, episode reward: 246.403, mean reward:  0.415 [-19.126, 100.000], mean action: 0.695 [0.000, 3.000],  loss: 4.298031, mae: 33.517833, mean_q: 43.631881, mean_eps: 0.100000\n",
      " 498708/1000000: episode: 1458, duration: 1.816s, episode steps: 362, steps per second: 199, episode reward: 276.838, mean reward:  0.765 [-10.258, 100.000], mean action: 1.116 [0.000, 3.000],  loss: 5.514913, mae: 43.730924, mean_q: 57.498871, mean_eps: 0.100000\n",
      " 499708/1000000: episode: 1459, duration: 5.320s, episode steps: 1000, steps per second: 188, episode reward: -41.897, mean reward: -0.042 [-19.924, 16.375], mean action: 1.633 [0.000, 3.000],  loss: 5.601293, mae: 38.673148, mean_q: 50.313594, mean_eps: 0.100000\n",
      " 499975/1000000: episode: 1460, duration: 1.324s, episode steps: 267, steps per second: 202, episode reward: 254.404, mean reward:  0.953 [-18.965, 100.000], mean action: 1.217 [0.000, 3.000],  loss: 2.564105, mae: 23.452605, mean_q: 24.947935, mean_eps: 0.100000\n",
      " 500294/1000000: episode: 1461, duration: 1.569s, episode steps: 319, steps per second: 203, episode reward: 271.502, mean reward:  0.851 [-17.595, 100.000], mean action: 0.859 [0.000, 3.000],  loss: 3.561220, mae: 30.052142, mean_q: 35.839866, mean_eps: 0.100000\n",
      " 500644/1000000: episode: 1462, duration: 1.741s, episode steps: 350, steps per second: 201, episode reward: 294.152, mean reward:  0.840 [-17.482, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 4.364704, mae: 41.831090, mean_q: 55.306527, mean_eps: 0.100000\n",
      " 500881/1000000: episode: 1463, duration: 1.152s, episode steps: 237, steps per second: 206, episode reward: 317.935, mean reward:  1.341 [-9.034, 100.000], mean action: 1.371 [0.000, 3.000],  loss: 8.785107, mae: 46.701279, mean_q: 63.476091, mean_eps: 0.100000\n",
      " 501003/1000000: episode: 1464, duration: 0.585s, episode steps: 122, steps per second: 208, episode reward: 49.647, mean reward:  0.407 [-100.000, 22.756], mean action: 1.738 [0.000, 3.000],  loss: 11.028166, mae: 47.556039, mean_q: 64.907065, mean_eps: 0.100000\n",
      " 501251/1000000: episode: 1465, duration: 1.185s, episode steps: 248, steps per second: 209, episode reward: 278.607, mean reward:  1.123 [-17.776, 100.000], mean action: 0.996 [0.000, 3.000],  loss: 8.854953, mae: 48.893855, mean_q: 66.456586, mean_eps: 0.100000\n",
      " 501463/1000000: episode: 1466, duration: 1.034s, episode steps: 212, steps per second: 205, episode reward: 279.855, mean reward:  1.320 [-17.804, 100.000], mean action: 1.075 [0.000, 3.000],  loss: 8.535743, mae: 50.321948, mean_q: 68.058484, mean_eps: 0.100000\n",
      " 501759/1000000: episode: 1467, duration: 1.456s, episode steps: 296, steps per second: 203, episode reward: 262.224, mean reward:  0.886 [-12.146, 100.000], mean action: 1.466 [0.000, 3.000],  loss: 8.669240, mae: 52.840904, mean_q: 70.877640, mean_eps: 0.100000\n",
      " 502118/1000000: episode: 1468, duration: 1.794s, episode steps: 359, steps per second: 200, episode reward: 259.958, mean reward:  0.724 [-18.392, 100.000], mean action: 1.240 [0.000, 3.000],  loss: 9.229015, mae: 52.541914, mean_q: 70.566602, mean_eps: 0.100000\n",
      " 502368/1000000: episode: 1469, duration: 1.239s, episode steps: 250, steps per second: 202, episode reward: 290.252, mean reward:  1.161 [-10.569, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 6.668081, mae: 49.187772, mean_q: 66.443973, mean_eps: 0.100000\n",
      " 502539/1000000: episode: 1470, duration: 0.826s, episode steps: 171, steps per second: 207, episode reward: 249.468, mean reward:  1.459 [-10.204, 100.000], mean action: 1.421 [0.000, 3.000],  loss: 5.768280, mae: 52.031665, mean_q: 70.318880, mean_eps: 0.100000\n",
      " 502755/1000000: episode: 1471, duration: 1.052s, episode steps: 216, steps per second: 205, episode reward: 258.897, mean reward:  1.199 [-11.463, 100.000], mean action: 1.468 [0.000, 3.000],  loss: 5.584274, mae: 52.664348, mean_q: 71.355553, mean_eps: 0.100000\n",
      " 502861/1000000: episode: 1472, duration: 0.509s, episode steps: 106, steps per second: 208, episode reward: 23.358, mean reward:  0.220 [-100.000, 22.902], mean action: 1.387 [0.000, 3.000],  loss: 7.324438, mae: 54.548960, mean_q: 73.809294, mean_eps: 0.100000\n",
      " 503162/1000000: episode: 1473, duration: 1.468s, episode steps: 301, steps per second: 205, episode reward: 257.048, mean reward:  0.854 [-19.138, 100.000], mean action: 0.953 [0.000, 3.000],  loss: 10.676748, mae: 59.430474, mean_q: 80.318081, mean_eps: 0.100000\n",
      " 503406/1000000: episode: 1474, duration: 1.177s, episode steps: 244, steps per second: 207, episode reward: 281.306, mean reward:  1.153 [-19.611, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 5.988477, mae: 59.550404, mean_q: 80.617936, mean_eps: 0.100000\n",
      " 503522/1000000: episode: 1475, duration: 0.554s, episode steps: 116, steps per second: 209, episode reward: 10.138, mean reward:  0.087 [-100.000, 18.316], mean action: 1.310 [0.000, 3.000],  loss: 8.584604, mae: 58.620626, mean_q: 79.140406, mean_eps: 0.100000\n",
      " 503753/1000000: episode: 1476, duration: 1.123s, episode steps: 231, steps per second: 206, episode reward: 264.290, mean reward:  1.144 [-17.426, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 8.590860, mae: 60.005056, mean_q: 80.585832, mean_eps: 0.100000\n",
      " 503855/1000000: episode: 1477, duration: 0.489s, episode steps: 102, steps per second: 209, episode reward: -22.267, mean reward: -0.218 [-100.000, 11.184], mean action: 1.480 [0.000, 3.000],  loss: 9.696153, mae: 58.484130, mean_q: 78.471368, mean_eps: 0.100000\n",
      " 504133/1000000: episode: 1478, duration: 1.353s, episode steps: 278, steps per second: 205, episode reward: 252.847, mean reward:  0.910 [-10.656, 100.000], mean action: 1.112 [0.000, 3.000],  loss: 17.440721, mae: 58.756406, mean_q: 79.366550, mean_eps: 0.100000\n",
      " 504249/1000000: episode: 1479, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: 14.611, mean reward:  0.126 [-100.000, 19.852], mean action: 1.629 [0.000, 3.000],  loss: 10.284465, mae: 59.380362, mean_q: 79.741424, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 504643/1000000: episode: 1480, duration: 1.977s, episode steps: 394, steps per second: 199, episode reward: 271.186, mean reward:  0.688 [-10.642, 100.000], mean action: 1.505 [0.000, 3.000],  loss: 9.523208, mae: 57.831676, mean_q: 77.169420, mean_eps: 0.100000\n",
      " 504991/1000000: episode: 1481, duration: 1.733s, episode steps: 348, steps per second: 201, episode reward: 181.134, mean reward:  0.520 [-13.052, 100.000], mean action: 1.575 [0.000, 3.000],  loss: 6.076838, mae: 53.946581, mean_q: 72.148893, mean_eps: 0.100000\n",
      " 505991/1000000: episode: 1482, duration: 5.413s, episode steps: 1000, steps per second: 185, episode reward:  1.248, mean reward:  0.001 [-16.582, 19.456], mean action: 1.941 [0.000, 3.000],  loss: 5.913277, mae: 40.948738, mean_q: 55.091598, mean_eps: 0.100000\n",
      " 506404/1000000: episode: 1483, duration: 2.106s, episode steps: 413, steps per second: 196, episode reward: -175.828, mean reward: -0.426 [-100.000, 14.315], mean action: 1.337 [0.000, 3.000],  loss: 2.700926, mae: 25.366415, mean_q: 34.023249, mean_eps: 0.100000\n",
      " 507404/1000000: episode: 1484, duration: 5.473s, episode steps: 1000, steps per second: 183, episode reward: 144.875, mean reward:  0.145 [-21.143, 21.994], mean action: 1.947 [0.000, 3.000],  loss: 2.709706, mae: 40.471092, mean_q: 51.672574, mean_eps: 0.100000\n",
      " 507967/1000000: episode: 1485, duration: 2.889s, episode steps: 563, steps per second: 195, episode reward: 264.886, mean reward:  0.470 [-20.629, 100.000], mean action: 0.726 [0.000, 3.000],  loss: 1.915275, mae: 42.804076, mean_q: 57.666269, mean_eps: 0.100000\n",
      " 508457/1000000: episode: 1486, duration: 2.453s, episode steps: 490, steps per second: 200, episode reward: 301.271, mean reward:  0.615 [-19.125, 100.000], mean action: 0.586 [0.000, 3.000],  loss: 4.925380, mae: 43.802835, mean_q: 59.470028, mean_eps: 0.100000\n",
      " 508683/1000000: episode: 1487, duration: 1.084s, episode steps: 226, steps per second: 208, episode reward: 266.873, mean reward:  1.181 [-7.491, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 4.219870, mae: 44.087820, mean_q: 59.925576, mean_eps: 0.100000\n",
      " 508880/1000000: episode: 1488, duration: 0.936s, episode steps: 197, steps per second: 211, episode reward: 264.439, mean reward:  1.342 [-3.067, 100.000], mean action: 1.051 [0.000, 3.000],  loss: 7.600901, mae: 49.615896, mean_q: 67.316015, mean_eps: 0.100000\n",
      " 508989/1000000: episode: 1489, duration: 0.522s, episode steps: 109, steps per second: 209, episode reward: -3.134, mean reward: -0.029 [-100.000, 17.093], mean action: 1.624 [0.000, 3.000],  loss: 6.695245, mae: 53.450618, mean_q: 72.617002, mean_eps: 0.100000\n",
      " 509151/1000000: episode: 1490, duration: 0.776s, episode steps: 162, steps per second: 209, episode reward: -275.128, mean reward: -1.698 [-100.000, 27.993], mean action: 1.420 [0.000, 3.000],  loss: 22.532582, mae: 52.927800, mean_q: 72.061865, mean_eps: 0.100000\n",
      " 509236/1000000: episode: 1491, duration: 0.401s, episode steps:  85, steps per second: 212, episode reward: -25.242, mean reward: -0.297 [-100.000,  8.558], mean action: 1.106 [0.000, 2.000],  loss: 48.297946, mae: 56.697661, mean_q: 71.564449, mean_eps: 0.100000\n",
      " 509630/1000000: episode: 1492, duration: 1.985s, episode steps: 394, steps per second: 199, episode reward: 276.581, mean reward:  0.702 [-19.810, 100.000], mean action: 1.069 [0.000, 3.000],  loss: 22.999297, mae: 62.038353, mean_q: 77.743748, mean_eps: 0.100000\n",
      " 509738/1000000: episode: 1493, duration: 0.516s, episode steps: 108, steps per second: 209, episode reward: -17.046, mean reward: -0.158 [-100.000, 11.647], mean action: 1.574 [0.000, 3.000],  loss: 20.947546, mae: 59.863912, mean_q: 75.346999, mean_eps: 0.100000\n",
      " 509876/1000000: episode: 1494, duration: 0.655s, episode steps: 138, steps per second: 211, episode reward: 34.541, mean reward:  0.250 [-100.000, 15.823], mean action: 1.514 [0.000, 3.000],  loss: 26.603599, mae: 59.334753, mean_q: 72.705117, mean_eps: 0.100000\n",
      " 509970/1000000: episode: 1495, duration: 0.452s, episode steps:  94, steps per second: 208, episode reward: -9.245, mean reward: -0.098 [-100.000, 28.825], mean action: 1.404 [0.000, 3.000],  loss: 34.550534, mae: 60.927363, mean_q: 75.981273, mean_eps: 0.100000\n",
      " 510261/1000000: episode: 1496, duration: 1.411s, episode steps: 291, steps per second: 206, episode reward: 247.093, mean reward:  0.849 [-19.500, 100.000], mean action: 0.904 [0.000, 3.000],  loss: 27.178839, mae: 59.732812, mean_q: 76.218830, mean_eps: 0.100000\n",
      " 510467/1000000: episode: 1497, duration: 0.991s, episode steps: 206, steps per second: 208, episode reward: 242.274, mean reward:  1.176 [-10.356, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 19.732358, mae: 55.301851, mean_q: 75.620291, mean_eps: 0.100000\n",
      " 510844/1000000: episode: 1498, duration: 1.848s, episode steps: 377, steps per second: 204, episode reward: -197.175, mean reward: -0.523 [-100.000, 20.928], mean action: 1.037 [0.000, 3.000],  loss: 11.817221, mae: 55.042938, mean_q: 73.313187, mean_eps: 0.100000\n",
      " 511157/1000000: episode: 1499, duration: 1.543s, episode steps: 313, steps per second: 203, episode reward: 297.213, mean reward:  0.950 [-17.703, 100.000], mean action: 1.169 [0.000, 3.000],  loss: 10.961829, mae: 50.727732, mean_q: 54.650039, mean_eps: 0.100000\n",
      " 511376/1000000: episode: 1500, duration: 1.057s, episode steps: 219, steps per second: 207, episode reward: 257.023, mean reward:  1.174 [-18.680, 100.000], mean action: 1.365 [0.000, 3.000],  loss: 7.031039, mae: 51.331323, mean_q: 54.851712, mean_eps: 0.100000\n",
      " 511773/1000000: episode: 1501, duration: 1.961s, episode steps: 397, steps per second: 202, episode reward: -332.013, mean reward: -0.836 [-100.000, 29.969], mean action: 1.204 [0.000, 3.000],  loss: 8.383429, mae: 48.942072, mean_q: 45.604863, mean_eps: 0.100000\n",
      " 512404/1000000: episode: 1502, duration: 3.418s, episode steps: 631, steps per second: 185, episode reward: 221.758, mean reward:  0.351 [-21.851, 100.000], mean action: 1.035 [0.000, 3.000],  loss: 10.511875, mae: 49.185768, mean_q: 40.588468, mean_eps: 0.100000\n",
      " 512696/1000000: episode: 1503, duration: 1.438s, episode steps: 292, steps per second: 203, episode reward: 256.973, mean reward:  0.880 [-7.685, 100.000], mean action: 1.247 [0.000, 3.000],  loss: 11.567605, mae: 46.672503, mean_q: 43.009869, mean_eps: 0.100000\n",
      " 512951/1000000: episode: 1504, duration: 1.219s, episode steps: 255, steps per second: 209, episode reward: 256.963, mean reward:  1.008 [-18.054, 100.000], mean action: 1.024 [0.000, 3.000],  loss: 9.160733, mae: 49.820331, mean_q: 64.743614, mean_eps: 0.100000\n",
      " 513261/1000000: episode: 1505, duration: 1.534s, episode steps: 310, steps per second: 202, episode reward: 262.958, mean reward:  0.848 [-19.365, 100.000], mean action: 0.939 [0.000, 3.000],  loss: 4.180279, mae: 50.492506, mean_q: 68.562629, mean_eps: 0.100000\n",
      " 513968/1000000: episode: 1506, duration: 3.553s, episode steps: 707, steps per second: 199, episode reward: 286.372, mean reward:  0.405 [-19.834, 100.000], mean action: 1.345 [0.000, 3.000],  loss: 4.789680, mae: 49.003479, mean_q: 66.839393, mean_eps: 0.100000\n",
      " 514206/1000000: episode: 1507, duration: 1.167s, episode steps: 238, steps per second: 204, episode reward: 281.626, mean reward:  1.183 [-11.313, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 4.463137, mae: 44.984718, mean_q: 61.772591, mean_eps: 0.100000\n",
      " 514527/1000000: episode: 1508, duration: 1.589s, episode steps: 321, steps per second: 202, episode reward: 243.481, mean reward:  0.759 [-18.051, 100.000], mean action: 1.034 [0.000, 3.000],  loss: 4.628362, mae: 49.513983, mean_q: 67.648698, mean_eps: 0.100000\n",
      " 514810/1000000: episode: 1509, duration: 1.390s, episode steps: 283, steps per second: 204, episode reward: 241.228, mean reward:  0.852 [-12.504, 100.000], mean action: 0.894 [0.000, 3.000],  loss: 2.338935, mae: 51.988845, mean_q: 70.570346, mean_eps: 0.100000\n",
      " 515067/1000000: episode: 1510, duration: 1.254s, episode steps: 257, steps per second: 205, episode reward: 262.259, mean reward:  1.020 [-10.225, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 4.836788, mae: 57.052039, mean_q: 77.188071, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 515464/1000000: episode: 1511, duration: 2.013s, episode steps: 397, steps per second: 197, episode reward: 238.870, mean reward:  0.602 [-18.448, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 3.477412, mae: 54.034055, mean_q: 73.369449, mean_eps: 0.100000\n",
      " 515669/1000000: episode: 1512, duration: 0.978s, episode steps: 205, steps per second: 210, episode reward: 257.211, mean reward:  1.255 [-2.897, 100.000], mean action: 1.098 [0.000, 3.000],  loss: 4.281754, mae: 51.601093, mean_q: 70.378838, mean_eps: 0.100000\n",
      " 516026/1000000: episode: 1513, duration: 1.766s, episode steps: 357, steps per second: 202, episode reward: 274.974, mean reward:  0.770 [-18.259, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 5.073842, mae: 52.620164, mean_q: 71.570565, mean_eps: 0.100000\n",
      " 516286/1000000: episode: 1514, duration: 1.254s, episode steps: 260, steps per second: 207, episode reward: 263.403, mean reward:  1.013 [-19.458, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 6.037747, mae: 52.882087, mean_q: 71.906423, mean_eps: 0.100000\n",
      " 516457/1000000: episode: 1515, duration: 0.835s, episode steps: 171, steps per second: 205, episode reward: 283.747, mean reward:  1.659 [-9.532, 100.000], mean action: 1.386 [0.000, 3.000],  loss: 4.603022, mae: 58.351389, mean_q: 79.052921, mean_eps: 0.100000\n",
      " 516701/1000000: episode: 1516, duration: 1.188s, episode steps: 244, steps per second: 205, episode reward: 249.326, mean reward:  1.022 [-8.973, 100.000], mean action: 1.148 [0.000, 3.000],  loss: 3.047899, mae: 59.475112, mean_q: 80.353640, mean_eps: 0.100000\n",
      " 517207/1000000: episode: 1517, duration: 2.496s, episode steps: 506, steps per second: 203, episode reward: 267.911, mean reward:  0.529 [-19.047, 100.000], mean action: 1.907 [0.000, 3.000],  loss: 4.014203, mae: 54.894643, mean_q: 74.240415, mean_eps: 0.100000\n",
      " 517511/1000000: episode: 1518, duration: 1.505s, episode steps: 304, steps per second: 202, episode reward: 214.612, mean reward:  0.706 [-16.400, 100.000], mean action: 2.197 [0.000, 3.000],  loss: 4.136486, mae: 47.513856, mean_q: 64.486045, mean_eps: 0.100000\n",
      " 518409/1000000: episode: 1519, duration: 4.678s, episode steps: 898, steps per second: 192, episode reward: 268.818, mean reward:  0.299 [-21.221, 100.000], mean action: 1.617 [0.000, 3.000],  loss: 8.832545, mae: 39.724099, mean_q: 54.525136, mean_eps: 0.100000\n",
      " 518772/1000000: episode: 1520, duration: 1.824s, episode steps: 363, steps per second: 199, episode reward: 264.396, mean reward:  0.728 [-19.109, 100.000], mean action: 1.251 [0.000, 3.000],  loss: 3.390145, mae: 47.082465, mean_q: 63.906227, mean_eps: 0.100000\n",
      " 519708/1000000: episode: 1521, duration: 5.046s, episode steps: 936, steps per second: 185, episode reward: 255.665, mean reward:  0.273 [-18.974, 100.000], mean action: 1.906 [0.000, 3.000],  loss: 2.313179, mae: 49.987873, mean_q: 67.812601, mean_eps: 0.100000\n",
      " 520206/1000000: episode: 1522, duration: 2.545s, episode steps: 498, steps per second: 196, episode reward: 272.183, mean reward:  0.547 [-19.454, 100.000], mean action: 1.060 [0.000, 3.000],  loss: 2.029051, mae: 47.373257, mean_q: 64.262714, mean_eps: 0.100000\n",
      " 520457/1000000: episode: 1523, duration: 1.214s, episode steps: 251, steps per second: 207, episode reward: 270.666, mean reward:  1.078 [-4.255, 100.000], mean action: 1.243 [0.000, 3.000],  loss: 2.482541, mae: 50.780749, mean_q: 68.756616, mean_eps: 0.100000\n",
      " 520835/1000000: episode: 1524, duration: 1.917s, episode steps: 378, steps per second: 197, episode reward: 217.729, mean reward:  0.576 [-21.404, 100.000], mean action: 2.090 [0.000, 3.000],  loss: 2.860089, mae: 51.123820, mean_q: 69.189671, mean_eps: 0.100000\n",
      " 521377/1000000: episode: 1525, duration: 2.733s, episode steps: 542, steps per second: 198, episode reward: 266.993, mean reward:  0.493 [-23.897, 100.000], mean action: 1.696 [0.000, 3.000],  loss: 7.605493, mae: 45.954977, mean_q: 62.443583, mean_eps: 0.100000\n",
      " 521911/1000000: episode: 1526, duration: 2.790s, episode steps: 534, steps per second: 191, episode reward: 287.124, mean reward:  0.538 [-19.820, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 4.123596, mae: 47.122407, mean_q: 63.878493, mean_eps: 0.100000\n",
      " 522134/1000000: episode: 1527, duration: 1.078s, episode steps: 223, steps per second: 207, episode reward: 300.278, mean reward:  1.347 [-10.986, 100.000], mean action: 1.260 [0.000, 3.000],  loss: 3.702722, mae: 49.194162, mean_q: 66.527368, mean_eps: 0.100000\n",
      " 522394/1000000: episode: 1528, duration: 1.267s, episode steps: 260, steps per second: 205, episode reward: 298.089, mean reward:  1.146 [-18.538, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 4.102282, mae: 53.738457, mean_q: 72.721285, mean_eps: 0.100000\n",
      " 522775/1000000: episode: 1529, duration: 1.925s, episode steps: 381, steps per second: 198, episode reward: 281.932, mean reward:  0.740 [-13.096, 100.000], mean action: 1.446 [0.000, 3.000],  loss: 3.826897, mae: 55.763833, mean_q: 75.545013, mean_eps: 0.100000\n",
      " 523134/1000000: episode: 1530, duration: 1.788s, episode steps: 359, steps per second: 201, episode reward: 233.750, mean reward:  0.651 [-18.001, 100.000], mean action: 2.373 [0.000, 3.000],  loss: 3.500097, mae: 57.883775, mean_q: 78.430026, mean_eps: 0.100000\n",
      " 523431/1000000: episode: 1531, duration: 1.494s, episode steps: 297, steps per second: 199, episode reward: 272.543, mean reward:  0.918 [-10.603, 100.000], mean action: 1.559 [0.000, 3.000],  loss: 3.230934, mae: 53.188503, mean_q: 72.141960, mean_eps: 0.100000\n",
      " 523662/1000000: episode: 1532, duration: 1.115s, episode steps: 231, steps per second: 207, episode reward: 276.861, mean reward:  1.199 [-17.451, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 5.743662, mae: 51.647887, mean_q: 69.976991, mean_eps: 0.100000\n",
      " 524002/1000000: episode: 1533, duration: 1.673s, episode steps: 340, steps per second: 203, episode reward: 282.067, mean reward:  0.830 [-18.864, 100.000], mean action: 1.238 [0.000, 3.000],  loss: 4.295117, mae: 52.403169, mean_q: 71.177609, mean_eps: 0.100000\n",
      " 524224/1000000: episode: 1534, duration: 1.065s, episode steps: 222, steps per second: 209, episode reward: 275.938, mean reward:  1.243 [-4.516, 100.000], mean action: 1.261 [0.000, 3.000],  loss: 5.514491, mae: 52.433490, mean_q: 71.247953, mean_eps: 0.100000\n",
      " 524346/1000000: episode: 1535, duration: 0.586s, episode steps: 122, steps per second: 208, episode reward: -16.365, mean reward: -0.134 [-100.000, 11.443], mean action: 1.352 [0.000, 3.000],  loss: 4.645278, mae: 55.569100, mean_q: 75.424339, mean_eps: 0.100000\n",
      " 524620/1000000: episode: 1536, duration: 1.320s, episode steps: 274, steps per second: 208, episode reward: 292.251, mean reward:  1.067 [-2.558, 100.000], mean action: 1.161 [0.000, 3.000],  loss: 17.765588, mae: 58.326452, mean_q: 79.590420, mean_eps: 0.100000\n",
      " 524817/1000000: episode: 1537, duration: 0.947s, episode steps: 197, steps per second: 208, episode reward: 275.297, mean reward:  1.397 [-19.404, 100.000], mean action: 1.086 [0.000, 3.000],  loss: 12.622587, mae: 57.580484, mean_q: 78.821135, mean_eps: 0.100000\n",
      " 525115/1000000: episode: 1538, duration: 1.462s, episode steps: 298, steps per second: 204, episode reward: 261.088, mean reward:  0.876 [-9.449, 100.000], mean action: 1.158 [0.000, 3.000],  loss: 10.110316, mae: 63.184715, mean_q: 86.082216, mean_eps: 0.100000\n",
      " 525300/1000000: episode: 1539, duration: 0.886s, episode steps: 185, steps per second: 209, episode reward: 250.716, mean reward:  1.355 [-7.415, 100.000], mean action: 1.330 [0.000, 3.000],  loss: 6.683297, mae: 63.647503, mean_q: 86.649040, mean_eps: 0.100000\n",
      " 525703/1000000: episode: 1540, duration: 2.050s, episode steps: 403, steps per second: 197, episode reward: 264.935, mean reward:  0.657 [-17.261, 100.000], mean action: 1.789 [0.000, 3.000],  loss: 4.549977, mae: 62.599533, mean_q: 85.071756, mean_eps: 0.100000\n",
      " 525893/1000000: episode: 1541, duration: 0.917s, episode steps: 190, steps per second: 207, episode reward: 290.754, mean reward:  1.530 [-10.737, 100.000], mean action: 1.484 [0.000, 3.000],  loss: 3.420522, mae: 59.274087, mean_q: 80.343444, mean_eps: 0.100000\n",
      " 526449/1000000: episode: 1542, duration: 2.784s, episode steps: 556, steps per second: 200, episode reward: 256.972, mean reward:  0.462 [-19.833, 100.000], mean action: 0.545 [0.000, 3.000],  loss: 3.179881, mae: 57.493327, mean_q: 77.838574, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 526779/1000000: episode: 1543, duration: 1.621s, episode steps: 330, steps per second: 204, episode reward: 268.026, mean reward:  0.812 [-11.736, 100.000], mean action: 0.961 [0.000, 3.000],  loss: 2.613096, mae: 56.686277, mean_q: 76.617365, mean_eps: 0.100000\n",
      " 527037/1000000: episode: 1544, duration: 1.243s, episode steps: 258, steps per second: 208, episode reward: 243.969, mean reward:  0.946 [-20.109, 100.000], mean action: 1.008 [0.000, 3.000],  loss: 1.965420, mae: 54.938605, mean_q: 74.385909, mean_eps: 0.100000\n",
      " 527147/1000000: episode: 1545, duration: 0.532s, episode steps: 110, steps per second: 207, episode reward: 48.181, mean reward:  0.438 [-100.000, 18.729], mean action: 2.000 [0.000, 3.000],  loss: 4.595919, mae: 53.598805, mean_q: 72.660656, mean_eps: 0.100000\n",
      " 527370/1000000: episode: 1546, duration: 1.059s, episode steps: 223, steps per second: 211, episode reward: 249.307, mean reward:  1.118 [-9.028, 100.000], mean action: 0.785 [0.000, 3.000],  loss: 15.978549, mae: 57.828348, mean_q: 79.644499, mean_eps: 0.100000\n",
      " 527894/1000000: episode: 1547, duration: 2.658s, episode steps: 524, steps per second: 197, episode reward: 170.056, mean reward:  0.325 [-24.148, 100.000], mean action: 1.773 [0.000, 3.000],  loss: 7.888539, mae: 57.278735, mean_q: 78.399291, mean_eps: 0.100000\n",
      " 528180/1000000: episode: 1548, duration: 1.383s, episode steps: 286, steps per second: 207, episode reward: 247.892, mean reward:  0.867 [-10.843, 100.000], mean action: 0.948 [0.000, 3.000],  loss: 9.983416, mae: 48.179434, mean_q: 64.638859, mean_eps: 0.100000\n",
      " 528448/1000000: episode: 1549, duration: 1.310s, episode steps: 268, steps per second: 205, episode reward: 267.940, mean reward:  1.000 [-2.286, 100.000], mean action: 1.325 [0.000, 3.000],  loss: 13.300804, mae: 44.179029, mean_q: 59.696212, mean_eps: 0.100000\n",
      " 528858/1000000: episode: 1550, duration: 2.067s, episode steps: 410, steps per second: 198, episode reward: 260.302, mean reward:  0.635 [-19.328, 100.000], mean action: 1.146 [0.000, 3.000],  loss: 11.057617, mae: 48.135839, mean_q: 65.390577, mean_eps: 0.100000\n",
      " 528965/1000000: episode: 1551, duration: 0.511s, episode steps: 107, steps per second: 210, episode reward: -7.905, mean reward: -0.074 [-100.000, 13.287], mean action: 1.589 [0.000, 3.000],  loss: 6.258379, mae: 57.500926, mean_q: 78.209578, mean_eps: 0.100000\n",
      " 529395/1000000: episode: 1552, duration: 2.146s, episode steps: 430, steps per second: 200, episode reward: 231.528, mean reward:  0.538 [-19.539, 100.000], mean action: 1.384 [0.000, 3.000],  loss: 6.515453, mae: 59.200859, mean_q: 79.708861, mean_eps: 0.100000\n",
      " 530395/1000000: episode: 1553, duration: 5.576s, episode steps: 1000, steps per second: 179, episode reward: 119.849, mean reward:  0.120 [-22.519, 23.257], mean action: 1.834 [0.000, 3.000],  loss: 4.842012, mae: 53.483592, mean_q: 71.950112, mean_eps: 0.100000\n",
      " 530890/1000000: episode: 1554, duration: 2.555s, episode steps: 495, steps per second: 194, episode reward: 280.967, mean reward:  0.568 [-17.321, 100.000], mean action: 1.636 [0.000, 3.000],  loss: 3.263235, mae: 49.695463, mean_q: 66.841418, mean_eps: 0.100000\n",
      " 531000/1000000: episode: 1555, duration: 0.518s, episode steps: 110, steps per second: 212, episode reward: -32.867, mean reward: -0.299 [-100.000, 11.877], mean action: 1.055 [0.000, 3.000],  loss: 2.497635, mae: 49.922671, mean_q: 67.232314, mean_eps: 0.100000\n",
      " 531292/1000000: episode: 1556, duration: 1.439s, episode steps: 292, steps per second: 203, episode reward: 275.364, mean reward:  0.943 [-9.354, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 4.562650, mae: 53.949725, mean_q: 71.914128, mean_eps: 0.100000\n",
      " 531510/1000000: episode: 1557, duration: 1.056s, episode steps: 218, steps per second: 207, episode reward: 247.556, mean reward:  1.136 [-18.646, 100.000], mean action: 1.229 [0.000, 3.000],  loss: 5.294978, mae: 57.808210, mean_q: 76.859984, mean_eps: 0.100000\n",
      " 531743/1000000: episode: 1558, duration: 1.114s, episode steps: 233, steps per second: 209, episode reward: 256.383, mean reward:  1.100 [-8.908, 100.000], mean action: 0.961 [0.000, 3.000],  loss: 4.047605, mae: 58.129981, mean_q: 77.179418, mean_eps: 0.100000\n",
      " 532040/1000000: episode: 1559, duration: 1.446s, episode steps: 297, steps per second: 205, episode reward: 298.267, mean reward:  1.004 [-18.282, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 6.008573, mae: 60.875077, mean_q: 80.867651, mean_eps: 0.100000\n",
      " 532443/1000000: episode: 1560, duration: 2.041s, episode steps: 403, steps per second: 197, episode reward: 234.836, mean reward:  0.583 [-20.273, 100.000], mean action: 1.551 [0.000, 3.000],  loss: 7.047964, mae: 57.268546, mean_q: 77.230121, mean_eps: 0.100000\n",
      " 533075/1000000: episode: 1561, duration: 3.357s, episode steps: 632, steps per second: 188, episode reward: 275.303, mean reward:  0.436 [-21.728, 100.000], mean action: 0.910 [0.000, 3.000],  loss: 4.252929, mae: 53.662565, mean_q: 72.758318, mean_eps: 0.100000\n",
      " 533784/1000000: episode: 1562, duration: 3.619s, episode steps: 709, steps per second: 196, episode reward: 277.695, mean reward:  0.392 [-20.539, 100.000], mean action: 0.597 [0.000, 3.000],  loss: 2.105013, mae: 53.448446, mean_q: 72.291343, mean_eps: 0.100000\n",
      " 533984/1000000: episode: 1563, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: 256.524, mean reward:  1.283 [-2.814, 100.000], mean action: 0.900 [0.000, 3.000],  loss: 2.435233, mae: 54.241723, mean_q: 73.138269, mean_eps: 0.100000\n",
      " 534263/1000000: episode: 1564, duration: 1.343s, episode steps: 279, steps per second: 208, episode reward: 288.815, mean reward:  1.035 [-18.485, 100.000], mean action: 0.871 [0.000, 3.000],  loss: 2.471804, mae: 54.456281, mean_q: 73.662257, mean_eps: 0.100000\n",
      " 534362/1000000: episode: 1565, duration: 0.476s, episode steps:  99, steps per second: 208, episode reward: 32.616, mean reward:  0.329 [-100.000,  9.096], mean action: 1.667 [0.000, 3.000],  loss: 6.879514, mae: 52.575799, mean_q: 71.246395, mean_eps: 0.100000\n",
      " 534469/1000000: episode: 1566, duration: 0.512s, episode steps: 107, steps per second: 209, episode reward: 20.363, mean reward:  0.190 [-100.000, 10.002], mean action: 1.402 [0.000, 3.000],  loss: 14.490079, mae: 56.066838, mean_q: 76.184163, mean_eps: 0.100000\n",
      " 534812/1000000: episode: 1567, duration: 1.712s, episode steps: 343, steps per second: 200, episode reward: 264.014, mean reward:  0.770 [-12.484, 100.000], mean action: 0.968 [0.000, 3.000],  loss: 18.725166, mae: 59.829046, mean_q: 81.589567, mean_eps: 0.100000\n",
      " 535107/1000000: episode: 1568, duration: 1.485s, episode steps: 295, steps per second: 199, episode reward: 261.853, mean reward:  0.888 [-17.328, 100.000], mean action: 1.698 [0.000, 3.000],  loss: 14.593713, mae: 57.989459, mean_q: 78.836567, mean_eps: 0.100000\n",
      " 535319/1000000: episode: 1569, duration: 1.021s, episode steps: 212, steps per second: 208, episode reward: 279.408, mean reward:  1.318 [-17.899, 100.000], mean action: 1.410 [0.000, 3.000],  loss: 12.062483, mae: 61.550255, mean_q: 83.174247, mean_eps: 0.100000\n",
      " 535452/1000000: episode: 1570, duration: 0.628s, episode steps: 133, steps per second: 212, episode reward: 29.249, mean reward:  0.220 [-100.000,  9.317], mean action: 1.323 [0.000, 3.000],  loss: 5.890868, mae: 61.409853, mean_q: 82.965255, mean_eps: 0.100000\n",
      " 536078/1000000: episode: 1571, duration: 3.217s, episode steps: 626, steps per second: 195, episode reward: 257.598, mean reward:  0.411 [-21.160, 100.000], mean action: 1.470 [0.000, 3.000],  loss: 5.058746, mae: 61.163898, mean_q: 81.725395, mean_eps: 0.100000\n",
      " 536692/1000000: episode: 1572, duration: 3.143s, episode steps: 614, steps per second: 195, episode reward: -169.847, mean reward: -0.277 [-100.000, 20.624], mean action: 1.430 [0.000, 3.000],  loss: 3.039945, mae: 54.308043, mean_q: 72.649252, mean_eps: 0.100000\n",
      " 536859/1000000: episode: 1573, duration: 0.804s, episode steps: 167, steps per second: 208, episode reward: 26.498, mean reward:  0.159 [-100.000, 10.433], mean action: 1.593 [0.000, 3.000],  loss: 4.460115, mae: 44.565244, mean_q: 56.779032, mean_eps: 0.100000\n",
      " 537710/1000000: episode: 1574, duration: 4.423s, episode steps: 851, steps per second: 192, episode reward: 230.029, mean reward:  0.270 [-18.189, 100.000], mean action: 1.801 [0.000, 3.000],  loss: 9.137644, mae: 38.044216, mean_q: 46.759042, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 538710/1000000: episode: 1575, duration: 5.610s, episode steps: 1000, steps per second: 178, episode reward: 125.390, mean reward:  0.125 [-24.839, 23.077], mean action: 1.346 [0.000, 3.000],  loss: 5.062231, mae: 34.933569, mean_q: 47.234899, mean_eps: 0.100000\n",
      " 539710/1000000: episode: 1576, duration: 5.260s, episode steps: 1000, steps per second: 190, episode reward: 157.616, mean reward:  0.158 [-23.653, 23.224], mean action: 1.325 [0.000, 3.000],  loss: 1.992464, mae: 40.879257, mean_q: 55.817676, mean_eps: 0.100000\n",
      " 540095/1000000: episode: 1577, duration: 1.953s, episode steps: 385, steps per second: 197, episode reward: 189.332, mean reward:  0.492 [-14.072, 100.000], mean action: 1.699 [0.000, 3.000],  loss: 1.639249, mae: 42.779435, mean_q: 57.853948, mean_eps: 0.100000\n",
      " 540368/1000000: episode: 1578, duration: 1.325s, episode steps: 273, steps per second: 206, episode reward: 270.244, mean reward:  0.990 [-18.693, 100.000], mean action: 1.311 [0.000, 3.000],  loss: 4.058168, mae: 43.787264, mean_q: 59.189135, mean_eps: 0.100000\n",
      " 541368/1000000: episode: 1579, duration: 5.431s, episode steps: 1000, steps per second: 184, episode reward: 134.454, mean reward:  0.134 [-18.475, 22.576], mean action: 1.367 [0.000, 3.000],  loss: 3.536844, mae: 45.422268, mean_q: 61.524281, mean_eps: 0.100000\n",
      " 541533/1000000: episode: 1580, duration: 0.801s, episode steps: 165, steps per second: 206, episode reward: 309.674, mean reward:  1.877 [-7.573, 100.000], mean action: 1.424 [0.000, 3.000],  loss: 1.545823, mae: 39.574032, mean_q: 53.520367, mean_eps: 0.100000\n",
      " 541830/1000000: episode: 1581, duration: 1.493s, episode steps: 297, steps per second: 199, episode reward: 282.213, mean reward:  0.950 [-2.507, 100.000], mean action: 1.616 [0.000, 3.000],  loss: 2.535747, mae: 39.685644, mean_q: 53.796068, mean_eps: 0.100000\n",
      " 542139/1000000: episode: 1582, duration: 1.645s, episode steps: 309, steps per second: 188, episode reward: 245.666, mean reward:  0.795 [-11.143, 100.000], mean action: 2.217 [0.000, 3.000],  loss: 3.411425, mae: 46.945244, mean_q: 63.620579, mean_eps: 0.100000\n",
      " 542407/1000000: episode: 1583, duration: 1.301s, episode steps: 268, steps per second: 206, episode reward: 282.605, mean reward:  1.054 [-18.518, 100.000], mean action: 1.041 [0.000, 3.000],  loss: 5.492694, mae: 53.695214, mean_q: 73.046651, mean_eps: 0.100000\n",
      " 542502/1000000: episode: 1584, duration: 0.451s, episode steps:  95, steps per second: 210, episode reward: -3.764, mean reward: -0.040 [-100.000, 15.598], mean action: 1.295 [0.000, 3.000],  loss: 5.874053, mae: 52.744978, mean_q: 71.855750, mean_eps: 0.100000\n",
      " 543502/1000000: episode: 1585, duration: 6.170s, episode steps: 1000, steps per second: 162, episode reward: -3.569, mean reward: -0.004 [-22.598, 23.360], mean action: 1.916 [0.000, 3.000],  loss: 6.454142, mae: 44.694669, mean_q: 61.551121, mean_eps: 0.100000\n",
      " 543662/1000000: episode: 1586, duration: 0.770s, episode steps: 160, steps per second: 208, episode reward: -7.779, mean reward: -0.049 [-100.000, 18.813], mean action: 1.525 [0.000, 3.000],  loss: 1.281600, mae: 27.296084, mean_q: 37.710226, mean_eps: 0.100000\n",
      " 543845/1000000: episode: 1587, duration: 0.870s, episode steps: 183, steps per second: 210, episode reward: 284.438, mean reward:  1.554 [-2.140, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 17.829514, mae: 32.403696, mean_q: 44.416831, mean_eps: 0.100000\n",
      " 544845/1000000: episode: 1588, duration: 5.266s, episode steps: 1000, steps per second: 190, episode reward: 52.672, mean reward:  0.053 [-22.208, 22.265], mean action: 1.272 [0.000, 3.000],  loss: 8.375383, mae: 43.463351, mean_q: 59.639963, mean_eps: 0.100000\n",
      " 545354/1000000: episode: 1589, duration: 2.611s, episode steps: 509, steps per second: 195, episode reward: 241.199, mean reward:  0.474 [-21.264, 100.000], mean action: 1.138 [0.000, 3.000],  loss: 2.047543, mae: 28.993573, mean_q: 37.824336, mean_eps: 0.100000\n",
      " 545599/1000000: episode: 1590, duration: 1.199s, episode steps: 245, steps per second: 204, episode reward: 244.958, mean reward:  1.000 [-20.462, 100.000], mean action: 1.437 [0.000, 3.000],  loss: 6.329992, mae: 31.632563, mean_q: 35.146442, mean_eps: 0.100000\n",
      " 546114/1000000: episode: 1591, duration: 2.757s, episode steps: 515, steps per second: 187, episode reward: 248.028, mean reward:  0.482 [-20.918, 100.000], mean action: 1.517 [0.000, 3.000],  loss: 12.313016, mae: 40.077793, mean_q: 49.122422, mean_eps: 0.100000\n",
      " 546432/1000000: episode: 1592, duration: 1.595s, episode steps: 318, steps per second: 199, episode reward: 289.884, mean reward:  0.912 [-9.238, 100.000], mean action: 0.969 [0.000, 3.000],  loss: 8.972111, mae: 44.815589, mean_q: 59.521018, mean_eps: 0.100000\n",
      " 546686/1000000: episode: 1593, duration: 1.232s, episode steps: 254, steps per second: 206, episode reward: 290.729, mean reward:  1.145 [-2.258, 100.000], mean action: 1.327 [0.000, 3.000],  loss: 3.521969, mae: 45.581241, mean_q: 62.115845, mean_eps: 0.100000\n",
      " 547306/1000000: episode: 1594, duration: 3.266s, episode steps: 620, steps per second: 190, episode reward: 275.488, mean reward:  0.444 [-19.903, 100.000], mean action: 1.165 [0.000, 3.000],  loss: 5.299599, mae: 45.203676, mean_q: 61.425375, mean_eps: 0.100000\n",
      " 547387/1000000: episode: 1595, duration: 0.390s, episode steps:  81, steps per second: 208, episode reward: 15.460, mean reward:  0.191 [-100.000, 21.243], mean action: 1.827 [0.000, 3.000],  loss: 8.145810, mae: 42.436562, mean_q: 57.575271, mean_eps: 0.100000\n",
      " 547480/1000000: episode: 1596, duration: 0.447s, episode steps:  93, steps per second: 208, episode reward: 47.593, mean reward:  0.512 [-100.000, 17.475], mean action: 1.742 [0.000, 3.000],  loss: 17.287809, mae: 46.092286, mean_q: 62.949404, mean_eps: 0.100000\n",
      " 547565/1000000: episode: 1597, duration: 0.409s, episode steps:  85, steps per second: 208, episode reward: 42.157, mean reward:  0.496 [-100.000, 20.916], mean action: 1.788 [0.000, 3.000],  loss: 16.005324, mae: 46.274887, mean_q: 63.613355, mean_eps: 0.100000\n",
      " 547894/1000000: episode: 1598, duration: 1.597s, episode steps: 329, steps per second: 206, episode reward: 290.649, mean reward:  0.883 [-17.760, 100.000], mean action: 0.839 [0.000, 3.000],  loss: 26.430625, mae: 48.125550, mean_q: 66.311643, mean_eps: 0.100000\n",
      " 547991/1000000: episode: 1599, duration: 0.467s, episode steps:  97, steps per second: 208, episode reward: 36.841, mean reward:  0.380 [-100.000, 12.910], mean action: 1.742 [0.000, 3.000],  loss: 22.551465, mae: 48.478334, mean_q: 67.161733, mean_eps: 0.100000\n",
      " 548080/1000000: episode: 1600, duration: 0.430s, episode steps:  89, steps per second: 207, episode reward:  5.270, mean reward:  0.059 [-100.000, 11.032], mean action: 1.764 [0.000, 3.000],  loss: 13.977656, mae: 54.182878, mean_q: 74.519750, mean_eps: 0.100000\n",
      " 548288/1000000: episode: 1601, duration: 0.994s, episode steps: 208, steps per second: 209, episode reward: 256.560, mean reward:  1.233 [-18.486, 100.000], mean action: 0.865 [0.000, 3.000],  loss: 24.173981, mae: 61.062962, mean_q: 83.329916, mean_eps: 0.100000\n",
      " 548399/1000000: episode: 1602, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -26.653, mean reward: -0.240 [-100.000,  9.697], mean action: 1.396 [0.000, 3.000],  loss: 26.025598, mae: 60.062624, mean_q: 82.083976, mean_eps: 0.100000\n",
      " 548718/1000000: episode: 1603, duration: 1.542s, episode steps: 319, steps per second: 207, episode reward: 253.456, mean reward:  0.795 [-17.343, 100.000], mean action: 0.821 [0.000, 3.000],  loss: 18.142540, mae: 54.514318, mean_q: 74.196381, mean_eps: 0.100000\n",
      " 548961/1000000: episode: 1604, duration: 1.175s, episode steps: 243, steps per second: 207, episode reward: 268.906, mean reward:  1.107 [-11.722, 100.000], mean action: 0.868 [0.000, 3.000],  loss: 20.063467, mae: 49.134035, mean_q: 65.515452, mean_eps: 0.100000\n",
      " 549423/1000000: episode: 1605, duration: 2.303s, episode steps: 462, steps per second: 201, episode reward: 257.172, mean reward:  0.557 [-20.484, 100.000], mean action: 1.002 [0.000, 3.000],  loss: 13.021030, mae: 44.220358, mean_q: 59.929641, mean_eps: 0.100000\n",
      " 549801/1000000: episode: 1606, duration: 1.887s, episode steps: 378, steps per second: 200, episode reward: 282.529, mean reward:  0.747 [-20.073, 100.000], mean action: 0.862 [0.000, 3.000],  loss: 7.179277, mae: 45.217950, mean_q: 61.644979, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 549958/1000000: episode: 1607, duration: 0.746s, episode steps: 157, steps per second: 211, episode reward: 255.951, mean reward:  1.630 [-2.474, 100.000], mean action: 1.013 [0.000, 3.000],  loss: 6.604807, mae: 48.315983, mean_q: 65.789003, mean_eps: 0.100000\n",
      " 550257/1000000: episode: 1608, duration: 1.488s, episode steps: 299, steps per second: 201, episode reward: 264.371, mean reward:  0.884 [-18.535, 100.000], mean action: 1.191 [0.000, 3.000],  loss: 4.416870, mae: 49.837092, mean_q: 67.723538, mean_eps: 0.100000\n",
      " 550449/1000000: episode: 1609, duration: 0.916s, episode steps: 192, steps per second: 210, episode reward: 274.736, mean reward:  1.431 [-19.828, 100.000], mean action: 0.990 [0.000, 3.000],  loss: 8.667480, mae: 54.240533, mean_q: 73.602987, mean_eps: 0.100000\n",
      " 550684/1000000: episode: 1610, duration: 1.135s, episode steps: 235, steps per second: 207, episode reward: 264.328, mean reward:  1.125 [-8.711, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 9.496609, mae: 55.403321, mean_q: 75.273392, mean_eps: 0.100000\n",
      " 550912/1000000: episode: 1611, duration: 1.091s, episode steps: 228, steps per second: 209, episode reward: 270.925, mean reward:  1.188 [-17.383, 100.000], mean action: 0.759 [0.000, 3.000],  loss: 8.780177, mae: 59.939626, mean_q: 81.439870, mean_eps: 0.100000\n",
      " 551314/1000000: episode: 1612, duration: 2.032s, episode steps: 402, steps per second: 198, episode reward: 296.918, mean reward:  0.739 [-17.556, 100.000], mean action: 1.017 [0.000, 3.000],  loss: 7.051430, mae: 60.063029, mean_q: 81.583344, mean_eps: 0.100000\n",
      " 551479/1000000: episode: 1613, duration: 0.794s, episode steps: 165, steps per second: 208, episode reward: 266.512, mean reward:  1.615 [-2.516, 100.000], mean action: 1.424 [0.000, 3.000],  loss: 6.505381, mae: 60.249567, mean_q: 81.703228, mean_eps: 0.100000\n",
      " 551598/1000000: episode: 1614, duration: 0.572s, episode steps: 119, steps per second: 208, episode reward: 33.808, mean reward:  0.284 [-100.000, 13.326], mean action: 1.714 [0.000, 3.000],  loss: 5.820481, mae: 59.824464, mean_q: 81.176475, mean_eps: 0.100000\n",
      " 551884/1000000: episode: 1615, duration: 1.396s, episode steps: 286, steps per second: 205, episode reward: 283.023, mean reward:  0.990 [-17.496, 100.000], mean action: 1.059 [0.000, 3.000],  loss: 21.562935, mae: 60.010773, mean_q: 81.438734, mean_eps: 0.100000\n",
      " 552423/1000000: episode: 1616, duration: 2.813s, episode steps: 539, steps per second: 192, episode reward: 297.874, mean reward:  0.553 [-21.424, 100.000], mean action: 1.545 [0.000, 3.000],  loss: 13.801484, mae: 58.380851, mean_q: 79.588367, mean_eps: 0.100000\n",
      " 552698/1000000: episode: 1617, duration: 1.329s, episode steps: 275, steps per second: 207, episode reward: 255.579, mean reward:  0.929 [-11.381, 100.000], mean action: 1.113 [0.000, 3.000],  loss: 9.031006, mae: 55.570607, mean_q: 75.699195, mean_eps: 0.100000\n",
      " 552885/1000000: episode: 1618, duration: 0.899s, episode steps: 187, steps per second: 208, episode reward: 257.094, mean reward:  1.375 [-2.994, 100.000], mean action: 1.374 [0.000, 3.000],  loss: 5.761814, mae: 54.212656, mean_q: 73.967089, mean_eps: 0.100000\n",
      " 553177/1000000: episode: 1619, duration: 1.418s, episode steps: 292, steps per second: 206, episode reward: 248.923, mean reward:  0.852 [-10.650, 100.000], mean action: 1.127 [0.000, 3.000],  loss: 8.036962, mae: 48.074164, mean_q: 65.796796, mean_eps: 0.100000\n",
      " 553401/1000000: episode: 1620, duration: 1.076s, episode steps: 224, steps per second: 208, episode reward: 271.306, mean reward:  1.211 [-19.017, 100.000], mean action: 0.893 [0.000, 3.000],  loss: 10.789464, mae: 48.133834, mean_q: 66.007101, mean_eps: 0.100000\n",
      " 553666/1000000: episode: 1621, duration: 1.292s, episode steps: 265, steps per second: 205, episode reward: 305.254, mean reward:  1.152 [-8.548, 100.000], mean action: 1.158 [0.000, 3.000],  loss: 11.755394, mae: 48.856768, mean_q: 66.967601, mean_eps: 0.100000\n",
      " 554132/1000000: episode: 1622, duration: 2.344s, episode steps: 466, steps per second: 199, episode reward: 249.904, mean reward:  0.536 [-18.645, 100.000], mean action: 0.946 [0.000, 3.000],  loss: 9.428858, mae: 50.109388, mean_q: 68.435165, mean_eps: 0.100000\n",
      " 554419/1000000: episode: 1623, duration: 1.394s, episode steps: 287, steps per second: 206, episode reward: 243.156, mean reward:  0.847 [-19.262, 100.000], mean action: 1.143 [0.000, 3.000],  loss: 5.778412, mae: 50.200412, mean_q: 68.307698, mean_eps: 0.100000\n",
      " 554770/1000000: episode: 1624, duration: 1.735s, episode steps: 351, steps per second: 202, episode reward: 235.313, mean reward:  0.670 [-19.103, 100.000], mean action: 1.171 [0.000, 3.000],  loss: 6.989692, mae: 43.137469, mean_q: 59.552122, mean_eps: 0.100000\n",
      " 555016/1000000: episode: 1625, duration: 1.183s, episode steps: 246, steps per second: 208, episode reward: -137.114, mean reward: -0.557 [-100.000, 20.560], mean action: 1.150 [0.000, 3.000],  loss: 8.783948, mae: 42.042355, mean_q: 57.982945, mean_eps: 0.100000\n",
      " 555401/1000000: episode: 1626, duration: 1.870s, episode steps: 385, steps per second: 206, episode reward: 278.008, mean reward:  0.722 [-9.571, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 8.758035, mae: 43.683824, mean_q: 58.934122, mean_eps: 0.100000\n",
      " 555621/1000000: episode: 1627, duration: 1.074s, episode steps: 220, steps per second: 205, episode reward:  0.414, mean reward:  0.002 [-100.000, 11.927], mean action: 1.991 [0.000, 3.000],  loss: 5.307941, mae: 47.780939, mean_q: 63.441665, mean_eps: 0.100000\n",
      " 555809/1000000: episode: 1628, duration: 0.933s, episode steps: 188, steps per second: 201, episode reward: 243.469, mean reward:  1.295 [-2.320, 100.000], mean action: 0.846 [0.000, 3.000],  loss: 19.481427, mae: 58.388293, mean_q: 77.055328, mean_eps: 0.100000\n",
      " 556042/1000000: episode: 1629, duration: 1.127s, episode steps: 233, steps per second: 207, episode reward: 251.154, mean reward:  1.078 [-9.478, 100.000], mean action: 1.082 [0.000, 3.000],  loss: 7.794703, mae: 59.568378, mean_q: 79.258634, mean_eps: 0.100000\n",
      " 556124/1000000: episode: 1630, duration: 0.389s, episode steps:  82, steps per second: 211, episode reward: -57.069, mean reward: -0.696 [-100.000, 13.473], mean action: 0.841 [0.000, 3.000],  loss: 9.932885, mae: 58.061122, mean_q: 78.902231, mean_eps: 0.100000\n",
      " 556418/1000000: episode: 1631, duration: 1.425s, episode steps: 294, steps per second: 206, episode reward: 255.340, mean reward:  0.869 [-18.907, 100.000], mean action: 0.986 [0.000, 3.000],  loss: 10.144821, mae: 60.931415, mean_q: 82.577491, mean_eps: 0.100000\n",
      " 556919/1000000: episode: 1632, duration: 2.496s, episode steps: 501, steps per second: 201, episode reward: 219.992, mean reward:  0.439 [-18.524, 100.000], mean action: 0.768 [0.000, 3.000],  loss: 8.130071, mae: 46.743662, mean_q: 63.271394, mean_eps: 0.100000\n",
      " 557408/1000000: episode: 1633, duration: 2.537s, episode steps: 489, steps per second: 193, episode reward: 288.666, mean reward:  0.590 [-20.465, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 8.995402, mae: 35.450576, mean_q: 48.274570, mean_eps: 0.100000\n",
      " 557817/1000000: episode: 1634, duration: 2.066s, episode steps: 409, steps per second: 198, episode reward: 272.070, mean reward:  0.665 [-18.525, 100.000], mean action: 1.504 [0.000, 3.000],  loss: 8.327235, mae: 35.870549, mean_q: 48.849707, mean_eps: 0.100000\n",
      " 558501/1000000: episode: 1635, duration: 3.536s, episode steps: 684, steps per second: 193, episode reward: 239.580, mean reward:  0.350 [-20.562, 100.000], mean action: 1.594 [0.000, 3.000],  loss: 4.526584, mae: 43.160430, mean_q: 58.469097, mean_eps: 0.100000\n",
      " 558836/1000000: episode: 1636, duration: 1.666s, episode steps: 335, steps per second: 201, episode reward: 239.162, mean reward:  0.714 [-18.765, 100.000], mean action: 0.913 [0.000, 3.000],  loss: 4.427554, mae: 44.972926, mean_q: 60.892101, mean_eps: 0.100000\n",
      " 559117/1000000: episode: 1637, duration: 1.387s, episode steps: 281, steps per second: 203, episode reward: 222.813, mean reward:  0.793 [-10.701, 100.000], mean action: 1.552 [0.000, 3.000],  loss: 3.906685, mae: 48.158774, mean_q: 65.522572, mean_eps: 0.100000\n",
      " 559309/1000000: episode: 1638, duration: 0.926s, episode steps: 192, steps per second: 207, episode reward: 285.278, mean reward:  1.486 [-9.416, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 3.993932, mae: 52.464154, mean_q: 71.552593, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 559522/1000000: episode: 1639, duration: 1.032s, episode steps: 213, steps per second: 206, episode reward: 282.362, mean reward:  1.326 [-17.611, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 6.579124, mae: 58.704017, mean_q: 80.093105, mean_eps: 0.100000\n",
      " 559691/1000000: episode: 1640, duration: 0.812s, episode steps: 169, steps per second: 208, episode reward: 289.316, mean reward:  1.712 [-17.248, 100.000], mean action: 1.207 [0.000, 3.000],  loss: 5.720137, mae: 59.090982, mean_q: 80.881635, mean_eps: 0.100000\n",
      " 559859/1000000: episode: 1641, duration: 0.796s, episode steps: 168, steps per second: 211, episode reward: 272.322, mean reward:  1.621 [-10.404, 100.000], mean action: 1.351 [0.000, 3.000],  loss: 5.616422, mae: 62.270415, mean_q: 85.147561, mean_eps: 0.100000\n",
      " 559984/1000000: episode: 1642, duration: 0.601s, episode steps: 125, steps per second: 208, episode reward: -105.901, mean reward: -0.847 [-100.000, 43.922], mean action: 1.408 [0.000, 3.000],  loss: 3.730018, mae: 63.248349, mean_q: 86.632984, mean_eps: 0.100000\n",
      " 560484/1000000: episode: 1643, duration: 2.587s, episode steps: 500, steps per second: 193, episode reward: 196.223, mean reward:  0.392 [-19.430, 100.000], mean action: 1.608 [0.000, 3.000],  loss: 9.013455, mae: 62.873718, mean_q: 85.695753, mean_eps: 0.100000\n",
      " 561073/1000000: episode: 1644, duration: 2.939s, episode steps: 589, steps per second: 200, episode reward: -275.149, mean reward: -0.467 [-100.000, 22.572], mean action: 1.594 [0.000, 3.000],  loss: 4.972463, mae: 51.785770, mean_q: 70.417128, mean_eps: 0.100000\n",
      " 562073/1000000: episode: 1645, duration: 5.754s, episode steps: 1000, steps per second: 174, episode reward: -47.885, mean reward: -0.048 [-3.777,  5.479], mean action: 1.876 [0.000, 3.000],  loss: 4.057878, mae: 35.687207, mean_q: 45.939243, mean_eps: 0.100000\n",
      " 563073/1000000: episode: 1646, duration: 5.324s, episode steps: 1000, steps per second: 188, episode reward: -65.958, mean reward: -0.066 [-8.241, 17.788], mean action: 1.806 [0.000, 3.000],  loss: 1.356762, mae: 34.714437, mean_q: 47.202138, mean_eps: 0.100000\n",
      " 563261/1000000: episode: 1647, duration: 0.911s, episode steps: 188, steps per second: 206, episode reward: 282.081, mean reward:  1.500 [-8.713, 100.000], mean action: 2.144 [0.000, 3.000],  loss: 3.327874, mae: 41.014086, mean_q: 55.613126, mean_eps: 0.100000\n",
      " 563516/1000000: episode: 1648, duration: 1.243s, episode steps: 255, steps per second: 205, episode reward: 56.564, mean reward:  0.222 [-100.000, 23.007], mean action: 1.514 [0.000, 3.000],  loss: 3.672872, mae: 45.506753, mean_q: 61.627365, mean_eps: 0.100000\n",
      " 563927/1000000: episode: 1649, duration: 2.100s, episode steps: 411, steps per second: 196, episode reward: 255.364, mean reward:  0.621 [-11.675, 100.000], mean action: 1.623 [0.000, 3.000],  loss: 13.796121, mae: 56.181217, mean_q: 76.331898, mean_eps: 0.100000\n",
      " 564592/1000000: episode: 1650, duration: 3.364s, episode steps: 665, steps per second: 198, episode reward: 256.276, mean reward:  0.385 [-18.763, 100.000], mean action: 1.298 [0.000, 3.000],  loss: 5.440452, mae: 57.297665, mean_q: 77.660247, mean_eps: 0.100000\n",
      " 564858/1000000: episode: 1651, duration: 1.286s, episode steps: 266, steps per second: 207, episode reward: 260.168, mean reward:  0.978 [-8.669, 100.000], mean action: 1.053 [0.000, 3.000],  loss: 2.963724, mae: 48.863467, mean_q: 66.103663, mean_eps: 0.100000\n",
      " 564969/1000000: episode: 1652, duration: 0.527s, episode steps: 111, steps per second: 210, episode reward: 13.016, mean reward:  0.117 [-100.000, 14.341], mean action: 1.459 [0.000, 3.000],  loss: 3.436236, mae: 51.968223, mean_q: 70.334365, mean_eps: 0.100000\n",
      " 565225/1000000: episode: 1653, duration: 1.258s, episode steps: 256, steps per second: 204, episode reward: 295.785, mean reward:  1.155 [-2.898, 100.000], mean action: 1.391 [0.000, 3.000],  loss: 7.122916, mae: 50.615791, mean_q: 68.388130, mean_eps: 0.100000\n",
      " 565605/1000000: episode: 1654, duration: 1.979s, episode steps: 380, steps per second: 192, episode reward: 220.518, mean reward:  0.580 [-18.765, 100.000], mean action: 2.284 [0.000, 3.000],  loss: 10.048131, mae: 54.040509, mean_q: 72.907685, mean_eps: 0.100000\n",
      " 566085/1000000: episode: 1655, duration: 2.570s, episode steps: 480, steps per second: 187, episode reward: 232.130, mean reward:  0.484 [-23.780, 100.000], mean action: 1.617 [0.000, 3.000],  loss: 10.736634, mae: 51.999958, mean_q: 70.448695, mean_eps: 0.100000\n",
      " 566413/1000000: episode: 1656, duration: 1.606s, episode steps: 328, steps per second: 204, episode reward: 245.652, mean reward:  0.749 [-17.951, 100.000], mean action: 1.055 [0.000, 3.000],  loss: 3.454631, mae: 50.675378, mean_q: 68.758790, mean_eps: 0.100000\n",
      " 567338/1000000: episode: 1657, duration: 4.985s, episode steps: 925, steps per second: 186, episode reward: 145.184, mean reward:  0.157 [-18.427, 100.000], mean action: 2.014 [0.000, 3.000],  loss: 3.714200, mae: 47.115349, mean_q: 64.048302, mean_eps: 0.100000\n",
      " 567584/1000000: episode: 1658, duration: 1.186s, episode steps: 246, steps per second: 207, episode reward: 236.938, mean reward:  0.963 [-20.094, 100.000], mean action: 1.077 [0.000, 3.000],  loss: 3.124082, mae: 39.384845, mean_q: 53.660321, mean_eps: 0.100000\n",
      " 567982/1000000: episode: 1659, duration: 2.002s, episode steps: 398, steps per second: 199, episode reward: 210.669, mean reward:  0.529 [-11.584, 100.000], mean action: 1.663 [0.000, 3.000],  loss: 5.852648, mae: 43.678685, mean_q: 59.482104, mean_eps: 0.100000\n",
      " 568279/1000000: episode: 1660, duration: 1.452s, episode steps: 297, steps per second: 205, episode reward: 263.131, mean reward:  0.886 [-12.739, 100.000], mean action: 0.896 [0.000, 3.000],  loss: 8.326268, mae: 49.303871, mean_q: 67.466236, mean_eps: 0.100000\n",
      " 568452/1000000: episode: 1661, duration: 0.836s, episode steps: 173, steps per second: 207, episode reward: 280.992, mean reward:  1.624 [-8.880, 100.000], mean action: 1.457 [0.000, 3.000],  loss: 8.606677, mae: 53.728529, mean_q: 73.575490, mean_eps: 0.100000\n",
      " 568744/1000000: episode: 1662, duration: 1.427s, episode steps: 292, steps per second: 205, episode reward: 277.814, mean reward:  0.951 [-17.400, 100.000], mean action: 0.726 [0.000, 3.000],  loss: 6.340599, mae: 53.666445, mean_q: 73.629661, mean_eps: 0.100000\n",
      " 569049/1000000: episode: 1663, duration: 1.489s, episode steps: 305, steps per second: 205, episode reward: 248.884, mean reward:  0.816 [-17.704, 100.000], mean action: 0.830 [0.000, 3.000],  loss: 6.203959, mae: 55.874516, mean_q: 76.193529, mean_eps: 0.100000\n",
      " 569233/1000000: episode: 1664, duration: 0.889s, episode steps: 184, steps per second: 207, episode reward: 291.580, mean reward:  1.585 [-7.797, 100.000], mean action: 1.190 [0.000, 3.000],  loss: 5.983389, mae: 56.165688, mean_q: 76.466517, mean_eps: 0.100000\n",
      " 569868/1000000: episode: 1665, duration: 3.270s, episode steps: 635, steps per second: 194, episode reward: 216.688, mean reward:  0.341 [-19.612, 100.000], mean action: 2.211 [0.000, 3.000],  loss: 3.810032, mae: 51.688010, mean_q: 70.808681, mean_eps: 0.100000\n",
      " 570766/1000000: episode: 1666, duration: 4.506s, episode steps: 898, steps per second: 199, episode reward: 285.088, mean reward:  0.317 [-20.617, 100.000], mean action: 0.827 [0.000, 3.000],  loss: 3.427549, mae: 41.069338, mean_q: 56.691597, mean_eps: 0.100000\n",
      " 571088/1000000: episode: 1667, duration: 1.580s, episode steps: 322, steps per second: 204, episode reward: 285.769, mean reward:  0.887 [-2.606, 100.000], mean action: 1.453 [0.000, 3.000],  loss: 1.622301, mae: 45.104663, mean_q: 61.551382, mean_eps: 0.100000\n",
      " 571370/1000000: episode: 1668, duration: 1.384s, episode steps: 282, steps per second: 204, episode reward: 281.636, mean reward:  0.999 [-18.956, 100.000], mean action: 1.149 [0.000, 3.000],  loss: 2.311135, mae: 48.146993, mean_q: 65.590728, mean_eps: 0.100000\n",
      " 571478/1000000: episode: 1669, duration: 0.523s, episode steps: 108, steps per second: 206, episode reward: 33.875, mean reward:  0.314 [-100.000, 19.778], mean action: 2.037 [1.000, 3.000],  loss: 4.061452, mae: 53.790834, mean_q: 73.086139, mean_eps: 0.100000\n",
      " 571587/1000000: episode: 1670, duration: 0.528s, episode steps: 109, steps per second: 207, episode reward: 49.622, mean reward:  0.455 [-100.000, 18.487], mean action: 1.908 [0.000, 3.000],  loss: 9.660560, mae: 58.827490, mean_q: 80.084654, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 571681/1000000: episode: 1671, duration: 0.453s, episode steps:  94, steps per second: 207, episode reward: -49.908, mean reward: -0.531 [-100.000, 15.284], mean action: 1.745 [0.000, 3.000],  loss: 21.439383, mae: 63.939923, mean_q: 86.883762, mean_eps: 0.100000\n",
      " 571775/1000000: episode: 1672, duration: 0.452s, episode steps:  94, steps per second: 208, episode reward: -15.607, mean reward: -0.166 [-100.000, 14.435], mean action: 1.947 [0.000, 3.000],  loss: 16.736012, mae: 68.485084, mean_q: 92.893810, mean_eps: 0.100000\n",
      " 571954/1000000: episode: 1673, duration: 0.865s, episode steps: 179, steps per second: 207, episode reward: 266.069, mean reward:  1.486 [-2.626, 100.000], mean action: 1.162 [0.000, 3.000],  loss: 15.985160, mae: 69.925676, mean_q: 94.689266, mean_eps: 0.100000\n",
      " 572150/1000000: episode: 1674, duration: 0.953s, episode steps: 196, steps per second: 206, episode reward: 280.059, mean reward:  1.429 [-9.400, 100.000], mean action: 1.408 [0.000, 3.000],  loss: 13.081128, mae: 72.106989, mean_q: 96.966863, mean_eps: 0.100000\n",
      " 572409/1000000: episode: 1675, duration: 1.260s, episode steps: 259, steps per second: 206, episode reward: 278.764, mean reward:  1.076 [-9.518, 100.000], mean action: 1.340 [0.000, 3.000],  loss: 18.750909, mae: 72.326128, mean_q: 97.391131, mean_eps: 0.100000\n",
      " 572536/1000000: episode: 1676, duration: 0.612s, episode steps: 127, steps per second: 208, episode reward: 26.643, mean reward:  0.210 [-100.000, 18.538], mean action: 1.921 [0.000, 3.000],  loss: 11.707899, mae: 72.597540, mean_q: 97.983377, mean_eps: 0.100000\n",
      " 572631/1000000: episode: 1677, duration: 0.458s, episode steps:  95, steps per second: 207, episode reward: -95.152, mean reward: -1.002 [-100.000, 36.220], mean action: 1.832 [0.000, 3.000],  loss: 22.678259, mae: 73.288407, mean_q: 98.971926, mean_eps: 0.100000\n",
      " 572727/1000000: episode: 1678, duration: 0.463s, episode steps:  96, steps per second: 207, episode reward: -170.028, mean reward: -1.771 [-100.000, 20.555], mean action: 2.042 [0.000, 3.000],  loss: 31.909058, mae: 73.276529, mean_q: 99.211174, mean_eps: 0.100000\n",
      " 572788/1000000: episode: 1679, duration: 0.294s, episode steps:  61, steps per second: 207, episode reward: -39.296, mean reward: -0.644 [-100.000,  7.092], mean action: 2.066 [0.000, 3.000],  loss: 49.248844, mae: 75.708904, mean_q: 101.927380, mean_eps: 0.100000\n",
      " 572924/1000000: episode: 1680, duration: 0.658s, episode steps: 136, steps per second: 207, episode reward: 29.008, mean reward:  0.213 [-100.000, 17.580], mean action: 1.971 [0.000, 3.000],  loss: 32.720889, mae: 75.065421, mean_q: 100.840234, mean_eps: 0.100000\n",
      " 573023/1000000: episode: 1681, duration: 0.480s, episode steps:  99, steps per second: 206, episode reward: 44.172, mean reward:  0.446 [-100.000, 17.467], mean action: 1.889 [0.000, 3.000],  loss: 34.698802, mae: 79.864048, mean_q: 106.659757, mean_eps: 0.100000\n",
      " 573112/1000000: episode: 1682, duration: 0.431s, episode steps:  89, steps per second: 206, episode reward: -44.243, mean reward: -0.497 [-100.000, 10.895], mean action: 1.809 [0.000, 3.000],  loss: 46.167377, mae: 79.873601, mean_q: 106.427581, mean_eps: 0.100000\n",
      " 573215/1000000: episode: 1683, duration: 0.493s, episode steps: 103, steps per second: 209, episode reward: -51.004, mean reward: -0.495 [-100.000, 28.449], mean action: 1.447 [0.000, 3.000],  loss: 48.920590, mae: 80.911135, mean_q: 107.327137, mean_eps: 0.100000\n",
      " 573399/1000000: episode: 1684, duration: 0.881s, episode steps: 184, steps per second: 209, episode reward: -14.132, mean reward: -0.077 [-100.000, 13.902], mean action: 1.565 [0.000, 3.000],  loss: 47.611206, mae: 81.807980, mean_q: 107.532043, mean_eps: 0.100000\n",
      " 573504/1000000: episode: 1685, duration: 0.509s, episode steps: 105, steps per second: 206, episode reward: 62.818, mean reward:  0.598 [-100.000, 11.810], mean action: 1.819 [0.000, 3.000],  loss: 39.329780, mae: 84.745553, mean_q: 109.671362, mean_eps: 0.100000\n",
      " 573654/1000000: episode: 1686, duration: 0.722s, episode steps: 150, steps per second: 208, episode reward: 14.722, mean reward:  0.098 [-100.000, 13.929], mean action: 1.833 [0.000, 3.000],  loss: 32.956664, mae: 83.065451, mean_q: 107.587566, mean_eps: 0.100000\n",
      " 574024/1000000: episode: 1687, duration: 1.863s, episode steps: 370, steps per second: 199, episode reward: 258.975, mean reward:  0.700 [-17.812, 100.000], mean action: 0.851 [0.000, 3.000],  loss: 23.204894, mae: 74.741612, mean_q: 95.155147, mean_eps: 0.100000\n",
      " 574273/1000000: episode: 1688, duration: 1.208s, episode steps: 249, steps per second: 206, episode reward: 289.300, mean reward:  1.162 [-8.887, 100.000], mean action: 1.522 [0.000, 3.000],  loss: 16.718729, mae: 58.950925, mean_q: 74.006717, mean_eps: 0.100000\n",
      " 574404/1000000: episode: 1689, duration: 0.632s, episode steps: 131, steps per second: 207, episode reward: -62.530, mean reward: -0.477 [-100.000,  8.415], mean action: 1.985 [0.000, 3.000],  loss: 13.729720, mae: 54.856025, mean_q: 70.467704, mean_eps: 0.100000\n",
      " 574507/1000000: episode: 1690, duration: 0.492s, episode steps: 103, steps per second: 210, episode reward: 15.416, mean reward:  0.150 [-100.000, 20.588], mean action: 1.777 [0.000, 3.000],  loss: 26.265369, mae: 53.153306, mean_q: 67.765561, mean_eps: 0.100000\n",
      " 574820/1000000: episode: 1691, duration: 1.586s, episode steps: 313, steps per second: 197, episode reward: 232.600, mean reward:  0.743 [-24.195, 100.000], mean action: 1.319 [0.000, 3.000],  loss: 18.887697, mae: 52.056894, mean_q: 66.309063, mean_eps: 0.100000\n",
      " 575100/1000000: episode: 1692, duration: 1.384s, episode steps: 280, steps per second: 202, episode reward: 287.689, mean reward:  1.027 [-17.736, 100.000], mean action: 1.379 [0.000, 3.000],  loss: 15.021454, mae: 56.483072, mean_q: 72.404741, mean_eps: 0.100000\n",
      " 575418/1000000: episode: 1693, duration: 1.590s, episode steps: 318, steps per second: 200, episode reward: 223.855, mean reward:  0.704 [-17.536, 100.000], mean action: 1.547 [0.000, 3.000],  loss: 11.870205, mae: 54.814857, mean_q: 70.392242, mean_eps: 0.100000\n",
      " 575913/1000000: episode: 1694, duration: 2.598s, episode steps: 495, steps per second: 191, episode reward: 202.291, mean reward:  0.409 [-20.447, 100.000], mean action: 1.598 [0.000, 3.000],  loss: 6.386381, mae: 47.037613, mean_q: 63.583551, mean_eps: 0.100000\n",
      " 576287/1000000: episode: 1695, duration: 1.880s, episode steps: 374, steps per second: 199, episode reward: 267.952, mean reward:  0.716 [-11.256, 100.000], mean action: 1.527 [0.000, 3.000],  loss: 6.820916, mae: 42.914095, mean_q: 58.166053, mean_eps: 0.100000\n",
      " 576690/1000000: episode: 1696, duration: 2.065s, episode steps: 403, steps per second: 195, episode reward: 268.390, mean reward:  0.666 [-17.359, 100.000], mean action: 1.161 [0.000, 3.000],  loss: 7.173350, mae: 42.445538, mean_q: 57.797424, mean_eps: 0.100000\n",
      " 577111/1000000: episode: 1697, duration: 2.189s, episode steps: 421, steps per second: 192, episode reward: 232.058, mean reward:  0.551 [-10.849, 100.000], mean action: 1.658 [0.000, 3.000],  loss: 6.943745, mae: 43.541091, mean_q: 59.302411, mean_eps: 0.100000\n",
      " 577506/1000000: episode: 1698, duration: 2.006s, episode steps: 395, steps per second: 197, episode reward: 208.658, mean reward:  0.528 [-20.646, 100.000], mean action: 1.927 [0.000, 3.000],  loss: 7.990172, mae: 40.852288, mean_q: 55.744877, mean_eps: 0.100000\n",
      " 578057/1000000: episode: 1699, duration: 2.903s, episode steps: 551, steps per second: 190, episode reward: 191.962, mean reward:  0.348 [-24.018, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 11.601537, mae: 38.278103, mean_q: 52.530133, mean_eps: 0.100000\n",
      " 578608/1000000: episode: 1700, duration: 2.847s, episode steps: 551, steps per second: 194, episode reward: 237.439, mean reward:  0.431 [-20.556, 100.000], mean action: 0.956 [0.000, 3.000],  loss: 7.378782, mae: 36.988511, mean_q: 50.547968, mean_eps: 0.100000\n",
      " 578822/1000000: episode: 1701, duration: 1.033s, episode steps: 214, steps per second: 207, episode reward: 269.798, mean reward:  1.261 [-10.292, 100.000], mean action: 1.472 [0.000, 3.000],  loss: 5.584053, mae: 37.350654, mean_q: 50.715225, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 579238/1000000: episode: 1702, duration: 2.133s, episode steps: 416, steps per second: 195, episode reward: 264.924, mean reward:  0.637 [-21.403, 100.000], mean action: 0.889 [0.000, 3.000],  loss: 4.790030, mae: 41.085512, mean_q: 55.801368, mean_eps: 0.100000\n",
      " 579530/1000000: episode: 1703, duration: 1.435s, episode steps: 292, steps per second: 203, episode reward: 254.107, mean reward:  0.870 [-10.470, 100.000], mean action: 1.271 [0.000, 3.000],  loss: 4.868965, mae: 43.791879, mean_q: 59.444742, mean_eps: 0.100000\n",
      " 579709/1000000: episode: 1704, duration: 0.857s, episode steps: 179, steps per second: 209, episode reward: 245.190, mean reward:  1.370 [-10.672, 100.000], mean action: 1.330 [0.000, 3.000],  loss: 5.381218, mae: 47.881897, mean_q: 64.925745, mean_eps: 0.100000\n",
      " 579828/1000000: episode: 1705, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: 16.977, mean reward:  0.143 [-100.000, 16.382], mean action: 1.731 [0.000, 3.000],  loss: 8.485482, mae: 47.483146, mean_q: 64.467991, mean_eps: 0.100000\n",
      " 580286/1000000: episode: 1706, duration: 2.333s, episode steps: 458, steps per second: 196, episode reward: 279.983, mean reward:  0.611 [-10.071, 100.000], mean action: 1.111 [0.000, 3.000],  loss: 12.874857, mae: 49.843545, mean_q: 67.787794, mean_eps: 0.100000\n",
      " 580606/1000000: episode: 1707, duration: 1.586s, episode steps: 320, steps per second: 202, episode reward: 273.923, mean reward:  0.856 [-19.578, 100.000], mean action: 1.266 [0.000, 3.000],  loss: 8.195155, mae: 49.949230, mean_q: 67.891462, mean_eps: 0.100000\n",
      " 580720/1000000: episode: 1708, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward:  1.358, mean reward:  0.012 [-100.000, 16.395], mean action: 1.421 [0.000, 3.000],  loss: 11.166803, mae: 50.293857, mean_q: 68.375638, mean_eps: 0.100000\n",
      " 581079/1000000: episode: 1709, duration: 1.750s, episode steps: 359, steps per second: 205, episode reward: 291.946, mean reward:  0.813 [-10.303, 100.000], mean action: 0.925 [0.000, 3.000],  loss: 12.405601, mae: 50.112618, mean_q: 68.300528, mean_eps: 0.100000\n",
      " 581493/1000000: episode: 1710, duration: 2.087s, episode steps: 414, steps per second: 198, episode reward: 274.428, mean reward:  0.663 [-17.875, 100.000], mean action: 1.087 [0.000, 3.000],  loss: 6.062510, mae: 51.553219, mean_q: 70.162688, mean_eps: 0.100000\n",
      " 581918/1000000: episode: 1711, duration: 2.170s, episode steps: 425, steps per second: 196, episode reward: 247.734, mean reward:  0.583 [-17.721, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 4.087215, mae: 50.791784, mean_q: 68.956838, mean_eps: 0.100000\n",
      " 582149/1000000: episode: 1712, duration: 1.127s, episode steps: 231, steps per second: 205, episode reward: 261.096, mean reward:  1.130 [-17.923, 100.000], mean action: 1.515 [0.000, 3.000],  loss: 3.702604, mae: 51.654569, mean_q: 69.831094, mean_eps: 0.100000\n",
      " 582401/1000000: episode: 1713, duration: 1.241s, episode steps: 252, steps per second: 203, episode reward: 274.215, mean reward:  1.088 [-19.070, 100.000], mean action: 1.456 [0.000, 3.000],  loss: 3.944129, mae: 53.444403, mean_q: 72.165668, mean_eps: 0.100000\n",
      " 582763/1000000: episode: 1714, duration: 1.833s, episode steps: 362, steps per second: 197, episode reward: 241.340, mean reward:  0.667 [-19.706, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 3.780876, mae: 55.681240, mean_q: 75.264713, mean_eps: 0.100000\n",
      " 583032/1000000: episode: 1715, duration: 1.330s, episode steps: 269, steps per second: 202, episode reward: 277.907, mean reward:  1.033 [-19.212, 100.000], mean action: 1.454 [0.000, 3.000],  loss: 4.838172, mae: 56.393023, mean_q: 76.368441, mean_eps: 0.100000\n",
      " 583133/1000000: episode: 1716, duration: 0.489s, episode steps: 101, steps per second: 206, episode reward: 23.047, mean reward:  0.228 [-100.000, 18.626], mean action: 1.990 [0.000, 3.000],  loss: 5.367129, mae: 57.393843, mean_q: 77.880926, mean_eps: 0.100000\n",
      " 583673/1000000: episode: 1717, duration: 2.679s, episode steps: 540, steps per second: 202, episode reward: 252.465, mean reward:  0.468 [-20.339, 100.000], mean action: 0.759 [0.000, 3.000],  loss: 4.557347, mae: 56.849104, mean_q: 76.394742, mean_eps: 0.100000\n",
      " 584012/1000000: episode: 1718, duration: 1.722s, episode steps: 339, steps per second: 197, episode reward: 212.444, mean reward:  0.627 [-19.688, 100.000], mean action: 1.392 [0.000, 3.000],  loss: 3.259145, mae: 54.419652, mean_q: 72.768722, mean_eps: 0.100000\n",
      " 584588/1000000: episode: 1719, duration: 3.035s, episode steps: 576, steps per second: 190, episode reward: 228.470, mean reward:  0.397 [-17.470, 100.000], mean action: 1.321 [0.000, 3.000],  loss: 2.609725, mae: 52.104505, mean_q: 70.189519, mean_eps: 0.100000\n",
      " 584923/1000000: episode: 1720, duration: 1.670s, episode steps: 335, steps per second: 201, episode reward: 283.815, mean reward:  0.847 [-17.790, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 3.176217, mae: 52.423358, mean_q: 70.781855, mean_eps: 0.100000\n",
      " 585514/1000000: episode: 1721, duration: 3.015s, episode steps: 591, steps per second: 196, episode reward: 217.640, mean reward:  0.368 [-19.632, 100.000], mean action: 0.707 [0.000, 3.000],  loss: 3.002843, mae: 51.150441, mean_q: 69.229259, mean_eps: 0.100000\n",
      " 586219/1000000: episode: 1722, duration: 3.795s, episode steps: 705, steps per second: 186, episode reward: 241.018, mean reward:  0.342 [-19.030, 100.000], mean action: 1.128 [0.000, 3.000],  loss: 1.713077, mae: 49.648701, mean_q: 67.121098, mean_eps: 0.100000\n",
      " 586483/1000000: episode: 1723, duration: 1.289s, episode steps: 264, steps per second: 205, episode reward: 246.139, mean reward:  0.932 [-11.361, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 3.122754, mae: 48.179056, mean_q: 65.082203, mean_eps: 0.100000\n",
      " 587041/1000000: episode: 1724, duration: 2.834s, episode steps: 558, steps per second: 197, episode reward: 262.552, mean reward:  0.471 [-21.701, 100.000], mean action: 0.624 [0.000, 3.000],  loss: 3.306024, mae: 48.351694, mean_q: 65.527387, mean_eps: 0.100000\n",
      " 587540/1000000: episode: 1725, duration: 2.551s, episode steps: 499, steps per second: 196, episode reward: 273.794, mean reward:  0.549 [-18.513, 100.000], mean action: 1.234 [0.000, 3.000],  loss: 3.105289, mae: 49.415855, mean_q: 66.820905, mean_eps: 0.100000\n",
      " 587658/1000000: episode: 1726, duration: 0.569s, episode steps: 118, steps per second: 207, episode reward: 30.276, mean reward:  0.257 [-100.000,  9.890], mean action: 1.619 [0.000, 3.000],  loss: 2.166310, mae: 47.754600, mean_q: 64.594038, mean_eps: 0.100000\n",
      " 587786/1000000: episode: 1727, duration: 0.620s, episode steps: 128, steps per second: 206, episode reward:  4.148, mean reward:  0.032 [-100.000,  9.086], mean action: 1.953 [0.000, 3.000],  loss: 8.158764, mae: 50.868770, mean_q: 68.354224, mean_eps: 0.100000\n",
      " 588409/1000000: episode: 1728, duration: 3.218s, episode steps: 623, steps per second: 194, episode reward: 259.673, mean reward:  0.417 [-18.239, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 5.297098, mae: 53.739948, mean_q: 71.225201, mean_eps: 0.100000\n",
      " 588642/1000000: episode: 1729, duration: 1.135s, episode steps: 233, steps per second: 205, episode reward: 26.855, mean reward:  0.115 [-100.000, 13.062], mean action: 1.541 [0.000, 3.000],  loss: 4.204036, mae: 50.583585, mean_q: 66.416169, mean_eps: 0.100000\n",
      " 588743/1000000: episode: 1730, duration: 0.487s, episode steps: 101, steps per second: 207, episode reward: 21.256, mean reward:  0.210 [-100.000, 16.186], mean action: 1.980 [0.000, 3.000],  loss: 5.329961, mae: 51.304139, mean_q: 67.270354, mean_eps: 0.100000\n",
      " 589194/1000000: episode: 1731, duration: 2.351s, episode steps: 451, steps per second: 192, episode reward: 235.964, mean reward:  0.523 [-19.239, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 5.931729, mae: 51.268700, mean_q: 67.195886, mean_eps: 0.100000\n",
      " 589441/1000000: episode: 1732, duration: 1.247s, episode steps: 247, steps per second: 198, episode reward: 286.533, mean reward:  1.160 [-7.197, 100.000], mean action: 1.389 [0.000, 3.000],  loss: 6.009366, mae: 51.968166, mean_q: 67.984360, mean_eps: 0.100000\n",
      " 589710/1000000: episode: 1733, duration: 1.339s, episode steps: 269, steps per second: 201, episode reward: 233.828, mean reward:  0.869 [-10.536, 100.000], mean action: 1.331 [0.000, 3.000],  loss: 5.165621, mae: 52.778709, mean_q: 69.715048, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 589996/1000000: episode: 1734, duration: 1.410s, episode steps: 286, steps per second: 203, episode reward: 245.452, mean reward:  0.858 [-17.936, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 4.450658, mae: 49.907013, mean_q: 67.683126, mean_eps: 0.100000\n",
      " 590299/1000000: episode: 1735, duration: 1.521s, episode steps: 303, steps per second: 199, episode reward: 247.161, mean reward:  0.816 [-19.377, 100.000], mean action: 1.314 [0.000, 3.000],  loss: 6.574877, mae: 51.443221, mean_q: 69.754820, mean_eps: 0.100000\n",
      " 590751/1000000: episode: 1736, duration: 2.343s, episode steps: 452, steps per second: 193, episode reward: 246.051, mean reward:  0.544 [-18.170, 100.000], mean action: 1.670 [0.000, 3.000],  loss: 6.788423, mae: 51.720587, mean_q: 69.997094, mean_eps: 0.100000\n",
      " 591360/1000000: episode: 1737, duration: 3.174s, episode steps: 609, steps per second: 192, episode reward: 252.072, mean reward:  0.414 [-20.860, 100.000], mean action: 1.207 [0.000, 3.000],  loss: 3.201758, mae: 51.829837, mean_q: 70.029326, mean_eps: 0.100000\n",
      " 592360/1000000: episode: 1738, duration: 5.378s, episode steps: 1000, steps per second: 186, episode reward: 110.391, mean reward:  0.110 [-17.577, 13.473], mean action: 1.074 [0.000, 3.000],  loss: 2.479942, mae: 44.794948, mean_q: 60.574435, mean_eps: 0.100000\n",
      " 592707/1000000: episode: 1739, duration: 1.729s, episode steps: 347, steps per second: 201, episode reward: 253.686, mean reward:  0.731 [-18.382, 100.000], mean action: 1.159 [0.000, 3.000],  loss: 0.892493, mae: 38.509412, mean_q: 51.985947, mean_eps: 0.100000\n",
      " 592942/1000000: episode: 1740, duration: 1.150s, episode steps: 235, steps per second: 204, episode reward: 239.942, mean reward:  1.021 [-23.750, 100.000], mean action: 1.306 [0.000, 3.000],  loss: 3.026515, mae: 41.423094, mean_q: 55.954250, mean_eps: 0.100000\n",
      " 593301/1000000: episode: 1741, duration: 1.824s, episode steps: 359, steps per second: 197, episode reward: 276.407, mean reward:  0.770 [-19.899, 100.000], mean action: 1.460 [0.000, 3.000],  loss: 4.105055, mae: 46.543879, mean_q: 63.396687, mean_eps: 0.100000\n",
      " 593843/1000000: episode: 1742, duration: 2.920s, episode steps: 542, steps per second: 186, episode reward: 275.314, mean reward:  0.508 [-19.796, 100.000], mean action: 0.756 [0.000, 3.000],  loss: 5.418956, mae: 47.430233, mean_q: 65.194749, mean_eps: 0.100000\n",
      " 594168/1000000: episode: 1743, duration: 1.616s, episode steps: 325, steps per second: 201, episode reward: 250.606, mean reward:  0.771 [-19.049, 100.000], mean action: 0.886 [0.000, 3.000],  loss: 5.246898, mae: 44.464486, mean_q: 61.028864, mean_eps: 0.100000\n",
      " 594486/1000000: episode: 1744, duration: 1.694s, episode steps: 318, steps per second: 188, episode reward: 269.777, mean reward:  0.848 [-19.740, 100.000], mean action: 1.302 [0.000, 3.000],  loss: 4.270687, mae: 46.528256, mean_q: 63.298061, mean_eps: 0.100000\n",
      " 594667/1000000: episode: 1745, duration: 0.965s, episode steps: 181, steps per second: 187, episode reward: 275.055, mean reward:  1.520 [-5.056, 100.000], mean action: 1.276 [0.000, 3.000],  loss: 6.148584, mae: 48.300863, mean_q: 65.596112, mean_eps: 0.100000\n",
      " 594906/1000000: episode: 1746, duration: 1.410s, episode steps: 239, steps per second: 169, episode reward: 259.236, mean reward:  1.085 [-17.208, 100.000], mean action: 0.987 [0.000, 3.000],  loss: 4.154514, mae: 52.600060, mean_q: 71.371940, mean_eps: 0.100000\n",
      " 595404/1000000: episode: 1747, duration: 3.053s, episode steps: 498, steps per second: 163, episode reward: 276.319, mean reward:  0.555 [-18.886, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 5.058103, mae: 51.549868, mean_q: 70.001027, mean_eps: 0.100000\n",
      " 595841/1000000: episode: 1748, duration: 2.611s, episode steps: 437, steps per second: 167, episode reward: 286.373, mean reward:  0.655 [-17.359, 100.000], mean action: 1.279 [0.000, 3.000],  loss: 3.872924, mae: 48.849362, mean_q: 66.300958, mean_eps: 0.100000\n",
      " 596023/1000000: episode: 1749, duration: 1.025s, episode steps: 182, steps per second: 178, episode reward: 242.305, mean reward:  1.331 [-17.472, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 5.511757, mae: 47.159554, mean_q: 63.940346, mean_eps: 0.100000\n",
      " 596254/1000000: episode: 1750, duration: 1.132s, episode steps: 231, steps per second: 204, episode reward: 243.870, mean reward:  1.056 [-18.519, 100.000], mean action: 1.026 [0.000, 3.000],  loss: 3.306255, mae: 48.223554, mean_q: 65.443066, mean_eps: 0.100000\n",
      " 596673/1000000: episode: 1751, duration: 2.139s, episode steps: 419, steps per second: 196, episode reward: 281.409, mean reward:  0.672 [-10.515, 100.000], mean action: 1.229 [0.000, 3.000],  loss: 4.836663, mae: 51.759704, mean_q: 70.262967, mean_eps: 0.100000\n",
      " 597170/1000000: episode: 1752, duration: 2.538s, episode steps: 497, steps per second: 196, episode reward: 299.389, mean reward:  0.602 [-17.504, 100.000], mean action: 0.769 [0.000, 3.000],  loss: 3.881202, mae: 49.682259, mean_q: 67.378373, mean_eps: 0.100000\n",
      " 597283/1000000: episode: 1753, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: 11.129, mean reward:  0.098 [-100.000, 11.838], mean action: 1.850 [0.000, 3.000],  loss: 2.799648, mae: 45.160431, mean_q: 61.120004, mean_eps: 0.100000\n",
      " 597382/1000000: episode: 1754, duration: 0.473s, episode steps:  99, steps per second: 209, episode reward: 38.493, mean reward:  0.389 [-100.000, 14.167], mean action: 1.697 [0.000, 3.000],  loss: 27.860260, mae: 46.001020, mean_q: 62.438653, mean_eps: 0.100000\n",
      " 597562/1000000: episode: 1755, duration: 0.865s, episode steps: 180, steps per second: 208, episode reward: 255.999, mean reward:  1.422 [-17.374, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 35.472110, mae: 48.549499, mean_q: 66.691780, mean_eps: 0.100000\n",
      " 597888/1000000: episode: 1756, duration: 1.630s, episode steps: 326, steps per second: 200, episode reward: 254.415, mean reward:  0.780 [-11.424, 100.000], mean action: 1.528 [0.000, 3.000],  loss: 24.238978, mae: 51.603347, mean_q: 71.075117, mean_eps: 0.100000\n",
      " 598138/1000000: episode: 1757, duration: 1.193s, episode steps: 250, steps per second: 209, episode reward: 275.144, mean reward:  1.101 [-18.698, 100.000], mean action: 1.020 [0.000, 3.000],  loss: 22.808614, mae: 54.783296, mean_q: 75.449995, mean_eps: 0.100000\n",
      " 599100/1000000: episode: 1758, duration: 5.046s, episode steps: 962, steps per second: 191, episode reward: 235.292, mean reward:  0.245 [-20.227, 100.000], mean action: 1.761 [0.000, 3.000],  loss: 8.482665, mae: 52.593594, mean_q: 72.362815, mean_eps: 0.100000\n",
      " 599555/1000000: episode: 1759, duration: 2.307s, episode steps: 455, steps per second: 197, episode reward: 283.351, mean reward:  0.623 [-11.572, 100.000], mean action: 1.426 [0.000, 3.000],  loss: 1.536789, mae: 46.215042, mean_q: 63.163320, mean_eps: 0.100000\n",
      " 599767/1000000: episode: 1760, duration: 1.024s, episode steps: 212, steps per second: 207, episode reward: 273.049, mean reward:  1.288 [-3.681, 100.000], mean action: 1.458 [0.000, 3.000],  loss: 1.261604, mae: 45.360950, mean_q: 61.853766, mean_eps: 0.100000\n",
      " 600237/1000000: episode: 1761, duration: 2.383s, episode steps: 470, steps per second: 197, episode reward: 174.585, mean reward:  0.371 [-17.717, 100.000], mean action: 1.255 [0.000, 3.000],  loss: 5.418450, mae: 47.291871, mean_q: 64.761120, mean_eps: 0.100000\n",
      " 600435/1000000: episode: 1762, duration: 0.954s, episode steps: 198, steps per second: 207, episode reward: 273.132, mean reward:  1.379 [-17.970, 100.000], mean action: 1.141 [0.000, 3.000],  loss: 10.311377, mae: 47.050100, mean_q: 64.625380, mean_eps: 0.100000\n",
      " 600741/1000000: episode: 1763, duration: 1.513s, episode steps: 306, steps per second: 202, episode reward: 289.413, mean reward:  0.946 [-18.896, 100.000], mean action: 1.454 [0.000, 3.000],  loss: 4.219011, mae: 50.907546, mean_q: 69.772771, mean_eps: 0.100000\n",
      " 600952/1000000: episode: 1764, duration: 1.015s, episode steps: 211, steps per second: 208, episode reward: 233.863, mean reward:  1.108 [-13.583, 100.000], mean action: 1.488 [0.000, 3.000],  loss: 7.439397, mae: 49.920592, mean_q: 68.341069, mean_eps: 0.100000\n",
      " 601041/1000000: episode: 1765, duration: 0.428s, episode steps:  89, steps per second: 208, episode reward: 13.615, mean reward:  0.153 [-100.000, 19.158], mean action: 1.775 [0.000, 3.000],  loss: 7.705748, mae: 50.622076, mean_q: 69.237557, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 601216/1000000: episode: 1766, duration: 0.847s, episode steps: 175, steps per second: 207, episode reward: 276.741, mean reward:  1.581 [-1.955, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 17.762729, mae: 58.954094, mean_q: 80.401716, mean_eps: 0.100000\n",
      " 601310/1000000: episode: 1767, duration: 0.452s, episode steps:  94, steps per second: 208, episode reward: 17.560, mean reward:  0.187 [-100.000, 16.783], mean action: 1.926 [0.000, 3.000],  loss: 14.073615, mae: 62.040280, mean_q: 84.497549, mean_eps: 0.100000\n",
      " 601422/1000000: episode: 1768, duration: 0.536s, episode steps: 112, steps per second: 209, episode reward: 25.711, mean reward:  0.230 [-100.000,  9.904], mean action: 1.509 [0.000, 3.000],  loss: 12.674551, mae: 63.695419, mean_q: 86.500306, mean_eps: 0.100000\n",
      " 601519/1000000: episode: 1769, duration: 0.462s, episode steps:  97, steps per second: 210, episode reward: -19.452, mean reward: -0.201 [-100.000, 15.862], mean action: 1.474 [0.000, 3.000],  loss: 16.737810, mae: 65.298185, mean_q: 88.443566, mean_eps: 0.100000\n",
      " 601824/1000000: episode: 1770, duration: 1.487s, episode steps: 305, steps per second: 205, episode reward: 251.310, mean reward:  0.824 [-10.674, 100.000], mean action: 1.066 [0.000, 3.000],  loss: 13.114513, mae: 66.212639, mean_q: 89.136931, mean_eps: 0.100000\n",
      " 602015/1000000: episode: 1771, duration: 0.920s, episode steps: 191, steps per second: 208, episode reward: 252.276, mean reward:  1.321 [-20.161, 100.000], mean action: 1.157 [0.000, 3.000],  loss: 11.267472, mae: 65.813972, mean_q: 87.942359, mean_eps: 0.100000\n",
      " 602250/1000000: episode: 1772, duration: 1.133s, episode steps: 235, steps per second: 207, episode reward: 249.961, mean reward:  1.064 [-18.192, 100.000], mean action: 0.894 [0.000, 3.000],  loss: 8.132397, mae: 63.560069, mean_q: 85.131821, mean_eps: 0.100000\n",
      " 602428/1000000: episode: 1773, duration: 0.867s, episode steps: 178, steps per second: 205, episode reward: 246.944, mean reward:  1.387 [-9.701, 100.000], mean action: 1.725 [0.000, 3.000],  loss: 6.259522, mae: 59.373553, mean_q: 79.718673, mean_eps: 0.100000\n",
      " 602511/1000000: episode: 1774, duration: 0.400s, episode steps:  83, steps per second: 208, episode reward: -19.612, mean reward: -0.236 [-100.000,  9.382], mean action: 1.711 [0.000, 3.000],  loss: 10.341492, mae: 57.899739, mean_q: 78.115932, mean_eps: 0.100000\n",
      " 602878/1000000: episode: 1775, duration: 1.825s, episode steps: 367, steps per second: 201, episode reward: 273.468, mean reward:  0.745 [-19.928, 100.000], mean action: 0.899 [0.000, 3.000],  loss: 8.044222, mae: 57.075896, mean_q: 76.967765, mean_eps: 0.100000\n",
      " 603080/1000000: episode: 1776, duration: 0.987s, episode steps: 202, steps per second: 205, episode reward: 262.431, mean reward:  1.299 [-3.151, 100.000], mean action: 1.653 [0.000, 3.000],  loss: 7.329355, mae: 51.558618, mean_q: 69.607304, mean_eps: 0.100000\n",
      " 603474/1000000: episode: 1777, duration: 1.941s, episode steps: 394, steps per second: 203, episode reward: 250.459, mean reward:  0.636 [-19.064, 100.000], mean action: 0.817 [0.000, 3.000],  loss: 7.416677, mae: 52.038363, mean_q: 69.859953, mean_eps: 0.100000\n",
      " 603803/1000000: episode: 1778, duration: 1.652s, episode steps: 329, steps per second: 199, episode reward: 279.655, mean reward:  0.850 [-9.677, 100.000], mean action: 1.620 [0.000, 3.000],  loss: 7.578360, mae: 50.916856, mean_q: 68.984232, mean_eps: 0.100000\n",
      " 603992/1000000: episode: 1779, duration: 0.910s, episode steps: 189, steps per second: 208, episode reward: 273.631, mean reward:  1.448 [-20.608, 100.000], mean action: 1.159 [0.000, 3.000],  loss: 6.141333, mae: 58.727284, mean_q: 79.621115, mean_eps: 0.100000\n",
      " 604244/1000000: episode: 1780, duration: 1.210s, episode steps: 252, steps per second: 208, episode reward: 286.520, mean reward:  1.137 [-4.313, 100.000], mean action: 1.155 [0.000, 3.000],  loss: 5.867156, mae: 58.123209, mean_q: 78.877253, mean_eps: 0.100000\n",
      " 604456/1000000: episode: 1781, duration: 1.014s, episode steps: 212, steps per second: 209, episode reward: 274.909, mean reward:  1.297 [-7.994, 100.000], mean action: 0.976 [0.000, 3.000],  loss: 3.567368, mae: 60.412511, mean_q: 81.850201, mean_eps: 0.100000\n",
      " 605456/1000000: episode: 1782, duration: 5.165s, episode steps: 1000, steps per second: 194, episode reward: 162.541, mean reward:  0.163 [-21.501, 22.888], mean action: 0.890 [0.000, 3.000],  loss: 3.350448, mae: 56.558404, mean_q: 76.807878, mean_eps: 0.100000\n",
      " 605723/1000000: episode: 1783, duration: 1.308s, episode steps: 267, steps per second: 204, episode reward: 263.210, mean reward:  0.986 [-19.570, 100.000], mean action: 1.075 [0.000, 3.000],  loss: 2.445752, mae: 45.287135, mean_q: 61.741263, mean_eps: 0.100000\n",
      " 605870/1000000: episode: 1784, duration: 0.705s, episode steps: 147, steps per second: 209, episode reward: -48.032, mean reward: -0.327 [-100.000, 10.983], mean action: 1.878 [0.000, 3.000],  loss: 4.249107, mae: 45.500593, mean_q: 62.170855, mean_eps: 0.100000\n",
      " 606048/1000000: episode: 1785, duration: 0.859s, episode steps: 178, steps per second: 207, episode reward: 246.774, mean reward:  1.386 [-8.936, 100.000], mean action: 1.612 [0.000, 3.000],  loss: 15.638397, mae: 50.213113, mean_q: 68.607931, mean_eps: 0.100000\n",
      " 606165/1000000: episode: 1786, duration: 0.563s, episode steps: 117, steps per second: 208, episode reward:  4.376, mean reward:  0.037 [-100.000, 10.736], mean action: 1.821 [0.000, 3.000],  loss: 25.045418, mae: 52.554415, mean_q: 71.845822, mean_eps: 0.100000\n",
      " 606384/1000000: episode: 1787, duration: 1.056s, episode steps: 219, steps per second: 207, episode reward: 279.305, mean reward:  1.275 [-10.232, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 21.203066, mae: 58.227892, mean_q: 79.520922, mean_eps: 0.100000\n",
      " 606560/1000000: episode: 1788, duration: 0.846s, episode steps: 176, steps per second: 208, episode reward: 255.411, mean reward:  1.451 [-9.156, 100.000], mean action: 1.199 [0.000, 3.000],  loss: 16.337480, mae: 62.377051, mean_q: 85.220071, mean_eps: 0.100000\n",
      " 606637/1000000: episode: 1789, duration: 0.375s, episode steps:  77, steps per second: 206, episode reward: 32.799, mean reward:  0.426 [-100.000, 22.416], mean action: 2.013 [0.000, 3.000],  loss: 15.717266, mae: 63.852317, mean_q: 87.252935, mean_eps: 0.100000\n",
      " 606835/1000000: episode: 1790, duration: 0.948s, episode steps: 198, steps per second: 209, episode reward: 254.872, mean reward:  1.287 [-19.760, 100.000], mean action: 1.167 [0.000, 3.000],  loss: 18.444523, mae: 68.011822, mean_q: 92.339412, mean_eps: 0.100000\n",
      " 607144/1000000: episode: 1791, duration: 1.507s, episode steps: 309, steps per second: 205, episode reward: 270.441, mean reward:  0.875 [-17.388, 100.000], mean action: 0.851 [0.000, 3.000],  loss: 11.498659, mae: 66.675804, mean_q: 90.507672, mean_eps: 0.100000\n",
      " 607239/1000000: episode: 1792, duration: 0.452s, episode steps:  95, steps per second: 210, episode reward: -25.991, mean reward: -0.274 [-100.000,  8.097], mean action: 1.484 [0.000, 3.000],  loss: 6.838948, mae: 65.247079, mean_q: 88.457490, mean_eps: 0.100000\n",
      " 607397/1000000: episode: 1793, duration: 0.761s, episode steps: 158, steps per second: 208, episode reward: 11.081, mean reward:  0.070 [-100.000, 10.711], mean action: 1.665 [0.000, 3.000],  loss: 4.548406, mae: 64.635564, mean_q: 87.388651, mean_eps: 0.100000\n",
      " 607765/1000000: episode: 1794, duration: 1.859s, episode steps: 368, steps per second: 198, episode reward: 249.614, mean reward:  0.678 [-10.293, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 6.407501, mae: 61.624189, mean_q: 82.741489, mean_eps: 0.100000\n",
      " 608016/1000000: episode: 1795, duration: 1.237s, episode steps: 251, steps per second: 203, episode reward: 259.442, mean reward:  1.034 [-11.006, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 6.245235, mae: 57.575806, mean_q: 77.103172, mean_eps: 0.100000\n",
      " 608337/1000000: episode: 1796, duration: 1.582s, episode steps: 321, steps per second: 203, episode reward: 299.482, mean reward:  0.933 [-17.334, 100.000], mean action: 0.950 [0.000, 3.000],  loss: 8.721469, mae: 54.964115, mean_q: 73.589816, mean_eps: 0.100000\n",
      " 608579/1000000: episode: 1797, duration: 1.185s, episode steps: 242, steps per second: 204, episode reward: 254.859, mean reward:  1.053 [-19.541, 100.000], mean action: 1.463 [0.000, 3.000],  loss: 6.402213, mae: 52.013175, mean_q: 70.456549, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 608871/1000000: episode: 1798, duration: 1.449s, episode steps: 292, steps per second: 202, episode reward: 287.140, mean reward:  0.983 [-9.911, 100.000], mean action: 1.346 [0.000, 3.000],  loss: 7.243328, mae: 54.575720, mean_q: 74.394079, mean_eps: 0.100000\n",
      " 609151/1000000: episode: 1799, duration: 1.358s, episode steps: 280, steps per second: 206, episode reward: 257.901, mean reward:  0.921 [-18.885, 100.000], mean action: 0.925 [0.000, 3.000],  loss: 6.862979, mae: 55.362147, mean_q: 75.483188, mean_eps: 0.100000\n",
      " 609370/1000000: episode: 1800, duration: 1.053s, episode steps: 219, steps per second: 208, episode reward: 264.012, mean reward:  1.206 [-9.187, 100.000], mean action: 0.941 [0.000, 3.000],  loss: 6.320304, mae: 57.312827, mean_q: 78.117795, mean_eps: 0.100000\n",
      " 609536/1000000: episode: 1801, duration: 0.795s, episode steps: 166, steps per second: 209, episode reward: 262.532, mean reward:  1.582 [-7.889, 100.000], mean action: 1.277 [0.000, 3.000],  loss: 5.574450, mae: 58.427067, mean_q: 79.585025, mean_eps: 0.100000\n",
      " 609701/1000000: episode: 1802, duration: 0.790s, episode steps: 165, steps per second: 209, episode reward: -6.780, mean reward: -0.041 [-100.000, 14.244], mean action: 1.461 [0.000, 3.000],  loss: 5.988103, mae: 60.186346, mean_q: 82.023008, mean_eps: 0.100000\n",
      " 609822/1000000: episode: 1803, duration: 0.577s, episode steps: 121, steps per second: 210, episode reward: 21.543, mean reward:  0.178 [-100.000, 18.412], mean action: 1.256 [0.000, 3.000],  loss: 13.779721, mae: 59.927015, mean_q: 81.568870, mean_eps: 0.100000\n",
      " 610125/1000000: episode: 1804, duration: 1.510s, episode steps: 303, steps per second: 201, episode reward: 269.805, mean reward:  0.890 [-17.534, 100.000], mean action: 1.026 [0.000, 3.000],  loss: 8.362802, mae: 59.419966, mean_q: 80.242671, mean_eps: 0.100000\n",
      " 610492/1000000: episode: 1805, duration: 1.847s, episode steps: 367, steps per second: 199, episode reward: 228.456, mean reward:  0.622 [-12.793, 100.000], mean action: 1.144 [0.000, 3.000],  loss: 6.425619, mae: 54.478554, mean_q: 73.599544, mean_eps: 0.100000\n",
      " 610779/1000000: episode: 1806, duration: 1.408s, episode steps: 287, steps per second: 204, episode reward: 280.709, mean reward:  0.978 [-9.410, 100.000], mean action: 1.331 [0.000, 3.000],  loss: 4.102026, mae: 48.697364, mean_q: 65.392200, mean_eps: 0.100000\n",
      " 611090/1000000: episode: 1807, duration: 1.537s, episode steps: 311, steps per second: 202, episode reward: 244.433, mean reward:  0.786 [-11.998, 100.000], mean action: 1.322 [0.000, 3.000],  loss: 4.274149, mae: 49.255169, mean_q: 67.171914, mean_eps: 0.100000\n",
      " 611486/1000000: episode: 1808, duration: 1.988s, episode steps: 396, steps per second: 199, episode reward: 273.822, mean reward:  0.691 [-18.567, 100.000], mean action: 1.646 [0.000, 3.000],  loss: 3.651052, mae: 53.914460, mean_q: 73.016353, mean_eps: 0.100000\n",
      " 611805/1000000: episode: 1809, duration: 1.562s, episode steps: 319, steps per second: 204, episode reward: 318.288, mean reward:  0.998 [-17.893, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 2.568199, mae: 57.983224, mean_q: 78.223825, mean_eps: 0.100000\n",
      " 612172/1000000: episode: 1810, duration: 1.830s, episode steps: 367, steps per second: 201, episode reward: 237.569, mean reward:  0.647 [-18.172, 100.000], mean action: 1.368 [0.000, 3.000],  loss: 3.117029, mae: 56.815260, mean_q: 76.582201, mean_eps: 0.100000\n",
      " 613172/1000000: episode: 1811, duration: 5.309s, episode steps: 1000, steps per second: 188, episode reward: 160.798, mean reward:  0.161 [-18.850, 14.525], mean action: 0.780 [0.000, 3.000],  loss: 2.446690, mae: 53.586985, mean_q: 72.176810, mean_eps: 0.100000\n",
      " 613597/1000000: episode: 1812, duration: 2.123s, episode steps: 425, steps per second: 200, episode reward: 284.284, mean reward:  0.669 [-17.894, 100.000], mean action: 1.365 [0.000, 3.000],  loss: 1.950724, mae: 49.044890, mean_q: 65.821725, mean_eps: 0.100000\n",
      " 613944/1000000: episode: 1813, duration: 1.727s, episode steps: 347, steps per second: 201, episode reward: 244.989, mean reward:  0.706 [-22.515, 100.000], mean action: 2.000 [0.000, 3.000],  loss: 3.600541, mae: 50.797165, mean_q: 68.244812, mean_eps: 0.100000\n",
      " 614605/1000000: episode: 1814, duration: 3.416s, episode steps: 661, steps per second: 193, episode reward: 263.239, mean reward:  0.398 [-20.245, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 5.606201, mae: 49.877576, mean_q: 67.199422, mean_eps: 0.100000\n",
      " 614996/1000000: episode: 1815, duration: 1.942s, episode steps: 391, steps per second: 201, episode reward: 247.759, mean reward:  0.634 [-18.309, 100.000], mean action: 0.772 [0.000, 3.000],  loss: 4.400173, mae: 48.587346, mean_q: 65.721066, mean_eps: 0.100000\n",
      " 615225/1000000: episode: 1816, duration: 1.111s, episode steps: 229, steps per second: 206, episode reward: 254.334, mean reward:  1.111 [-18.339, 100.000], mean action: 1.096 [0.000, 3.000],  loss: 3.221898, mae: 50.370320, mean_q: 68.112733, mean_eps: 0.100000\n",
      " 615541/1000000: episode: 1817, duration: 1.570s, episode steps: 316, steps per second: 201, episode reward: 289.658, mean reward:  0.917 [-19.068, 100.000], mean action: 0.908 [0.000, 3.000],  loss: 4.103184, mae: 55.801652, mean_q: 75.490739, mean_eps: 0.100000\n",
      " 615849/1000000: episode: 1818, duration: 1.516s, episode steps: 308, steps per second: 203, episode reward: 277.904, mean reward:  0.902 [-17.502, 100.000], mean action: 1.188 [0.000, 3.000],  loss: 5.111033, mae: 56.610167, mean_q: 76.592687, mean_eps: 0.100000\n",
      " 616507/1000000: episode: 1819, duration: 3.360s, episode steps: 658, steps per second: 196, episode reward: 274.762, mean reward:  0.418 [-20.232, 100.000], mean action: 1.173 [0.000, 3.000],  loss: 3.858033, mae: 52.812785, mean_q: 71.454394, mean_eps: 0.100000\n",
      " 616703/1000000: episode: 1820, duration: 0.930s, episode steps: 196, steps per second: 211, episode reward: 257.288, mean reward:  1.313 [-11.403, 100.000], mean action: 0.974 [0.000, 3.000],  loss: 4.271953, mae: 47.596258, mean_q: 64.483453, mean_eps: 0.100000\n",
      " 617229/1000000: episode: 1821, duration: 2.693s, episode steps: 526, steps per second: 195, episode reward: 252.806, mean reward:  0.481 [-20.079, 100.000], mean action: 0.966 [0.000, 3.000],  loss: 2.867601, mae: 48.250865, mean_q: 65.373261, mean_eps: 0.100000\n",
      " 617823/1000000: episode: 1822, duration: 3.078s, episode steps: 594, steps per second: 193, episode reward: 270.835, mean reward:  0.456 [-20.042, 100.000], mean action: 0.633 [0.000, 3.000],  loss: 3.709694, mae: 48.390575, mean_q: 65.630391, mean_eps: 0.100000\n",
      " 618284/1000000: episode: 1823, duration: 2.287s, episode steps: 461, steps per second: 202, episode reward: 252.488, mean reward:  0.548 [-10.829, 100.000], mean action: 1.733 [0.000, 3.000],  loss: 3.527387, mae: 46.969051, mean_q: 63.782401, mean_eps: 0.100000\n",
      " 619284/1000000: episode: 1824, duration: 5.353s, episode steps: 1000, steps per second: 187, episode reward: 142.685, mean reward:  0.143 [-19.523, 14.998], mean action: 1.328 [0.000, 3.000],  loss: 2.469947, mae: 46.732474, mean_q: 63.200922, mean_eps: 0.100000\n",
      " 619708/1000000: episode: 1825, duration: 2.097s, episode steps: 424, steps per second: 202, episode reward: 250.594, mean reward:  0.591 [-22.090, 100.000], mean action: 0.816 [0.000, 3.000],  loss: 1.462920, mae: 42.397612, mean_q: 57.345228, mean_eps: 0.100000\n",
      " 620396/1000000: episode: 1826, duration: 3.707s, episode steps: 688, steps per second: 186, episode reward: 255.583, mean reward:  0.371 [-17.665, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 2.902252, mae: 41.311117, mean_q: 56.010248, mean_eps: 0.100000\n",
      " 620593/1000000: episode: 1827, duration: 0.970s, episode steps: 197, steps per second: 203, episode reward: 201.770, mean reward:  1.024 [-11.193, 100.000], mean action: 1.497 [0.000, 3.000],  loss: 5.084901, mae: 32.790486, mean_q: 45.000128, mean_eps: 0.100000\n",
      " 621500/1000000: episode: 1828, duration: 4.674s, episode steps: 907, steps per second: 194, episode reward: 247.452, mean reward:  0.273 [-19.376, 100.000], mean action: 0.808 [0.000, 3.000],  loss: 4.617733, mae: 38.722088, mean_q: 53.212314, mean_eps: 0.100000\n",
      " 621729/1000000: episode: 1829, duration: 1.124s, episode steps: 229, steps per second: 204, episode reward: 245.496, mean reward:  1.072 [-13.174, 100.000], mean action: 1.659 [0.000, 3.000],  loss: 2.877189, mae: 41.463088, mean_q: 56.499764, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 621982/1000000: episode: 1830, duration: 1.234s, episode steps: 253, steps per second: 205, episode reward: 241.827, mean reward:  0.956 [-3.930, 100.000], mean action: 0.980 [0.000, 3.000],  loss: 3.452458, mae: 43.879757, mean_q: 59.802378, mean_eps: 0.100000\n",
      " 622754/1000000: episode: 1831, duration: 4.092s, episode steps: 772, steps per second: 189, episode reward: 188.646, mean reward:  0.244 [-19.799, 100.000], mean action: 1.250 [0.000, 3.000],  loss: 5.024753, mae: 51.294707, mean_q: 70.020869, mean_eps: 0.100000\n",
      " 622978/1000000: episode: 1832, duration: 1.093s, episode steps: 224, steps per second: 205, episode reward: 242.748, mean reward:  1.084 [-9.333, 100.000], mean action: 2.049 [0.000, 3.000],  loss: 4.416165, mae: 46.792622, mean_q: 64.066035, mean_eps: 0.100000\n",
      " 623395/1000000: episode: 1833, duration: 2.070s, episode steps: 417, steps per second: 201, episode reward: 218.601, mean reward:  0.524 [-17.571, 100.000], mean action: 1.199 [0.000, 3.000],  loss: 4.895766, mae: 46.501034, mean_q: 63.679992, mean_eps: 0.100000\n",
      " 623734/1000000: episode: 1834, duration: 1.681s, episode steps: 339, steps per second: 202, episode reward: 268.768, mean reward:  0.793 [-17.213, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 4.750101, mae: 49.657915, mean_q: 67.847213, mean_eps: 0.100000\n",
      " 624126/1000000: episode: 1835, duration: 1.937s, episode steps: 392, steps per second: 202, episode reward: 271.466, mean reward:  0.693 [-19.934, 100.000], mean action: 0.977 [0.000, 3.000],  loss: 4.799129, mae: 50.505137, mean_q: 68.908894, mean_eps: 0.100000\n",
      " 624609/1000000: episode: 1836, duration: 2.454s, episode steps: 483, steps per second: 197, episode reward: 270.141, mean reward:  0.559 [-19.604, 100.000], mean action: 2.182 [0.000, 3.000],  loss: 4.980494, mae: 49.007796, mean_q: 66.509074, mean_eps: 0.100000\n",
      " 624892/1000000: episode: 1837, duration: 1.391s, episode steps: 283, steps per second: 203, episode reward: 257.634, mean reward:  0.910 [-17.203, 100.000], mean action: 0.862 [0.000, 3.000],  loss: 3.452103, mae: 49.668036, mean_q: 67.243217, mean_eps: 0.100000\n",
      " 625433/1000000: episode: 1838, duration: 2.762s, episode steps: 541, steps per second: 196, episode reward: 267.768, mean reward:  0.495 [-18.263, 100.000], mean action: 2.392 [0.000, 3.000],  loss: 3.369345, mae: 49.455995, mean_q: 67.003878, mean_eps: 0.100000\n",
      " 625778/1000000: episode: 1839, duration: 1.702s, episode steps: 345, steps per second: 203, episode reward: 278.297, mean reward:  0.807 [-18.402, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 3.574004, mae: 50.100676, mean_q: 67.988576, mean_eps: 0.100000\n",
      " 626005/1000000: episode: 1840, duration: 1.103s, episode steps: 227, steps per second: 206, episode reward: 280.066, mean reward:  1.234 [-17.463, 100.000], mean action: 0.943 [0.000, 3.000],  loss: 3.579841, mae: 51.667561, mean_q: 70.076245, mean_eps: 0.100000\n",
      " 626184/1000000: episode: 1841, duration: 0.868s, episode steps: 179, steps per second: 206, episode reward: 272.059, mean reward:  1.520 [-2.834, 100.000], mean action: 1.335 [0.000, 3.000],  loss: 2.662711, mae: 52.952205, mean_q: 71.794279, mean_eps: 0.100000\n",
      " 626478/1000000: episode: 1842, duration: 1.460s, episode steps: 294, steps per second: 201, episode reward: 283.827, mean reward:  0.965 [-13.734, 100.000], mean action: 1.704 [0.000, 3.000],  loss: 4.669937, mae: 59.365657, mean_q: 80.189918, mean_eps: 0.100000\n",
      " 626681/1000000: episode: 1843, duration: 0.970s, episode steps: 203, steps per second: 209, episode reward: 274.532, mean reward:  1.352 [-8.014, 100.000], mean action: 1.118 [0.000, 3.000],  loss: 5.761733, mae: 60.134247, mean_q: 81.321244, mean_eps: 0.100000\n",
      " 626840/1000000: episode: 1844, duration: 0.749s, episode steps: 159, steps per second: 212, episode reward: 268.362, mean reward:  1.688 [-8.039, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 7.373325, mae: 63.119260, mean_q: 85.529728, mean_eps: 0.100000\n",
      " 627090/1000000: episode: 1845, duration: 1.225s, episode steps: 250, steps per second: 204, episode reward: 255.204, mean reward:  1.021 [-17.546, 100.000], mean action: 1.952 [0.000, 3.000],  loss: 6.341065, mae: 64.842600, mean_q: 87.749537, mean_eps: 0.100000\n",
      " 627467/1000000: episode: 1846, duration: 1.910s, episode steps: 377, steps per second: 197, episode reward: 259.510, mean reward:  0.688 [-17.743, 100.000], mean action: 2.382 [0.000, 3.000],  loss: 4.935312, mae: 60.281319, mean_q: 81.766029, mean_eps: 0.100000\n",
      " 627657/1000000: episode: 1847, duration: 0.912s, episode steps: 190, steps per second: 208, episode reward: 292.779, mean reward:  1.541 [-5.961, 100.000], mean action: 1.421 [0.000, 3.000],  loss: 5.796659, mae: 57.045371, mean_q: 77.487704, mean_eps: 0.100000\n",
      " 627768/1000000: episode: 1848, duration: 0.530s, episode steps: 111, steps per second: 209, episode reward: 40.241, mean reward:  0.363 [-100.000, 13.438], mean action: 1.577 [0.000, 3.000],  loss: 7.026370, mae: 58.248828, mean_q: 78.984001, mean_eps: 0.100000\n",
      " 627949/1000000: episode: 1849, duration: 0.854s, episode steps: 181, steps per second: 212, episode reward: 261.022, mean reward:  1.442 [-9.106, 100.000], mean action: 0.895 [0.000, 3.000],  loss: 17.267110, mae: 59.116082, mean_q: 80.300981, mean_eps: 0.100000\n",
      " 628205/1000000: episode: 1850, duration: 1.243s, episode steps: 256, steps per second: 206, episode reward: 252.183, mean reward:  0.985 [-8.764, 100.000], mean action: 0.906 [0.000, 3.000],  loss: 14.750390, mae: 61.566368, mean_q: 83.696974, mean_eps: 0.100000\n",
      " 628359/1000000: episode: 1851, duration: 0.739s, episode steps: 154, steps per second: 209, episode reward:  9.783, mean reward:  0.064 [-100.000, 11.115], mean action: 1.461 [0.000, 3.000],  loss: 14.158623, mae: 63.439584, mean_q: 86.485550, mean_eps: 0.100000\n",
      " 628470/1000000: episode: 1852, duration: 0.533s, episode steps: 111, steps per second: 208, episode reward: -38.438, mean reward: -0.346 [-100.000,  8.542], mean action: 1.622 [0.000, 3.000],  loss: 21.946242, mae: 67.910938, mean_q: 92.262741, mean_eps: 0.100000\n",
      " 628731/1000000: episode: 1853, duration: 1.282s, episode steps: 261, steps per second: 204, episode reward: 304.663, mean reward:  1.167 [-10.677, 100.000], mean action: 1.031 [0.000, 3.000],  loss: 20.982249, mae: 69.213412, mean_q: 93.996439, mean_eps: 0.100000\n",
      " 628863/1000000: episode: 1854, duration: 0.646s, episode steps: 132, steps per second: 204, episode reward: 81.334, mean reward:  0.616 [-100.000, 12.963], mean action: 1.735 [0.000, 3.000],  loss: 14.224709, mae: 64.513710, mean_q: 88.005345, mean_eps: 0.100000\n",
      " 629863/1000000: episode: 1855, duration: 5.408s, episode steps: 1000, steps per second: 185, episode reward: 159.925, mean reward:  0.160 [-21.779, 23.462], mean action: 2.261 [0.000, 3.000],  loss: 21.077715, mae: 53.606862, mean_q: 72.911287, mean_eps: 0.100000\n",
      " 630771/1000000: episode: 1856, duration: 4.973s, episode steps: 908, steps per second: 183, episode reward: 220.516, mean reward:  0.243 [-19.606, 100.000], mean action: 2.268 [0.000, 3.000],  loss: 1.850713, mae: 40.425784, mean_q: 54.472702, mean_eps: 0.100000\n",
      " 630936/1000000: episode: 1857, duration: 0.800s, episode steps: 165, steps per second: 206, episode reward: -48.224, mean reward: -0.292 [-100.000, 12.324], mean action: 1.806 [0.000, 3.000],  loss: 1.413830, mae: 42.410816, mean_q: 57.181194, mean_eps: 0.100000\n",
      " 631415/1000000: episode: 1858, duration: 2.438s, episode steps: 479, steps per second: 196, episode reward: 284.559, mean reward:  0.594 [-18.435, 100.000], mean action: 0.956 [0.000, 3.000],  loss: 2.531896, mae: 46.414269, mean_q: 61.773627, mean_eps: 0.100000\n",
      " 631648/1000000: episode: 1859, duration: 1.134s, episode steps: 233, steps per second: 205, episode reward: 248.201, mean reward:  1.065 [-18.219, 100.000], mean action: 1.305 [0.000, 3.000],  loss: 3.589677, mae: 53.015572, mean_q: 70.577523, mean_eps: 0.100000\n",
      " 631770/1000000: episode: 1860, duration: 0.582s, episode steps: 122, steps per second: 210, episode reward:  3.633, mean reward:  0.030 [-100.000, 11.105], mean action: 1.533 [0.000, 3.000],  loss: 5.570394, mae: 58.069083, mean_q: 77.532920, mean_eps: 0.100000\n",
      " 632014/1000000: episode: 1861, duration: 1.181s, episode steps: 244, steps per second: 207, episode reward: 274.557, mean reward:  1.125 [-18.528, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 11.884199, mae: 58.584275, mean_q: 78.466710, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 632140/1000000: episode: 1862, duration: 0.607s, episode steps: 126, steps per second: 207, episode reward: 73.777, mean reward:  0.586 [-100.000, 10.547], mean action: 1.849 [0.000, 3.000],  loss: 15.996411, mae: 57.777289, mean_q: 78.460142, mean_eps: 0.100000\n",
      " 632403/1000000: episode: 1863, duration: 1.291s, episode steps: 263, steps per second: 204, episode reward: 277.544, mean reward:  1.055 [-11.750, 100.000], mean action: 0.878 [0.000, 3.000],  loss: 16.895613, mae: 62.905673, mean_q: 85.532612, mean_eps: 0.100000\n",
      " 632914/1000000: episode: 1864, duration: 2.587s, episode steps: 511, steps per second: 197, episode reward: 242.873, mean reward:  0.475 [-18.862, 100.000], mean action: 0.945 [0.000, 3.000],  loss: 9.718548, mae: 58.411325, mean_q: 79.422077, mean_eps: 0.100000\n",
      " 633602/1000000: episode: 1865, duration: 3.676s, episode steps: 688, steps per second: 187, episode reward: 257.752, mean reward:  0.375 [-20.041, 100.000], mean action: 0.831 [0.000, 3.000],  loss: 7.091497, mae: 43.898411, mean_q: 60.035441, mean_eps: 0.100000\n",
      " 633753/1000000: episode: 1866, duration: 0.727s, episode steps: 151, steps per second: 208, episode reward: 32.893, mean reward:  0.218 [-100.000, 10.640], mean action: 1.834 [0.000, 3.000],  loss: 2.983374, mae: 44.170889, mean_q: 60.127866, mean_eps: 0.100000\n",
      " 634044/1000000: episode: 1867, duration: 1.465s, episode steps: 291, steps per second: 199, episode reward: 246.006, mean reward:  0.845 [-4.543, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 5.996721, mae: 51.751717, mean_q: 69.647719, mean_eps: 0.100000\n",
      " 634394/1000000: episode: 1868, duration: 1.778s, episode steps: 350, steps per second: 197, episode reward: 238.978, mean reward:  0.683 [-7.384, 100.000], mean action: 1.360 [0.000, 3.000],  loss: 4.807523, mae: 53.376169, mean_q: 71.687227, mean_eps: 0.100000\n",
      " 634698/1000000: episode: 1869, duration: 1.523s, episode steps: 304, steps per second: 200, episode reward: 223.650, mean reward:  0.736 [-8.671, 100.000], mean action: 1.444 [0.000, 3.000],  loss: 4.062968, mae: 56.194623, mean_q: 74.948857, mean_eps: 0.100000\n",
      " 635309/1000000: episode: 1870, duration: 3.326s, episode steps: 611, steps per second: 184, episode reward: 222.584, mean reward:  0.364 [-22.291, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 7.388846, mae: 53.836236, mean_q: 72.298877, mean_eps: 0.100000\n",
      " 635573/1000000: episode: 1871, duration: 1.300s, episode steps: 264, steps per second: 203, episode reward: 243.808, mean reward:  0.924 [-17.418, 100.000], mean action: 1.447 [0.000, 3.000],  loss: 5.799175, mae: 51.701825, mean_q: 69.616095, mean_eps: 0.100000\n",
      " 635664/1000000: episode: 1872, duration: 0.442s, episode steps:  91, steps per second: 206, episode reward: 17.591, mean reward:  0.193 [-100.000, 19.762], mean action: 1.923 [0.000, 3.000],  loss: 7.574278, mae: 53.719188, mean_q: 72.658973, mean_eps: 0.100000\n",
      " 635971/1000000: episode: 1873, duration: 1.547s, episode steps: 307, steps per second: 198, episode reward: 202.647, mean reward:  0.660 [-19.801, 100.000], mean action: 2.088 [0.000, 3.000],  loss: 7.618226, mae: 52.883657, mean_q: 71.332963, mean_eps: 0.100000\n",
      " 636261/1000000: episode: 1874, duration: 1.447s, episode steps: 290, steps per second: 200, episode reward: 296.002, mean reward:  1.021 [-10.925, 100.000], mean action: 1.814 [0.000, 3.000],  loss: 9.006150, mae: 51.865857, mean_q: 70.267633, mean_eps: 0.100000\n",
      " 636370/1000000: episode: 1875, duration: 0.539s, episode steps: 109, steps per second: 202, episode reward: -78.689, mean reward: -0.722 [-100.000, 11.768], mean action: 1.394 [0.000, 3.000],  loss: 10.536283, mae: 53.703609, mean_q: 72.674298, mean_eps: 0.100000\n",
      " 636627/1000000: episode: 1876, duration: 1.281s, episode steps: 257, steps per second: 201, episode reward: 279.285, mean reward:  1.087 [-8.393, 100.000], mean action: 1.136 [0.000, 3.000],  loss: 8.732664, mae: 56.445200, mean_q: 75.353322, mean_eps: 0.100000\n",
      " 636841/1000000: episode: 1877, duration: 1.056s, episode steps: 214, steps per second: 203, episode reward: 270.523, mean reward:  1.264 [-9.801, 100.000], mean action: 1.121 [0.000, 3.000],  loss: 6.912217, mae: 55.514546, mean_q: 73.961998, mean_eps: 0.100000\n",
      " 637149/1000000: episode: 1878, duration: 1.529s, episode steps: 308, steps per second: 201, episode reward: 283.125, mean reward:  0.919 [-10.714, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 5.337554, mae: 60.243989, mean_q: 80.158506, mean_eps: 0.100000\n",
      " 637718/1000000: episode: 1879, duration: 2.884s, episode steps: 569, steps per second: 197, episode reward: 291.421, mean reward:  0.512 [-23.948, 100.000], mean action: 0.895 [0.000, 3.000],  loss: 3.125586, mae: 59.780928, mean_q: 80.145675, mean_eps: 0.100000\n",
      " 637989/1000000: episode: 1880, duration: 1.333s, episode steps: 271, steps per second: 203, episode reward: 277.188, mean reward:  1.023 [-15.276, 100.000], mean action: 1.672 [0.000, 3.000],  loss: 3.406484, mae: 54.666363, mean_q: 73.896921, mean_eps: 0.100000\n",
      " 638439/1000000: episode: 1881, duration: 2.396s, episode steps: 450, steps per second: 188, episode reward: 282.066, mean reward:  0.627 [-12.259, 100.000], mean action: 1.198 [0.000, 3.000],  loss: 2.983922, mae: 52.950027, mean_q: 71.830037, mean_eps: 0.100000\n",
      " 638706/1000000: episode: 1882, duration: 1.316s, episode steps: 267, steps per second: 203, episode reward: 232.699, mean reward:  0.872 [-11.715, 100.000], mean action: 1.255 [0.000, 3.000],  loss: 4.298840, mae: 54.984203, mean_q: 74.500210, mean_eps: 0.100000\n",
      " 639332/1000000: episode: 1883, duration: 3.284s, episode steps: 626, steps per second: 191, episode reward: 272.916, mean reward:  0.436 [-18.903, 100.000], mean action: 0.743 [0.000, 3.000],  loss: 4.088642, mae: 54.806550, mean_q: 74.072897, mean_eps: 0.100000\n",
      " 639562/1000000: episode: 1884, duration: 1.128s, episode steps: 230, steps per second: 204, episode reward: 267.579, mean reward:  1.163 [-9.270, 100.000], mean action: 1.491 [0.000, 3.000],  loss: 2.937378, mae: 53.897531, mean_q: 72.840967, mean_eps: 0.100000\n",
      " 639903/1000000: episode: 1885, duration: 1.771s, episode steps: 341, steps per second: 193, episode reward: 269.027, mean reward:  0.789 [-19.280, 100.000], mean action: 1.314 [0.000, 3.000],  loss: 2.706061, mae: 54.587283, mean_q: 73.573771, mean_eps: 0.100000\n",
      " 640161/1000000: episode: 1886, duration: 1.256s, episode steps: 258, steps per second: 205, episode reward: 229.029, mean reward:  0.888 [-18.699, 100.000], mean action: 1.275 [0.000, 3.000],  loss: 2.980938, mae: 54.005476, mean_q: 72.759714, mean_eps: 0.100000\n",
      " 640455/1000000: episode: 1887, duration: 1.484s, episode steps: 294, steps per second: 198, episode reward: 251.643, mean reward:  0.856 [-11.652, 100.000], mean action: 1.619 [0.000, 3.000],  loss: 4.509972, mae: 56.321463, mean_q: 75.830666, mean_eps: 0.100000\n",
      " 640666/1000000: episode: 1888, duration: 1.023s, episode steps: 211, steps per second: 206, episode reward: 250.037, mean reward:  1.185 [-9.985, 100.000], mean action: 1.313 [0.000, 3.000],  loss: 5.377997, mae: 54.707521, mean_q: 73.744460, mean_eps: 0.100000\n",
      " 640982/1000000: episode: 1889, duration: 1.562s, episode steps: 316, steps per second: 202, episode reward: 251.412, mean reward:  0.796 [-9.157, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 4.426304, mae: 56.038864, mean_q: 75.592047, mean_eps: 0.100000\n",
      " 641579/1000000: episode: 1890, duration: 2.992s, episode steps: 597, steps per second: 200, episode reward: 296.403, mean reward:  0.496 [-20.102, 100.000], mean action: 0.836 [0.000, 3.000],  loss: 3.633385, mae: 54.266560, mean_q: 73.348271, mean_eps: 0.100000\n",
      " 641959/1000000: episode: 1891, duration: 1.912s, episode steps: 380, steps per second: 199, episode reward: 239.384, mean reward:  0.630 [-18.952, 100.000], mean action: 0.924 [0.000, 3.000],  loss: 2.722970, mae: 50.929356, mean_q: 68.795892, mean_eps: 0.100000\n",
      " 642169/1000000: episode: 1892, duration: 1.021s, episode steps: 210, steps per second: 206, episode reward: 279.175, mean reward:  1.329 [-8.230, 100.000], mean action: 1.471 [0.000, 3.000],  loss: 3.311802, mae: 50.066458, mean_q: 67.676559, mean_eps: 0.100000\n",
      " 642357/1000000: episode: 1893, duration: 0.912s, episode steps: 188, steps per second: 206, episode reward: 283.913, mean reward:  1.510 [-8.467, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 3.538040, mae: 54.291520, mean_q: 73.195588, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 642708/1000000: episode: 1894, duration: 1.731s, episode steps: 351, steps per second: 203, episode reward: 247.917, mean reward:  0.706 [-18.422, 100.000], mean action: 1.031 [0.000, 3.000],  loss: 4.175323, mae: 56.853016, mean_q: 76.659869, mean_eps: 0.100000\n",
      " 642973/1000000: episode: 1895, duration: 1.288s, episode steps: 265, steps per second: 206, episode reward: 253.155, mean reward:  0.955 [-2.812, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 3.613800, mae: 57.373025, mean_q: 77.377046, mean_eps: 0.100000\n",
      " 643176/1000000: episode: 1896, duration: 0.980s, episode steps: 203, steps per second: 207, episode reward: 61.918, mean reward:  0.305 [-100.000, 12.703], mean action: 1.744 [0.000, 3.000],  loss: 4.100935, mae: 59.091191, mean_q: 79.735720, mean_eps: 0.100000\n",
      " 643380/1000000: episode: 1897, duration: 0.989s, episode steps: 204, steps per second: 206, episode reward: 278.025, mean reward:  1.363 [-11.078, 100.000], mean action: 1.333 [0.000, 3.000],  loss: 17.107811, mae: 60.449617, mean_q: 82.058767, mean_eps: 0.100000\n",
      " 643712/1000000: episode: 1898, duration: 1.654s, episode steps: 332, steps per second: 201, episode reward: 237.432, mean reward:  0.715 [-18.389, 100.000], mean action: 1.054 [0.000, 3.000],  loss: 8.491697, mae: 61.027478, mean_q: 83.041834, mean_eps: 0.100000\n",
      " 644561/1000000: episode: 1899, duration: 4.442s, episode steps: 849, steps per second: 191, episode reward: 208.724, mean reward:  0.246 [-20.164, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 7.191263, mae: 50.509354, mean_q: 69.125386, mean_eps: 0.100000\n",
      " 644972/1000000: episode: 1900, duration: 2.047s, episode steps: 411, steps per second: 201, episode reward: 233.689, mean reward:  0.569 [-19.240, 100.000], mean action: 1.253 [0.000, 3.000],  loss: 3.187020, mae: 41.571407, mean_q: 56.636253, mean_eps: 0.100000\n",
      " 645268/1000000: episode: 1901, duration: 1.461s, episode steps: 296, steps per second: 203, episode reward: 238.308, mean reward:  0.805 [-20.552, 100.000], mean action: 0.966 [0.000, 3.000],  loss: 3.665729, mae: 46.643798, mean_q: 63.429550, mean_eps: 0.100000\n",
      " 645638/1000000: episode: 1902, duration: 1.847s, episode steps: 370, steps per second: 200, episode reward: 272.792, mean reward:  0.737 [-11.251, 100.000], mean action: 1.270 [0.000, 3.000],  loss: 3.977078, mae: 54.332214, mean_q: 73.451413, mean_eps: 0.100000\n",
      " 645764/1000000: episode: 1903, duration: 0.602s, episode steps: 126, steps per second: 209, episode reward:  3.848, mean reward:  0.031 [-100.000, 16.487], mean action: 1.524 [0.000, 3.000],  loss: 4.264542, mae: 54.163385, mean_q: 73.049464, mean_eps: 0.100000\n",
      " 645862/1000000: episode: 1904, duration: 0.475s, episode steps:  98, steps per second: 206, episode reward: 43.934, mean reward:  0.448 [-100.000, 14.180], mean action: 1.857 [0.000, 3.000],  loss: 8.152655, mae: 56.048427, mean_q: 75.683853, mean_eps: 0.100000\n",
      " 646264/1000000: episode: 1905, duration: 2.071s, episode steps: 402, steps per second: 194, episode reward: 243.218, mean reward:  0.605 [-10.511, 100.000], mean action: 1.614 [0.000, 3.000],  loss: 11.763821, mae: 57.826738, mean_q: 77.919841, mean_eps: 0.100000\n",
      " 647264/1000000: episode: 1906, duration: 5.614s, episode steps: 1000, steps per second: 178, episode reward: 80.232, mean reward:  0.080 [-23.660, 24.809], mean action: 1.503 [0.000, 3.000],  loss: 3.920814, mae: 46.435943, mean_q: 62.592720, mean_eps: 0.100000\n",
      " 648264/1000000: episode: 1907, duration: 5.341s, episode steps: 1000, steps per second: 187, episode reward: 116.804, mean reward:  0.117 [-22.194, 24.822], mean action: 1.216 [0.000, 3.000],  loss: 3.024841, mae: 41.925784, mean_q: 56.774050, mean_eps: 0.100000\n",
      " 648471/1000000: episode: 1908, duration: 0.985s, episode steps: 207, steps per second: 210, episode reward: 243.233, mean reward:  1.175 [-11.380, 100.000], mean action: 1.140 [0.000, 3.000],  loss: 2.937191, mae: 38.376090, mean_q: 51.760032, mean_eps: 0.100000\n",
      " 648683/1000000: episode: 1909, duration: 1.040s, episode steps: 212, steps per second: 204, episode reward: 271.334, mean reward:  1.280 [-19.825, 100.000], mean action: 2.047 [0.000, 3.000],  loss: 4.153399, mae: 41.657237, mean_q: 56.142998, mean_eps: 0.100000\n",
      " 649017/1000000: episode: 1910, duration: 1.661s, episode steps: 334, steps per second: 201, episode reward: 246.281, mean reward:  0.737 [-19.748, 100.000], mean action: 1.455 [0.000, 3.000],  loss: 5.339456, mae: 47.137041, mean_q: 63.733057, mean_eps: 0.100000\n",
      " 649207/1000000: episode: 1911, duration: 0.914s, episode steps: 190, steps per second: 208, episode reward: 286.765, mean reward:  1.509 [-9.359, 100.000], mean action: 1.132 [0.000, 3.000],  loss: 7.098371, mae: 53.131622, mean_q: 71.968888, mean_eps: 0.100000\n",
      " 649457/1000000: episode: 1912, duration: 1.211s, episode steps: 250, steps per second: 206, episode reward: 267.298, mean reward:  1.069 [-8.067, 100.000], mean action: 0.876 [0.000, 3.000],  loss: 7.482043, mae: 57.624079, mean_q: 78.091906, mean_eps: 0.100000\n",
      " 649563/1000000: episode: 1913, duration: 0.514s, episode steps: 106, steps per second: 206, episode reward: 41.988, mean reward:  0.396 [-100.000, 15.884], mean action: 1.868 [0.000, 3.000],  loss: 6.321611, mae: 57.296611, mean_q: 77.507650, mean_eps: 0.100000\n",
      " 650503/1000000: episode: 1914, duration: 4.838s, episode steps: 940, steps per second: 194, episode reward: 248.118, mean reward:  0.264 [-17.904, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 9.469981, mae: 53.405194, mean_q: 72.169353, mean_eps: 0.100000\n",
      " 650872/1000000: episode: 1915, duration: 1.860s, episode steps: 369, steps per second: 198, episode reward: 235.000, mean reward:  0.637 [-17.431, 100.000], mean action: 2.293 [0.000, 3.000],  loss: 1.700782, mae: 43.800149, mean_q: 59.537660, mean_eps: 0.100000\n",
      " 651234/1000000: episode: 1916, duration: 1.825s, episode steps: 362, steps per second: 198, episode reward: 237.130, mean reward:  0.655 [-21.260, 100.000], mean action: 2.343 [0.000, 3.000],  loss: 2.610696, mae: 47.678355, mean_q: 64.926321, mean_eps: 0.100000\n",
      " 651521/1000000: episode: 1917, duration: 1.401s, episode steps: 287, steps per second: 205, episode reward: 291.280, mean reward:  1.015 [-9.233, 100.000], mean action: 0.885 [0.000, 3.000],  loss: 4.766538, mae: 51.807128, mean_q: 70.125821, mean_eps: 0.100000\n",
      " 651983/1000000: episode: 1918, duration: 2.257s, episode steps: 462, steps per second: 205, episode reward: 306.248, mean reward:  0.663 [-17.825, 100.000], mean action: 0.671 [0.000, 3.000],  loss: 3.914870, mae: 51.349785, mean_q: 70.002977, mean_eps: 0.100000\n",
      " 652707/1000000: episode: 1919, duration: 3.755s, episode steps: 724, steps per second: 193, episode reward: 276.747, mean reward:  0.382 [-20.185, 100.000], mean action: 1.608 [0.000, 3.000],  loss: 3.287451, mae: 48.352139, mean_q: 65.874389, mean_eps: 0.100000\n",
      " 653005/1000000: episode: 1920, duration: 1.469s, episode steps: 298, steps per second: 203, episode reward: 240.511, mean reward:  0.807 [-19.848, 100.000], mean action: 0.752 [0.000, 3.000],  loss: 2.601132, mae: 45.673471, mean_q: 62.068522, mean_eps: 0.100000\n",
      " 653139/1000000: episode: 1921, duration: 0.645s, episode steps: 134, steps per second: 208, episode reward: 11.766, mean reward:  0.088 [-100.000, 10.135], mean action: 1.694 [0.000, 3.000],  loss: 1.652662, mae: 45.227349, mean_q: 61.490690, mean_eps: 0.100000\n",
      " 653360/1000000: episode: 1922, duration: 1.063s, episode steps: 221, steps per second: 208, episode reward: 265.785, mean reward:  1.203 [-11.570, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 14.610530, mae: 49.581442, mean_q: 67.724147, mean_eps: 0.100000\n",
      " 653659/1000000: episode: 1923, duration: 1.457s, episode steps: 299, steps per second: 205, episode reward: 283.111, mean reward:  0.947 [-10.734, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 15.604382, mae: 52.338303, mean_q: 71.345497, mean_eps: 0.100000\n",
      " 654489/1000000: episode: 1924, duration: 4.394s, episode steps: 830, steps per second: 189, episode reward: 236.783, mean reward:  0.285 [-18.859, 100.000], mean action: 2.149 [0.000, 3.000],  loss: 9.581829, mae: 48.324902, mean_q: 66.113266, mean_eps: 0.100000\n",
      " 654599/1000000: episode: 1925, duration: 0.536s, episode steps: 110, steps per second: 205, episode reward: 31.484, mean reward:  0.286 [-100.000, 19.822], mean action: 1.864 [0.000, 3.000],  loss: 3.928876, mae: 41.769873, mean_q: 56.981550, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 654856/1000000: episode: 1926, duration: 1.252s, episode steps: 257, steps per second: 205, episode reward: 287.836, mean reward:  1.120 [-18.260, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 8.198965, mae: 42.583827, mean_q: 58.020522, mean_eps: 0.100000\n",
      " 655019/1000000: episode: 1927, duration: 0.783s, episode steps: 163, steps per second: 208, episode reward: 257.656, mean reward:  1.581 [-11.047, 100.000], mean action: 1.423 [0.000, 3.000],  loss: 8.216785, mae: 43.944114, mean_q: 59.739362, mean_eps: 0.100000\n",
      " 655288/1000000: episode: 1928, duration: 1.305s, episode steps: 269, steps per second: 206, episode reward: 272.459, mean reward:  1.013 [-9.443, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 6.129936, mae: 48.280066, mean_q: 65.629916, mean_eps: 0.100000\n",
      " 655653/1000000: episode: 1929, duration: 1.810s, episode steps: 365, steps per second: 202, episode reward: 300.558, mean reward:  0.823 [-19.510, 100.000], mean action: 1.512 [0.000, 3.000],  loss: 6.119201, mae: 52.722852, mean_q: 71.835376, mean_eps: 0.100000\n",
      " 655964/1000000: episode: 1930, duration: 1.516s, episode steps: 311, steps per second: 205, episode reward: 270.827, mean reward:  0.871 [-9.038, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 6.723316, mae: 53.165220, mean_q: 72.726216, mean_eps: 0.100000\n",
      " 656186/1000000: episode: 1931, duration: 1.083s, episode steps: 222, steps per second: 205, episode reward: 286.999, mean reward:  1.293 [-2.570, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 6.206266, mae: 54.971855, mean_q: 75.210661, mean_eps: 0.100000\n",
      " 656533/1000000: episode: 1932, duration: 1.724s, episode steps: 347, steps per second: 201, episode reward: 237.497, mean reward:  0.684 [-17.844, 100.000], mean action: 0.761 [0.000, 3.000],  loss: 5.224747, mae: 57.268506, mean_q: 78.031494, mean_eps: 0.100000\n",
      " 656728/1000000: episode: 1933, duration: 0.930s, episode steps: 195, steps per second: 210, episode reward: 265.725, mean reward:  1.363 [-9.696, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 5.257529, mae: 59.823508, mean_q: 81.177752, mean_eps: 0.100000\n",
      " 657025/1000000: episode: 1934, duration: 1.447s, episode steps: 297, steps per second: 205, episode reward: 256.765, mean reward:  0.865 [-18.221, 100.000], mean action: 0.710 [0.000, 3.000],  loss: 5.335408, mae: 59.436388, mean_q: 80.614764, mean_eps: 0.100000\n",
      " 657870/1000000: episode: 1935, duration: 4.517s, episode steps: 845, steps per second: 187, episode reward: 247.671, mean reward:  0.293 [-19.204, 100.000], mean action: 2.470 [0.000, 3.000],  loss: 4.105831, mae: 49.548789, mean_q: 67.552397, mean_eps: 0.100000\n",
      " 658160/1000000: episode: 1936, duration: 1.407s, episode steps: 290, steps per second: 206, episode reward: 256.342, mean reward:  0.884 [-18.403, 100.000], mean action: 0.707 [0.000, 3.000],  loss: 3.133390, mae: 40.842941, mean_q: 55.940043, mean_eps: 0.100000\n",
      " 658474/1000000: episode: 1937, duration: 1.563s, episode steps: 314, steps per second: 201, episode reward: 280.713, mean reward:  0.894 [-17.795, 100.000], mean action: 0.866 [0.000, 3.000],  loss: 4.092671, mae: 44.365146, mean_q: 61.063555, mean_eps: 0.100000\n",
      " 658707/1000000: episode: 1938, duration: 1.121s, episode steps: 233, steps per second: 208, episode reward: 278.870, mean reward:  1.197 [-11.184, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 4.225894, mae: 51.020313, mean_q: 69.958359, mean_eps: 0.100000\n",
      " 658975/1000000: episode: 1939, duration: 1.293s, episode steps: 268, steps per second: 207, episode reward: 266.618, mean reward:  0.995 [-2.913, 100.000], mean action: 0.903 [0.000, 3.000],  loss: 4.288298, mae: 57.394630, mean_q: 78.187681, mean_eps: 0.100000\n",
      " 659276/1000000: episode: 1940, duration: 1.470s, episode steps: 301, steps per second: 205, episode reward: 262.351, mean reward:  0.872 [-17.611, 100.000], mean action: 0.997 [0.000, 3.000],  loss: 5.587604, mae: 57.397772, mean_q: 77.932869, mean_eps: 0.100000\n",
      " 659450/1000000: episode: 1941, duration: 0.830s, episode steps: 174, steps per second: 210, episode reward: 275.835, mean reward:  1.585 [-2.837, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 4.988257, mae: 58.167270, mean_q: 78.750491, mean_eps: 0.100000\n",
      " 659568/1000000: episode: 1942, duration: 0.567s, episode steps: 118, steps per second: 208, episode reward: 57.605, mean reward:  0.488 [-100.000, 20.071], mean action: 1.653 [0.000, 3.000],  loss: 7.675170, mae: 63.033508, mean_q: 85.130478, mean_eps: 0.100000\n",
      " 659665/1000000: episode: 1943, duration: 0.471s, episode steps:  97, steps per second: 206, episode reward: 13.395, mean reward:  0.138 [-100.000, 19.229], mean action: 1.835 [0.000, 3.000],  loss: 12.437710, mae: 62.871222, mean_q: 84.898856, mean_eps: 0.100000\n",
      " 659864/1000000: episode: 1944, duration: 0.958s, episode steps: 199, steps per second: 208, episode reward: 256.104, mean reward:  1.287 [-5.043, 100.000], mean action: 1.286 [0.000, 3.000],  loss: 30.463161, mae: 64.714463, mean_q: 87.251072, mean_eps: 0.100000\n",
      " 660068/1000000: episode: 1945, duration: 1.018s, episode steps: 204, steps per second: 200, episode reward: 244.719, mean reward:  1.200 [-11.480, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 32.594132, mae: 66.183837, mean_q: 89.288284, mean_eps: 0.100000\n",
      " 660221/1000000: episode: 1946, duration: 0.737s, episode steps: 153, steps per second: 208, episode reward: -21.721, mean reward: -0.142 [-100.000, 25.231], mean action: 1.686 [0.000, 3.000],  loss: 18.372709, mae: 65.098817, mean_q: 88.036238, mean_eps: 0.100000\n",
      " 660331/1000000: episode: 1947, duration: 0.543s, episode steps: 110, steps per second: 203, episode reward: 10.966, mean reward:  0.100 [-100.000, 16.119], mean action: 1.591 [0.000, 3.000],  loss: 21.271425, mae: 67.033842, mean_q: 90.397479, mean_eps: 0.100000\n",
      " 660523/1000000: episode: 1948, duration: 0.966s, episode steps: 192, steps per second: 199, episode reward: 263.002, mean reward:  1.370 [-8.820, 100.000], mean action: 0.984 [0.000, 3.000],  loss: 34.222534, mae: 67.672736, mean_q: 91.172119, mean_eps: 0.100000\n",
      " 660755/1000000: episode: 1949, duration: 1.123s, episode steps: 232, steps per second: 207, episode reward: 285.827, mean reward:  1.232 [-7.859, 100.000], mean action: 1.509 [0.000, 3.000],  loss: 17.151571, mae: 65.591234, mean_q: 88.518950, mean_eps: 0.100000\n",
      " 660884/1000000: episode: 1950, duration: 0.618s, episode steps: 129, steps per second: 209, episode reward: 46.170, mean reward:  0.358 [-100.000, 15.849], mean action: 1.698 [0.000, 3.000],  loss: 8.122096, mae: 65.113976, mean_q: 87.710353, mean_eps: 0.100000\n",
      " 661222/1000000: episode: 1951, duration: 1.657s, episode steps: 338, steps per second: 204, episode reward: 274.328, mean reward:  0.812 [-11.588, 100.000], mean action: 0.891 [0.000, 3.000],  loss: 18.268812, mae: 66.393544, mean_q: 89.232669, mean_eps: 0.100000\n",
      " 661478/1000000: episode: 1952, duration: 1.254s, episode steps: 256, steps per second: 204, episode reward: 230.016, mean reward:  0.899 [-19.494, 100.000], mean action: 1.527 [0.000, 3.000],  loss: 17.023736, mae: 61.819623, mean_q: 83.835953, mean_eps: 0.100000\n",
      " 661785/1000000: episode: 1953, duration: 1.525s, episode steps: 307, steps per second: 201, episode reward: 179.409, mean reward:  0.584 [-21.612, 100.000], mean action: 2.306 [0.000, 3.000],  loss: 19.025404, mae: 54.705245, mean_q: 74.023426, mean_eps: 0.100000\n",
      " 662010/1000000: episode: 1954, duration: 1.102s, episode steps: 225, steps per second: 204, episode reward: 250.292, mean reward:  1.112 [-10.767, 100.000], mean action: 2.191 [0.000, 3.000],  loss: 18.893793, mae: 46.302281, mean_q: 63.074309, mean_eps: 0.100000\n",
      " 662266/1000000: episode: 1955, duration: 1.266s, episode steps: 256, steps per second: 202, episode reward: 204.075, mean reward:  0.797 [-10.184, 100.000], mean action: 1.691 [0.000, 3.000],  loss: 16.239833, mae: 45.414115, mean_q: 62.441351, mean_eps: 0.100000\n",
      " 663060/1000000: episode: 1956, duration: 4.008s, episode steps: 794, steps per second: 198, episode reward: 248.721, mean reward:  0.313 [-19.892, 100.000], mean action: 0.929 [0.000, 3.000],  loss: 8.711886, mae: 46.528771, mean_q: 63.656188, mean_eps: 0.100000\n",
      " 663665/1000000: episode: 1957, duration: 3.087s, episode steps: 605, steps per second: 196, episode reward: 263.278, mean reward:  0.435 [-20.456, 100.000], mean action: 0.798 [0.000, 3.000],  loss: 4.077482, mae: 42.504633, mean_q: 57.635197, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 663808/1000000: episode: 1958, duration: 0.681s, episode steps: 143, steps per second: 210, episode reward: -146.110, mean reward: -1.022 [-100.000, 39.784], mean action: 1.147 [0.000, 3.000],  loss: 4.442440, mae: 44.348356, mean_q: 60.461382, mean_eps: 0.100000\n",
      " 663909/1000000: episode: 1959, duration: 0.479s, episode steps: 101, steps per second: 211, episode reward: 13.040, mean reward:  0.129 [-100.000, 17.417], mean action: 1.257 [0.000, 3.000],  loss: 15.374622, mae: 48.323549, mean_q: 66.027782, mean_eps: 0.100000\n",
      " 664004/1000000: episode: 1960, duration: 0.450s, episode steps:  95, steps per second: 211, episode reward: -33.283, mean reward: -0.350 [-100.000, 11.330], mean action: 1.305 [0.000, 3.000],  loss: 16.759987, mae: 53.026553, mean_q: 72.213567, mean_eps: 0.100000\n",
      " 664230/1000000: episode: 1961, duration: 1.087s, episode steps: 226, steps per second: 208, episode reward: 261.701, mean reward:  1.158 [-2.702, 100.000], mean action: 0.872 [0.000, 3.000],  loss: 14.902267, mae: 56.603140, mean_q: 76.608507, mean_eps: 0.100000\n",
      " 664336/1000000: episode: 1962, duration: 0.508s, episode steps: 106, steps per second: 209, episode reward: 30.066, mean reward:  0.284 [-100.000, 16.203], mean action: 1.660 [0.000, 3.000],  loss: 9.639138, mae: 57.760356, mean_q: 78.101033, mean_eps: 0.100000\n",
      " 664849/1000000: episode: 1963, duration: 2.635s, episode steps: 513, steps per second: 195, episode reward: 277.170, mean reward:  0.540 [-17.419, 100.000], mean action: 0.671 [0.000, 3.000],  loss: 15.201364, mae: 61.976423, mean_q: 83.030891, mean_eps: 0.100000\n",
      " 664964/1000000: episode: 1964, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -34.323, mean reward: -0.298 [-100.000, 15.167], mean action: 1.557 [0.000, 3.000],  loss: 15.195645, mae: 58.143278, mean_q: 78.426929, mean_eps: 0.100000\n",
      " 665278/1000000: episode: 1965, duration: 1.548s, episode steps: 314, steps per second: 203, episode reward: 252.442, mean reward:  0.804 [-17.925, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 12.866747, mae: 55.520627, mean_q: 74.740833, mean_eps: 0.100000\n",
      " 665458/1000000: episode: 1966, duration: 0.885s, episode steps: 180, steps per second: 203, episode reward: 278.803, mean reward:  1.549 [-10.166, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 13.541008, mae: 53.089466, mean_q: 71.218921, mean_eps: 0.100000\n",
      " 666202/1000000: episode: 1967, duration: 3.843s, episode steps: 744, steps per second: 194, episode reward: 284.575, mean reward:  0.382 [-19.681, 100.000], mean action: 1.007 [0.000, 3.000],  loss: 5.060998, mae: 55.898775, mean_q: 75.220054, mean_eps: 0.100000\n",
      " 666419/1000000: episode: 1968, duration: 1.057s, episode steps: 217, steps per second: 205, episode reward: 247.618, mean reward:  1.141 [-7.494, 100.000], mean action: 1.479 [0.000, 3.000],  loss: 3.567623, mae: 54.483023, mean_q: 73.532611, mean_eps: 0.100000\n",
      " 666571/1000000: episode: 1969, duration: 0.735s, episode steps: 152, steps per second: 207, episode reward: 271.174, mean reward:  1.784 [-2.374, 100.000], mean action: 1.559 [0.000, 3.000],  loss: 2.912102, mae: 54.212870, mean_q: 73.099047, mean_eps: 0.100000\n",
      " 666892/1000000: episode: 1970, duration: 1.598s, episode steps: 321, steps per second: 201, episode reward: 299.012, mean reward:  0.932 [-18.675, 100.000], mean action: 1.604 [0.000, 3.000],  loss: 5.391860, mae: 53.509084, mean_q: 72.035609, mean_eps: 0.100000\n",
      " 667452/1000000: episode: 1971, duration: 2.927s, episode steps: 560, steps per second: 191, episode reward: 163.036, mean reward:  0.291 [-19.973, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 5.671505, mae: 54.850504, mean_q: 74.964975, mean_eps: 0.100000\n",
      " 667926/1000000: episode: 1972, duration: 2.482s, episode steps: 474, steps per second: 191, episode reward: 229.196, mean reward:  0.484 [-21.391, 100.000], mean action: 0.945 [0.000, 3.000],  loss: 4.250221, mae: 48.243244, mean_q: 66.081928, mean_eps: 0.100000\n",
      " 668085/1000000: episode: 1973, duration: 0.763s, episode steps: 159, steps per second: 208, episode reward:  4.038, mean reward:  0.025 [-100.000, 16.324], mean action: 1.478 [0.000, 3.000],  loss: 3.251429, mae: 47.892964, mean_q: 65.432316, mean_eps: 0.100000\n",
      " 668430/1000000: episode: 1974, duration: 1.700s, episode steps: 345, steps per second: 203, episode reward: 232.748, mean reward:  0.675 [-18.486, 100.000], mean action: 0.733 [0.000, 3.000],  loss: 15.398858, mae: 59.213487, mean_q: 80.051492, mean_eps: 0.100000\n",
      " 668720/1000000: episode: 1975, duration: 1.396s, episode steps: 290, steps per second: 208, episode reward: 268.610, mean reward:  0.926 [-8.751, 100.000], mean action: 1.048 [0.000, 3.000],  loss: 15.034392, mae: 56.642278, mean_q: 76.962408, mean_eps: 0.100000\n",
      " 669014/1000000: episode: 1976, duration: 1.447s, episode steps: 294, steps per second: 203, episode reward: -42.004, mean reward: -0.143 [-100.000, 11.525], mean action: 1.378 [0.000, 3.000],  loss: 14.213797, mae: 59.430430, mean_q: 80.695311, mean_eps: 0.100000\n",
      " 669256/1000000: episode: 1977, duration: 1.182s, episode steps: 242, steps per second: 205, episode reward: 236.534, mean reward:  0.977 [-17.076, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 7.440876, mae: 49.997733, mean_q: 65.460129, mean_eps: 0.100000\n",
      " 669514/1000000: episode: 1978, duration: 1.245s, episode steps: 258, steps per second: 207, episode reward: 303.947, mean reward:  1.178 [-17.364, 100.000], mean action: 0.864 [0.000, 3.000],  loss: 8.458377, mae: 51.503304, mean_q: 67.007745, mean_eps: 0.100000\n",
      " 670514/1000000: episode: 1979, duration: 5.160s, episode steps: 1000, steps per second: 194, episode reward: 130.811, mean reward:  0.131 [-21.063, 22.160], mean action: 1.809 [0.000, 3.000],  loss: 4.673041, mae: 49.273824, mean_q: 65.767117, mean_eps: 0.100000\n",
      " 670877/1000000: episode: 1980, duration: 1.811s, episode steps: 363, steps per second: 200, episode reward: 303.272, mean reward:  0.835 [-21.222, 100.000], mean action: 1.595 [0.000, 3.000],  loss: 2.252606, mae: 37.097006, mean_q: 50.328779, mean_eps: 0.100000\n",
      " 671220/1000000: episode: 1981, duration: 1.761s, episode steps: 343, steps per second: 195, episode reward: 280.677, mean reward:  0.818 [-18.708, 100.000], mean action: 1.682 [0.000, 3.000],  loss: 3.776212, mae: 44.206910, mean_q: 60.053328, mean_eps: 0.100000\n",
      " 671702/1000000: episode: 1982, duration: 2.444s, episode steps: 482, steps per second: 197, episode reward: 303.471, mean reward:  0.630 [-19.612, 100.000], mean action: 0.786 [0.000, 3.000],  loss: 3.995097, mae: 58.484785, mean_q: 79.534329, mean_eps: 0.100000\n",
      " 671934/1000000: episode: 1983, duration: 1.115s, episode steps: 232, steps per second: 208, episode reward: 240.512, mean reward:  1.037 [-9.285, 100.000], mean action: 0.823 [0.000, 3.000],  loss: 2.968740, mae: 62.246023, mean_q: 84.530013, mean_eps: 0.100000\n",
      " 672447/1000000: episode: 1984, duration: 2.525s, episode steps: 513, steps per second: 203, episode reward: 279.706, mean reward:  0.545 [-19.235, 100.000], mean action: 0.797 [0.000, 3.000],  loss: 2.239831, mae: 62.604141, mean_q: 84.707074, mean_eps: 0.100000\n",
      " 672757/1000000: episode: 1985, duration: 1.560s, episode steps: 310, steps per second: 199, episode reward: 299.496, mean reward:  0.966 [-10.204, 100.000], mean action: 1.690 [0.000, 3.000],  loss: 2.959019, mae: 59.156884, mean_q: 80.043181, mean_eps: 0.100000\n",
      " 673072/1000000: episode: 1986, duration: 1.537s, episode steps: 315, steps per second: 205, episode reward: 293.618, mean reward:  0.932 [-18.531, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 2.751603, mae: 57.672478, mean_q: 78.434494, mean_eps: 0.100000\n",
      " 673489/1000000: episode: 1987, duration: 2.066s, episode steps: 417, steps per second: 202, episode reward: 274.832, mean reward:  0.659 [-18.283, 100.000], mean action: 0.827 [0.000, 3.000],  loss: 3.544237, mae: 57.797276, mean_q: 78.910333, mean_eps: 0.100000\n",
      " 673991/1000000: episode: 1988, duration: 2.478s, episode steps: 502, steps per second: 203, episode reward: 269.402, mean reward:  0.537 [-18.596, 100.000], mean action: 0.416 [0.000, 3.000],  loss: 3.028742, mae: 55.694192, mean_q: 76.131915, mean_eps: 0.100000\n",
      " 674194/1000000: episode: 1989, duration: 0.984s, episode steps: 203, steps per second: 206, episode reward: 274.013, mean reward:  1.350 [-12.145, 100.000], mean action: 1.261 [0.000, 3.000],  loss: 3.517887, mae: 52.567294, mean_q: 71.828863, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 674481/1000000: episode: 1990, duration: 1.415s, episode steps: 287, steps per second: 203, episode reward: 237.179, mean reward:  0.826 [-11.238, 100.000], mean action: 1.220 [0.000, 3.000],  loss: 3.731390, mae: 55.231019, mean_q: 75.493707, mean_eps: 0.100000\n",
      " 674695/1000000: episode: 1991, duration: 1.024s, episode steps: 214, steps per second: 209, episode reward: 298.808, mean reward:  1.396 [-2.984, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 3.252566, mae: 58.657695, mean_q: 79.939252, mean_eps: 0.100000\n",
      " 674766/1000000: episode: 1992, duration: 0.348s, episode steps:  71, steps per second: 204, episode reward:  5.990, mean reward:  0.084 [-100.000, 19.471], mean action: 1.831 [0.000, 3.000],  loss: 6.009500, mae: 61.063409, mean_q: 83.200235, mean_eps: 0.100000\n",
      " 674947/1000000: episode: 1993, duration: 0.855s, episode steps: 181, steps per second: 212, episode reward: 279.912, mean reward:  1.546 [-3.350, 100.000], mean action: 1.006 [0.000, 3.000],  loss: 13.760381, mae: 67.146550, mean_q: 91.455896, mean_eps: 0.100000\n",
      " 675347/1000000: episode: 1994, duration: 1.998s, episode steps: 400, steps per second: 200, episode reward: 263.395, mean reward:  0.658 [-21.315, 100.000], mean action: 0.818 [0.000, 3.000],  loss: 12.438300, mae: 70.341475, mean_q: 95.873715, mean_eps: 0.100000\n",
      " 676165/1000000: episode: 1995, duration: 4.147s, episode steps: 818, steps per second: 197, episode reward: 267.984, mean reward:  0.328 [-20.587, 100.000], mean action: 1.621 [0.000, 3.000],  loss: 3.845855, mae: 62.162695, mean_q: 84.020430, mean_eps: 0.100000\n",
      " 676334/1000000: episode: 1996, duration: 0.812s, episode steps: 169, steps per second: 208, episode reward: 272.686, mean reward:  1.614 [-2.499, 100.000], mean action: 1.331 [0.000, 3.000],  loss: 1.639774, mae: 53.939331, mean_q: 72.633205, mean_eps: 0.100000\n",
      " 676545/1000000: episode: 1997, duration: 1.022s, episode steps: 211, steps per second: 206, episode reward: 318.773, mean reward:  1.511 [-18.613, 100.000], mean action: 1.517 [0.000, 3.000],  loss: 3.157957, mae: 55.251014, mean_q: 74.546683, mean_eps: 0.100000\n",
      " 676860/1000000: episode: 1998, duration: 1.554s, episode steps: 315, steps per second: 203, episode reward: 268.630, mean reward:  0.853 [-10.259, 100.000], mean action: 0.768 [0.000, 3.000],  loss: 3.198134, mae: 61.536183, mean_q: 83.141072, mean_eps: 0.100000\n",
      " 677015/1000000: episode: 1999, duration: 0.745s, episode steps: 155, steps per second: 208, episode reward: 279.755, mean reward:  1.805 [-10.410, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 4.718293, mae: 67.948353, mean_q: 91.844519, mean_eps: 0.100000\n",
      " 677406/1000000: episode: 2000, duration: 1.927s, episode steps: 391, steps per second: 203, episode reward: 274.131, mean reward:  0.701 [-19.282, 100.000], mean action: 0.675 [0.000, 3.000],  loss: 4.067841, mae: 72.608569, mean_q: 98.083113, mean_eps: 0.100000\n",
      " 677521/1000000: episode: 2001, duration: 0.550s, episode steps: 115, steps per second: 209, episode reward: 18.051, mean reward:  0.157 [-100.000, 13.013], mean action: 1.252 [0.000, 3.000],  loss: 4.162774, mae: 66.504318, mean_q: 89.713388, mean_eps: 0.100000\n",
      " 677696/1000000: episode: 2002, duration: 0.843s, episode steps: 175, steps per second: 208, episode reward: 286.567, mean reward:  1.638 [-19.368, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 15.144951, mae: 67.737579, mean_q: 91.510132, mean_eps: 0.100000\n",
      " 677894/1000000: episode: 2003, duration: 0.960s, episode steps: 198, steps per second: 206, episode reward: -150.774, mean reward: -0.761 [-100.000, 14.793], mean action: 1.571 [0.000, 3.000],  loss: 11.350645, mae: 69.518480, mean_q: 94.186300, mean_eps: 0.100000\n",
      " 678086/1000000: episode: 2004, duration: 0.917s, episode steps: 192, steps per second: 209, episode reward: 250.365, mean reward:  1.304 [-2.449, 100.000], mean action: 0.880 [0.000, 3.000],  loss: 13.448727, mae: 70.463026, mean_q: 95.806338, mean_eps: 0.100000\n",
      " 678274/1000000: episode: 2005, duration: 0.897s, episode steps: 188, steps per second: 210, episode reward: 269.864, mean reward:  1.435 [-2.885, 100.000], mean action: 0.867 [0.000, 3.000],  loss: 11.006916, mae: 70.013426, mean_q: 95.143692, mean_eps: 0.100000\n",
      " 678415/1000000: episode: 2006, duration: 0.671s, episode steps: 141, steps per second: 210, episode reward: 286.108, mean reward:  2.029 [-2.191, 100.000], mean action: 1.170 [0.000, 3.000],  loss: 10.677594, mae: 74.603438, mean_q: 100.881386, mean_eps: 0.100000\n",
      " 678512/1000000: episode: 2007, duration: 0.459s, episode steps:  97, steps per second: 211, episode reward: -1.526, mean reward: -0.016 [-100.000, 15.793], mean action: 1.361 [0.000, 3.000],  loss: 7.940653, mae: 76.746151, mean_q: 103.913994, mean_eps: 0.100000\n",
      " 678590/1000000: episode: 2008, duration: 0.374s, episode steps:  78, steps per second: 209, episode reward: -3.535, mean reward: -0.045 [-100.000, 13.319], mean action: 1.513 [0.000, 3.000],  loss: 21.443537, mae: 76.249531, mean_q: 103.193432, mean_eps: 0.100000\n",
      " 678686/1000000: episode: 2009, duration: 0.457s, episode steps:  96, steps per second: 210, episode reward: 14.342, mean reward:  0.149 [-100.000, 22.661], mean action: 1.042 [0.000, 3.000],  loss: 16.297592, mae: 74.961500, mean_q: 101.867869, mean_eps: 0.100000\n",
      " 679033/1000000: episode: 2010, duration: 1.717s, episode steps: 347, steps per second: 202, episode reward: 236.325, mean reward:  0.681 [-19.567, 100.000], mean action: 1.179 [0.000, 3.000],  loss: 22.130504, mae: 76.144845, mean_q: 102.926213, mean_eps: 0.100000\n",
      " 680033/1000000: episode: 2011, duration: 5.573s, episode steps: 1000, steps per second: 179, episode reward: 54.281, mean reward:  0.054 [-22.794, 24.016], mean action: 2.343 [0.000, 3.000],  loss: 15.358473, mae: 61.504142, mean_q: 83.701922, mean_eps: 0.100000\n",
      " 680130/1000000: episode: 2012, duration: 0.469s, episode steps:  97, steps per second: 207, episode reward: 34.649, mean reward:  0.357 [-100.000, 10.273], mean action: 1.680 [0.000, 3.000],  loss: 5.526686, mae: 50.143633, mean_q: 68.671000, mean_eps: 0.100000\n",
      " 680962/1000000: episode: 2013, duration: 4.291s, episode steps: 832, steps per second: 194, episode reward: 261.473, mean reward:  0.314 [-19.941, 100.000], mean action: 1.099 [0.000, 3.000],  loss: 8.125451, mae: 53.281499, mean_q: 72.681270, mean_eps: 0.100000\n",
      " 681072/1000000: episode: 2014, duration: 0.535s, episode steps: 110, steps per second: 206, episode reward: 41.811, mean reward:  0.380 [-100.000, 22.117], mean action: 1.927 [0.000, 3.000],  loss: 6.027301, mae: 58.288925, mean_q: 79.453503, mean_eps: 0.100000\n",
      " 681208/1000000: episode: 2015, duration: 0.660s, episode steps: 136, steps per second: 206, episode reward: 28.690, mean reward:  0.211 [-100.000, 14.696], mean action: 1.838 [0.000, 3.000],  loss: 11.644569, mae: 59.266771, mean_q: 80.832923, mean_eps: 0.100000\n",
      " 681336/1000000: episode: 2016, duration: 0.612s, episode steps: 128, steps per second: 209, episode reward: 21.973, mean reward:  0.172 [-100.000, 14.649], mean action: 1.547 [0.000, 3.000],  loss: 8.907988, mae: 60.412628, mean_q: 82.321348, mean_eps: 0.100000\n",
      " 681432/1000000: episode: 2017, duration: 0.463s, episode steps:  96, steps per second: 208, episode reward: -87.217, mean reward: -0.909 [-100.000,  8.390], mean action: 1.969 [0.000, 3.000],  loss: 13.094351, mae: 64.738238, mean_q: 87.825752, mean_eps: 0.100000\n",
      " 681693/1000000: episode: 2018, duration: 1.277s, episode steps: 261, steps per second: 204, episode reward: 278.990, mean reward:  1.069 [-17.366, 100.000], mean action: 0.985 [0.000, 3.000],  loss: 12.827700, mae: 69.582813, mean_q: 93.199608, mean_eps: 0.100000\n",
      " 681838/1000000: episode: 2019, duration: 0.698s, episode steps: 145, steps per second: 208, episode reward: 22.637, mean reward:  0.156 [-100.000, 17.202], mean action: 1.531 [0.000, 3.000],  loss: 12.492873, mae: 73.148309, mean_q: 97.528855, mean_eps: 0.100000\n",
      " 682042/1000000: episode: 2020, duration: 0.982s, episode steps: 204, steps per second: 208, episode reward: 278.699, mean reward:  1.366 [-3.444, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 18.877846, mae: 78.150784, mean_q: 103.480677, mean_eps: 0.100000\n",
      " 682419/1000000: episode: 2021, duration: 1.910s, episode steps: 377, steps per second: 197, episode reward: 222.935, mean reward:  0.591 [-4.310, 100.000], mean action: 1.435 [0.000, 3.000],  loss: 12.809944, mae: 72.854897, mean_q: 96.838552, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 682923/1000000: episode: 2022, duration: 2.555s, episode steps: 504, steps per second: 197, episode reward: 284.831, mean reward:  0.565 [-21.490, 100.000], mean action: 1.054 [0.000, 3.000],  loss: 9.611984, mae: 66.542798, mean_q: 89.603873, mean_eps: 0.100000\n",
      " 683148/1000000: episode: 2023, duration: 1.099s, episode steps: 225, steps per second: 205, episode reward: 278.883, mean reward:  1.239 [-7.948, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 2.641421, mae: 63.622233, mean_q: 85.567331, mean_eps: 0.100000\n",
      " 683295/1000000: episode: 2024, duration: 0.700s, episode steps: 147, steps per second: 210, episode reward: 22.033, mean reward:  0.150 [-100.000, 14.738], mean action: 1.755 [0.000, 3.000],  loss: 4.845872, mae: 62.952556, mean_q: 84.731413, mean_eps: 0.100000\n",
      " 683644/1000000: episode: 2025, duration: 1.765s, episode steps: 349, steps per second: 198, episode reward: 242.065, mean reward:  0.694 [-19.919, 100.000], mean action: 1.404 [0.000, 3.000],  loss: 15.360822, mae: 63.325590, mean_q: 85.351132, mean_eps: 0.100000\n",
      " 683757/1000000: episode: 2026, duration: 0.546s, episode steps: 113, steps per second: 207, episode reward: -8.196, mean reward: -0.073 [-100.000, 21.029], mean action: 1.602 [0.000, 3.000],  loss: 17.652004, mae: 61.787630, mean_q: 83.545353, mean_eps: 0.100000\n",
      " 683872/1000000: episode: 2027, duration: 0.549s, episode steps: 115, steps per second: 209, episode reward: 31.907, mean reward:  0.277 [-100.000, 10.824], mean action: 1.478 [0.000, 3.000],  loss: 10.524617, mae: 64.885057, mean_q: 85.256567, mean_eps: 0.100000\n",
      " 683978/1000000: episode: 2028, duration: 0.509s, episode steps: 106, steps per second: 208, episode reward: -23.175, mean reward: -0.219 [-100.000, 18.953], mean action: 1.349 [0.000, 3.000],  loss: 17.357147, mae: 66.865375, mean_q: 88.276461, mean_eps: 0.100000\n",
      " 684325/1000000: episode: 2029, duration: 1.755s, episode steps: 347, steps per second: 198, episode reward: 232.622, mean reward:  0.670 [-19.184, 100.000], mean action: 0.870 [0.000, 3.000],  loss: 17.460741, mae: 66.326428, mean_q: 84.447831, mean_eps: 0.100000\n",
      " 684577/1000000: episode: 2030, duration: 1.278s, episode steps: 252, steps per second: 197, episode reward: 249.614, mean reward:  0.991 [-8.725, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 9.858700, mae: 63.657016, mean_q: 82.257542, mean_eps: 0.100000\n",
      " 684783/1000000: episode: 2031, duration: 0.990s, episode steps: 206, steps per second: 208, episode reward: 274.775, mean reward:  1.334 [-9.210, 100.000], mean action: 1.102 [0.000, 3.000],  loss: 6.701949, mae: 64.279093, mean_q: 82.743592, mean_eps: 0.100000\n",
      " 684972/1000000: episode: 2032, duration: 0.910s, episode steps: 189, steps per second: 208, episode reward: 236.779, mean reward:  1.253 [-8.943, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 7.030262, mae: 61.795805, mean_q: 80.514813, mean_eps: 0.100000\n",
      " 685081/1000000: episode: 2033, duration: 0.528s, episode steps: 109, steps per second: 207, episode reward:  2.940, mean reward:  0.027 [-100.000,  9.026], mean action: 1.881 [0.000, 3.000],  loss: 5.344985, mae: 61.236818, mean_q: 82.641768, mean_eps: 0.100000\n",
      " 685377/1000000: episode: 2034, duration: 1.452s, episode steps: 296, steps per second: 204, episode reward: 282.479, mean reward:  0.954 [-10.793, 100.000], mean action: 1.706 [0.000, 3.000],  loss: 10.331005, mae: 65.810467, mean_q: 88.382259, mean_eps: 0.100000\n",
      " 685564/1000000: episode: 2035, duration: 0.901s, episode steps: 187, steps per second: 208, episode reward: 296.158, mean reward:  1.584 [-2.370, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 9.124281, mae: 68.300681, mean_q: 92.109643, mean_eps: 0.100000\n",
      " 685667/1000000: episode: 2036, duration: 0.494s, episode steps: 103, steps per second: 209, episode reward: 32.850, mean reward:  0.319 [-100.000, 25.064], mean action: 1.728 [0.000, 3.000],  loss: 7.635833, mae: 70.052106, mean_q: 94.524972, mean_eps: 0.100000\n",
      " 686058/1000000: episode: 2037, duration: 1.966s, episode steps: 391, steps per second: 199, episode reward: 230.045, mean reward:  0.588 [-19.238, 100.000], mean action: 0.908 [0.000, 3.000],  loss: 9.219401, mae: 70.195498, mean_q: 93.927596, mean_eps: 0.100000\n",
      " 686246/1000000: episode: 2038, duration: 0.909s, episode steps: 188, steps per second: 207, episode reward: 255.262, mean reward:  1.358 [-9.232, 100.000], mean action: 1.303 [0.000, 3.000],  loss: 7.675223, mae: 68.165139, mean_q: 91.539659, mean_eps: 0.100000\n",
      " 686504/1000000: episode: 2039, duration: 1.278s, episode steps: 258, steps per second: 202, episode reward: 270.308, mean reward:  1.048 [-17.908, 100.000], mean action: 1.054 [0.000, 3.000],  loss: 8.429890, mae: 66.045171, mean_q: 88.582889, mean_eps: 0.100000\n",
      " 686663/1000000: episode: 2040, duration: 0.768s, episode steps: 159, steps per second: 207, episode reward: 26.756, mean reward:  0.168 [-100.000,  9.010], mean action: 1.811 [0.000, 3.000],  loss: 6.461269, mae: 65.264004, mean_q: 87.716206, mean_eps: 0.100000\n",
      " 686754/1000000: episode: 2041, duration: 0.439s, episode steps:  91, steps per second: 207, episode reward:  4.670, mean reward:  0.051 [-100.000, 16.990], mean action: 1.670 [0.000, 3.000],  loss: 14.915433, mae: 65.705857, mean_q: 89.374484, mean_eps: 0.100000\n",
      " 686820/1000000: episode: 2042, duration: 0.321s, episode steps:  66, steps per second: 206, episode reward: -42.781, mean reward: -0.648 [-100.000,  4.503], mean action: 1.409 [0.000, 3.000],  loss: 13.247869, mae: 66.005096, mean_q: 90.272799, mean_eps: 0.100000\n",
      " 687245/1000000: episode: 2043, duration: 2.131s, episode steps: 425, steps per second: 199, episode reward: 251.012, mean reward:  0.591 [-10.628, 100.000], mean action: 1.440 [0.000, 3.000],  loss: 24.477529, mae: 69.963611, mean_q: 94.990837, mean_eps: 0.100000\n",
      " 687575/1000000: episode: 2044, duration: 1.683s, episode steps: 330, steps per second: 196, episode reward: 211.980, mean reward:  0.642 [-9.189, 100.000], mean action: 1.364 [0.000, 3.000],  loss: 18.125057, mae: 62.406839, mean_q: 84.528170, mean_eps: 0.100000\n",
      " 688508/1000000: episode: 2045, duration: 5.332s, episode steps: 933, steps per second: 175, episode reward: 173.783, mean reward:  0.186 [-20.507, 100.000], mean action: 1.236 [0.000, 3.000],  loss: 5.224147, mae: 52.677621, mean_q: 71.732339, mean_eps: 0.100000\n",
      " 688661/1000000: episode: 2046, duration: 0.734s, episode steps: 153, steps per second: 209, episode reward: -114.749, mean reward: -0.750 [-100.000, 19.081], mean action: 1.438 [0.000, 3.000],  loss: 2.713176, mae: 46.421283, mean_q: 61.780176, mean_eps: 0.100000\n",
      " 689002/1000000: episode: 2047, duration: 1.717s, episode steps: 341, steps per second: 199, episode reward: 253.579, mean reward:  0.744 [-18.388, 100.000], mean action: 1.109 [0.000, 3.000],  loss: 6.388019, mae: 42.012457, mean_q: 50.368006, mean_eps: 0.100000\n",
      " 689300/1000000: episode: 2048, duration: 1.460s, episode steps: 298, steps per second: 204, episode reward: 42.922, mean reward:  0.144 [-100.000, 10.056], mean action: 1.497 [0.000, 3.000],  loss: 9.058969, mae: 40.483948, mean_q: 48.062411, mean_eps: 0.100000\n",
      " 689483/1000000: episode: 2049, duration: 0.883s, episode steps: 183, steps per second: 207, episode reward: -81.854, mean reward: -0.447 [-100.000, 17.877], mean action: 1.590 [0.000, 3.000],  loss: 17.798878, mae: 42.482967, mean_q: 49.295019, mean_eps: 0.100000\n",
      " 689678/1000000: episode: 2050, duration: 0.950s, episode steps: 195, steps per second: 205, episode reward: 25.850, mean reward:  0.133 [-100.000, 15.050], mean action: 1.938 [0.000, 3.000],  loss: 23.200689, mae: 43.549267, mean_q: 49.727474, mean_eps: 0.100000\n",
      " 690186/1000000: episode: 2051, duration: 2.783s, episode steps: 508, steps per second: 183, episode reward: 212.491, mean reward:  0.418 [-20.065, 100.000], mean action: 1.098 [0.000, 3.000],  loss: 14.718040, mae: 44.051615, mean_q: 56.005806, mean_eps: 0.100000\n",
      " 690723/1000000: episode: 2052, duration: 3.253s, episode steps: 537, steps per second: 165, episode reward: 219.719, mean reward:  0.409 [-18.716, 100.000], mean action: 1.784 [0.000, 3.000],  loss: 9.709294, mae: 39.691730, mean_q: 52.084054, mean_eps: 0.100000\n",
      " 691145/1000000: episode: 2053, duration: 2.409s, episode steps: 422, steps per second: 175, episode reward: 255.955, mean reward:  0.607 [-18.857, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 5.324972, mae: 32.403887, mean_q: 43.333216, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 691247/1000000: episode: 2054, duration: 0.492s, episode steps: 102, steps per second: 207, episode reward: -1.448, mean reward: -0.014 [-100.000, 20.143], mean action: 1.706 [0.000, 3.000],  loss: 9.203048, mae: 35.563886, mean_q: 47.576366, mean_eps: 0.100000\n",
      " 691394/1000000: episode: 2055, duration: 0.719s, episode steps: 147, steps per second: 204, episode reward: -24.706, mean reward: -0.168 [-100.000, 11.535], mean action: 1.844 [0.000, 3.000],  loss: 9.674605, mae: 36.229848, mean_q: 48.646908, mean_eps: 0.100000\n",
      " 692059/1000000: episode: 2056, duration: 3.779s, episode steps: 665, steps per second: 176, episode reward: 176.454, mean reward:  0.265 [-17.668, 100.000], mean action: 2.377 [0.000, 3.000],  loss: 13.273639, mae: 41.489856, mean_q: 45.155051, mean_eps: 0.100000\n",
      " 692458/1000000: episode: 2057, duration: 2.141s, episode steps: 399, steps per second: 186, episode reward: 227.388, mean reward:  0.570 [-19.214, 100.000], mean action: 1.175 [0.000, 3.000],  loss: 23.902986, mae: 41.011974, mean_q: 17.189291, mean_eps: 0.100000\n",
      " 692796/1000000: episode: 2058, duration: 1.789s, episode steps: 338, steps per second: 189, episode reward: 277.310, mean reward:  0.820 [-19.390, 100.000], mean action: 1.269 [0.000, 3.000],  loss: 17.776343, mae: 36.907343, mean_q: 17.300830, mean_eps: 0.100000\n",
      " 693477/1000000: episode: 2059, duration: 3.724s, episode steps: 681, steps per second: 183, episode reward: 266.908, mean reward:  0.392 [-18.858, 100.000], mean action: 0.755 [0.000, 3.000],  loss: 9.351512, mae: 44.631898, mean_q: 56.273471, mean_eps: 0.100000\n",
      " 693856/1000000: episode: 2060, duration: 1.947s, episode steps: 379, steps per second: 195, episode reward: 228.727, mean reward:  0.604 [-18.947, 100.000], mean action: 1.264 [0.000, 3.000],  loss: 2.282838, mae: 51.219061, mean_q: 69.508829, mean_eps: 0.100000\n",
      " 694085/1000000: episode: 2061, duration: 1.142s, episode steps: 229, steps per second: 201, episode reward: 240.741, mean reward:  1.051 [-9.657, 100.000], mean action: 1.616 [0.000, 3.000],  loss: 2.960565, mae: 51.333785, mean_q: 69.643963, mean_eps: 0.100000\n",
      " 694555/1000000: episode: 2062, duration: 2.635s, episode steps: 470, steps per second: 178, episode reward: 265.367, mean reward:  0.565 [-18.951, 100.000], mean action: 1.200 [0.000, 3.000],  loss: 3.182198, mae: 55.170939, mean_q: 74.581664, mean_eps: 0.100000\n",
      " 694824/1000000: episode: 2063, duration: 1.348s, episode steps: 269, steps per second: 200, episode reward: 246.394, mean reward:  0.916 [-6.086, 100.000], mean action: 1.565 [0.000, 3.000],  loss: 3.616168, mae: 55.435771, mean_q: 74.923399, mean_eps: 0.100000\n",
      " 695186/1000000: episode: 2064, duration: 1.966s, episode steps: 362, steps per second: 184, episode reward: 233.357, mean reward:  0.645 [-18.078, 100.000], mean action: 1.470 [0.000, 3.000],  loss: 4.149773, mae: 56.951138, mean_q: 76.852036, mean_eps: 0.100000\n",
      " 695382/1000000: episode: 2065, duration: 0.950s, episode steps: 196, steps per second: 206, episode reward: 261.476, mean reward:  1.334 [-9.573, 100.000], mean action: 1.270 [0.000, 3.000],  loss: 2.829393, mae: 54.577267, mean_q: 73.778156, mean_eps: 0.100000\n",
      " 695744/1000000: episode: 2066, duration: 1.807s, episode steps: 362, steps per second: 200, episode reward: 268.827, mean reward:  0.743 [-18.000, 100.000], mean action: 1.180 [0.000, 3.000],  loss: 4.502929, mae: 57.905366, mean_q: 78.159176, mean_eps: 0.100000\n",
      " 696236/1000000: episode: 2067, duration: 2.456s, episode steps: 492, steps per second: 200, episode reward: 278.469, mean reward:  0.566 [-21.012, 100.000], mean action: 0.876 [0.000, 3.000],  loss: 4.221249, mae: 56.144913, mean_q: 76.195371, mean_eps: 0.100000\n",
      " 696822/1000000: episode: 2068, duration: 3.158s, episode steps: 586, steps per second: 186, episode reward: 195.712, mean reward:  0.334 [-21.775, 100.000], mean action: 1.143 [0.000, 3.000],  loss: 2.922009, mae: 51.573474, mean_q: 70.098264, mean_eps: 0.100000\n",
      " 697291/1000000: episode: 2069, duration: 2.520s, episode steps: 469, steps per second: 186, episode reward: 269.817, mean reward:  0.575 [-18.167, 100.000], mean action: 1.640 [0.000, 3.000],  loss: 2.201875, mae: 45.176129, mean_q: 61.283133, mean_eps: 0.100000\n",
      " 697586/1000000: episode: 2070, duration: 1.470s, episode steps: 295, steps per second: 201, episode reward: 272.015, mean reward:  0.922 [-17.190, 100.000], mean action: 1.275 [0.000, 3.000],  loss: 2.718005, mae: 43.545053, mean_q: 59.079705, mean_eps: 0.100000\n",
      " 697931/1000000: episode: 2071, duration: 1.770s, episode steps: 345, steps per second: 195, episode reward: 256.792, mean reward:  0.744 [-17.599, 100.000], mean action: 1.148 [0.000, 3.000],  loss: 4.021068, mae: 47.182670, mean_q: 63.894727, mean_eps: 0.100000\n",
      " 698210/1000000: episode: 2072, duration: 1.399s, episode steps: 279, steps per second: 199, episode reward: 254.490, mean reward:  0.912 [-18.249, 100.000], mean action: 1.441 [0.000, 3.000],  loss: 3.393033, mae: 49.680682, mean_q: 67.286818, mean_eps: 0.100000\n",
      " 698400/1000000: episode: 2073, duration: 0.968s, episode steps: 190, steps per second: 196, episode reward: 258.881, mean reward:  1.363 [-8.053, 100.000], mean action: 1.421 [0.000, 3.000],  loss: 4.378122, mae: 52.689579, mean_q: 71.369438, mean_eps: 0.100000\n",
      " 698927/1000000: episode: 2074, duration: 2.759s, episode steps: 527, steps per second: 191, episode reward: 243.319, mean reward:  0.462 [-17.673, 100.000], mean action: 1.091 [0.000, 3.000],  loss: 5.119943, mae: 52.654582, mean_q: 71.346795, mean_eps: 0.100000\n",
      " 699220/1000000: episode: 2075, duration: 1.488s, episode steps: 293, steps per second: 197, episode reward: 265.726, mean reward:  0.907 [-2.788, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 4.953146, mae: 50.899524, mean_q: 69.002825, mean_eps: 0.100000\n",
      " 699517/1000000: episode: 2076, duration: 1.487s, episode steps: 297, steps per second: 200, episode reward: 292.835, mean reward:  0.986 [-8.491, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 3.051167, mae: 50.470335, mean_q: 68.579774, mean_eps: 0.100000\n",
      " 699909/1000000: episode: 2077, duration: 2.003s, episode steps: 392, steps per second: 196, episode reward: 257.005, mean reward:  0.656 [-10.305, 100.000], mean action: 1.895 [0.000, 3.000],  loss: 2.664565, mae: 53.472764, mean_q: 72.562046, mean_eps: 0.100000\n",
      " 700230/1000000: episode: 2078, duration: 1.657s, episode steps: 321, steps per second: 194, episode reward: 278.077, mean reward:  0.866 [-11.483, 100.000], mean action: 1.330 [0.000, 3.000],  loss: 4.159578, mae: 54.894784, mean_q: 74.463269, mean_eps: 0.100000\n",
      " 700692/1000000: episode: 2079, duration: 2.333s, episode steps: 462, steps per second: 198, episode reward: 295.175, mean reward:  0.639 [-18.890, 100.000], mean action: 1.303 [0.000, 3.000],  loss: 2.947569, mae: 55.288755, mean_q: 74.804330, mean_eps: 0.100000\n",
      " 701235/1000000: episode: 2080, duration: 2.857s, episode steps: 543, steps per second: 190, episode reward: 235.411, mean reward:  0.434 [-18.907, 100.000], mean action: 0.737 [0.000, 3.000],  loss: 2.907876, mae: 56.185596, mean_q: 76.172833, mean_eps: 0.100000\n",
      " 701567/1000000: episode: 2081, duration: 1.697s, episode steps: 332, steps per second: 196, episode reward: 243.210, mean reward:  0.733 [-20.422, 100.000], mean action: 1.139 [0.000, 3.000],  loss: 1.502378, mae: 56.308690, mean_q: 76.224104, mean_eps: 0.100000\n",
      " 701861/1000000: episode: 2082, duration: 1.505s, episode steps: 294, steps per second: 195, episode reward: 264.588, mean reward:  0.900 [-10.211, 100.000], mean action: 1.796 [0.000, 3.000],  loss: 2.349881, mae: 55.132547, mean_q: 74.689590, mean_eps: 0.100000\n",
      " 702057/1000000: episode: 2083, duration: 0.940s, episode steps: 196, steps per second: 208, episode reward: 245.608, mean reward:  1.253 [-2.793, 100.000], mean action: 1.347 [0.000, 3.000],  loss: 3.340482, mae: 56.859797, mean_q: 76.838824, mean_eps: 0.100000\n",
      " 702385/1000000: episode: 2084, duration: 1.637s, episode steps: 328, steps per second: 200, episode reward: 296.169, mean reward:  0.903 [-17.831, 100.000], mean action: 1.259 [0.000, 3.000],  loss: 3.670128, mae: 59.720377, mean_q: 80.710302, mean_eps: 0.100000\n",
      " 702708/1000000: episode: 2085, duration: 1.577s, episode steps: 323, steps per second: 205, episode reward: 300.964, mean reward:  0.932 [-11.993, 100.000], mean action: 1.248 [0.000, 3.000],  loss: 3.528409, mae: 61.314443, mean_q: 82.791331, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 702963/1000000: episode: 2086, duration: 1.237s, episode steps: 255, steps per second: 206, episode reward: 289.044, mean reward:  1.134 [-7.932, 100.000], mean action: 1.204 [0.000, 3.000],  loss: 2.701023, mae: 60.647279, mean_q: 81.971750, mean_eps: 0.100000\n",
      " 703225/1000000: episode: 2087, duration: 1.284s, episode steps: 262, steps per second: 204, episode reward: 273.142, mean reward:  1.043 [-10.779, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 3.250675, mae: 60.840891, mean_q: 82.338628, mean_eps: 0.100000\n",
      " 704075/1000000: episode: 2088, duration: 4.627s, episode steps: 850, steps per second: 184, episode reward: 200.526, mean reward:  0.236 [-17.939, 100.000], mean action: 2.461 [0.000, 3.000],  loss: 2.553165, mae: 51.827716, mean_q: 70.647018, mean_eps: 0.100000\n",
      " 704330/1000000: episode: 2089, duration: 1.279s, episode steps: 255, steps per second: 199, episode reward: 260.434, mean reward:  1.021 [-17.937, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 3.340913, mae: 35.999876, mean_q: 49.592842, mean_eps: 0.100000\n",
      " 704529/1000000: episode: 2090, duration: 0.979s, episode steps: 199, steps per second: 203, episode reward: 284.750, mean reward:  1.431 [-17.356, 100.000], mean action: 1.206 [0.000, 3.000],  loss: 4.522637, mae: 37.446618, mean_q: 51.569425, mean_eps: 0.100000\n",
      " 704771/1000000: episode: 2091, duration: 1.227s, episode steps: 242, steps per second: 197, episode reward: 267.166, mean reward:  1.104 [-9.570, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 4.763975, mae: 46.789292, mean_q: 64.133884, mean_eps: 0.100000\n",
      " 705132/1000000: episode: 2092, duration: 1.890s, episode steps: 361, steps per second: 191, episode reward: 289.024, mean reward:  0.801 [-18.958, 100.000], mean action: 0.925 [0.000, 3.000],  loss: 4.515322, mae: 59.582874, mean_q: 81.213145, mean_eps: 0.100000\n",
      " 705563/1000000: episode: 2093, duration: 2.558s, episode steps: 431, steps per second: 168, episode reward: 249.183, mean reward:  0.578 [-19.711, 100.000], mean action: 1.039 [0.000, 3.000],  loss: 2.314226, mae: 63.015974, mean_q: 85.772321, mean_eps: 0.100000\n",
      " 705670/1000000: episode: 2094, duration: 0.531s, episode steps: 107, steps per second: 202, episode reward:  1.871, mean reward:  0.017 [-100.000, 10.626], mean action: 1.617 [0.000, 3.000],  loss: 2.100487, mae: 60.490545, mean_q: 82.271960, mean_eps: 0.100000\n",
      " 706056/1000000: episode: 2095, duration: 2.027s, episode steps: 386, steps per second: 190, episode reward: 298.645, mean reward:  0.774 [-13.016, 100.000], mean action: 1.448 [0.000, 3.000],  loss: 3.363507, mae: 59.320559, mean_q: 79.143001, mean_eps: 0.100000\n",
      " 706260/1000000: episode: 2096, duration: 1.019s, episode steps: 204, steps per second: 200, episode reward: 281.987, mean reward:  1.382 [-8.376, 100.000], mean action: 1.672 [0.000, 3.000],  loss: 3.798494, mae: 60.754882, mean_q: 80.503074, mean_eps: 0.100000\n",
      " 706449/1000000: episode: 2097, duration: 0.909s, episode steps: 189, steps per second: 208, episode reward: 277.763, mean reward:  1.470 [-10.180, 100.000], mean action: 1.492 [0.000, 3.000],  loss: 4.544120, mae: 61.438460, mean_q: 81.037484, mean_eps: 0.100000\n",
      " 706698/1000000: episode: 2098, duration: 1.277s, episode steps: 249, steps per second: 195, episode reward: 305.411, mean reward:  1.227 [-8.194, 100.000], mean action: 1.285 [0.000, 3.000],  loss: 4.488007, mae: 65.560533, mean_q: 86.900369, mean_eps: 0.100000\n",
      " 707082/1000000: episode: 2099, duration: 2.121s, episode steps: 384, steps per second: 181, episode reward: 303.897, mean reward:  0.791 [-19.024, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 3.490727, mae: 67.868327, mean_q: 91.954275, mean_eps: 0.100000\n",
      " 707424/1000000: episode: 2100, duration: 1.771s, episode steps: 342, steps per second: 193, episode reward: 254.681, mean reward:  0.745 [-8.785, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 2.654461, mae: 66.956870, mean_q: 90.709283, mean_eps: 0.100000\n",
      " 707621/1000000: episode: 2101, duration: 0.984s, episode steps: 197, steps per second: 200, episode reward: 328.544, mean reward:  1.668 [-18.336, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 2.574047, mae: 64.466827, mean_q: 87.443505, mean_eps: 0.100000\n",
      " 707886/1000000: episode: 2102, duration: 1.387s, episode steps: 265, steps per second: 191, episode reward: 227.704, mean reward:  0.859 [-19.894, 100.000], mean action: 1.208 [0.000, 3.000],  loss: 2.840042, mae: 62.056005, mean_q: 84.017792, mean_eps: 0.100000\n",
      " 708082/1000000: episode: 2103, duration: 0.985s, episode steps: 196, steps per second: 199, episode reward: 276.439, mean reward:  1.410 [-11.164, 100.000], mean action: 1.291 [0.000, 3.000],  loss: 3.443035, mae: 61.486102, mean_q: 83.044052, mean_eps: 0.100000\n",
      " 708468/1000000: episode: 2104, duration: 2.039s, episode steps: 386, steps per second: 189, episode reward: 269.950, mean reward:  0.699 [-19.699, 100.000], mean action: 1.212 [0.000, 3.000],  loss: 2.510501, mae: 61.633604, mean_q: 83.256681, mean_eps: 0.100000\n",
      " 708683/1000000: episode: 2105, duration: 1.103s, episode steps: 215, steps per second: 195, episode reward: 240.446, mean reward:  1.118 [-2.376, 100.000], mean action: 1.344 [0.000, 3.000],  loss: 2.850983, mae: 62.104865, mean_q: 83.721551, mean_eps: 0.100000\n",
      " 709683/1000000: episode: 2106, duration: 5.492s, episode steps: 1000, steps per second: 182, episode reward: 84.483, mean reward:  0.084 [-20.483, 23.182], mean action: 2.244 [0.000, 3.000],  loss: 2.046726, mae: 55.638027, mean_q: 75.304027, mean_eps: 0.100000\n",
      " 710074/1000000: episode: 2107, duration: 2.130s, episode steps: 391, steps per second: 184, episode reward: 242.579, mean reward:  0.620 [-19.454, 100.000], mean action: 2.315 [0.000, 3.000],  loss: 1.443537, mae: 43.417301, mean_q: 59.136766, mean_eps: 0.100000\n",
      " 710465/1000000: episode: 2108, duration: 2.291s, episode steps: 391, steps per second: 171, episode reward: 240.479, mean reward:  0.615 [-17.233, 100.000], mean action: 1.872 [0.000, 3.000],  loss: 4.241503, mae: 46.146446, mean_q: 62.979201, mean_eps: 0.100000\n",
      " 710833/1000000: episode: 2109, duration: 1.881s, episode steps: 368, steps per second: 196, episode reward: 270.241, mean reward:  0.734 [-19.157, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 5.613486, mae: 48.951386, mean_q: 66.558717, mean_eps: 0.100000\n",
      " 711174/1000000: episode: 2110, duration: 1.764s, episode steps: 341, steps per second: 193, episode reward: 221.524, mean reward:  0.650 [-12.270, 100.000], mean action: 2.152 [0.000, 3.000],  loss: 6.065438, mae: 48.444991, mean_q: 65.849597, mean_eps: 0.100000\n",
      " 711359/1000000: episode: 2111, duration: 0.949s, episode steps: 185, steps per second: 195, episode reward: 205.241, mean reward:  1.109 [-2.666, 100.000], mean action: 1.276 [0.000, 3.000],  loss: 7.209314, mae: 44.995379, mean_q: 61.286833, mean_eps: 0.100000\n",
      " 711738/1000000: episode: 2112, duration: 2.174s, episode steps: 379, steps per second: 174, episode reward: 236.194, mean reward:  0.623 [-20.613, 100.000], mean action: 1.435 [0.000, 3.000],  loss: 7.271549, mae: 49.256151, mean_q: 66.898358, mean_eps: 0.100000\n",
      " 711866/1000000: episode: 2113, duration: 0.694s, episode steps: 128, steps per second: 185, episode reward: 15.757, mean reward:  0.123 [-100.000, 18.694], mean action: 1.422 [0.000, 3.000],  loss: 6.167427, mae: 52.328186, mean_q: 70.949838, mean_eps: 0.100000\n",
      " 712866/1000000: episode: 2114, duration: 5.986s, episode steps: 1000, steps per second: 167, episode reward: 135.766, mean reward:  0.136 [-20.353, 22.019], mean action: 2.399 [0.000, 3.000],  loss: 21.500409, mae: 50.840546, mean_q: 68.725073, mean_eps: 0.100000\n",
      " 713094/1000000: episode: 2115, duration: 1.278s, episode steps: 228, steps per second: 178, episode reward: 240.381, mean reward:  1.054 [-17.933, 100.000], mean action: 1.482 [0.000, 3.000],  loss: 2.257645, mae: 40.814826, mean_q: 55.143235, mean_eps: 0.100000\n",
      " 713239/1000000: episode: 2116, duration: 0.812s, episode steps: 145, steps per second: 179, episode reward: 32.228, mean reward:  0.222 [-100.000, 47.162], mean action: 1.538 [0.000, 3.000],  loss: 3.168156, mae: 43.789107, mean_q: 59.480302, mean_eps: 0.100000\n",
      " 713539/1000000: episode: 2117, duration: 1.542s, episode steps: 300, steps per second: 195, episode reward: 270.037, mean reward:  0.900 [-20.834, 100.000], mean action: 1.040 [0.000, 3.000],  loss: 21.253705, mae: 53.022321, mean_q: 71.631958, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 713733/1000000: episode: 2118, duration: 1.023s, episode steps: 194, steps per second: 190, episode reward: 38.084, mean reward:  0.196 [-100.000, 16.431], mean action: 1.784 [0.000, 3.000],  loss: 13.807974, mae: 57.854144, mean_q: 78.329717, mean_eps: 0.100000\n",
      " 714434/1000000: episode: 2119, duration: 4.149s, episode steps: 701, steps per second: 169, episode reward: 162.906, mean reward:  0.232 [-19.695, 100.000], mean action: 1.588 [0.000, 3.000],  loss: 17.939852, mae: 52.469748, mean_q: 69.343163, mean_eps: 0.100000\n",
      " 714651/1000000: episode: 2120, duration: 1.078s, episode steps: 217, steps per second: 201, episode reward: 261.989, mean reward:  1.207 [-19.328, 100.000], mean action: 1.037 [0.000, 3.000],  loss: 15.271166, mae: 35.333183, mean_q: 41.305784, mean_eps: 0.100000\n",
      " 714777/1000000: episode: 2121, duration: 0.609s, episode steps: 126, steps per second: 207, episode reward: 18.760, mean reward:  0.149 [-100.000, 31.651], mean action: 1.175 [0.000, 3.000],  loss: 18.380661, mae: 31.694230, mean_q: 37.729016, mean_eps: 0.100000\n",
      " 715054/1000000: episode: 2122, duration: 1.412s, episode steps: 277, steps per second: 196, episode reward: 276.560, mean reward:  0.998 [-8.812, 100.000], mean action: 0.668 [0.000, 3.000],  loss: 12.607092, mae: 34.022741, mean_q: 41.911534, mean_eps: 0.100000\n",
      " 715378/1000000: episode: 2123, duration: 1.627s, episode steps: 324, steps per second: 199, episode reward: 251.136, mean reward:  0.775 [-17.361, 100.000], mean action: 0.917 [0.000, 3.000],  loss: 11.009139, mae: 48.424026, mean_q: 63.688176, mean_eps: 0.100000\n",
      " 715651/1000000: episode: 2124, duration: 1.383s, episode steps: 273, steps per second: 197, episode reward: 215.998, mean reward:  0.791 [-14.157, 100.000], mean action: 1.337 [0.000, 3.000],  loss: 5.952814, mae: 55.110219, mean_q: 73.257029, mean_eps: 0.100000\n",
      " 716138/1000000: episode: 2125, duration: 2.613s, episode steps: 487, steps per second: 186, episode reward: 228.321, mean reward:  0.469 [-19.766, 100.000], mean action: 1.337 [0.000, 3.000],  loss: 10.745000, mae: 44.036674, mean_q: 54.571909, mean_eps: 0.100000\n",
      " 716252/1000000: episode: 2126, duration: 0.551s, episode steps: 114, steps per second: 207, episode reward: -3.834, mean reward: -0.034 [-100.000, 19.366], mean action: 1.377 [0.000, 3.000],  loss: 9.996069, mae: 35.580278, mean_q: 43.979762, mean_eps: 0.100000\n",
      " 716344/1000000: episode: 2127, duration: 0.446s, episode steps:  92, steps per second: 206, episode reward: -153.613, mean reward: -1.670 [-100.000, 77.486], mean action: 2.011 [0.000, 3.000],  loss: 22.256005, mae: 39.499502, mean_q: 50.472126, mean_eps: 0.100000\n",
      " 716840/1000000: episode: 2128, duration: 2.609s, episode steps: 496, steps per second: 190, episode reward: 230.654, mean reward:  0.465 [-20.386, 100.000], mean action: 1.149 [0.000, 3.000],  loss: 23.600427, mae: 44.934158, mean_q: 59.213197, mean_eps: 0.100000\n",
      " 717022/1000000: episode: 2129, duration: 0.887s, episode steps: 182, steps per second: 205, episode reward: 21.673, mean reward:  0.119 [-100.000, 22.548], mean action: 1.769 [0.000, 3.000],  loss: 12.129299, mae: 55.756991, mean_q: 73.998928, mean_eps: 0.100000\n",
      " 717433/1000000: episode: 2130, duration: 2.073s, episode steps: 411, steps per second: 198, episode reward: 276.724, mean reward:  0.673 [-19.487, 100.000], mean action: 1.012 [0.000, 3.000],  loss: 10.258336, mae: 62.982529, mean_q: 83.194555, mean_eps: 0.100000\n",
      " 717964/1000000: episode: 2131, duration: 2.803s, episode steps: 531, steps per second: 189, episode reward: 267.112, mean reward:  0.503 [-19.914, 100.000], mean action: 1.339 [0.000, 3.000],  loss: 7.080649, mae: 56.963570, mean_q: 76.609772, mean_eps: 0.100000\n",
      " 718278/1000000: episode: 2132, duration: 1.608s, episode steps: 314, steps per second: 195, episode reward: 284.913, mean reward:  0.907 [-10.208, 100.000], mean action: 1.494 [0.000, 3.000],  loss: 5.155491, mae: 48.822537, mean_q: 65.684571, mean_eps: 0.100000\n",
      " 718568/1000000: episode: 2133, duration: 1.634s, episode steps: 290, steps per second: 177, episode reward: 156.852, mean reward:  0.541 [-16.306, 100.000], mean action: 1.614 [0.000, 3.000],  loss: 5.508423, mae: 48.208670, mean_q: 61.656315, mean_eps: 0.100000\n",
      " 718839/1000000: episode: 2134, duration: 1.351s, episode steps: 271, steps per second: 201, episode reward: 275.838, mean reward:  1.018 [-11.464, 100.000], mean action: 1.037 [0.000, 3.000],  loss: 19.067578, mae: 53.586430, mean_q: 54.035714, mean_eps: 0.100000\n",
      " 718974/1000000: episode: 2135, duration: 0.680s, episode steps: 135, steps per second: 199, episode reward: -4.980, mean reward: -0.037 [-100.000, 13.962], mean action: 1.570 [0.000, 3.000],  loss: 19.894333, mae: 58.755276, mean_q: 60.836139, mean_eps: 0.100000\n",
      " 719974/1000000: episode: 2136, duration: 6.240s, episode steps: 1000, steps per second: 160, episode reward: 150.788, mean reward:  0.151 [-20.822, 14.008], mean action: 1.932 [0.000, 3.000],  loss: 15.092762, mae: 53.370820, mean_q: 63.760982, mean_eps: 0.100000\n",
      " 720154/1000000: episode: 2137, duration: 0.914s, episode steps: 180, steps per second: 197, episode reward: -20.076, mean reward: -0.112 [-100.000, 18.449], mean action: 1.839 [0.000, 3.000],  loss: 2.892258, mae: 44.738297, mean_q: 60.468718, mean_eps: 0.100000\n",
      " 721154/1000000: episode: 2138, duration: 6.212s, episode steps: 1000, steps per second: 161, episode reward: 33.085, mean reward:  0.033 [-18.725, 22.687], mean action: 2.111 [0.000, 3.000],  loss: 8.913481, mae: 35.483443, mean_q: 47.863053, mean_eps: 0.100000\n",
      " 722154/1000000: episode: 2139, duration: 6.143s, episode steps: 1000, steps per second: 163, episode reward: -18.275, mean reward: -0.018 [-19.777, 13.454], mean action: 1.741 [0.000, 3.000],  loss: 2.890941, mae: 25.880304, mean_q: 28.051336, mean_eps: 0.100000\n",
      " 722608/1000000: episode: 2140, duration: 2.443s, episode steps: 454, steps per second: 186, episode reward: 221.703, mean reward:  0.488 [-19.674, 100.000], mean action: 2.090 [0.000, 3.000],  loss: 1.974002, mae: 16.254653, mean_q: 13.997558, mean_eps: 0.100000\n",
      " 723237/1000000: episode: 2141, duration: 3.581s, episode steps: 629, steps per second: 176, episode reward: 202.508, mean reward:  0.322 [-20.022, 100.000], mean action: 1.146 [0.000, 3.000],  loss: 3.710371, mae: 22.611136, mean_q: 30.466436, mean_eps: 0.100000\n",
      " 723561/1000000: episode: 2142, duration: 1.875s, episode steps: 324, steps per second: 173, episode reward: 252.799, mean reward:  0.780 [-11.621, 100.000], mean action: 1.383 [0.000, 3.000],  loss: 6.836905, mae: 33.172833, mean_q: 45.667780, mean_eps: 0.100000\n",
      " 723909/1000000: episode: 2143, duration: 2.108s, episode steps: 348, steps per second: 165, episode reward: 219.927, mean reward:  0.632 [-19.553, 100.000], mean action: 1.925 [0.000, 3.000],  loss: 6.906561, mae: 39.850223, mean_q: 54.526397, mean_eps: 0.100000\n",
      " 724909/1000000: episode: 2144, duration: 5.910s, episode steps: 1000, steps per second: 169, episode reward: 123.121, mean reward:  0.123 [-18.441, 23.290], mean action: 1.259 [0.000, 3.000],  loss: 7.729459, mae: 42.949999, mean_q: 58.599704, mean_eps: 0.100000\n",
      " 725732/1000000: episode: 2145, duration: 5.137s, episode steps: 823, steps per second: 160, episode reward: 212.694, mean reward:  0.258 [-21.885, 100.000], mean action: 1.948 [0.000, 3.000],  loss: 1.509390, mae: 32.186602, mean_q: 43.685192, mean_eps: 0.100000\n",
      " 725893/1000000: episode: 2146, duration: 0.871s, episode steps: 161, steps per second: 185, episode reward: -11.069, mean reward: -0.069 [-100.000, 18.761], mean action: 1.317 [0.000, 3.000],  loss: 4.540310, mae: 30.909730, mean_q: 42.166216, mean_eps: 0.100000\n",
      " 726458/1000000: episode: 2147, duration: 3.060s, episode steps: 565, steps per second: 185, episode reward: 174.703, mean reward:  0.309 [-17.658, 100.000], mean action: 1.910 [0.000, 3.000],  loss: 8.621960, mae: 33.717885, mean_q: 46.289021, mean_eps: 0.100000\n",
      " 726749/1000000: episode: 2148, duration: 1.486s, episode steps: 291, steps per second: 196, episode reward: 221.110, mean reward:  0.760 [-17.048, 100.000], mean action: 1.103 [0.000, 3.000],  loss: 8.936047, mae: 33.701048, mean_q: 44.851926, mean_eps: 0.100000\n",
      " 727113/1000000: episode: 2149, duration: 1.841s, episode steps: 364, steps per second: 198, episode reward: 297.102, mean reward:  0.816 [-17.539, 100.000], mean action: 0.890 [0.000, 3.000],  loss: 7.850032, mae: 35.078320, mean_q: 47.567520, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 727222/1000000: episode: 2150, duration: 0.580s, episode steps: 109, steps per second: 188, episode reward:  8.769, mean reward:  0.080 [-100.000, 15.411], mean action: 1.954 [0.000, 3.000],  loss: 5.868264, mae: 40.355130, mean_q: 54.859703, mean_eps: 0.100000\n",
      " 727592/1000000: episode: 2151, duration: 2.172s, episode steps: 370, steps per second: 170, episode reward: 237.329, mean reward:  0.641 [-23.084, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 11.387382, mae: 51.441640, mean_q: 69.762293, mean_eps: 0.100000\n",
      " 728084/1000000: episode: 2152, duration: 2.724s, episode steps: 492, steps per second: 181, episode reward: 241.991, mean reward:  0.492 [-23.812, 100.000], mean action: 1.171 [0.000, 3.000],  loss: 7.753812, mae: 51.019404, mean_q: 68.953016, mean_eps: 0.100000\n",
      " 728393/1000000: episode: 2153, duration: 1.652s, episode steps: 309, steps per second: 187, episode reward: 265.833, mean reward:  0.860 [-9.416, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 6.758941, mae: 46.896426, mean_q: 63.574371, mean_eps: 0.100000\n",
      " 728673/1000000: episode: 2154, duration: 1.453s, episode steps: 280, steps per second: 193, episode reward: 272.390, mean reward:  0.973 [-9.286, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 3.899608, mae: 48.407557, mean_q: 65.591718, mean_eps: 0.100000\n",
      " 728870/1000000: episode: 2155, duration: 1.131s, episode steps: 197, steps per second: 174, episode reward: 283.474, mean reward:  1.439 [-4.658, 100.000], mean action: 1.437 [0.000, 3.000],  loss: 4.151485, mae: 52.194126, mean_q: 70.481637, mean_eps: 0.100000\n",
      " 729116/1000000: episode: 2156, duration: 1.311s, episode steps: 246, steps per second: 188, episode reward: 276.946, mean reward:  1.126 [-17.382, 100.000], mean action: 1.419 [0.000, 3.000],  loss: 4.504960, mae: 59.089703, mean_q: 79.652783, mean_eps: 0.100000\n",
      " 729320/1000000: episode: 2157, duration: 1.101s, episode steps: 204, steps per second: 185, episode reward: 253.804, mean reward:  1.244 [-8.126, 100.000], mean action: 1.480 [0.000, 3.000],  loss: 3.278217, mae: 61.269176, mean_q: 82.520932, mean_eps: 0.100000\n",
      " 729579/1000000: episode: 2158, duration: 1.300s, episode steps: 259, steps per second: 199, episode reward: 272.061, mean reward:  1.050 [-9.096, 100.000], mean action: 1.386 [0.000, 3.000],  loss: 3.417695, mae: 63.349729, mean_q: 85.289734, mean_eps: 0.100000\n",
      " 729674/1000000: episode: 2159, duration: 0.481s, episode steps:  95, steps per second: 197, episode reward: -35.223, mean reward: -0.371 [-100.000, 20.218], mean action: 1.463 [0.000, 3.000],  loss: 3.213149, mae: 63.703661, mean_q: 85.842706, mean_eps: 0.100000\n",
      " 730077/1000000: episode: 2160, duration: 2.130s, episode steps: 403, steps per second: 189, episode reward: 255.984, mean reward:  0.635 [-17.475, 100.000], mean action: 0.814 [0.000, 3.000],  loss: 4.240368, mae: 60.256426, mean_q: 81.182504, mean_eps: 0.100000\n",
      " 730287/1000000: episode: 2161, duration: 1.113s, episode steps: 210, steps per second: 189, episode reward: 248.654, mean reward:  1.184 [-3.408, 100.000], mean action: 1.562 [0.000, 3.000],  loss: 2.757750, mae: 55.450183, mean_q: 74.258999, mean_eps: 0.100000\n",
      " 730591/1000000: episode: 2162, duration: 1.569s, episode steps: 304, steps per second: 194, episode reward: 271.144, mean reward:  0.892 [-17.463, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 3.996845, mae: 56.252391, mean_q: 75.146074, mean_eps: 0.100000\n",
      " 731535/1000000: episode: 2163, duration: 5.262s, episode steps: 944, steps per second: 179, episode reward: 273.466, mean reward:  0.290 [-17.787, 100.000], mean action: 1.038 [0.000, 3.000],  loss: 1.633796, mae: 57.654483, mean_q: 77.495100, mean_eps: 0.100000\n",
      " 732535/1000000: episode: 2164, duration: 5.844s, episode steps: 1000, steps per second: 171, episode reward: 189.566, mean reward:  0.190 [-19.997, 22.476], mean action: 1.342 [0.000, 3.000],  loss: 1.038359, mae: 55.333141, mean_q: 74.701521, mean_eps: 0.100000\n",
      " 732671/1000000: episode: 2165, duration: 0.660s, episode steps: 136, steps per second: 206, episode reward:  6.654, mean reward:  0.049 [-100.000,  9.210], mean action: 1.647 [0.000, 3.000],  loss: 4.063702, mae: 52.846142, mean_q: 71.287810, mean_eps: 0.100000\n",
      " 732835/1000000: episode: 2166, duration: 0.808s, episode steps: 164, steps per second: 203, episode reward: 26.149, mean reward:  0.159 [-100.000, 11.068], mean action: 1.665 [0.000, 3.000],  loss: 4.268155, mae: 51.377382, mean_q: 69.551329, mean_eps: 0.100000\n",
      " 732935/1000000: episode: 2167, duration: 0.479s, episode steps: 100, steps per second: 209, episode reward: -17.670, mean reward: -0.177 [-100.000, 18.204], mean action: 1.440 [0.000, 3.000],  loss: 24.772044, mae: 52.096347, mean_q: 71.119028, mean_eps: 0.100000\n",
      " 733198/1000000: episode: 2168, duration: 1.407s, episode steps: 263, steps per second: 187, episode reward: 266.632, mean reward:  1.014 [-17.803, 100.000], mean action: 1.570 [0.000, 3.000],  loss: 12.309253, mae: 54.785522, mean_q: 74.385901, mean_eps: 0.100000\n",
      " 733541/1000000: episode: 2169, duration: 1.926s, episode steps: 343, steps per second: 178, episode reward: 238.887, mean reward:  0.696 [-17.210, 100.000], mean action: 2.099 [0.000, 3.000],  loss: 10.217931, mae: 58.622274, mean_q: 78.478342, mean_eps: 0.100000\n",
      " 734178/1000000: episode: 2170, duration: 3.539s, episode steps: 637, steps per second: 180, episode reward: 229.734, mean reward:  0.361 [-19.256, 100.000], mean action: 0.932 [0.000, 3.000],  loss: 6.644435, mae: 53.518681, mean_q: 71.679112, mean_eps: 0.100000\n",
      " 734570/1000000: episode: 2171, duration: 2.150s, episode steps: 392, steps per second: 182, episode reward: 231.102, mean reward:  0.590 [-16.849, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 3.506496, mae: 46.471769, mean_q: 62.688909, mean_eps: 0.100000\n",
      " 734978/1000000: episode: 2172, duration: 2.453s, episode steps: 408, steps per second: 166, episode reward: 221.219, mean reward:  0.542 [-12.153, 100.000], mean action: 1.662 [0.000, 3.000],  loss: 3.174469, mae: 45.669301, mean_q: 61.436504, mean_eps: 0.100000\n",
      " 735296/1000000: episode: 2173, duration: 1.643s, episode steps: 318, steps per second: 194, episode reward: 256.676, mean reward:  0.807 [-3.232, 100.000], mean action: 1.569 [0.000, 3.000],  loss: 5.080522, mae: 49.235381, mean_q: 66.055836, mean_eps: 0.100000\n",
      " 735461/1000000: episode: 2174, duration: 0.853s, episode steps: 165, steps per second: 193, episode reward: -13.949, mean reward: -0.085 [-100.000, 13.446], mean action: 1.333 [0.000, 3.000],  loss: 4.429728, mae: 49.005696, mean_q: 65.903410, mean_eps: 0.100000\n",
      " 736461/1000000: episode: 2175, duration: 6.427s, episode steps: 1000, steps per second: 156, episode reward: 67.348, mean reward:  0.067 [-22.149, 23.080], mean action: 1.123 [0.000, 3.000],  loss: 11.073068, mae: 47.623235, mean_q: 64.379823, mean_eps: 0.100000\n",
      " 736695/1000000: episode: 2176, duration: 1.217s, episode steps: 234, steps per second: 192, episode reward: 234.530, mean reward:  1.002 [-17.910, 100.000], mean action: 2.359 [0.000, 3.000],  loss: 4.597795, mae: 41.552288, mean_q: 56.529491, mean_eps: 0.100000\n",
      " 736847/1000000: episode: 2177, duration: 0.801s, episode steps: 152, steps per second: 190, episode reward: 12.188, mean reward:  0.080 [-100.000, 15.131], mean action: 1.849 [0.000, 3.000],  loss: 4.388250, mae: 45.649908, mean_q: 62.019227, mean_eps: 0.100000\n",
      " 737123/1000000: episode: 2178, duration: 1.382s, episode steps: 276, steps per second: 200, episode reward: 221.546, mean reward:  0.803 [-10.468, 100.000], mean action: 1.138 [0.000, 3.000],  loss: 8.211937, mae: 50.394462, mean_q: 67.509536, mean_eps: 0.100000\n",
      " 738123/1000000: episode: 2179, duration: 5.962s, episode steps: 1000, steps per second: 168, episode reward: 123.949, mean reward:  0.124 [-19.865, 24.110], mean action: 1.720 [0.000, 3.000],  loss: 6.849053, mae: 52.742458, mean_q: 70.960643, mean_eps: 0.100000\n",
      " 738338/1000000: episode: 2180, duration: 1.218s, episode steps: 215, steps per second: 176, episode reward: -30.033, mean reward: -0.140 [-100.000, 17.229], mean action: 1.814 [0.000, 3.000],  loss: 2.829967, mae: 43.266306, mean_q: 58.552285, mean_eps: 0.100000\n",
      " 738435/1000000: episode: 2181, duration: 0.610s, episode steps:  97, steps per second: 159, episode reward:  6.411, mean reward:  0.066 [-100.000, 18.347], mean action: 1.979 [0.000, 3.000],  loss: 6.217055, mae: 47.772788, mean_q: 64.418307, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 738603/1000000: episode: 2182, duration: 0.944s, episode steps: 168, steps per second: 178, episode reward:  1.512, mean reward:  0.009 [-100.000, 11.957], mean action: 1.970 [0.000, 3.000],  loss: 7.967348, mae: 48.679738, mean_q: 64.443628, mean_eps: 0.100000\n",
      " 738811/1000000: episode: 2183, duration: 1.152s, episode steps: 208, steps per second: 181, episode reward: 282.840, mean reward:  1.360 [-9.105, 100.000], mean action: 1.284 [0.000, 3.000],  loss: 13.636581, mae: 53.402728, mean_q: 70.701057, mean_eps: 0.100000\n",
      " 739116/1000000: episode: 2184, duration: 1.756s, episode steps: 305, steps per second: 174, episode reward: 247.021, mean reward:  0.810 [-22.250, 100.000], mean action: 1.436 [0.000, 3.000],  loss: 11.863812, mae: 57.528706, mean_q: 75.953878, mean_eps: 0.100000\n",
      " 739729/1000000: episode: 2185, duration: 3.449s, episode steps: 613, steps per second: 178, episode reward: 250.812, mean reward:  0.409 [-20.608, 100.000], mean action: 0.896 [0.000, 3.000],  loss: 9.642925, mae: 53.964401, mean_q: 72.071469, mean_eps: 0.100000\n",
      " 739969/1000000: episode: 2186, duration: 1.200s, episode steps: 240, steps per second: 200, episode reward: 237.637, mean reward:  0.990 [-10.815, 100.000], mean action: 2.138 [0.000, 3.000],  loss: 5.626149, mae: 48.901849, mean_q: 66.581980, mean_eps: 0.100000\n",
      " 740475/1000000: episode: 2187, duration: 2.714s, episode steps: 506, steps per second: 186, episode reward: 271.345, mean reward:  0.536 [-20.496, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 5.220243, mae: 50.098963, mean_q: 68.299212, mean_eps: 0.100000\n",
      " 741169/1000000: episode: 2188, duration: 3.709s, episode steps: 694, steps per second: 187, episode reward: 227.413, mean reward:  0.328 [-22.035, 100.000], mean action: 1.084 [0.000, 3.000],  loss: 3.945592, mae: 49.112628, mean_q: 66.615225, mean_eps: 0.100000\n",
      " 741629/1000000: episode: 2189, duration: 2.548s, episode steps: 460, steps per second: 181, episode reward: 293.591, mean reward:  0.638 [-18.053, 100.000], mean action: 1.635 [0.000, 3.000],  loss: 3.210906, mae: 46.094763, mean_q: 62.352450, mean_eps: 0.100000\n",
      " 741857/1000000: episode: 2190, duration: 1.192s, episode steps: 228, steps per second: 191, episode reward: 260.516, mean reward:  1.143 [-11.235, 100.000], mean action: 1.623 [0.000, 3.000],  loss: 4.340439, mae: 47.064159, mean_q: 63.644592, mean_eps: 0.100000\n",
      " 742258/1000000: episode: 2191, duration: 2.095s, episode steps: 401, steps per second: 191, episode reward: -55.889, mean reward: -0.139 [-100.000, 16.658], mean action: 0.771 [0.000, 3.000],  loss: 4.127203, mae: 51.985057, mean_q: 67.359372, mean_eps: 0.100000\n",
      " 742522/1000000: episode: 2192, duration: 1.366s, episode steps: 264, steps per second: 193, episode reward: 289.726, mean reward:  1.097 [-7.961, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 7.601473, mae: 50.130759, mean_q: 48.485635, mean_eps: 0.100000\n",
      " 742843/1000000: episode: 2193, duration: 1.633s, episode steps: 321, steps per second: 197, episode reward: 296.388, mean reward:  0.923 [-9.228, 100.000], mean action: 1.153 [0.000, 3.000],  loss: 7.145932, mae: 48.649114, mean_q: 48.036799, mean_eps: 0.100000\n",
      " 743070/1000000: episode: 2194, duration: 1.266s, episode steps: 227, steps per second: 179, episode reward: 278.493, mean reward:  1.227 [-9.682, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 8.296100, mae: 45.857874, mean_q: 45.400988, mean_eps: 0.100000\n",
      " 743347/1000000: episode: 2195, duration: 1.498s, episode steps: 277, steps per second: 185, episode reward: 244.083, mean reward:  0.881 [-9.582, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 9.043701, mae: 50.065188, mean_q: 61.372137, mean_eps: 0.100000\n",
      " 743685/1000000: episode: 2196, duration: 1.785s, episode steps: 338, steps per second: 189, episode reward: 279.785, mean reward:  0.828 [-17.538, 100.000], mean action: 1.056 [0.000, 3.000],  loss: 4.231769, mae: 52.422345, mean_q: 71.193632, mean_eps: 0.100000\n",
      " 743871/1000000: episode: 2197, duration: 0.918s, episode steps: 186, steps per second: 203, episode reward: 256.053, mean reward:  1.377 [-8.619, 100.000], mean action: 1.280 [0.000, 3.000],  loss: 7.215757, mae: 54.668723, mean_q: 74.080038, mean_eps: 0.100000\n",
      " 744232/1000000: episode: 2198, duration: 1.913s, episode steps: 361, steps per second: 189, episode reward: 276.156, mean reward:  0.765 [-17.899, 100.000], mean action: 0.898 [0.000, 3.000],  loss: 3.833549, mae: 56.149928, mean_q: 76.060308, mean_eps: 0.100000\n",
      " 744502/1000000: episode: 2199, duration: 1.505s, episode steps: 270, steps per second: 179, episode reward: 263.471, mean reward:  0.976 [-11.564, 100.000], mean action: 1.326 [0.000, 3.000],  loss: 3.291679, mae: 55.797772, mean_q: 75.673962, mean_eps: 0.100000\n",
      " 744834/1000000: episode: 2200, duration: 1.716s, episode steps: 332, steps per second: 193, episode reward: 286.201, mean reward:  0.862 [-19.908, 100.000], mean action: 0.901 [0.000, 3.000],  loss: 4.269467, mae: 57.143650, mean_q: 77.376330, mean_eps: 0.100000\n",
      " 745021/1000000: episode: 2201, duration: 0.920s, episode steps: 187, steps per second: 203, episode reward: 250.031, mean reward:  1.337 [-14.335, 100.000], mean action: 1.230 [0.000, 3.000],  loss: 3.280935, mae: 57.266027, mean_q: 77.520387, mean_eps: 0.100000\n",
      " 745283/1000000: episode: 2202, duration: 1.304s, episode steps: 262, steps per second: 201, episode reward: 275.972, mean reward:  1.053 [-11.306, 100.000], mean action: 1.092 [0.000, 3.000],  loss: 4.397163, mae: 60.256854, mean_q: 81.416205, mean_eps: 0.100000\n",
      " 745477/1000000: episode: 2203, duration: 1.155s, episode steps: 194, steps per second: 168, episode reward: 288.572, mean reward:  1.487 [-8.500, 100.000], mean action: 1.108 [0.000, 3.000],  loss: 4.450238, mae: 61.146169, mean_q: 82.711223, mean_eps: 0.100000\n",
      " 745886/1000000: episode: 2204, duration: 2.378s, episode steps: 409, steps per second: 172, episode reward: 292.468, mean reward:  0.715 [-20.471, 100.000], mean action: 1.599 [0.000, 3.000],  loss: 4.220999, mae: 63.750309, mean_q: 86.254097, mean_eps: 0.100000\n",
      " 746265/1000000: episode: 2205, duration: 2.170s, episode steps: 379, steps per second: 175, episode reward: 304.036, mean reward:  0.802 [-17.935, 100.000], mean action: 0.826 [0.000, 3.000],  loss: 3.815762, mae: 62.978198, mean_q: 85.609341, mean_eps: 0.100000\n",
      " 746700/1000000: episode: 2206, duration: 2.444s, episode steps: 435, steps per second: 178, episode reward: 289.306, mean reward:  0.665 [-18.321, 100.000], mean action: 1.352 [0.000, 3.000],  loss: 3.389736, mae: 57.104604, mean_q: 77.905124, mean_eps: 0.100000\n",
      " 746908/1000000: episode: 2207, duration: 1.047s, episode steps: 208, steps per second: 199, episode reward: 251.225, mean reward:  1.208 [-8.978, 100.000], mean action: 1.014 [0.000, 3.000],  loss: 4.947213, mae: 53.325728, mean_q: 72.734975, mean_eps: 0.100000\n",
      " 747154/1000000: episode: 2208, duration: 1.186s, episode steps: 246, steps per second: 207, episode reward: 257.681, mean reward:  1.047 [-17.684, 100.000], mean action: 0.996 [0.000, 3.000],  loss: 5.088350, mae: 54.021779, mean_q: 73.561324, mean_eps: 0.100000\n",
      " 747439/1000000: episode: 2209, duration: 1.392s, episode steps: 285, steps per second: 205, episode reward: 288.417, mean reward:  1.012 [-18.100, 100.000], mean action: 1.193 [0.000, 3.000],  loss: 4.289686, mae: 57.960482, mean_q: 78.802455, mean_eps: 0.100000\n",
      " 748090/1000000: episode: 2210, duration: 3.258s, episode steps: 651, steps per second: 200, episode reward: 250.781, mean reward:  0.385 [-20.626, 100.000], mean action: 0.995 [0.000, 3.000],  loss: 3.457395, mae: 57.941137, mean_q: 78.498626, mean_eps: 0.100000\n",
      " 748337/1000000: episode: 2211, duration: 1.220s, episode steps: 247, steps per second: 202, episode reward: 269.833, mean reward:  1.092 [-11.642, 100.000], mean action: 1.130 [0.000, 3.000],  loss: 3.174892, mae: 54.013304, mean_q: 73.186132, mean_eps: 0.100000\n",
      " 748559/1000000: episode: 2212, duration: 1.077s, episode steps: 222, steps per second: 206, episode reward: 316.765, mean reward:  1.427 [-9.201, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 2.125641, mae: 56.437449, mean_q: 76.437686, mean_eps: 0.100000\n",
      " 748726/1000000: episode: 2213, duration: 0.798s, episode steps: 167, steps per second: 209, episode reward: 282.154, mean reward:  1.690 [-18.559, 100.000], mean action: 1.156 [0.000, 3.000],  loss: 3.855456, mae: 57.264334, mean_q: 77.652492, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 748982/1000000: episode: 2214, duration: 1.245s, episode steps: 256, steps per second: 206, episode reward: 273.802, mean reward:  1.070 [-17.279, 100.000], mean action: 0.949 [0.000, 3.000],  loss: 5.298089, mae: 61.464458, mean_q: 83.178199, mean_eps: 0.100000\n",
      " 749293/1000000: episode: 2215, duration: 1.652s, episode steps: 311, steps per second: 188, episode reward: 272.243, mean reward:  0.875 [-21.005, 100.000], mean action: 1.357 [0.000, 3.000],  loss: 3.432223, mae: 64.006515, mean_q: 86.729179, mean_eps: 0.100000\n",
      " 749419/1000000: episode: 2216, duration: 0.616s, episode steps: 126, steps per second: 205, episode reward: 22.209, mean reward:  0.176 [-100.000, 16.502], mean action: 1.683 [0.000, 3.000],  loss: 4.457102, mae: 62.662704, mean_q: 84.899993, mean_eps: 0.100000\n",
      " 749564/1000000: episode: 2217, duration: 0.707s, episode steps: 145, steps per second: 205, episode reward: 65.745, mean reward:  0.453 [-100.000, 17.428], mean action: 1.724 [0.000, 3.000],  loss: 15.641681, mae: 64.152786, mean_q: 86.831978, mean_eps: 0.100000\n",
      " 749660/1000000: episode: 2218, duration: 0.469s, episode steps:  96, steps per second: 205, episode reward: 55.850, mean reward:  0.582 [-100.000, 14.320], mean action: 1.865 [0.000, 3.000],  loss: 22.017844, mae: 66.468779, mean_q: 90.169157, mean_eps: 0.100000\n",
      " 749768/1000000: episode: 2219, duration: 0.575s, episode steps: 108, steps per second: 188, episode reward: 20.015, mean reward:  0.185 [-100.000, 16.405], mean action: 1.444 [0.000, 3.000],  loss: 28.897180, mae: 69.835219, mean_q: 94.661124, mean_eps: 0.100000\n",
      " 749858/1000000: episode: 2220, duration: 0.501s, episode steps:  90, steps per second: 179, episode reward: -8.318, mean reward: -0.092 [-100.000, 18.622], mean action: 1.556 [0.000, 3.000],  loss: 57.322446, mae: 70.366540, mean_q: 95.507494, mean_eps: 0.100000\n",
      " 749973/1000000: episode: 2221, duration: 0.612s, episode steps: 115, steps per second: 188, episode reward: 50.051, mean reward:  0.435 [-100.000, 23.075], mean action: 1.452 [0.000, 3.000],  loss: 52.298057, mae: 73.286460, mean_q: 99.342185, mean_eps: 0.100000\n",
      " 750132/1000000: episode: 2222, duration: 0.802s, episode steps: 159, steps per second: 198, episode reward: 72.447, mean reward:  0.456 [-100.000, 16.193], mean action: 1.748 [0.000, 3.000],  loss: 59.309359, mae: 75.330866, mean_q: 102.090394, mean_eps: 0.100000\n",
      " 750232/1000000: episode: 2223, duration: 0.537s, episode steps: 100, steps per second: 186, episode reward: 40.649, mean reward:  0.406 [-100.000, 16.377], mean action: 1.510 [0.000, 3.000],  loss: 45.821703, mae: 78.949757, mean_q: 106.763647, mean_eps: 0.100000\n",
      " 750322/1000000: episode: 2224, duration: 0.489s, episode steps:  90, steps per second: 184, episode reward: 42.023, mean reward:  0.467 [-100.000, 18.805], mean action: 1.656 [0.000, 3.000],  loss: 64.197409, mae: 82.688481, mean_q: 111.380791, mean_eps: 0.100000\n",
      " 750470/1000000: episode: 2225, duration: 0.712s, episode steps: 148, steps per second: 208, episode reward: 44.896, mean reward:  0.303 [-100.000, 19.094], mean action: 1.128 [0.000, 3.000],  loss: 51.267594, mae: 83.203184, mean_q: 111.617520, mean_eps: 0.100000\n",
      " 750555/1000000: episode: 2226, duration: 0.414s, episode steps:  85, steps per second: 205, episode reward: 33.653, mean reward:  0.396 [-100.000, 18.985], mean action: 1.565 [0.000, 3.000],  loss: 49.961912, mae: 82.939180, mean_q: 109.989943, mean_eps: 0.100000\n",
      " 750640/1000000: episode: 2227, duration: 0.496s, episode steps:  85, steps per second: 171, episode reward: 20.756, mean reward:  0.244 [-100.000, 20.771], mean action: 1.424 [0.000, 3.000],  loss: 49.721009, mae: 82.701060, mean_q: 109.656796, mean_eps: 0.100000\n",
      " 750726/1000000: episode: 2228, duration: 0.458s, episode steps:  86, steps per second: 188, episode reward: 42.582, mean reward:  0.495 [-100.000, 19.961], mean action: 1.779 [0.000, 3.000],  loss: 27.510140, mae: 84.219769, mean_q: 111.208985, mean_eps: 0.100000\n",
      " 750877/1000000: episode: 2229, duration: 0.793s, episode steps: 151, steps per second: 190, episode reward: -19.044, mean reward: -0.126 [-100.000, 16.883], mean action: 1.675 [0.000, 3.000],  loss: 31.721225, mae: 82.723731, mean_q: 107.679173, mean_eps: 0.100000\n",
      " 750991/1000000: episode: 2230, duration: 0.574s, episode steps: 114, steps per second: 199, episode reward: 30.458, mean reward:  0.267 [-100.000, 14.182], mean action: 1.351 [0.000, 3.000],  loss: 19.905798, mae: 82.602412, mean_q: 106.635005, mean_eps: 0.100000\n",
      " 751102/1000000: episode: 2231, duration: 0.570s, episode steps: 111, steps per second: 195, episode reward: -20.898, mean reward: -0.188 [-100.000,  9.976], mean action: 1.243 [0.000, 3.000],  loss: 15.867109, mae: 82.167445, mean_q: 104.938268, mean_eps: 0.100000\n",
      " 751184/1000000: episode: 2232, duration: 0.413s, episode steps:  82, steps per second: 198, episode reward: 43.603, mean reward:  0.532 [-100.000, 22.385], mean action: 1.732 [0.000, 3.000],  loss: 13.470166, mae: 82.101500, mean_q: 103.966976, mean_eps: 0.100000\n",
      " 751267/1000000: episode: 2233, duration: 0.401s, episode steps:  83, steps per second: 207, episode reward: -0.064, mean reward: -0.001 [-100.000, 10.807], mean action: 1.687 [0.000, 3.000],  loss: 12.611205, mae: 82.374741, mean_q: 105.012934, mean_eps: 0.100000\n",
      " 751351/1000000: episode: 2234, duration: 0.399s, episode steps:  84, steps per second: 210, episode reward:  5.731, mean reward:  0.068 [-100.000, 12.299], mean action: 1.429 [0.000, 3.000],  loss: 8.942212, mae: 81.886908, mean_q: 103.883196, mean_eps: 0.100000\n",
      " 751454/1000000: episode: 2235, duration: 0.570s, episode steps: 103, steps per second: 181, episode reward: -44.823, mean reward: -0.435 [-100.000,  9.116], mean action: 1.136 [0.000, 3.000],  loss: 7.558232, mae: 82.050239, mean_q: 103.366310, mean_eps: 0.100000\n",
      " 751539/1000000: episode: 2236, duration: 0.412s, episode steps:  85, steps per second: 206, episode reward:  6.415, mean reward:  0.075 [-100.000, 20.023], mean action: 1.482 [0.000, 3.000],  loss: 7.106297, mae: 80.546848, mean_q: 101.952152, mean_eps: 0.100000\n",
      " 751638/1000000: episode: 2237, duration: 0.483s, episode steps:  99, steps per second: 205, episode reward: 45.557, mean reward:  0.460 [-100.000, 16.644], mean action: 1.424 [0.000, 3.000],  loss: 6.239705, mae: 79.708219, mean_q: 99.405431, mean_eps: 0.100000\n",
      " 751738/1000000: episode: 2238, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward:  0.739, mean reward:  0.007 [-100.000, 13.992], mean action: 1.530 [0.000, 3.000],  loss: 7.836214, mae: 78.402658, mean_q: 98.364021, mean_eps: 0.100000\n",
      " 751874/1000000: episode: 2239, duration: 0.728s, episode steps: 136, steps per second: 187, episode reward: 42.475, mean reward:  0.312 [-100.000, 16.576], mean action: 1.515 [0.000, 3.000],  loss: 7.323783, mae: 78.874719, mean_q: 98.591331, mean_eps: 0.100000\n",
      " 751983/1000000: episode: 2240, duration: 0.603s, episode steps: 109, steps per second: 181, episode reward: -28.917, mean reward: -0.265 [-100.000, 10.152], mean action: 1.468 [0.000, 3.000],  loss: 5.933650, mae: 78.049845, mean_q: 96.394890, mean_eps: 0.100000\n",
      " 752078/1000000: episode: 2241, duration: 0.476s, episode steps:  95, steps per second: 200, episode reward:  9.774, mean reward:  0.103 [-100.000, 18.549], mean action: 1.400 [0.000, 3.000],  loss: 5.351915, mae: 76.454867, mean_q: 94.518416, mean_eps: 0.100000\n",
      " 752190/1000000: episode: 2242, duration: 0.547s, episode steps: 112, steps per second: 205, episode reward: -8.434, mean reward: -0.075 [-100.000, 16.843], mean action: 1.580 [0.000, 3.000],  loss: 8.078515, mae: 75.855320, mean_q: 91.131319, mean_eps: 0.100000\n",
      " 752313/1000000: episode: 2243, duration: 0.586s, episode steps: 123, steps per second: 210, episode reward: -13.479, mean reward: -0.110 [-100.000, 18.959], mean action: 1.228 [0.000, 3.000],  loss: 4.850532, mae: 73.903489, mean_q: 89.834883, mean_eps: 0.100000\n",
      " 752428/1000000: episode: 2244, duration: 0.561s, episode steps: 115, steps per second: 205, episode reward: -8.225, mean reward: -0.072 [-100.000, 10.436], mean action: 1.496 [0.000, 3.000],  loss: 5.545893, mae: 73.116546, mean_q: 88.327516, mean_eps: 0.100000\n",
      " 752526/1000000: episode: 2245, duration: 0.472s, episode steps:  98, steps per second: 207, episode reward: 22.942, mean reward:  0.234 [-100.000, 17.590], mean action: 1.776 [0.000, 3.000],  loss: 4.071062, mae: 73.968659, mean_q: 89.290321, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 752629/1000000: episode: 2246, duration: 0.492s, episode steps: 103, steps per second: 209, episode reward: 41.788, mean reward:  0.406 [-100.000, 17.303], mean action: 1.913 [0.000, 3.000],  loss: 4.821419, mae: 72.768938, mean_q: 85.308418, mean_eps: 0.100000\n",
      " 752714/1000000: episode: 2247, duration: 0.419s, episode steps:  85, steps per second: 203, episode reward: 42.726, mean reward:  0.503 [-100.000, 16.316], mean action: 1.918 [0.000, 3.000],  loss: 4.427139, mae: 71.624902, mean_q: 83.408575, mean_eps: 0.100000\n",
      " 752827/1000000: episode: 2248, duration: 0.568s, episode steps: 113, steps per second: 199, episode reward: 18.991, mean reward:  0.168 [-100.000, 17.586], mean action: 1.389 [0.000, 3.000],  loss: 4.448051, mae: 70.963670, mean_q: 81.732244, mean_eps: 0.100000\n",
      " 752926/1000000: episode: 2249, duration: 0.469s, episode steps:  99, steps per second: 211, episode reward: 15.731, mean reward:  0.159 [-100.000, 10.991], mean action: 1.606 [0.000, 3.000],  loss: 5.477353, mae: 71.635035, mean_q: 81.015610, mean_eps: 0.100000\n",
      " 753020/1000000: episode: 2250, duration: 0.467s, episode steps:  94, steps per second: 201, episode reward: 80.772, mean reward:  0.859 [-100.000, 14.917], mean action: 2.021 [0.000, 3.000],  loss: 4.865174, mae: 70.606799, mean_q: 79.630560, mean_eps: 0.100000\n",
      " 753134/1000000: episode: 2251, duration: 0.595s, episode steps: 114, steps per second: 192, episode reward: 13.583, mean reward:  0.119 [-100.000, 16.583], mean action: 1.263 [0.000, 3.000],  loss: 5.716775, mae: 69.330197, mean_q: 79.319470, mean_eps: 0.100000\n",
      " 753236/1000000: episode: 2252, duration: 0.547s, episode steps: 102, steps per second: 187, episode reward: 28.213, mean reward:  0.277 [-100.000, 12.565], mean action: 1.382 [0.000, 3.000],  loss: 6.191935, mae: 68.255604, mean_q: 75.737677, mean_eps: 0.100000\n",
      " 754040/1000000: episode: 2253, duration: 4.779s, episode steps: 804, steps per second: 168, episode reward: 238.965, mean reward:  0.297 [-21.905, 100.000], mean action: 1.545 [0.000, 3.000],  loss: 4.758519, mae: 57.154071, mean_q: 37.400231, mean_eps: 0.100000\n",
      " 754305/1000000: episode: 2254, duration: 1.374s, episode steps: 265, steps per second: 193, episode reward: 256.535, mean reward:  0.968 [-10.927, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 14.829729, mae: 40.725020, mean_q: -8.988714, mean_eps: 0.100000\n",
      " 754454/1000000: episode: 2255, duration: 0.726s, episode steps: 149, steps per second: 205, episode reward: 276.985, mean reward:  1.859 [-2.597, 100.000], mean action: 1.161 [0.000, 3.000],  loss: 18.106797, mae: 36.816947, mean_q: -12.869772, mean_eps: 0.100000\n",
      " 754680/1000000: episode: 2256, duration: 1.252s, episode steps: 226, steps per second: 180, episode reward: 229.676, mean reward:  1.016 [-10.744, 100.000], mean action: 1.504 [0.000, 3.000],  loss: 20.160532, mae: 37.949683, mean_q: 3.273041, mean_eps: 0.100000\n",
      " 755031/1000000: episode: 2257, duration: 2.017s, episode steps: 351, steps per second: 174, episode reward: 244.043, mean reward:  0.695 [-17.341, 100.000], mean action: 1.077 [0.000, 3.000],  loss: 45.245686, mae: 37.615695, mean_q: 22.687744, mean_eps: 0.100000\n",
      " 755286/1000000: episode: 2258, duration: 1.335s, episode steps: 255, steps per second: 191, episode reward: 303.714, mean reward:  1.191 [-13.393, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 28.803422, mae: 34.415154, mean_q: 28.709502, mean_eps: 0.100000\n",
      " 755592/1000000: episode: 2259, duration: 1.603s, episode steps: 306, steps per second: 191, episode reward: 285.784, mean reward:  0.934 [-10.183, 100.000], mean action: 1.265 [0.000, 3.000],  loss: 22.596961, mae: 31.438756, mean_q: 30.068860, mean_eps: 0.100000\n",
      " 756170/1000000: episode: 2260, duration: 3.551s, episode steps: 578, steps per second: 163, episode reward: 243.035, mean reward:  0.420 [-17.543, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 15.486524, mae: 24.128896, mean_q: 24.413670, mean_eps: 0.100000\n",
      " 756486/1000000: episode: 2261, duration: 1.935s, episode steps: 316, steps per second: 163, episode reward: 294.646, mean reward:  0.932 [-21.707, 100.000], mean action: 1.399 [0.000, 3.000],  loss: 18.059215, mae: 18.496649, mean_q: 21.002623, mean_eps: 0.100000\n",
      " 756872/1000000: episode: 2262, duration: 2.365s, episode steps: 386, steps per second: 163, episode reward: 270.935, mean reward:  0.702 [-9.144, 100.000], mean action: 1.329 [0.000, 3.000],  loss: 16.081620, mae: 16.871987, mean_q: 21.174038, mean_eps: 0.100000\n",
      " 757187/1000000: episode: 2263, duration: 1.675s, episode steps: 315, steps per second: 188, episode reward: 271.287, mean reward:  0.861 [-18.311, 100.000], mean action: 1.127 [0.000, 3.000],  loss: 16.029046, mae: 21.901883, mean_q: 30.340142, mean_eps: 0.100000\n",
      " 757435/1000000: episode: 2264, duration: 1.411s, episode steps: 248, steps per second: 176, episode reward: 244.365, mean reward:  0.985 [-9.330, 100.000], mean action: 1.226 [0.000, 3.000],  loss: 17.901446, mae: 22.936150, mean_q: 30.704637, mean_eps: 0.100000\n",
      " 757689/1000000: episode: 2265, duration: 1.366s, episode steps: 254, steps per second: 186, episode reward: 277.067, mean reward:  1.091 [-8.590, 100.000], mean action: 1.449 [0.000, 3.000],  loss: 16.490886, mae: 24.520984, mean_q: 31.852416, mean_eps: 0.100000\n",
      " 758282/1000000: episode: 2266, duration: 3.412s, episode steps: 593, steps per second: 174, episode reward: 262.987, mean reward:  0.443 [-18.716, 100.000], mean action: 0.941 [0.000, 3.000],  loss: 12.926147, mae: 25.403151, mean_q: 33.792188, mean_eps: 0.100000\n",
      " 758660/1000000: episode: 2267, duration: 2.008s, episode steps: 378, steps per second: 188, episode reward: 274.520, mean reward:  0.726 [-17.286, 100.000], mean action: 1.053 [0.000, 3.000],  loss: 10.708252, mae: 24.178775, mean_q: 33.180288, mean_eps: 0.100000\n",
      " 758925/1000000: episode: 2268, duration: 1.485s, episode steps: 265, steps per second: 178, episode reward: 232.702, mean reward:  0.878 [-10.289, 100.000], mean action: 1.381 [0.000, 3.000],  loss: 9.768655, mae: 23.887016, mean_q: 32.862583, mean_eps: 0.100000\n",
      " 759170/1000000: episode: 2269, duration: 1.287s, episode steps: 245, steps per second: 190, episode reward: 247.310, mean reward:  1.009 [-11.866, 100.000], mean action: 1.363 [0.000, 3.000],  loss: 14.725601, mae: 28.680880, mean_q: 39.293641, mean_eps: 0.100000\n",
      " 759474/1000000: episode: 2270, duration: 1.588s, episode steps: 304, steps per second: 191, episode reward: 271.564, mean reward:  0.893 [-6.618, 100.000], mean action: 1.401 [0.000, 3.000],  loss: 11.033038, mae: 31.246271, mean_q: 42.746332, mean_eps: 0.100000\n",
      " 759716/1000000: episode: 2271, duration: 1.395s, episode steps: 242, steps per second: 173, episode reward: 243.372, mean reward:  1.006 [-17.360, 100.000], mean action: 1.025 [0.000, 3.000],  loss: 15.006424, mae: 32.254322, mean_q: 44.049897, mean_eps: 0.100000\n",
      " 760083/1000000: episode: 2272, duration: 2.097s, episode steps: 367, steps per second: 175, episode reward: 288.287, mean reward:  0.786 [-17.376, 100.000], mean action: 0.989 [0.000, 3.000],  loss: 10.878622, mae: 32.175782, mean_q: 44.071836, mean_eps: 0.100000\n",
      " 760335/1000000: episode: 2273, duration: 1.343s, episode steps: 252, steps per second: 188, episode reward: 253.902, mean reward:  1.008 [-17.175, 100.000], mean action: 1.123 [0.000, 3.000],  loss: 7.455950, mae: 32.447273, mean_q: 44.647707, mean_eps: 0.100000\n",
      " 760620/1000000: episode: 2274, duration: 1.689s, episode steps: 285, steps per second: 169, episode reward: 260.208, mean reward:  0.913 [-18.267, 100.000], mean action: 0.944 [0.000, 3.000],  loss: 11.823594, mae: 35.985994, mean_q: 49.590008, mean_eps: 0.100000\n",
      " 761059/1000000: episode: 2275, duration: 2.403s, episode steps: 439, steps per second: 183, episode reward: 263.793, mean reward:  0.601 [-20.587, 100.000], mean action: 0.859 [0.000, 3.000],  loss: 9.344308, mae: 35.638293, mean_q: 49.227709, mean_eps: 0.100000\n",
      " 761355/1000000: episode: 2276, duration: 1.565s, episode steps: 296, steps per second: 189, episode reward: 279.580, mean reward:  0.945 [-10.575, 100.000], mean action: 1.503 [0.000, 3.000],  loss: 8.365981, mae: 36.218402, mean_q: 49.847839, mean_eps: 0.100000\n",
      " 761624/1000000: episode: 2277, duration: 1.387s, episode steps: 269, steps per second: 194, episode reward: 246.592, mean reward:  0.917 [-8.944, 100.000], mean action: 1.145 [0.000, 3.000],  loss: 7.510251, mae: 37.586261, mean_q: 51.492304, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 762116/1000000: episode: 2278, duration: 2.724s, episode steps: 492, steps per second: 181, episode reward: 238.028, mean reward:  0.484 [-19.312, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 5.851912, mae: 37.738619, mean_q: 51.622590, mean_eps: 0.100000\n",
      " 762363/1000000: episode: 2279, duration: 1.240s, episode steps: 247, steps per second: 199, episode reward: 248.926, mean reward:  1.008 [-17.820, 100.000], mean action: 1.413 [0.000, 3.000],  loss: 5.814324, mae: 36.172987, mean_q: 49.600969, mean_eps: 0.100000\n",
      " 762585/1000000: episode: 2280, duration: 1.302s, episode steps: 222, steps per second: 171, episode reward: 247.114, mean reward:  1.113 [-14.968, 100.000], mean action: 2.243 [0.000, 3.000],  loss: 6.851011, mae: 35.697089, mean_q: 49.168369, mean_eps: 0.100000\n",
      " 763585/1000000: episode: 2281, duration: 5.842s, episode steps: 1000, steps per second: 171, episode reward: 138.108, mean reward:  0.138 [-18.810, 26.486], mean action: 1.498 [0.000, 3.000],  loss: 5.378817, mae: 37.493204, mean_q: 51.522838, mean_eps: 0.100000\n",
      " 763917/1000000: episode: 2282, duration: 1.703s, episode steps: 332, steps per second: 195, episode reward: 289.420, mean reward:  0.872 [-17.533, 100.000], mean action: 2.437 [0.000, 3.000],  loss: 1.632178, mae: 33.667570, mean_q: 46.403889, mean_eps: 0.100000\n",
      " 764278/1000000: episode: 2283, duration: 1.850s, episode steps: 361, steps per second: 195, episode reward: 291.941, mean reward:  0.809 [-17.835, 100.000], mean action: 1.463 [0.000, 3.000],  loss: 4.158834, mae: 36.118291, mean_q: 49.717527, mean_eps: 0.100000\n",
      " 764412/1000000: episode: 2284, duration: 0.658s, episode steps: 134, steps per second: 204, episode reward:  5.669, mean reward:  0.042 [-100.000, 12.057], mean action: 1.910 [0.000, 3.000],  loss: 4.679684, mae: 38.413328, mean_q: 52.762037, mean_eps: 0.100000\n",
      " 764676/1000000: episode: 2285, duration: 1.334s, episode steps: 264, steps per second: 198, episode reward: 265.949, mean reward:  1.007 [-17.515, 100.000], mean action: 0.947 [0.000, 3.000],  loss: 14.283926, mae: 41.910017, mean_q: 57.331407, mean_eps: 0.100000\n",
      " 765038/1000000: episode: 2286, duration: 2.032s, episode steps: 362, steps per second: 178, episode reward: 281.621, mean reward:  0.778 [-19.237, 100.000], mean action: 0.923 [0.000, 3.000],  loss: 8.301000, mae: 43.087293, mean_q: 58.661737, mean_eps: 0.100000\n",
      " 765244/1000000: episode: 2287, duration: 1.200s, episode steps: 206, steps per second: 172, episode reward: -140.698, mean reward: -0.683 [-100.000, 33.561], mean action: 2.102 [0.000, 3.000],  loss: 7.063936, mae: 43.787756, mean_q: 59.668754, mean_eps: 0.100000\n",
      " 765467/1000000: episode: 2288, duration: 1.199s, episode steps: 223, steps per second: 186, episode reward: 251.488, mean reward:  1.128 [-10.748, 100.000], mean action: 1.152 [0.000, 3.000],  loss: 17.084358, mae: 46.793040, mean_q: 63.985057, mean_eps: 0.100000\n",
      " 765724/1000000: episode: 2289, duration: 1.292s, episode steps: 257, steps per second: 199, episode reward: 274.585, mean reward:  1.068 [-3.292, 100.000], mean action: 1.584 [0.000, 3.000],  loss: 7.007534, mae: 48.195485, mean_q: 65.923922, mean_eps: 0.100000\n",
      " 766046/1000000: episode: 2290, duration: 1.893s, episode steps: 322, steps per second: 170, episode reward: 276.893, mean reward:  0.860 [-9.531, 100.000], mean action: 1.475 [0.000, 3.000],  loss: 5.902043, mae: 50.768229, mean_q: 69.111615, mean_eps: 0.100000\n",
      " 766304/1000000: episode: 2291, duration: 1.397s, episode steps: 258, steps per second: 185, episode reward: 258.048, mean reward:  1.000 [-11.539, 100.000], mean action: 1.163 [0.000, 3.000],  loss: 4.658488, mae: 52.660341, mean_q: 71.411710, mean_eps: 0.100000\n",
      " 766634/1000000: episode: 2292, duration: 1.785s, episode steps: 330, steps per second: 185, episode reward: 275.876, mean reward:  0.836 [-18.043, 100.000], mean action: 1.215 [0.000, 3.000],  loss: 5.041991, mae: 51.986353, mean_q: 70.400865, mean_eps: 0.100000\n",
      " 766929/1000000: episode: 2293, duration: 1.644s, episode steps: 295, steps per second: 179, episode reward: 271.588, mean reward:  0.921 [-7.796, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 4.901901, mae: 52.068225, mean_q: 70.368435, mean_eps: 0.100000\n",
      " 767221/1000000: episode: 2294, duration: 1.691s, episode steps: 292, steps per second: 173, episode reward: 276.435, mean reward:  0.947 [-2.820, 100.000], mean action: 1.596 [0.000, 3.000],  loss: 5.076237, mae: 50.117639, mean_q: 67.844609, mean_eps: 0.100000\n",
      " 767568/1000000: episode: 2295, duration: 1.844s, episode steps: 347, steps per second: 188, episode reward: 266.984, mean reward:  0.769 [-18.848, 100.000], mean action: 0.833 [0.000, 3.000],  loss: 3.605020, mae: 49.695937, mean_q: 67.267520, mean_eps: 0.100000\n",
      " 767767/1000000: episode: 2296, duration: 1.004s, episode steps: 199, steps per second: 198, episode reward: 256.345, mean reward:  1.288 [-18.287, 100.000], mean action: 1.432 [0.000, 3.000],  loss: 3.104302, mae: 50.414212, mean_q: 68.289352, mean_eps: 0.100000\n",
      " 768075/1000000: episode: 2297, duration: 1.579s, episode steps: 308, steps per second: 195, episode reward: 285.337, mean reward:  0.926 [-6.821, 100.000], mean action: 1.458 [0.000, 3.000],  loss: 3.523703, mae: 49.549664, mean_q: 67.074804, mean_eps: 0.100000\n",
      " 768387/1000000: episode: 2298, duration: 1.618s, episode steps: 312, steps per second: 193, episode reward: 278.504, mean reward:  0.893 [-17.049, 100.000], mean action: 1.067 [0.000, 3.000],  loss: 3.607448, mae: 51.648461, mean_q: 69.862368, mean_eps: 0.100000\n",
      " 768633/1000000: episode: 2299, duration: 1.444s, episode steps: 246, steps per second: 170, episode reward: 240.616, mean reward:  0.978 [-19.165, 100.000], mean action: 1.654 [0.000, 3.000],  loss: 2.842905, mae: 52.961870, mean_q: 71.585843, mean_eps: 0.100000\n",
      " 768932/1000000: episode: 2300, duration: 1.595s, episode steps: 299, steps per second: 187, episode reward: 247.870, mean reward:  0.829 [-17.387, 100.000], mean action: 0.796 [0.000, 3.000],  loss: 3.883705, mae: 53.825070, mean_q: 72.847658, mean_eps: 0.100000\n",
      " 769206/1000000: episode: 2301, duration: 1.435s, episode steps: 274, steps per second: 191, episode reward: 266.879, mean reward:  0.974 [-9.591, 100.000], mean action: 0.891 [0.000, 3.000],  loss: 4.042356, mae: 53.471665, mean_q: 72.494881, mean_eps: 0.100000\n",
      " 769768/1000000: episode: 2302, duration: 3.220s, episode steps: 562, steps per second: 175, episode reward: 235.974, mean reward:  0.420 [-18.344, 100.000], mean action: 0.947 [0.000, 3.000],  loss: 3.902580, mae: 51.544062, mean_q: 70.013639, mean_eps: 0.100000\n",
      " 770070/1000000: episode: 2303, duration: 1.540s, episode steps: 302, steps per second: 196, episode reward: 311.976, mean reward:  1.033 [-11.933, 100.000], mean action: 1.374 [0.000, 3.000],  loss: 2.750195, mae: 48.670690, mean_q: 66.200894, mean_eps: 0.100000\n",
      " 771070/1000000: episode: 2304, duration: 5.835s, episode steps: 1000, steps per second: 171, episode reward: 135.381, mean reward:  0.135 [-17.997, 21.905], mean action: 2.351 [0.000, 3.000],  loss: 2.521441, mae: 44.381387, mean_q: 60.491475, mean_eps: 0.100000\n",
      " 771409/1000000: episode: 2305, duration: 1.728s, episode steps: 339, steps per second: 196, episode reward: 301.624, mean reward:  0.890 [-10.436, 100.000], mean action: 0.979 [0.000, 3.000],  loss: 1.322027, mae: 38.128333, mean_q: 52.068772, mean_eps: 0.100000\n",
      " 771641/1000000: episode: 2306, duration: 1.355s, episode steps: 232, steps per second: 171, episode reward: 257.458, mean reward:  1.110 [-17.394, 100.000], mean action: 1.125 [0.000, 3.000],  loss: 2.263522, mae: 44.761394, mean_q: 60.971309, mean_eps: 0.100000\n",
      " 771871/1000000: episode: 2307, duration: 1.209s, episode steps: 230, steps per second: 190, episode reward: 294.020, mean reward:  1.278 [-5.225, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 3.295705, mae: 51.858878, mean_q: 70.469935, mean_eps: 0.100000\n",
      " 772086/1000000: episode: 2308, duration: 1.125s, episode steps: 215, steps per second: 191, episode reward: 248.707, mean reward:  1.157 [-17.019, 100.000], mean action: 1.219 [0.000, 3.000],  loss: 5.331077, mae: 59.982672, mean_q: 81.365093, mean_eps: 0.100000\n",
      " 772737/1000000: episode: 2309, duration: 3.498s, episode steps: 651, steps per second: 186, episode reward: 268.052, mean reward:  0.412 [-21.359, 100.000], mean action: 0.994 [0.000, 3.000],  loss: 4.761296, mae: 58.352429, mean_q: 79.243508, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 772979/1000000: episode: 2310, duration: 1.286s, episode steps: 242, steps per second: 188, episode reward: 277.480, mean reward:  1.147 [-10.335, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 3.125724, mae: 50.519134, mean_q: 68.826110, mean_eps: 0.100000\n",
      " 773291/1000000: episode: 2311, duration: 1.669s, episode steps: 312, steps per second: 187, episode reward: 285.658, mean reward:  0.916 [-18.072, 100.000], mean action: 0.933 [0.000, 3.000],  loss: 3.323819, mae: 51.373873, mean_q: 69.921883, mean_eps: 0.100000\n",
      " 774160/1000000: episode: 2312, duration: 4.663s, episode steps: 869, steps per second: 186, episode reward: 244.169, mean reward:  0.281 [-18.013, 100.000], mean action: 1.956 [0.000, 3.000],  loss: 2.551159, mae: 53.433344, mean_q: 72.471504, mean_eps: 0.100000\n",
      " 775160/1000000: episode: 2313, duration: 5.581s, episode steps: 1000, steps per second: 179, episode reward: 134.198, mean reward:  0.134 [-17.798, 21.756], mean action: 1.865 [0.000, 3.000],  loss: 1.773809, mae: 42.118319, mean_q: 57.330312, mean_eps: 0.100000\n",
      " 775566/1000000: episode: 2314, duration: 2.166s, episode steps: 406, steps per second: 187, episode reward: 307.938, mean reward:  0.758 [-11.170, 100.000], mean action: 0.825 [0.000, 3.000],  loss: 1.301376, mae: 39.070252, mean_q: 53.104809, mean_eps: 0.100000\n",
      " 775662/1000000: episode: 2315, duration: 0.522s, episode steps:  96, steps per second: 184, episode reward: 10.624, mean reward:  0.111 [-100.000, 17.467], mean action: 1.198 [0.000, 3.000],  loss: 1.791304, mae: 44.378709, mean_q: 60.148075, mean_eps: 0.100000\n",
      " 775754/1000000: episode: 2316, duration: 0.442s, episode steps:  92, steps per second: 208, episode reward: -10.649, mean reward: -0.116 [-100.000, 12.977], mean action: 1.630 [0.000, 3.000],  loss: 26.906749, mae: 48.292242, mean_q: 66.087695, mean_eps: 0.100000\n",
      " 775838/1000000: episode: 2317, duration: 0.411s, episode steps:  84, steps per second: 204, episode reward: -19.986, mean reward: -0.238 [-100.000, 15.015], mean action: 1.357 [0.000, 3.000],  loss: 32.281053, mae: 54.094946, mean_q: 74.666678, mean_eps: 0.100000\n",
      " 775975/1000000: episode: 2318, duration: 0.658s, episode steps: 137, steps per second: 208, episode reward: -15.683, mean reward: -0.114 [-100.000, 10.603], mean action: 1.752 [0.000, 3.000],  loss: 27.978084, mae: 59.908385, mean_q: 82.561227, mean_eps: 0.100000\n",
      " 776078/1000000: episode: 2319, duration: 0.492s, episode steps: 103, steps per second: 209, episode reward:  3.376, mean reward:  0.033 [-100.000, 18.028], mean action: 1.544 [0.000, 3.000],  loss: 49.502156, mae: 65.516800, mean_q: 90.692081, mean_eps: 0.100000\n",
      " 776143/1000000: episode: 2320, duration: 0.343s, episode steps:  65, steps per second: 190, episode reward: -18.154, mean reward: -0.279 [-100.000, 12.754], mean action: 1.185 [0.000, 3.000],  loss: 69.169338, mae: 68.864309, mean_q: 95.804516, mean_eps: 0.100000\n",
      " 776582/1000000: episode: 2321, duration: 2.371s, episode steps: 439, steps per second: 185, episode reward: 261.812, mean reward:  0.596 [-17.761, 100.000], mean action: 0.718 [0.000, 3.000],  loss: 40.491038, mae: 67.956044, mean_q: 93.262061, mean_eps: 0.100000\n",
      " 776673/1000000: episode: 2322, duration: 0.442s, episode steps:  91, steps per second: 206, episode reward:  3.790, mean reward:  0.042 [-100.000, 14.321], mean action: 1.890 [0.000, 3.000],  loss: 23.080989, mae: 58.571997, mean_q: 78.599550, mean_eps: 0.100000\n",
      " 776772/1000000: episode: 2323, duration: 0.487s, episode steps:  99, steps per second: 203, episode reward:  7.485, mean reward:  0.076 [-100.000, 20.888], mean action: 1.747 [0.000, 3.000],  loss: 32.608873, mae: 58.725865, mean_q: 79.092756, mean_eps: 0.100000\n",
      " 776896/1000000: episode: 2324, duration: 0.608s, episode steps: 124, steps per second: 204, episode reward: 16.916, mean reward:  0.136 [-100.000, 14.926], mean action: 1.790 [0.000, 3.000],  loss: 27.825059, mae: 55.787501, mean_q: 75.391358, mean_eps: 0.100000\n",
      " 776990/1000000: episode: 2325, duration: 0.455s, episode steps:  94, steps per second: 207, episode reward:  7.540, mean reward:  0.080 [-100.000, 10.935], mean action: 1.404 [0.000, 3.000],  loss: 34.942128, mae: 56.776773, mean_q: 76.327502, mean_eps: 0.100000\n",
      " 777079/1000000: episode: 2326, duration: 0.449s, episode steps:  89, steps per second: 198, episode reward: 48.384, mean reward:  0.544 [-100.000, 16.921], mean action: 1.719 [0.000, 3.000],  loss: 39.754026, mae: 54.089017, mean_q: 71.738431, mean_eps: 0.100000\n",
      " 777184/1000000: episode: 2327, duration: 0.548s, episode steps: 105, steps per second: 191, episode reward: 38.142, mean reward:  0.363 [-100.000, 14.660], mean action: 1.924 [0.000, 3.000],  loss: 30.240323, mae: 54.596878, mean_q: 72.265056, mean_eps: 0.100000\n",
      " 777367/1000000: episode: 2328, duration: 0.931s, episode steps: 183, steps per second: 197, episode reward:  4.156, mean reward:  0.023 [-100.000, 11.598], mean action: 1.470 [0.000, 3.000],  loss: 33.486079, mae: 54.171068, mean_q: 71.590494, mean_eps: 0.100000\n",
      " 777466/1000000: episode: 2329, duration: 0.497s, episode steps:  99, steps per second: 199, episode reward: 11.566, mean reward:  0.117 [-100.000, 20.012], mean action: 1.475 [0.000, 3.000],  loss: 22.584761, mae: 63.815763, mean_q: 85.385869, mean_eps: 0.100000\n",
      " 777642/1000000: episode: 2330, duration: 0.908s, episode steps: 176, steps per second: 194, episode reward: 283.838, mean reward:  1.613 [-3.633, 100.000], mean action: 0.881 [0.000, 3.000],  loss: 40.683305, mae: 71.159113, mean_q: 92.797601, mean_eps: 0.100000\n",
      " 777731/1000000: episode: 2331, duration: 0.484s, episode steps:  89, steps per second: 184, episode reward: -14.351, mean reward: -0.161 [-100.000, 12.537], mean action: 1.270 [0.000, 3.000],  loss: 29.299010, mae: 70.393292, mean_q: 92.333108, mean_eps: 0.100000\n",
      " 777838/1000000: episode: 2332, duration: 0.504s, episode steps: 107, steps per second: 212, episode reward: 19.513, mean reward:  0.182 [-100.000, 20.784], mean action: 1.056 [0.000, 3.000],  loss: 30.549616, mae: 71.110922, mean_q: 93.880351, mean_eps: 0.100000\n",
      " 777983/1000000: episode: 2333, duration: 0.705s, episode steps: 145, steps per second: 206, episode reward: 20.699, mean reward:  0.143 [-100.000, 17.363], mean action: 1.331 [0.000, 3.000],  loss: 31.590004, mae: 69.757466, mean_q: 90.513425, mean_eps: 0.100000\n",
      " 778123/1000000: episode: 2334, duration: 0.696s, episode steps: 140, steps per second: 201, episode reward: -5.921, mean reward: -0.042 [-100.000, 17.434], mean action: 1.757 [0.000, 3.000],  loss: 12.301247, mae: 67.012018, mean_q: 87.517585, mean_eps: 0.100000\n",
      " 778209/1000000: episode: 2335, duration: 0.412s, episode steps:  86, steps per second: 209, episode reward: -38.078, mean reward: -0.443 [-100.000, 16.147], mean action: 1.105 [0.000, 3.000],  loss: 28.698599, mae: 66.005513, mean_q: 84.890320, mean_eps: 0.100000\n",
      " 778320/1000000: episode: 2336, duration: 0.546s, episode steps: 111, steps per second: 203, episode reward: 17.937, mean reward:  0.162 [-100.000, 10.095], mean action: 1.667 [0.000, 3.000],  loss: 20.042461, mae: 67.946850, mean_q: 88.637926, mean_eps: 0.100000\n",
      " 778391/1000000: episode: 2337, duration: 0.341s, episode steps:  71, steps per second: 208, episode reward: 20.690, mean reward:  0.291 [-100.000, 21.362], mean action: 1.634 [0.000, 3.000],  loss: 14.461843, mae: 67.994853, mean_q: 87.141623, mean_eps: 0.100000\n",
      " 778481/1000000: episode: 2338, duration: 0.442s, episode steps:  90, steps per second: 204, episode reward: 22.353, mean reward:  0.248 [-100.000, 19.003], mean action: 1.167 [0.000, 3.000],  loss: 21.916979, mae: 67.627174, mean_q: 87.622198, mean_eps: 0.100000\n",
      " 778575/1000000: episode: 2339, duration: 0.502s, episode steps:  94, steps per second: 187, episode reward: -205.247, mean reward: -2.183 [-100.000, 76.823], mean action: 0.936 [0.000, 3.000],  loss: 10.426492, mae: 70.213323, mean_q: 90.174071, mean_eps: 0.100000\n",
      " 778669/1000000: episode: 2340, duration: 0.443s, episode steps:  94, steps per second: 212, episode reward:  5.393, mean reward:  0.057 [-100.000,  9.767], mean action: 1.170 [0.000, 3.000],  loss: 13.854786, mae: 75.024411, mean_q: 92.504876, mean_eps: 0.100000\n",
      " 778787/1000000: episode: 2341, duration: 0.573s, episode steps: 118, steps per second: 206, episode reward: -6.574, mean reward: -0.056 [-100.000, 14.695], mean action: 1.331 [0.000, 3.000],  loss: 16.457884, mae: 74.126323, mean_q: 88.662110, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 778875/1000000: episode: 2342, duration: 0.468s, episode steps:  88, steps per second: 188, episode reward: -149.506, mean reward: -1.699 [-100.000, 60.868], mean action: 1.216 [0.000, 3.000],  loss: 12.138391, mae: 75.772705, mean_q: 87.573305, mean_eps: 0.100000\n",
      " 779014/1000000: episode: 2343, duration: 0.674s, episode steps: 139, steps per second: 206, episode reward: -172.864, mean reward: -1.244 [-100.000, 17.735], mean action: 2.108 [0.000, 3.000],  loss: 11.773391, mae: 74.801099, mean_q: 85.015954, mean_eps: 0.100000\n",
      " 779120/1000000: episode: 2344, duration: 0.530s, episode steps: 106, steps per second: 200, episode reward: -16.191, mean reward: -0.153 [-100.000,  9.253], mean action: 1.887 [0.000, 3.000],  loss: 16.869893, mae: 74.671570, mean_q: 80.807441, mean_eps: 0.100000\n",
      " 779217/1000000: episode: 2345, duration: 0.532s, episode steps:  97, steps per second: 182, episode reward: -162.863, mean reward: -1.679 [-100.000, 14.168], mean action: 0.887 [0.000, 3.000],  loss: 17.464831, mae: 73.851308, mean_q: 81.885416, mean_eps: 0.100000\n",
      " 779766/1000000: episode: 2346, duration: 3.281s, episode steps: 549, steps per second: 167, episode reward: 224.681, mean reward:  0.409 [-17.495, 100.000], mean action: 0.933 [0.000, 3.000],  loss: 52.761988, mae: 66.816748, mean_q: 49.403289, mean_eps: 0.100000\n",
      " 779979/1000000: episode: 2347, duration: 1.230s, episode steps: 213, steps per second: 173, episode reward: 285.121, mean reward:  1.339 [-18.167, 100.000], mean action: 1.315 [0.000, 3.000],  loss: 44.689218, mae: 60.361223, mean_q: 4.625603, mean_eps: 0.100000\n",
      " 780087/1000000: episode: 2348, duration: 0.560s, episode steps: 108, steps per second: 193, episode reward: -0.388, mean reward: -0.004 [-100.000, 11.599], mean action: 1.676 [0.000, 3.000],  loss: 59.731446, mae: 55.182982, mean_q: 2.033107, mean_eps: 0.100000\n",
      " 781087/1000000: episode: 2349, duration: 5.779s, episode steps: 1000, steps per second: 173, episode reward: -8.587, mean reward: -0.009 [-18.746, 14.031], mean action: 1.692 [0.000, 3.000],  loss: 23.332054, mae: 37.758510, mean_q: 15.210205, mean_eps: 0.100000\n",
      " 782087/1000000: episode: 2350, duration: 6.529s, episode steps: 1000, steps per second: 153, episode reward: 18.640, mean reward:  0.019 [-21.080, 22.476], mean action: 1.604 [0.000, 3.000],  loss: 5.327965, mae: 11.366932, mean_q: 12.399458, mean_eps: 0.100000\n",
      " 782181/1000000: episode: 2351, duration: 0.457s, episode steps:  94, steps per second: 206, episode reward: 20.146, mean reward:  0.214 [-100.000, 17.339], mean action: 1.766 [0.000, 3.000],  loss: 3.760280, mae: 10.977159, mean_q: 13.125498, mean_eps: 0.100000\n",
      " 782345/1000000: episode: 2352, duration: 0.780s, episode steps: 164, steps per second: 210, episode reward: 258.532, mean reward:  1.576 [-11.394, 100.000], mean action: 1.201 [0.000, 3.000],  loss: 4.015297, mae: 15.351621, mean_q: 17.786851, mean_eps: 0.100000\n",
      " 783345/1000000: episode: 2353, duration: 6.555s, episode steps: 1000, steps per second: 153, episode reward: -18.622, mean reward: -0.019 [-11.387, 14.951], mean action: 1.688 [0.000, 3.000],  loss: 6.479473, mae: 18.898826, mean_q: 23.479212, mean_eps: 0.100000\n",
      " 784345/1000000: episode: 2354, duration: 6.401s, episode steps: 1000, steps per second: 156, episode reward: -1.109, mean reward: -0.001 [-18.904, 13.001], mean action: 1.732 [0.000, 3.000],  loss: 1.933671, mae: 10.658174, mean_q: 14.890573, mean_eps: 0.100000\n",
      " 785345/1000000: episode: 2355, duration: 5.828s, episode steps: 1000, steps per second: 172, episode reward: 49.219, mean reward:  0.049 [-20.921, 22.407], mean action: 1.768 [0.000, 3.000],  loss: 1.330255, mae: 10.276387, mean_q: 11.806779, mean_eps: 0.100000\n",
      " 785728/1000000: episode: 2356, duration: 1.923s, episode steps: 383, steps per second: 199, episode reward: 210.887, mean reward:  0.551 [-20.626, 100.000], mean action: 1.695 [0.000, 3.000],  loss: 0.851351, mae: 10.794585, mean_q: 9.896655, mean_eps: 0.100000\n",
      " 786390/1000000: episode: 2357, duration: 3.607s, episode steps: 662, steps per second: 184, episode reward: 236.279, mean reward:  0.357 [-17.488, 100.000], mean action: 0.962 [0.000, 3.000],  loss: 4.024492, mae: 15.730136, mean_q: 16.276884, mean_eps: 0.100000\n",
      " 786619/1000000: episode: 2358, duration: 1.124s, episode steps: 229, steps per second: 204, episode reward: 249.412, mean reward:  1.089 [-18.617, 100.000], mean action: 1.122 [0.000, 3.000],  loss: 11.611630, mae: 15.756042, mean_q: 14.597948, mean_eps: 0.100000\n",
      " 787157/1000000: episode: 2359, duration: 3.026s, episode steps: 538, steps per second: 178, episode reward: 194.310, mean reward:  0.361 [-21.542, 100.000], mean action: 1.641 [0.000, 3.000],  loss: 9.076601, mae: 14.684531, mean_q: 13.010852, mean_eps: 0.100000\n",
      " 787553/1000000: episode: 2360, duration: 2.171s, episode steps: 396, steps per second: 182, episode reward: 294.963, mean reward:  0.745 [-18.863, 100.000], mean action: 1.149 [0.000, 3.000],  loss: 16.771920, mae: 18.481500, mean_q: 12.901688, mean_eps: 0.100000\n",
      " 787945/1000000: episode: 2361, duration: 2.022s, episode steps: 392, steps per second: 194, episode reward: 250.435, mean reward:  0.639 [-9.917, 100.000], mean action: 0.745 [0.000, 3.000],  loss: 19.974689, mae: 18.219081, mean_q: 15.459584, mean_eps: 0.100000\n",
      " 788196/1000000: episode: 2362, duration: 1.494s, episode steps: 251, steps per second: 168, episode reward: 249.539, mean reward:  0.994 [-18.726, 100.000], mean action: 1.251 [0.000, 3.000],  loss: 20.463164, mae: 19.702027, mean_q: 23.174703, mean_eps: 0.100000\n",
      " 788685/1000000: episode: 2363, duration: 2.571s, episode steps: 489, steps per second: 190, episode reward: 265.611, mean reward:  0.543 [-19.254, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 11.684357, mae: 23.187322, mean_q: 32.024831, mean_eps: 0.100000\n",
      " 789113/1000000: episode: 2364, duration: 2.171s, episode steps: 428, steps per second: 197, episode reward: 284.832, mean reward:  0.665 [-18.344, 100.000], mean action: 1.213 [0.000, 3.000],  loss: 10.065958, mae: 27.256127, mean_q: 37.558287, mean_eps: 0.100000\n",
      " 789324/1000000: episode: 2365, duration: 1.023s, episode steps: 211, steps per second: 206, episode reward: 283.675, mean reward:  1.344 [-10.080, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 11.056517, mae: 32.429143, mean_q: 44.496218, mean_eps: 0.100000\n",
      " 789589/1000000: episode: 2366, duration: 1.318s, episode steps: 265, steps per second: 201, episode reward: 261.872, mean reward:  0.988 [-18.236, 100.000], mean action: 1.472 [0.000, 3.000],  loss: 11.709892, mae: 33.474240, mean_q: 45.894317, mean_eps: 0.100000\n",
      " 789769/1000000: episode: 2367, duration: 0.855s, episode steps: 180, steps per second: 211, episode reward: 275.732, mean reward:  1.532 [-8.984, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 7.680864, mae: 38.428170, mean_q: 52.469915, mean_eps: 0.100000\n",
      " 790145/1000000: episode: 2368, duration: 1.872s, episode steps: 376, steps per second: 201, episode reward: 240.756, mean reward:  0.640 [-17.765, 100.000], mean action: 0.931 [0.000, 3.000],  loss: 11.669701, mae: 37.604897, mean_q: 51.368180, mean_eps: 0.100000\n",
      " 790470/1000000: episode: 2369, duration: 1.596s, episode steps: 325, steps per second: 204, episode reward: 271.498, mean reward:  0.835 [-5.941, 100.000], mean action: 1.452 [0.000, 3.000],  loss: 9.789993, mae: 35.148591, mean_q: 48.009580, mean_eps: 0.100000\n",
      " 790948/1000000: episode: 2370, duration: 2.449s, episode steps: 478, steps per second: 195, episode reward: 285.866, mean reward:  0.598 [-17.751, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 9.142493, mae: 35.410489, mean_q: 48.403277, mean_eps: 0.100000\n",
      " 791476/1000000: episode: 2371, duration: 2.749s, episode steps: 528, steps per second: 192, episode reward: 252.895, mean reward:  0.479 [-17.184, 100.000], mean action: 0.994 [0.000, 3.000],  loss: 6.392173, mae: 30.339165, mean_q: 41.602784, mean_eps: 0.100000\n",
      " 791711/1000000: episode: 2372, duration: 1.141s, episode steps: 235, steps per second: 206, episode reward: 261.295, mean reward:  1.112 [-2.560, 100.000], mean action: 1.319 [0.000, 3.000],  loss: 6.152073, mae: 23.181077, mean_q: 31.995481, mean_eps: 0.100000\n",
      " 791958/1000000: episode: 2373, duration: 1.207s, episode steps: 247, steps per second: 205, episode reward: 263.124, mean reward:  1.065 [-9.242, 100.000], mean action: 1.640 [0.000, 3.000],  loss: 11.116910, mae: 24.578617, mean_q: 33.887204, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 792141/1000000: episode: 2374, duration: 0.874s, episode steps: 183, steps per second: 209, episode reward: 236.689, mean reward:  1.293 [-5.796, 100.000], mean action: 1.164 [0.000, 3.000],  loss: 7.568877, mae: 26.165607, mean_q: 36.203798, mean_eps: 0.100000\n",
      " 792355/1000000: episode: 2375, duration: 1.020s, episode steps: 214, steps per second: 210, episode reward: 276.097, mean reward:  1.290 [-10.396, 100.000], mean action: 1.136 [0.000, 3.000],  loss: 13.383867, mae: 32.634815, mean_q: 44.754290, mean_eps: 0.100000\n",
      " 792625/1000000: episode: 2376, duration: 1.305s, episode steps: 270, steps per second: 207, episode reward: 251.842, mean reward:  0.933 [-17.707, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 12.314886, mae: 40.494540, mean_q: 55.096431, mean_eps: 0.100000\n",
      " 793142/1000000: episode: 2377, duration: 2.739s, episode steps: 517, steps per second: 189, episode reward: 277.153, mean reward:  0.536 [-19.866, 100.000], mean action: 0.747 [0.000, 3.000],  loss: 8.111364, mae: 41.058230, mean_q: 55.923903, mean_eps: 0.100000\n",
      " 793617/1000000: episode: 2378, duration: 2.418s, episode steps: 475, steps per second: 196, episode reward: 298.249, mean reward:  0.628 [-11.356, 100.000], mean action: 0.773 [0.000, 3.000],  loss: 5.179706, mae: 34.858926, mean_q: 47.849590, mean_eps: 0.100000\n",
      " 793842/1000000: episode: 2379, duration: 1.087s, episode steps: 225, steps per second: 207, episode reward: 262.903, mean reward:  1.168 [-9.510, 100.000], mean action: 0.991 [0.000, 3.000],  loss: 4.485094, mae: 32.864394, mean_q: 45.121838, mean_eps: 0.100000\n",
      " 794362/1000000: episode: 2380, duration: 2.605s, episode steps: 520, steps per second: 200, episode reward: 250.767, mean reward:  0.482 [-19.226, 100.000], mean action: 0.875 [0.000, 3.000],  loss: 5.598795, mae: 33.876570, mean_q: 46.373096, mean_eps: 0.100000\n",
      " 794564/1000000: episode: 2381, duration: 0.955s, episode steps: 202, steps per second: 211, episode reward: 268.443, mean reward:  1.329 [-3.038, 100.000], mean action: 1.416 [0.000, 3.000],  loss: 7.182014, mae: 33.199037, mean_q: 45.175696, mean_eps: 0.100000\n",
      " 794820/1000000: episode: 2382, duration: 1.268s, episode steps: 256, steps per second: 202, episode reward: 309.566, mean reward:  1.209 [-10.040, 100.000], mean action: 1.426 [0.000, 3.000],  loss: 7.144730, mae: 38.071664, mean_q: 51.765606, mean_eps: 0.100000\n",
      " 795355/1000000: episode: 2383, duration: 2.712s, episode steps: 535, steps per second: 197, episode reward: 262.520, mean reward:  0.491 [-18.266, 100.000], mean action: 0.929 [0.000, 3.000],  loss: 7.011962, mae: 45.635760, mean_q: 62.045913, mean_eps: 0.100000\n",
      " 795809/1000000: episode: 2384, duration: 2.314s, episode steps: 454, steps per second: 196, episode reward: 221.361, mean reward:  0.488 [-20.983, 100.000], mean action: 0.965 [0.000, 3.000],  loss: 2.979672, mae: 49.793778, mean_q: 67.917552, mean_eps: 0.100000\n",
      " 796143/1000000: episode: 2385, duration: 1.634s, episode steps: 334, steps per second: 204, episode reward: 282.263, mean reward:  0.845 [-17.888, 100.000], mean action: 0.719 [0.000, 3.000],  loss: 3.924212, mae: 43.699675, mean_q: 59.551363, mean_eps: 0.100000\n",
      " 796356/1000000: episode: 2386, duration: 1.030s, episode steps: 213, steps per second: 207, episode reward: 298.395, mean reward:  1.401 [-11.954, 100.000], mean action: 1.254 [0.000, 3.000],  loss: 3.630936, mae: 47.565372, mean_q: 64.775295, mean_eps: 0.100000\n",
      " 796844/1000000: episode: 2387, duration: 2.472s, episode steps: 488, steps per second: 197, episode reward: 260.852, mean reward:  0.535 [-18.836, 100.000], mean action: 0.830 [0.000, 3.000],  loss: 3.994886, mae: 52.615066, mean_q: 71.425699, mean_eps: 0.100000\n",
      " 797225/1000000: episode: 2388, duration: 1.954s, episode steps: 381, steps per second: 195, episode reward: 254.281, mean reward:  0.667 [-17.444, 100.000], mean action: 1.522 [0.000, 3.000],  loss: 3.461997, mae: 53.363025, mean_q: 72.151802, mean_eps: 0.100000\n",
      " 797465/1000000: episode: 2389, duration: 1.157s, episode steps: 240, steps per second: 207, episode reward: 298.278, mean reward:  1.243 [-7.694, 100.000], mean action: 1.292 [0.000, 3.000],  loss: 2.228040, mae: 51.704166, mean_q: 69.961549, mean_eps: 0.100000\n",
      " 797907/1000000: episode: 2390, duration: 2.156s, episode steps: 442, steps per second: 205, episode reward: 214.740, mean reward:  0.486 [-18.655, 100.000], mean action: 0.722 [0.000, 3.000],  loss: 2.611328, mae: 53.176528, mean_q: 71.892551, mean_eps: 0.100000\n",
      " 798218/1000000: episode: 2391, duration: 1.500s, episode steps: 311, steps per second: 207, episode reward: -81.491, mean reward: -0.262 [-100.000, 14.575], mean action: 1.334 [0.000, 3.000],  loss: 4.619750, mae: 54.665359, mean_q: 74.182951, mean_eps: 0.100000\n",
      " 799068/1000000: episode: 2392, duration: 4.482s, episode steps: 850, steps per second: 190, episode reward: 230.576, mean reward:  0.271 [-19.489, 100.000], mean action: 1.027 [0.000, 3.000],  loss: 5.961890, mae: 45.684421, mean_q: 62.282066, mean_eps: 0.100000\n",
      " 799636/1000000: episode: 2393, duration: 2.923s, episode steps: 568, steps per second: 194, episode reward: -191.051, mean reward: -0.336 [-100.000, 20.577], mean action: 1.322 [0.000, 3.000],  loss: 4.666798, mae: 31.929104, mean_q: 42.601874, mean_eps: 0.100000\n",
      " 799756/1000000: episode: 2394, duration: 0.572s, episode steps: 120, steps per second: 210, episode reward: -151.708, mean reward: -1.264 [-100.000, 44.286], mean action: 1.475 [0.000, 3.000],  loss: 7.688096, mae: 30.368442, mean_q: 40.666540, mean_eps: 0.100000\n",
      " 800014/1000000: episode: 2395, duration: 1.238s, episode steps: 258, steps per second: 208, episode reward: 240.059, mean reward:  0.930 [-18.469, 100.000], mean action: 0.783 [0.000, 3.000],  loss: 16.557615, mae: 37.481867, mean_q: 50.067633, mean_eps: 0.100000\n",
      " 800250/1000000: episode: 2396, duration: 1.141s, episode steps: 236, steps per second: 207, episode reward: 273.234, mean reward:  1.158 [-11.001, 100.000], mean action: 1.169 [0.000, 3.000],  loss: 7.266806, mae: 41.001128, mean_q: 54.290144, mean_eps: 0.100000\n",
      " 800370/1000000: episode: 2397, duration: 0.573s, episode steps: 120, steps per second: 209, episode reward: 14.918, mean reward:  0.124 [-100.000, 16.836], mean action: 1.417 [0.000, 3.000],  loss: 4.501358, mae: 47.994659, mean_q: 64.412023, mean_eps: 0.100000\n",
      " 800489/1000000: episode: 2398, duration: 0.563s, episode steps: 119, steps per second: 211, episode reward:  7.112, mean reward:  0.060 [-100.000, 18.767], mean action: 1.160 [0.000, 3.000],  loss: 13.940964, mae: 55.809981, mean_q: 74.261941, mean_eps: 0.100000\n",
      " 800633/1000000: episode: 2399, duration: 0.691s, episode steps: 144, steps per second: 209, episode reward: -174.923, mean reward: -1.215 [-100.000, 20.548], mean action: 2.111 [0.000, 3.000],  loss: 12.474555, mae: 62.505748, mean_q: 82.807723, mean_eps: 0.100000\n",
      " 800765/1000000: episode: 2400, duration: 0.629s, episode steps: 132, steps per second: 210, episode reward: 13.522, mean reward:  0.102 [-100.000, 11.744], mean action: 1.424 [0.000, 3.000],  loss: 13.396004, mae: 63.719867, mean_q: 83.058288, mean_eps: 0.100000\n",
      " 801255/1000000: episode: 2401, duration: 2.551s, episode steps: 490, steps per second: 192, episode reward: 245.017, mean reward:  0.500 [-18.079, 100.000], mean action: 1.557 [0.000, 3.000],  loss: 22.510310, mae: 62.526922, mean_q: 80.633611, mean_eps: 0.100000\n",
      " 801386/1000000: episode: 2402, duration: 0.622s, episode steps: 131, steps per second: 211, episode reward: 11.369, mean reward:  0.087 [-100.000, 12.840], mean action: 1.527 [0.000, 3.000],  loss: 21.756848, mae: 58.761218, mean_q: 73.815017, mean_eps: 0.100000\n",
      " 801505/1000000: episode: 2403, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: -9.066, mean reward: -0.076 [-100.000, 17.817], mean action: 1.849 [0.000, 3.000],  loss: 19.502229, mae: 58.439731, mean_q: 72.905224, mean_eps: 0.100000\n",
      " 801805/1000000: episode: 2404, duration: 1.466s, episode steps: 300, steps per second: 205, episode reward: 279.304, mean reward:  0.931 [-17.364, 100.000], mean action: 1.290 [0.000, 3.000],  loss: 17.333034, mae: 59.094855, mean_q: 76.417346, mean_eps: 0.100000\n",
      " 802522/1000000: episode: 2405, duration: 3.840s, episode steps: 717, steps per second: 187, episode reward: 147.635, mean reward:  0.206 [-20.396, 100.000], mean action: 1.968 [0.000, 3.000],  loss: 7.889186, mae: 50.009636, mean_q: 61.911929, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 802896/1000000: episode: 2406, duration: 1.901s, episode steps: 374, steps per second: 197, episode reward: 243.777, mean reward:  0.652 [-19.703, 100.000], mean action: 1.021 [0.000, 3.000],  loss: 6.161225, mae: 36.243493, mean_q: 43.775425, mean_eps: 0.100000\n",
      " 803229/1000000: episode: 2407, duration: 1.648s, episode steps: 333, steps per second: 202, episode reward: 244.755, mean reward:  0.735 [-22.382, 100.000], mean action: 1.390 [0.000, 3.000],  loss: 8.539254, mae: 41.927316, mean_q: 54.899363, mean_eps: 0.100000\n",
      " 803976/1000000: episode: 2408, duration: 4.143s, episode steps: 747, steps per second: 180, episode reward: 96.293, mean reward:  0.129 [-22.929, 100.000], mean action: 1.764 [0.000, 3.000],  loss: 4.858894, mae: 55.202437, mean_q: 75.196398, mean_eps: 0.100000\n",
      " 804258/1000000: episode: 2409, duration: 1.354s, episode steps: 282, steps per second: 208, episode reward: 252.317, mean reward:  0.895 [-17.885, 100.000], mean action: 0.688 [0.000, 3.000],  loss: 5.825801, mae: 46.920393, mean_q: 63.963541, mean_eps: 0.100000\n",
      " 804932/1000000: episode: 2410, duration: 3.368s, episode steps: 674, steps per second: 200, episode reward: 267.610, mean reward:  0.397 [-19.853, 100.000], mean action: 0.709 [0.000, 3.000],  loss: 3.797835, mae: 49.607976, mean_q: 67.392367, mean_eps: 0.100000\n",
      " 805029/1000000: episode: 2411, duration: 0.466s, episode steps:  97, steps per second: 208, episode reward:  7.961, mean reward:  0.082 [-100.000, 18.963], mean action: 1.557 [0.000, 3.000],  loss: 3.183879, mae: 56.349074, mean_q: 76.087375, mean_eps: 0.100000\n",
      " 805600/1000000: episode: 2412, duration: 2.947s, episode steps: 571, steps per second: 194, episode reward: 268.577, mean reward:  0.470 [-20.573, 100.000], mean action: 0.694 [0.000, 3.000],  loss: 3.150170, mae: 53.719814, mean_q: 72.181226, mean_eps: 0.100000\n",
      " 806109/1000000: episode: 2413, duration: 2.566s, episode steps: 509, steps per second: 198, episode reward: -231.454, mean reward: -0.455 [-100.000, 18.754], mean action: 1.550 [0.000, 3.000],  loss: 2.977461, mae: 48.972366, mean_q: 65.758049, mean_eps: 0.100000\n",
      " 806400/1000000: episode: 2414, duration: 1.465s, episode steps: 291, steps per second: 199, episode reward: 283.305, mean reward:  0.974 [-18.504, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 3.621145, mae: 43.751495, mean_q: 56.958134, mean_eps: 0.100000\n",
      " 806793/1000000: episode: 2415, duration: 1.982s, episode steps: 393, steps per second: 198, episode reward: 259.404, mean reward:  0.660 [-18.723, 100.000], mean action: 1.168 [0.000, 3.000],  loss: 4.754076, mae: 47.617576, mean_q: 60.937129, mean_eps: 0.100000\n",
      " 807212/1000000: episode: 2416, duration: 2.169s, episode steps: 419, steps per second: 193, episode reward: 235.113, mean reward:  0.561 [-17.510, 100.000], mean action: 1.251 [0.000, 3.000],  loss: 6.782665, mae: 52.109039, mean_q: 66.631520, mean_eps: 0.100000\n",
      " 807425/1000000: episode: 2417, duration: 1.020s, episode steps: 213, steps per second: 209, episode reward: 272.520, mean reward:  1.279 [-17.543, 100.000], mean action: 1.160 [0.000, 3.000],  loss: 3.833105, mae: 51.921237, mean_q: 70.275781, mean_eps: 0.100000\n",
      " 807898/1000000: episode: 2418, duration: 2.368s, episode steps: 473, steps per second: 200, episode reward: 252.611, mean reward:  0.534 [-20.974, 100.000], mean action: 1.211 [0.000, 3.000],  loss: 4.679233, mae: 48.870819, mean_q: 66.233458, mean_eps: 0.100000\n",
      " 808330/1000000: episode: 2419, duration: 2.210s, episode steps: 432, steps per second: 195, episode reward: 267.433, mean reward:  0.619 [-19.823, 100.000], mean action: 0.979 [0.000, 3.000],  loss: 4.818551, mae: 49.112008, mean_q: 66.472936, mean_eps: 0.100000\n",
      " 809330/1000000: episode: 2420, duration: 5.449s, episode steps: 1000, steps per second: 184, episode reward: -2.217, mean reward: -0.002 [-20.971, 18.000], mean action: 2.339 [0.000, 3.000],  loss: 2.880791, mae: 35.531017, mean_q: 43.150950, mean_eps: 0.100000\n",
      " 810097/1000000: episode: 2421, duration: 3.790s, episode steps: 767, steps per second: 202, episode reward: 212.826, mean reward:  0.277 [-22.802, 100.000], mean action: 1.082 [0.000, 3.000],  loss: 1.744918, mae: 20.162608, mean_q: 18.527672, mean_eps: 0.100000\n",
      " 810423/1000000: episode: 2422, duration: 1.596s, episode steps: 326, steps per second: 204, episode reward: 292.591, mean reward:  0.898 [-19.332, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 1.841344, mae: 31.427809, mean_q: 42.115054, mean_eps: 0.100000\n",
      " 810606/1000000: episode: 2423, duration: 0.881s, episode steps: 183, steps per second: 208, episode reward: 261.400, mean reward:  1.428 [-9.631, 100.000], mean action: 1.372 [0.000, 3.000],  loss: 7.290100, mae: 35.792817, mean_q: 48.996126, mean_eps: 0.100000\n",
      " 811083/1000000: episode: 2424, duration: 2.410s, episode steps: 477, steps per second: 198, episode reward: 248.239, mean reward:  0.520 [-16.921, 100.000], mean action: 1.157 [0.000, 3.000],  loss: 6.564741, mae: 46.247540, mean_q: 62.701235, mean_eps: 0.100000\n",
      " 811606/1000000: episode: 2425, duration: 2.650s, episode steps: 523, steps per second: 197, episode reward: 280.458, mean reward:  0.536 [-17.288, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 3.105616, mae: 54.133679, mean_q: 73.072583, mean_eps: 0.100000\n",
      " 812606/1000000: episode: 2426, duration: 5.217s, episode steps: 1000, steps per second: 192, episode reward: 125.416, mean reward:  0.125 [-18.446, 12.011], mean action: 0.887 [0.000, 3.000],  loss: 2.558056, mae: 45.001662, mean_q: 60.759910, mean_eps: 0.100000\n",
      " 812913/1000000: episode: 2427, duration: 1.523s, episode steps: 307, steps per second: 202, episode reward: 247.840, mean reward:  0.807 [-18.921, 100.000], mean action: 1.466 [0.000, 3.000],  loss: 1.649514, mae: 33.554475, mean_q: 45.619556, mean_eps: 0.100000\n",
      " 813199/1000000: episode: 2428, duration: 1.378s, episode steps: 286, steps per second: 208, episode reward: 270.597, mean reward:  0.946 [-17.474, 100.000], mean action: 0.997 [0.000, 3.000],  loss: 3.118696, mae: 38.631225, mean_q: 52.442033, mean_eps: 0.100000\n",
      " 813447/1000000: episode: 2429, duration: 1.194s, episode steps: 248, steps per second: 208, episode reward: 249.380, mean reward:  1.006 [-9.434, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 4.194953, mae: 45.389834, mean_q: 61.573473, mean_eps: 0.100000\n",
      " 813664/1000000: episode: 2430, duration: 1.035s, episode steps: 217, steps per second: 210, episode reward: 280.263, mean reward:  1.292 [-11.189, 100.000], mean action: 1.101 [0.000, 3.000],  loss: 5.640028, mae: 53.921558, mean_q: 72.924393, mean_eps: 0.100000\n",
      " 813976/1000000: episode: 2431, duration: 1.550s, episode steps: 312, steps per second: 201, episode reward: 264.900, mean reward:  0.849 [-19.051, 100.000], mean action: 1.006 [0.000, 3.000],  loss: 4.811855, mae: 58.298620, mean_q: 78.836260, mean_eps: 0.100000\n",
      " 814260/1000000: episode: 2432, duration: 1.400s, episode steps: 284, steps per second: 203, episode reward: 269.178, mean reward:  0.948 [-11.125, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 5.114854, mae: 57.559775, mean_q: 77.813211, mean_eps: 0.100000\n",
      " 814534/1000000: episode: 2433, duration: 1.349s, episode steps: 274, steps per second: 203, episode reward: 276.259, mean reward:  1.008 [-2.918, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 4.465780, mae: 57.554591, mean_q: 77.794308, mean_eps: 0.100000\n",
      " 814746/1000000: episode: 2434, duration: 1.012s, episode steps: 212, steps per second: 210, episode reward: 256.459, mean reward:  1.210 [-11.714, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 4.033411, mae: 55.851169, mean_q: 75.458778, mean_eps: 0.100000\n",
      " 815008/1000000: episode: 2435, duration: 1.256s, episode steps: 262, steps per second: 209, episode reward: 255.655, mean reward:  0.976 [-9.400, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 3.319029, mae: 57.681687, mean_q: 77.887924, mean_eps: 0.100000\n",
      " 815291/1000000: episode: 2436, duration: 1.357s, episode steps: 283, steps per second: 209, episode reward: 296.818, mean reward:  1.049 [-9.181, 100.000], mean action: 1.233 [0.000, 3.000],  loss: 4.076473, mae: 60.223092, mean_q: 81.249810, mean_eps: 0.100000\n",
      " 816015/1000000: episode: 2437, duration: 3.731s, episode steps: 724, steps per second: 194, episode reward: 239.739, mean reward:  0.331 [-20.122, 100.000], mean action: 1.001 [0.000, 3.000],  loss: 2.468418, mae: 53.570978, mean_q: 72.289590, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 816466/1000000: episode: 2438, duration: 2.258s, episode steps: 451, steps per second: 200, episode reward: 276.810, mean reward:  0.614 [-20.063, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 2.449517, mae: 38.556612, mean_q: 52.502108, mean_eps: 0.100000\n",
      " 816673/1000000: episode: 2439, duration: 0.989s, episode steps: 207, steps per second: 209, episode reward: -52.716, mean reward: -0.255 [-100.000, 21.025], mean action: 1.126 [0.000, 3.000],  loss: 5.047029, mae: 36.042564, mean_q: 49.282399, mean_eps: 0.100000\n",
      " 816941/1000000: episode: 2440, duration: 1.301s, episode steps: 268, steps per second: 206, episode reward: 306.361, mean reward:  1.143 [-19.114, 100.000], mean action: 1.519 [0.000, 3.000],  loss: 22.776740, mae: 48.567906, mean_q: 66.082613, mean_eps: 0.100000\n",
      " 817043/1000000: episode: 2441, duration: 0.488s, episode steps: 102, steps per second: 209, episode reward: 34.798, mean reward:  0.341 [-100.000, 15.428], mean action: 1.549 [0.000, 3.000],  loss: 11.741081, mae: 57.016154, mean_q: 77.434838, mean_eps: 0.100000\n",
      " 818043/1000000: episode: 2442, duration: 5.345s, episode steps: 1000, steps per second: 187, episode reward: 133.868, mean reward:  0.134 [-18.960, 21.469], mean action: 1.312 [0.000, 3.000],  loss: 12.707712, mae: 50.600101, mean_q: 68.305151, mean_eps: 0.100000\n",
      " 819043/1000000: episode: 2443, duration: 5.290s, episode steps: 1000, steps per second: 189, episode reward: 97.611, mean reward:  0.098 [-23.663, 22.448], mean action: 1.879 [0.000, 3.000],  loss: 0.915483, mae: 34.688343, mean_q: 47.383690, mean_eps: 0.100000\n",
      " 819367/1000000: episode: 2444, duration: 1.602s, episode steps: 324, steps per second: 202, episode reward: 188.388, mean reward:  0.581 [-15.534, 100.000], mean action: 1.448 [0.000, 3.000],  loss: 1.073660, mae: 35.616934, mean_q: 48.869152, mean_eps: 0.100000\n",
      " 820330/1000000: episode: 2445, duration: 5.145s, episode steps: 963, steps per second: 187, episode reward: 239.509, mean reward:  0.249 [-19.470, 100.000], mean action: 1.072 [0.000, 3.000],  loss: 3.048665, mae: 34.780290, mean_q: 47.362481, mean_eps: 0.100000\n",
      " 820525/1000000: episode: 2446, duration: 0.930s, episode steps: 195, steps per second: 210, episode reward: 254.265, mean reward:  1.304 [-3.413, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 3.650879, mae: 31.547094, mean_q: 42.947318, mean_eps: 0.100000\n",
      " 821380/1000000: episode: 2447, duration: 4.543s, episode steps: 855, steps per second: 188, episode reward: 213.172, mean reward:  0.249 [-20.650, 100.000], mean action: 1.874 [0.000, 3.000],  loss: 2.786800, mae: 39.988911, mean_q: 54.285090, mean_eps: 0.100000\n",
      " 821563/1000000: episode: 2448, duration: 0.877s, episode steps: 183, steps per second: 209, episode reward: 255.607, mean reward:  1.397 [-12.092, 100.000], mean action: 1.186 [0.000, 3.000],  loss: 4.332291, mae: 40.200529, mean_q: 54.451051, mean_eps: 0.100000\n",
      " 822158/1000000: episode: 2449, duration: 3.128s, episode steps: 595, steps per second: 190, episode reward: 280.964, mean reward:  0.472 [-19.488, 100.000], mean action: 0.951 [0.000, 3.000],  loss: 3.514455, mae: 40.741534, mean_q: 54.978922, mean_eps: 0.100000\n",
      " 822323/1000000: episode: 2450, duration: 0.796s, episode steps: 165, steps per second: 207, episode reward: 274.646, mean reward:  1.665 [-11.022, 100.000], mean action: 1.485 [0.000, 3.000],  loss: 4.389072, mae: 46.797718, mean_q: 63.018692, mean_eps: 0.100000\n",
      " 822646/1000000: episode: 2451, duration: 1.616s, episode steps: 323, steps per second: 200, episode reward: 300.001, mean reward:  0.929 [-18.240, 100.000], mean action: 1.217 [0.000, 3.000],  loss: 4.072134, mae: 52.416914, mean_q: 70.720706, mean_eps: 0.100000\n",
      " 822834/1000000: episode: 2452, duration: 0.908s, episode steps: 188, steps per second: 207, episode reward: -152.970, mean reward: -0.814 [-100.000, 34.956], mean action: 1.633 [0.000, 3.000],  loss: 3.505023, mae: 53.711004, mean_q: 72.403502, mean_eps: 0.100000\n",
      " 822927/1000000: episode: 2453, duration: 0.448s, episode steps:  93, steps per second: 208, episode reward: 28.250, mean reward:  0.304 [-100.000, 14.696], mean action: 1.892 [0.000, 3.000],  loss: 4.079644, mae: 56.576091, mean_q: 75.265844, mean_eps: 0.100000\n",
      " 823321/1000000: episode: 2454, duration: 1.934s, episode steps: 394, steps per second: 204, episode reward: 290.365, mean reward:  0.737 [-11.534, 100.000], mean action: 1.251 [0.000, 3.000],  loss: 4.610218, mae: 60.943672, mean_q: 79.077054, mean_eps: 0.100000\n",
      " 823856/1000000: episode: 2455, duration: 2.785s, episode steps: 535, steps per second: 192, episode reward: 198.393, mean reward:  0.371 [-17.249, 100.000], mean action: 1.366 [0.000, 3.000],  loss: 4.205595, mae: 50.961226, mean_q: 65.054706, mean_eps: 0.100000\n",
      " 824030/1000000: episode: 2456, duration: 0.831s, episode steps: 174, steps per second: 209, episode reward: 290.225, mean reward:  1.668 [-9.178, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 4.259647, mae: 44.241031, mean_q: 59.637419, mean_eps: 0.100000\n",
      " 824405/1000000: episode: 2457, duration: 1.856s, episode steps: 375, steps per second: 202, episode reward: 296.697, mean reward:  0.791 [-9.726, 100.000], mean action: 0.997 [0.000, 3.000],  loss: 4.948695, mae: 48.499028, mean_q: 65.767566, mean_eps: 0.100000\n",
      " 824618/1000000: episode: 2458, duration: 1.033s, episode steps: 213, steps per second: 206, episode reward: 241.973, mean reward:  1.136 [-9.891, 100.000], mean action: 1.423 [0.000, 3.000],  loss: 4.166183, mae: 53.286523, mean_q: 72.204933, mean_eps: 0.100000\n",
      " 825107/1000000: episode: 2459, duration: 2.469s, episode steps: 489, steps per second: 198, episode reward: 246.587, mean reward:  0.504 [-18.734, 100.000], mean action: 1.168 [0.000, 3.000],  loss: 4.053599, mae: 57.164673, mean_q: 77.461961, mean_eps: 0.100000\n",
      " 825755/1000000: episode: 2460, duration: 3.254s, episode steps: 648, steps per second: 199, episode reward: 221.944, mean reward:  0.343 [-17.753, 100.000], mean action: 1.241 [0.000, 3.000],  loss: 2.433257, mae: 46.064752, mean_q: 62.619425, mean_eps: 0.100000\n",
      " 825970/1000000: episode: 2461, duration: 1.031s, episode steps: 215, steps per second: 209, episode reward: 256.917, mean reward:  1.195 [-11.502, 100.000], mean action: 1.326 [0.000, 3.000],  loss: 2.474545, mae: 43.210802, mean_q: 58.901662, mean_eps: 0.100000\n",
      " 826337/1000000: episode: 2462, duration: 1.848s, episode steps: 367, steps per second: 199, episode reward: 237.463, mean reward:  0.647 [-9.579, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 2.080156, mae: 48.003783, mean_q: 65.075265, mean_eps: 0.100000\n",
      " 826545/1000000: episode: 2463, duration: 1.003s, episode steps: 208, steps per second: 207, episode reward: 277.056, mean reward:  1.332 [-9.622, 100.000], mean action: 1.428 [0.000, 3.000],  loss: 3.873251, mae: 53.693534, mean_q: 72.605731, mean_eps: 0.100000\n",
      " 826969/1000000: episode: 2464, duration: 2.116s, episode steps: 424, steps per second: 200, episode reward: 236.898, mean reward:  0.559 [-18.870, 100.000], mean action: 1.064 [0.000, 3.000],  loss: 3.456705, mae: 61.353978, mean_q: 82.795677, mean_eps: 0.100000\n",
      " 827050/1000000: episode: 2465, duration: 0.390s, episode steps:  81, steps per second: 208, episode reward: -27.632, mean reward: -0.341 [-100.000, 17.981], mean action: 1.889 [0.000, 3.000],  loss: 3.135060, mae: 61.049724, mean_q: 82.482694, mean_eps: 0.100000\n",
      " 828050/1000000: episode: 2466, duration: 5.119s, episode steps: 1000, steps per second: 195, episode reward: 139.253, mean reward:  0.139 [-23.328, 23.285], mean action: 1.100 [0.000, 3.000],  loss: 2.569146, mae: 58.236433, mean_q: 78.238892, mean_eps: 0.100000\n",
      " 828444/1000000: episode: 2467, duration: 2.024s, episode steps: 394, steps per second: 195, episode reward: 258.236, mean reward:  0.655 [-18.358, 100.000], mean action: 1.546 [0.000, 3.000],  loss: 2.340642, mae: 48.955371, mean_q: 65.952299, mean_eps: 0.100000\n",
      " 828550/1000000: episode: 2468, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: 27.424, mean reward:  0.259 [-100.000, 18.538], mean action: 1.736 [0.000, 3.000],  loss: 2.968393, mae: 51.495353, mean_q: 69.269507, mean_eps: 0.100000\n",
      " 829277/1000000: episode: 2469, duration: 3.787s, episode steps: 727, steps per second: 192, episode reward: 243.222, mean reward:  0.335 [-20.112, 100.000], mean action: 1.365 [0.000, 3.000],  loss: 4.134801, mae: 54.198435, mean_q: 72.472157, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 829624/1000000: episode: 2470, duration: 1.728s, episode steps: 347, steps per second: 201, episode reward: 255.337, mean reward:  0.736 [-18.630, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 2.476235, mae: 50.805517, mean_q: 67.903760, mean_eps: 0.100000\n",
      " 829983/1000000: episode: 2471, duration: 1.780s, episode steps: 359, steps per second: 202, episode reward: 252.739, mean reward:  0.704 [-9.586, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 1.876192, mae: 49.704542, mean_q: 67.090911, mean_eps: 0.100000\n",
      " 830244/1000000: episode: 2472, duration: 1.274s, episode steps: 261, steps per second: 205, episode reward: 260.567, mean reward:  0.998 [-12.107, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 3.379984, mae: 53.492703, mean_q: 72.223397, mean_eps: 0.100000\n",
      " 830754/1000000: episode: 2473, duration: 2.592s, episode steps: 510, steps per second: 197, episode reward: 260.083, mean reward:  0.510 [-18.764, 100.000], mean action: 2.400 [0.000, 3.000],  loss: 4.169484, mae: 54.386617, mean_q: 73.581086, mean_eps: 0.100000\n",
      " 831124/1000000: episode: 2474, duration: 1.828s, episode steps: 370, steps per second: 202, episode reward: 278.906, mean reward:  0.754 [-17.078, 100.000], mean action: 1.103 [0.000, 3.000],  loss: 3.375456, mae: 52.417300, mean_q: 70.893102, mean_eps: 0.100000\n",
      " 831477/1000000: episode: 2475, duration: 1.733s, episode steps: 353, steps per second: 204, episode reward: 243.303, mean reward:  0.689 [-18.238, 100.000], mean action: 0.841 [0.000, 3.000],  loss: 3.946128, mae: 51.871541, mean_q: 70.231243, mean_eps: 0.100000\n",
      " 831841/1000000: episode: 2476, duration: 1.827s, episode steps: 364, steps per second: 199, episode reward: 211.804, mean reward:  0.582 [-19.468, 100.000], mean action: 2.104 [0.000, 3.000],  loss: 4.187007, mae: 54.442374, mean_q: 73.851075, mean_eps: 0.100000\n",
      " 832251/1000000: episode: 2477, duration: 2.084s, episode steps: 410, steps per second: 197, episode reward: 265.395, mean reward:  0.647 [-19.231, 100.000], mean action: 0.800 [0.000, 3.000],  loss: 4.313229, mae: 51.640064, mean_q: 70.112888, mean_eps: 0.100000\n",
      " 832525/1000000: episode: 2478, duration: 1.323s, episode steps: 274, steps per second: 207, episode reward: 250.237, mean reward:  0.913 [-17.843, 100.000], mean action: 1.099 [0.000, 3.000],  loss: 4.413676, mae: 51.102500, mean_q: 69.301654, mean_eps: 0.100000\n",
      " 833009/1000000: episode: 2479, duration: 2.473s, episode steps: 484, steps per second: 196, episode reward: 261.947, mean reward:  0.541 [-17.471, 100.000], mean action: 0.795 [0.000, 3.000],  loss: 4.756751, mae: 51.425879, mean_q: 69.714337, mean_eps: 0.100000\n",
      " 833430/1000000: episode: 2480, duration: 2.047s, episode steps: 421, steps per second: 206, episode reward: 292.433, mean reward:  0.695 [-19.439, 100.000], mean action: 0.903 [0.000, 3.000],  loss: 4.337596, mae: 49.844301, mean_q: 67.561354, mean_eps: 0.100000\n",
      " 833774/1000000: episode: 2481, duration: 1.679s, episode steps: 344, steps per second: 205, episode reward: 272.899, mean reward:  0.793 [-17.492, 100.000], mean action: 0.840 [0.000, 3.000],  loss: 2.850977, mae: 46.228393, mean_q: 62.698108, mean_eps: 0.100000\n",
      " 834131/1000000: episode: 2482, duration: 1.787s, episode steps: 357, steps per second: 200, episode reward: 189.802, mean reward:  0.532 [-17.986, 100.000], mean action: 2.325 [0.000, 3.000],  loss: 6.470213, mae: 47.447289, mean_q: 64.768176, mean_eps: 0.100000\n",
      " 834385/1000000: episode: 2483, duration: 1.227s, episode steps: 254, steps per second: 207, episode reward: -49.036, mean reward: -0.193 [-100.000, 20.945], mean action: 1.457 [0.000, 3.000],  loss: 6.699572, mae: 46.507498, mean_q: 63.551858, mean_eps: 0.100000\n",
      " 835154/1000000: episode: 2484, duration: 4.060s, episode steps: 769, steps per second: 189, episode reward: 271.284, mean reward:  0.353 [-19.810, 100.000], mean action: 0.836 [0.000, 3.000],  loss: 8.462169, mae: 52.032869, mean_q: 70.790337, mean_eps: 0.100000\n",
      " 835420/1000000: episode: 2485, duration: 1.315s, episode steps: 266, steps per second: 202, episode reward: 259.753, mean reward:  0.977 [-19.069, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 1.879894, mae: 51.056902, mean_q: 69.139728, mean_eps: 0.100000\n",
      " 835779/1000000: episode: 2486, duration: 1.783s, episode steps: 359, steps per second: 201, episode reward: 269.215, mean reward:  0.750 [-17.535, 100.000], mean action: 1.373 [0.000, 3.000],  loss: 3.029409, mae: 47.014553, mean_q: 63.637345, mean_eps: 0.100000\n",
      " 836237/1000000: episode: 2487, duration: 2.338s, episode steps: 458, steps per second: 196, episode reward: 289.668, mean reward:  0.632 [-19.818, 100.000], mean action: 1.138 [0.000, 3.000],  loss: 3.876501, mae: 50.575053, mean_q: 68.378573, mean_eps: 0.100000\n",
      " 836503/1000000: episode: 2488, duration: 1.298s, episode steps: 266, steps per second: 205, episode reward: 310.365, mean reward:  1.167 [-8.206, 100.000], mean action: 1.846 [0.000, 3.000],  loss: 3.946898, mae: 49.674007, mean_q: 67.058340, mean_eps: 0.100000\n",
      " 837071/1000000: episode: 2489, duration: 2.963s, episode steps: 568, steps per second: 192, episode reward: 249.337, mean reward:  0.439 [-9.365, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 4.328506, mae: 50.274306, mean_q: 67.981058, mean_eps: 0.100000\n",
      " 837501/1000000: episode: 2490, duration: 2.135s, episode steps: 430, steps per second: 201, episode reward: 241.441, mean reward:  0.561 [-18.351, 100.000], mean action: 0.742 [0.000, 3.000],  loss: 3.172276, mae: 54.963370, mean_q: 74.116541, mean_eps: 0.100000\n",
      " 837782/1000000: episode: 2491, duration: 1.375s, episode steps: 281, steps per second: 204, episode reward: 269.624, mean reward:  0.960 [-10.446, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 2.340610, mae: 54.300850, mean_q: 73.181059, mean_eps: 0.100000\n",
      " 838035/1000000: episode: 2492, duration: 1.248s, episode steps: 253, steps per second: 203, episode reward: 273.800, mean reward:  1.082 [-2.386, 100.000], mean action: 1.261 [0.000, 3.000],  loss: 3.240698, mae: 56.041658, mean_q: 75.586336, mean_eps: 0.100000\n",
      " 838307/1000000: episode: 2493, duration: 1.317s, episode steps: 272, steps per second: 207, episode reward: 292.603, mean reward:  1.076 [-9.666, 100.000], mean action: 1.312 [0.000, 3.000],  loss: 3.122577, mae: 55.322499, mean_q: 74.605540, mean_eps: 0.100000\n",
      " 839066/1000000: episode: 2494, duration: 3.841s, episode steps: 759, steps per second: 198, episode reward: 147.977, mean reward:  0.195 [-19.607, 100.000], mean action: 1.584 [0.000, 3.000],  loss: 5.459953, mae: 49.950976, mean_q: 66.999380, mean_eps: 0.100000\n",
      " 839354/1000000: episode: 2495, duration: 1.436s, episode steps: 288, steps per second: 201, episode reward: 275.858, mean reward:  0.958 [-13.092, 100.000], mean action: 1.562 [0.000, 3.000],  loss: 8.950966, mae: 34.726832, mean_q: 46.035110, mean_eps: 0.100000\n",
      " 839636/1000000: episode: 2496, duration: 1.364s, episode steps: 282, steps per second: 207, episode reward: 276.525, mean reward:  0.981 [-10.655, 100.000], mean action: 0.982 [0.000, 3.000],  loss: 7.022537, mae: 32.663800, mean_q: 44.375077, mean_eps: 0.100000\n",
      " 839750/1000000: episode: 2497, duration: 0.543s, episode steps: 114, steps per second: 210, episode reward: 21.420, mean reward:  0.188 [-100.000, 18.035], mean action: 1.702 [0.000, 3.000],  loss: 7.783881, mae: 38.127202, mean_q: 51.874042, mean_eps: 0.100000\n",
      " 839869/1000000: episode: 2498, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: 41.621, mean reward:  0.350 [-100.000, 16.814], mean action: 1.966 [0.000, 3.000],  loss: 8.610407, mae: 46.052780, mean_q: 62.569820, mean_eps: 0.100000\n",
      " 840016/1000000: episode: 2499, duration: 0.701s, episode steps: 147, steps per second: 210, episode reward: 35.896, mean reward:  0.244 [-100.000, 17.421], mean action: 1.721 [0.000, 3.000],  loss: 22.215219, mae: 54.648802, mean_q: 73.924606, mean_eps: 0.100000\n",
      " 840154/1000000: episode: 2500, duration: 0.659s, episode steps: 138, steps per second: 209, episode reward: 284.480, mean reward:  2.061 [-17.420, 100.000], mean action: 1.203 [0.000, 3.000],  loss: 25.281945, mae: 62.354137, mean_q: 84.359096, mean_eps: 0.100000\n",
      " 840293/1000000: episode: 2501, duration: 0.660s, episode steps: 139, steps per second: 211, episode reward: 24.530, mean reward:  0.176 [-100.000,  9.884], mean action: 1.417 [0.000, 3.000],  loss: 19.104606, mae: 63.613046, mean_q: 86.061837, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 840476/1000000: episode: 2502, duration: 0.875s, episode steps: 183, steps per second: 209, episode reward: 234.211, mean reward:  1.280 [-9.624, 100.000], mean action: 1.060 [0.000, 3.000],  loss: 18.079161, mae: 66.405828, mean_q: 89.685833, mean_eps: 0.100000\n",
      " 840570/1000000: episode: 2503, duration: 0.448s, episode steps:  94, steps per second: 210, episode reward: 12.158, mean reward:  0.129 [-100.000, 19.034], mean action: 1.383 [0.000, 3.000],  loss: 30.691325, mae: 67.654490, mean_q: 90.576093, mean_eps: 0.100000\n",
      " 840666/1000000: episode: 2504, duration: 0.457s, episode steps:  96, steps per second: 210, episode reward: 24.054, mean reward:  0.251 [-100.000, 17.584], mean action: 1.719 [0.000, 3.000],  loss: 16.624940, mae: 72.616672, mean_q: 96.328177, mean_eps: 0.100000\n",
      " 841170/1000000: episode: 2505, duration: 2.583s, episode steps: 504, steps per second: 195, episode reward: 160.561, mean reward:  0.319 [-19.512, 100.000], mean action: 0.950 [0.000, 3.000],  loss: 15.094151, mae: 68.474133, mean_q: 90.403601, mean_eps: 0.100000\n",
      " 841729/1000000: episode: 2506, duration: 2.799s, episode steps: 559, steps per second: 200, episode reward: 260.256, mean reward:  0.466 [-19.659, 100.000], mean action: 0.821 [0.000, 3.000],  loss: 6.064513, mae: 53.506105, mean_q: 70.455129, mean_eps: 0.100000\n",
      " 842046/1000000: episode: 2507, duration: 1.579s, episode steps: 317, steps per second: 201, episode reward: 263.150, mean reward:  0.830 [-9.162, 100.000], mean action: 1.372 [0.000, 3.000],  loss: 5.131193, mae: 45.424632, mean_q: 61.086197, mean_eps: 0.100000\n",
      " 842217/1000000: episode: 2508, duration: 0.860s, episode steps: 171, steps per second: 199, episode reward: 233.587, mean reward:  1.366 [-7.928, 100.000], mean action: 1.556 [0.000, 3.000],  loss: 6.044295, mae: 53.137360, mean_q: 72.374424, mean_eps: 0.100000\n",
      " 842793/1000000: episode: 2509, duration: 3.056s, episode steps: 576, steps per second: 188, episode reward: 264.748, mean reward:  0.460 [-18.339, 100.000], mean action: 1.024 [0.000, 3.000],  loss: 3.889190, mae: 54.858424, mean_q: 74.385262, mean_eps: 0.100000\n",
      " 843050/1000000: episode: 2510, duration: 1.240s, episode steps: 257, steps per second: 207, episode reward: 275.016, mean reward:  1.070 [-10.208, 100.000], mean action: 1.128 [0.000, 3.000],  loss: 4.886172, mae: 55.050071, mean_q: 74.447604, mean_eps: 0.100000\n",
      " 843320/1000000: episode: 2511, duration: 1.300s, episode steps: 270, steps per second: 208, episode reward: 283.957, mean reward:  1.052 [-18.626, 100.000], mean action: 1.296 [0.000, 3.000],  loss: 3.850965, mae: 54.236366, mean_q: 73.471162, mean_eps: 0.100000\n",
      " 843590/1000000: episode: 2512, duration: 1.306s, episode steps: 270, steps per second: 207, episode reward: 283.060, mean reward:  1.048 [-17.639, 100.000], mean action: 1.367 [0.000, 3.000],  loss: 4.451554, mae: 56.053466, mean_q: 76.196502, mean_eps: 0.100000\n",
      " 843915/1000000: episode: 2513, duration: 1.621s, episode steps: 325, steps per second: 201, episode reward: 252.436, mean reward:  0.777 [-11.213, 100.000], mean action: 1.492 [0.000, 3.000],  loss: 4.695914, mae: 59.685477, mean_q: 80.992848, mean_eps: 0.100000\n",
      " 844610/1000000: episode: 2514, duration: 3.690s, episode steps: 695, steps per second: 188, episode reward: 253.519, mean reward:  0.365 [-19.311, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 3.081338, mae: 56.371728, mean_q: 76.384126, mean_eps: 0.100000\n",
      " 844793/1000000: episode: 2515, duration: 0.876s, episode steps: 183, steps per second: 209, episode reward: 288.558, mean reward:  1.577 [-8.828, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 3.874831, mae: 51.301615, mean_q: 69.587208, mean_eps: 0.100000\n",
      " 845420/1000000: episode: 2516, duration: 3.405s, episode steps: 627, steps per second: 184, episode reward: 268.033, mean reward:  0.427 [-20.236, 100.000], mean action: 1.595 [0.000, 3.000],  loss: 3.426836, mae: 52.897937, mean_q: 71.763794, mean_eps: 0.100000\n",
      " 846302/1000000: episode: 2517, duration: 4.719s, episode steps: 882, steps per second: 187, episode reward: 233.739, mean reward:  0.265 [-17.731, 100.000], mean action: 2.061 [0.000, 3.000],  loss: 2.181898, mae: 49.037253, mean_q: 66.384993, mean_eps: 0.100000\n",
      " 846540/1000000: episode: 2518, duration: 1.148s, episode steps: 238, steps per second: 207, episode reward: 283.912, mean reward:  1.193 [-16.834, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 3.135864, mae: 42.522068, mean_q: 57.700555, mean_eps: 0.100000\n",
      " 846705/1000000: episode: 2519, duration: 0.783s, episode steps: 165, steps per second: 211, episode reward: 244.915, mean reward:  1.484 [-8.977, 100.000], mean action: 1.291 [0.000, 3.000],  loss: 2.941831, mae: 41.011612, mean_q: 55.647628, mean_eps: 0.100000\n",
      " 847028/1000000: episode: 2520, duration: 1.591s, episode steps: 323, steps per second: 203, episode reward: 217.580, mean reward:  0.674 [-11.880, 100.000], mean action: 0.978 [0.000, 3.000],  loss: 5.823189, mae: 46.170211, mean_q: 62.685407, mean_eps: 0.100000\n",
      " 848028/1000000: episode: 2521, duration: 5.248s, episode steps: 1000, steps per second: 191, episode reward: -16.803, mean reward: -0.017 [-20.103, 25.859], mean action: 1.735 [0.000, 3.000],  loss: 3.401304, mae: 44.558097, mean_q: 60.689482, mean_eps: 0.100000\n",
      " 848386/1000000: episode: 2522, duration: 1.768s, episode steps: 358, steps per second: 203, episode reward: 289.422, mean reward:  0.808 [-18.495, 100.000], mean action: 1.318 [0.000, 3.000],  loss: 2.479464, mae: 33.292685, mean_q: 45.579415, mean_eps: 0.100000\n",
      " 848654/1000000: episode: 2523, duration: 1.296s, episode steps: 268, steps per second: 207, episode reward: 265.842, mean reward:  0.992 [-1.885, 100.000], mean action: 1.015 [0.000, 3.000],  loss: 2.500775, mae: 44.587630, mean_q: 60.705414, mean_eps: 0.100000\n",
      " 848961/1000000: episode: 2524, duration: 1.554s, episode steps: 307, steps per second: 198, episode reward: 274.938, mean reward:  0.896 [-18.867, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 3.784182, mae: 55.632224, mean_q: 75.416718, mean_eps: 0.100000\n",
      " 849480/1000000: episode: 2525, duration: 2.694s, episode steps: 519, steps per second: 193, episode reward: 250.883, mean reward:  0.483 [-17.505, 100.000], mean action: 1.202 [0.000, 3.000],  loss: 2.641223, mae: 60.138981, mean_q: 81.421385, mean_eps: 0.100000\n",
      " 849578/1000000: episode: 2526, duration: 0.474s, episode steps:  98, steps per second: 207, episode reward: -165.676, mean reward: -1.691 [-100.000, 15.540], mean action: 1.847 [1.000, 3.000],  loss: 3.009033, mae: 57.402060, mean_q: 77.799085, mean_eps: 0.100000\n",
      " 849671/1000000: episode: 2527, duration: 0.445s, episode steps:  93, steps per second: 209, episode reward: 20.066, mean reward:  0.216 [-100.000, 16.719], mean action: 1.946 [0.000, 3.000],  loss: 14.799167, mae: 60.317920, mean_q: 81.836972, mean_eps: 0.100000\n",
      " 849966/1000000: episode: 2528, duration: 1.431s, episode steps: 295, steps per second: 206, episode reward: 250.144, mean reward:  0.848 [-11.331, 100.000], mean action: 0.736 [0.000, 3.000],  loss: 17.416071, mae: 60.118476, mean_q: 81.673134, mean_eps: 0.100000\n",
      " 850412/1000000: episode: 2529, duration: 2.257s, episode steps: 446, steps per second: 198, episode reward: 245.022, mean reward:  0.549 [-19.272, 100.000], mean action: 1.294 [0.000, 3.000],  loss: 8.661882, mae: 59.144657, mean_q: 80.169070, mean_eps: 0.100000\n",
      " 850712/1000000: episode: 2530, duration: 1.480s, episode steps: 300, steps per second: 203, episode reward: 255.985, mean reward:  0.853 [-18.025, 100.000], mean action: 1.373 [0.000, 3.000],  loss: 7.032193, mae: 58.316254, mean_q: 79.150632, mean_eps: 0.100000\n",
      " 851094/1000000: episode: 2531, duration: 1.918s, episode steps: 382, steps per second: 199, episode reward: 235.714, mean reward:  0.617 [-17.504, 100.000], mean action: 0.995 [0.000, 3.000],  loss: 3.864551, mae: 56.121319, mean_q: 76.286366, mean_eps: 0.100000\n",
      " 852094/1000000: episode: 2532, duration: 5.275s, episode steps: 1000, steps per second: 190, episode reward: 112.066, mean reward:  0.112 [-18.191, 14.764], mean action: 0.939 [0.000, 3.000],  loss: 2.564607, mae: 52.169743, mean_q: 70.757605, mean_eps: 0.100000\n",
      " 852251/1000000: episode: 2533, duration: 0.750s, episode steps: 157, steps per second: 209, episode reward: 286.808, mean reward:  1.827 [-11.619, 100.000], mean action: 1.153 [0.000, 3.000],  loss: 1.976062, mae: 46.156323, mean_q: 62.579841, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 852579/1000000: episode: 2534, duration: 1.634s, episode steps: 328, steps per second: 201, episode reward: 246.567, mean reward:  0.752 [-10.330, 100.000], mean action: 1.582 [0.000, 3.000],  loss: 3.309000, mae: 50.449251, mean_q: 68.286079, mean_eps: 0.100000\n",
      " 853283/1000000: episode: 2535, duration: 3.593s, episode steps: 704, steps per second: 196, episode reward: 275.038, mean reward:  0.391 [-20.858, 100.000], mean action: 1.080 [0.000, 3.000],  loss: 3.355846, mae: 60.244890, mean_q: 81.714543, mean_eps: 0.100000\n",
      " 853830/1000000: episode: 2536, duration: 2.829s, episode steps: 547, steps per second: 193, episode reward: -213.463, mean reward: -0.390 [-100.000, 25.405], mean action: 1.654 [0.000, 3.000],  loss: 2.776901, mae: 58.048259, mean_q: 79.029667, mean_eps: 0.100000\n",
      " 854329/1000000: episode: 2537, duration: 2.517s, episode steps: 499, steps per second: 198, episode reward: 302.327, mean reward:  0.606 [-9.535, 100.000], mean action: 1.192 [0.000, 3.000],  loss: 5.490870, mae: 57.007859, mean_q: 77.587822, mean_eps: 0.100000\n",
      " 854703/1000000: episode: 2538, duration: 1.889s, episode steps: 374, steps per second: 198, episode reward: -223.273, mean reward: -0.597 [-100.000, 12.213], mean action: 1.866 [0.000, 3.000],  loss: 3.686264, mae: 54.688840, mean_q: 74.049523, mean_eps: 0.100000\n",
      " 855001/1000000: episode: 2539, duration: 1.490s, episode steps: 298, steps per second: 200, episode reward: 292.415, mean reward:  0.981 [-12.984, 100.000], mean action: 1.638 [0.000, 3.000],  loss: 4.514831, mae: 55.788654, mean_q: 74.082280, mean_eps: 0.100000\n",
      " 855344/1000000: episode: 2540, duration: 1.731s, episode steps: 343, steps per second: 198, episode reward: 235.214, mean reward:  0.686 [-13.801, 100.000], mean action: 1.650 [0.000, 3.000],  loss: 4.921539, mae: 56.374229, mean_q: 74.446341, mean_eps: 0.100000\n",
      " 855758/1000000: episode: 2541, duration: 2.096s, episode steps: 414, steps per second: 198, episode reward: 252.738, mean reward:  0.610 [-20.997, 100.000], mean action: 0.894 [0.000, 3.000],  loss: 5.528682, mae: 56.126816, mean_q: 74.522312, mean_eps: 0.100000\n",
      " 855988/1000000: episode: 2542, duration: 1.099s, episode steps: 230, steps per second: 209, episode reward: 271.488, mean reward:  1.180 [-17.581, 100.000], mean action: 1.139 [0.000, 3.000],  loss: 5.381469, mae: 55.824101, mean_q: 76.009952, mean_eps: 0.100000\n",
      " 856185/1000000: episode: 2543, duration: 0.950s, episode steps: 197, steps per second: 207, episode reward: 261.181, mean reward:  1.326 [-9.510, 100.000], mean action: 1.112 [0.000, 3.000],  loss: 3.968468, mae: 57.537341, mean_q: 78.194324, mean_eps: 0.100000\n",
      " 856549/1000000: episode: 2544, duration: 1.799s, episode steps: 364, steps per second: 202, episode reward: 277.305, mean reward:  0.762 [-17.683, 100.000], mean action: 0.860 [0.000, 3.000],  loss: 3.485395, mae: 59.635492, mean_q: 80.707293, mean_eps: 0.100000\n",
      " 856865/1000000: episode: 2545, duration: 1.577s, episode steps: 316, steps per second: 200, episode reward: 281.174, mean reward:  0.890 [-18.347, 100.000], mean action: 1.139 [0.000, 3.000],  loss: 3.291300, mae: 60.958860, mean_q: 82.412399, mean_eps: 0.100000\n",
      " 857189/1000000: episode: 2546, duration: 1.609s, episode steps: 324, steps per second: 201, episode reward: 255.924, mean reward:  0.790 [-18.516, 100.000], mean action: 0.988 [0.000, 3.000],  loss: 4.238532, mae: 55.924394, mean_q: 75.624620, mean_eps: 0.100000\n",
      " 857448/1000000: episode: 2547, duration: 1.255s, episode steps: 259, steps per second: 206, episode reward: 253.346, mean reward:  0.978 [-19.470, 100.000], mean action: 1.166 [0.000, 3.000],  loss: 3.162837, mae: 52.741449, mean_q: 71.349254, mean_eps: 0.100000\n",
      " 857757/1000000: episode: 2548, duration: 1.552s, episode steps: 309, steps per second: 199, episode reward: 256.687, mean reward:  0.831 [-18.228, 100.000], mean action: 1.094 [0.000, 3.000],  loss: 4.086250, mae: 53.553557, mean_q: 72.460806, mean_eps: 0.100000\n",
      " 858105/1000000: episode: 2549, duration: 1.753s, episode steps: 348, steps per second: 199, episode reward: 254.884, mean reward:  0.732 [-18.537, 100.000], mean action: 1.063 [0.000, 3.000],  loss: 3.947496, mae: 53.901534, mean_q: 72.902293, mean_eps: 0.100000\n",
      " 858497/1000000: episode: 2550, duration: 1.935s, episode steps: 392, steps per second: 203, episode reward: 236.466, mean reward:  0.603 [-18.710, 100.000], mean action: 0.788 [0.000, 3.000],  loss: 3.867926, mae: 53.879321, mean_q: 72.877732, mean_eps: 0.100000\n",
      " 858750/1000000: episode: 2551, duration: 1.219s, episode steps: 253, steps per second: 208, episode reward: 285.699, mean reward:  1.129 [-17.764, 100.000], mean action: 1.154 [0.000, 3.000],  loss: 4.985187, mae: 52.279293, mean_q: 70.662656, mean_eps: 0.100000\n",
      " 859154/1000000: episode: 2552, duration: 2.070s, episode steps: 404, steps per second: 195, episode reward: 229.251, mean reward:  0.567 [-24.136, 100.000], mean action: 1.554 [0.000, 3.000],  loss: 2.848219, mae: 55.353619, mean_q: 74.947874, mean_eps: 0.100000\n",
      " 859433/1000000: episode: 2553, duration: 1.342s, episode steps: 279, steps per second: 208, episode reward: 271.719, mean reward:  0.974 [-8.123, 100.000], mean action: 0.810 [0.000, 3.000],  loss: 5.540504, mae: 54.908457, mean_q: 74.409152, mean_eps: 0.100000\n",
      " 859652/1000000: episode: 2554, duration: 1.045s, episode steps: 219, steps per second: 210, episode reward: 283.837, mean reward:  1.296 [-11.719, 100.000], mean action: 1.091 [0.000, 3.000],  loss: 4.039475, mae: 59.210930, mean_q: 80.114192, mean_eps: 0.100000\n",
      " 860057/1000000: episode: 2555, duration: 2.031s, episode steps: 405, steps per second: 199, episode reward: 267.133, mean reward:  0.660 [-18.974, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 5.357314, mae: 57.774578, mean_q: 78.038078, mean_eps: 0.100000\n",
      " 860468/1000000: episode: 2556, duration: 2.041s, episode steps: 411, steps per second: 201, episode reward: 286.540, mean reward:  0.697 [-10.440, 100.000], mean action: 1.268 [0.000, 3.000],  loss: 3.637288, mae: 58.389743, mean_q: 78.924165, mean_eps: 0.100000\n",
      " 860620/1000000: episode: 2557, duration: 0.718s, episode steps: 152, steps per second: 212, episode reward: 256.684, mean reward:  1.689 [-2.690, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 3.346404, mae: 56.507299, mean_q: 76.610461, mean_eps: 0.100000\n",
      " 860936/1000000: episode: 2558, duration: 1.544s, episode steps: 316, steps per second: 205, episode reward: 239.599, mean reward:  0.758 [-9.548, 100.000], mean action: 1.142 [0.000, 3.000],  loss: 3.386571, mae: 56.685616, mean_q: 76.907766, mean_eps: 0.100000\n",
      " 861201/1000000: episode: 2559, duration: 1.298s, episode steps: 265, steps per second: 204, episode reward: 278.286, mean reward:  1.050 [-17.867, 100.000], mean action: 1.626 [0.000, 3.000],  loss: 3.377665, mae: 56.656778, mean_q: 77.066917, mean_eps: 0.100000\n",
      " 861375/1000000: episode: 2560, duration: 0.829s, episode steps: 174, steps per second: 210, episode reward: 285.486, mean reward:  1.641 [-2.655, 100.000], mean action: 1.075 [0.000, 3.000],  loss: 4.902029, mae: 57.041102, mean_q: 77.377348, mean_eps: 0.100000\n",
      " 861577/1000000: episode: 2561, duration: 0.958s, episode steps: 202, steps per second: 211, episode reward: 255.292, mean reward:  1.264 [-12.401, 100.000], mean action: 0.931 [0.000, 3.000],  loss: 4.466336, mae: 59.591856, mean_q: 80.763455, mean_eps: 0.100000\n",
      " 861866/1000000: episode: 2562, duration: 1.411s, episode steps: 289, steps per second: 205, episode reward: 283.498, mean reward:  0.981 [-19.348, 100.000], mean action: 0.896 [0.000, 3.000],  loss: 4.724992, mae: 59.771318, mean_q: 81.212481, mean_eps: 0.100000\n",
      " 861956/1000000: episode: 2563, duration: 0.432s, episode steps:  90, steps per second: 208, episode reward: 49.832, mean reward:  0.554 [-100.000, 15.614], mean action: 1.889 [0.000, 3.000],  loss: 5.633702, mae: 63.804465, mean_q: 86.746398, mean_eps: 0.100000\n",
      " 862044/1000000: episode: 2564, duration: 0.425s, episode steps:  88, steps per second: 207, episode reward: 40.764, mean reward:  0.463 [-100.000, 17.126], mean action: 2.045 [0.000, 3.000],  loss: 22.331194, mae: 65.739859, mean_q: 89.545540, mean_eps: 0.100000\n",
      " 862512/1000000: episode: 2565, duration: 2.331s, episode steps: 468, steps per second: 201, episode reward: 298.723, mean reward:  0.638 [-19.809, 100.000], mean action: 0.720 [0.000, 3.000],  loss: 11.946403, mae: 65.012161, mean_q: 88.698079, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 862723/1000000: episode: 2566, duration: 1.024s, episode steps: 211, steps per second: 206, episode reward: 253.688, mean reward:  1.202 [-10.856, 100.000], mean action: 1.370 [0.000, 3.000],  loss: 7.675034, mae: 58.095917, mean_q: 79.105539, mean_eps: 0.100000\n",
      " 863058/1000000: episode: 2567, duration: 1.641s, episode steps: 335, steps per second: 204, episode reward: 235.597, mean reward:  0.703 [-18.379, 100.000], mean action: 0.746 [0.000, 3.000],  loss: 6.543511, mae: 57.047712, mean_q: 77.446412, mean_eps: 0.100000\n",
      " 863261/1000000: episode: 2568, duration: 0.972s, episode steps: 203, steps per second: 209, episode reward: 260.220, mean reward:  1.282 [-8.687, 100.000], mean action: 1.296 [0.000, 3.000],  loss: 4.470345, mae: 54.865860, mean_q: 75.033382, mean_eps: 0.100000\n",
      " 863409/1000000: episode: 2569, duration: 0.705s, episode steps: 148, steps per second: 210, episode reward:  3.520, mean reward:  0.024 [-100.000, 22.858], mean action: 1.405 [0.000, 3.000],  loss: 6.079369, mae: 58.762236, mean_q: 80.399177, mean_eps: 0.100000\n",
      " 863526/1000000: episode: 2570, duration: 0.564s, episode steps: 117, steps per second: 208, episode reward: 33.039, mean reward:  0.282 [-100.000, 18.393], mean action: 1.564 [0.000, 3.000],  loss: 10.313558, mae: 62.805443, mean_q: 85.450634, mean_eps: 0.100000\n",
      " 863995/1000000: episode: 2571, duration: 2.349s, episode steps: 469, steps per second: 200, episode reward: 264.855, mean reward:  0.565 [-19.364, 100.000], mean action: 0.710 [0.000, 3.000],  loss: 17.777458, mae: 64.382378, mean_q: 87.495294, mean_eps: 0.100000\n",
      " 864108/1000000: episode: 2572, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: 29.717, mean reward:  0.263 [-100.000, 11.584], mean action: 1.301 [0.000, 3.000],  loss: 13.031042, mae: 62.371072, mean_q: 84.816263, mean_eps: 0.100000\n",
      " 864433/1000000: episode: 2573, duration: 1.579s, episode steps: 325, steps per second: 206, episode reward: 250.823, mean reward:  0.772 [-9.270, 100.000], mean action: 0.825 [0.000, 3.000],  loss: 20.176182, mae: 62.063445, mean_q: 84.428550, mean_eps: 0.100000\n",
      " 864529/1000000: episode: 2574, duration: 0.460s, episode steps:  96, steps per second: 208, episode reward: -147.900, mean reward: -1.541 [-100.000,  6.593], mean action: 1.635 [0.000, 3.000],  loss: 10.295254, mae: 57.470951, mean_q: 78.798629, mean_eps: 0.100000\n",
      " 865529/1000000: episode: 2575, duration: 5.150s, episode steps: 1000, steps per second: 194, episode reward: 103.324, mean reward:  0.103 [-17.553, 11.956], mean action: 1.668 [0.000, 3.000],  loss: 11.441285, mae: 42.023994, mean_q: 51.853770, mean_eps: 0.100000\n",
      " 866529/1000000: episode: 2576, duration: 5.950s, episode steps: 1000, steps per second: 168, episode reward: 100.923, mean reward:  0.101 [-23.122, 26.240], mean action: 1.591 [0.000, 3.000],  loss: 2.051947, mae: 31.577666, mean_q: 35.247023, mean_eps: 0.100000\n",
      " 867529/1000000: episode: 2577, duration: 5.616s, episode steps: 1000, steps per second: 178, episode reward: 119.104, mean reward:  0.119 [-18.651, 21.939], mean action: 1.707 [0.000, 3.000],  loss: 1.985170, mae: 45.896885, mean_q: 61.886249, mean_eps: 0.100000\n",
      " 867920/1000000: episode: 2578, duration: 1.952s, episode steps: 391, steps per second: 200, episode reward: 272.753, mean reward:  0.698 [-17.333, 100.000], mean action: 0.931 [0.000, 3.000],  loss: 1.542103, mae: 43.158566, mean_q: 58.210836, mean_eps: 0.100000\n",
      " 868279/1000000: episode: 2579, duration: 1.799s, episode steps: 359, steps per second: 200, episode reward: 300.961, mean reward:  0.838 [-19.597, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 4.078760, mae: 46.807905, mean_q: 63.443192, mean_eps: 0.100000\n",
      " 868546/1000000: episode: 2580, duration: 1.296s, episode steps: 267, steps per second: 206, episode reward: 295.864, mean reward:  1.108 [-9.757, 100.000], mean action: 1.019 [0.000, 3.000],  loss: 5.034175, mae: 52.012040, mean_q: 70.882507, mean_eps: 0.100000\n",
      " 868852/1000000: episode: 2581, duration: 1.504s, episode steps: 306, steps per second: 203, episode reward: 269.701, mean reward:  0.881 [-17.507, 100.000], mean action: 1.003 [0.000, 3.000],  loss: 5.905379, mae: 52.576842, mean_q: 71.434112, mean_eps: 0.100000\n",
      " 869123/1000000: episode: 2582, duration: 1.315s, episode steps: 271, steps per second: 206, episode reward: 271.324, mean reward:  1.001 [-17.521, 100.000], mean action: 0.989 [0.000, 3.000],  loss: 5.859437, mae: 54.414211, mean_q: 73.678442, mean_eps: 0.100000\n",
      " 869735/1000000: episode: 2583, duration: 3.092s, episode steps: 612, steps per second: 198, episode reward: 280.280, mean reward:  0.458 [-19.569, 100.000], mean action: 1.458 [0.000, 3.000],  loss: 4.900486, mae: 51.512144, mean_q: 69.706979, mean_eps: 0.100000\n",
      " 869995/1000000: episode: 2584, duration: 1.264s, episode steps: 260, steps per second: 206, episode reward: 256.285, mean reward:  0.986 [-17.666, 100.000], mean action: 1.008 [0.000, 3.000],  loss: 3.157975, mae: 46.103578, mean_q: 62.705393, mean_eps: 0.100000\n",
      " 870362/1000000: episode: 2585, duration: 1.853s, episode steps: 367, steps per second: 198, episode reward: 224.934, mean reward:  0.613 [-18.928, 100.000], mean action: 1.455 [0.000, 3.000],  loss: 5.955613, mae: 44.590773, mean_q: 61.085271, mean_eps: 0.100000\n",
      " 870600/1000000: episode: 2586, duration: 1.143s, episode steps: 238, steps per second: 208, episode reward: 237.333, mean reward:  0.997 [-18.258, 100.000], mean action: 0.933 [0.000, 3.000],  loss: 9.232229, mae: 46.432101, mean_q: 63.808682, mean_eps: 0.100000\n",
      " 870677/1000000: episode: 2587, duration: 0.368s, episode steps:  77, steps per second: 209, episode reward: -40.049, mean reward: -0.520 [-100.000, 12.366], mean action: 1.818 [0.000, 3.000],  loss: 12.453906, mae: 49.314086, mean_q: 67.816257, mean_eps: 0.100000\n",
      " 871025/1000000: episode: 2588, duration: 1.740s, episode steps: 348, steps per second: 200, episode reward: 269.222, mean reward:  0.774 [-20.524, 100.000], mean action: 1.425 [0.000, 3.000],  loss: 12.575839, mae: 54.703980, mean_q: 75.013835, mean_eps: 0.100000\n",
      " 871236/1000000: episode: 2589, duration: 1.007s, episode steps: 211, steps per second: 209, episode reward: 263.840, mean reward:  1.250 [-9.989, 100.000], mean action: 0.991 [0.000, 3.000],  loss: 9.696455, mae: 57.167035, mean_q: 78.181084, mean_eps: 0.100000\n",
      " 871325/1000000: episode: 2590, duration: 0.425s, episode steps:  89, steps per second: 209, episode reward: -56.679, mean reward: -0.637 [-100.000, 36.103], mean action: 1.843 [0.000, 3.000],  loss: 10.050114, mae: 59.053010, mean_q: 80.190858, mean_eps: 0.100000\n",
      " 871488/1000000: episode: 2591, duration: 0.778s, episode steps: 163, steps per second: 210, episode reward: -2.987, mean reward: -0.018 [-100.000, 13.490], mean action: 1.650 [0.000, 3.000],  loss: 10.472829, mae: 62.205601, mean_q: 83.534709, mean_eps: 0.100000\n",
      " 871807/1000000: episode: 2592, duration: 1.592s, episode steps: 319, steps per second: 200, episode reward: 278.450, mean reward:  0.873 [-19.564, 100.000], mean action: 1.154 [0.000, 3.000],  loss: 14.255873, mae: 65.017880, mean_q: 86.666599, mean_eps: 0.100000\n",
      " 872004/1000000: episode: 2593, duration: 0.952s, episode steps: 197, steps per second: 207, episode reward: 278.757, mean reward:  1.415 [-3.233, 100.000], mean action: 1.365 [0.000, 3.000],  loss: 13.969111, mae: 62.937144, mean_q: 84.183144, mean_eps: 0.100000\n",
      " 872244/1000000: episode: 2594, duration: 1.155s, episode steps: 240, steps per second: 208, episode reward: 250.697, mean reward:  1.045 [-2.509, 100.000], mean action: 0.779 [0.000, 3.000],  loss: 9.530185, mae: 63.236492, mean_q: 84.260330, mean_eps: 0.100000\n",
      " 873244/1000000: episode: 2595, duration: 5.568s, episode steps: 1000, steps per second: 180, episode reward: -24.497, mean reward: -0.024 [-19.775, 26.445], mean action: 1.553 [0.000, 3.000],  loss: 5.349857, mae: 54.307538, mean_q: 73.869221, mean_eps: 0.100000\n",
      " 873628/1000000: episode: 2596, duration: 1.889s, episode steps: 384, steps per second: 203, episode reward: 276.909, mean reward:  0.721 [-19.948, 100.000], mean action: 0.964 [0.000, 3.000],  loss: 3.621075, mae: 37.322512, mean_q: 50.971658, mean_eps: 0.100000\n",
      " 873854/1000000: episode: 2597, duration: 1.092s, episode steps: 226, steps per second: 207, episode reward: 271.455, mean reward:  1.201 [-2.765, 100.000], mean action: 1.642 [0.000, 3.000],  loss: 3.813530, mae: 43.301044, mean_q: 58.912664, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 874154/1000000: episode: 2598, duration: 1.464s, episode steps: 300, steps per second: 205, episode reward: 281.574, mean reward:  0.939 [-10.558, 100.000], mean action: 1.047 [0.000, 3.000],  loss: 3.511605, mae: 51.917685, mean_q: 70.432306, mean_eps: 0.100000\n",
      " 874444/1000000: episode: 2599, duration: 1.405s, episode steps: 290, steps per second: 206, episode reward: 267.792, mean reward:  0.923 [-18.274, 100.000], mean action: 0.845 [0.000, 3.000],  loss: 4.245429, mae: 59.574932, mean_q: 80.719358, mean_eps: 0.100000\n",
      " 874655/1000000: episode: 2600, duration: 1.015s, episode steps: 211, steps per second: 208, episode reward: 275.243, mean reward:  1.304 [-10.484, 100.000], mean action: 0.976 [0.000, 3.000],  loss: 5.066140, mae: 61.847545, mean_q: 83.649409, mean_eps: 0.100000\n",
      " 875093/1000000: episode: 2601, duration: 2.289s, episode steps: 438, steps per second: 191, episode reward: 271.725, mean reward:  0.620 [-13.146, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 4.324997, mae: 60.288314, mean_q: 81.570795, mean_eps: 0.100000\n",
      " 875340/1000000: episode: 2602, duration: 1.184s, episode steps: 247, steps per second: 209, episode reward: 257.049, mean reward:  1.041 [-8.520, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 3.265123, mae: 57.986741, mean_q: 78.390291, mean_eps: 0.100000\n",
      " 875429/1000000: episode: 2603, duration: 0.425s, episode steps:  89, steps per second: 209, episode reward: -22.480, mean reward: -0.253 [-100.000, 34.684], mean action: 1.371 [0.000, 3.000],  loss: 4.319673, mae: 59.012040, mean_q: 79.705074, mean_eps: 0.100000\n",
      " 876133/1000000: episode: 2604, duration: 3.762s, episode steps: 704, steps per second: 187, episode reward: 247.558, mean reward:  0.352 [-19.434, 100.000], mean action: 1.385 [0.000, 3.000],  loss: 13.976799, mae: 57.593005, mean_q: 77.880447, mean_eps: 0.100000\n",
      " 876223/1000000: episode: 2605, duration: 0.428s, episode steps:  90, steps per second: 210, episode reward: -245.052, mean reward: -2.723 [-100.000, 47.168], mean action: 1.078 [0.000, 3.000],  loss: 10.669846, mae: 52.951819, mean_q: 71.570431, mean_eps: 0.100000\n",
      " 876317/1000000: episode: 2606, duration: 0.454s, episode steps:  94, steps per second: 207, episode reward: -305.689, mean reward: -3.252 [-100.000, 63.960], mean action: 1.436 [0.000, 3.000],  loss: 25.183334, mae: 56.076540, mean_q: 75.844977, mean_eps: 0.100000\n",
      " 876389/1000000: episode: 2607, duration: 0.354s, episode steps:  72, steps per second: 204, episode reward: -3.586, mean reward: -0.050 [-100.000, 10.285], mean action: 1.639 [0.000, 3.000],  loss: 20.575302, mae: 59.445977, mean_q: 80.754764, mean_eps: 0.100000\n",
      " 876471/1000000: episode: 2608, duration: 0.403s, episode steps:  82, steps per second: 203, episode reward: -61.277, mean reward: -0.747 [-100.000,  9.584], mean action: 1.439 [0.000, 3.000],  loss: 40.672077, mae: 57.306405, mean_q: 78.134290, mean_eps: 0.100000\n",
      " 876564/1000000: episode: 2609, duration: 0.460s, episode steps:  93, steps per second: 202, episode reward: -47.319, mean reward: -0.509 [-100.000,  7.840], mean action: 1.323 [0.000, 3.000],  loss: 40.663945, mae: 57.115594, mean_q: 77.837134, mean_eps: 0.100000\n",
      " 876670/1000000: episode: 2610, duration: 0.514s, episode steps: 106, steps per second: 206, episode reward: -166.263, mean reward: -1.569 [-100.000, 11.055], mean action: 1.340 [0.000, 3.000],  loss: 25.713575, mae: 60.861032, mean_q: 82.956068, mean_eps: 0.100000\n",
      " 876741/1000000: episode: 2611, duration: 0.338s, episode steps:  71, steps per second: 210, episode reward: -73.526, mean reward: -1.036 [-100.000, 13.380], mean action: 1.211 [0.000, 3.000],  loss: 46.424207, mae: 64.241153, mean_q: 84.003910, mean_eps: 0.100000\n",
      " 876832/1000000: episode: 2612, duration: 0.452s, episode steps:  91, steps per second: 201, episode reward: -54.954, mean reward: -0.604 [-100.000, 12.426], mean action: 1.099 [0.000, 3.000],  loss: 25.542740, mae: 68.632387, mean_q: 89.827232, mean_eps: 0.100000\n",
      " 876936/1000000: episode: 2613, duration: 0.499s, episode steps: 104, steps per second: 208, episode reward: -29.834, mean reward: -0.287 [-100.000, 11.111], mean action: 1.337 [0.000, 3.000],  loss: 25.317742, mae: 70.756771, mean_q: 92.014632, mean_eps: 0.100000\n",
      " 877053/1000000: episode: 2614, duration: 0.562s, episode steps: 117, steps per second: 208, episode reward: 52.106, mean reward:  0.445 [-100.000, 40.766], mean action: 1.607 [0.000, 3.000],  loss: 20.390219, mae: 74.264853, mean_q: 94.856637, mean_eps: 0.100000\n",
      " 877122/1000000: episode: 2615, duration: 0.339s, episode steps:  69, steps per second: 204, episode reward: -72.935, mean reward: -1.057 [-100.000,  9.290], mean action: 1.246 [0.000, 3.000],  loss: 23.772046, mae: 77.148646, mean_q: 94.583705, mean_eps: 0.100000\n",
      " 877211/1000000: episode: 2616, duration: 0.422s, episode steps:  89, steps per second: 211, episode reward: 45.919, mean reward:  0.516 [-100.000, 38.358], mean action: 1.169 [0.000, 3.000],  loss: 23.733122, mae: 78.438595, mean_q: 96.150343, mean_eps: 0.100000\n",
      " 877286/1000000: episode: 2617, duration: 0.359s, episode steps:  75, steps per second: 209, episode reward: -8.296, mean reward: -0.111 [-100.000,  9.926], mean action: 1.560 [0.000, 3.000],  loss: 22.639823, mae: 80.008488, mean_q: 96.718537, mean_eps: 0.100000\n",
      " 877372/1000000: episode: 2618, duration: 0.409s, episode steps:  86, steps per second: 210, episode reward: -73.674, mean reward: -0.857 [-100.000,  9.100], mean action: 0.988 [0.000, 3.000],  loss: 34.978491, mae: 81.240585, mean_q: 98.126805, mean_eps: 0.100000\n",
      " 877462/1000000: episode: 2619, duration: 0.428s, episode steps:  90, steps per second: 210, episode reward: -49.579, mean reward: -0.551 [-100.000, 14.766], mean action: 1.189 [0.000, 3.000],  loss: 26.739330, mae: 79.851348, mean_q: 96.739402, mean_eps: 0.100000\n",
      " 877537/1000000: episode: 2620, duration: 0.356s, episode steps:  75, steps per second: 211, episode reward: -24.706, mean reward: -0.329 [-100.000, 12.622], mean action: 1.293 [0.000, 3.000],  loss: 26.279262, mae: 80.473211, mean_q: 95.825726, mean_eps: 0.100000\n",
      " 877640/1000000: episode: 2621, duration: 0.489s, episode steps: 103, steps per second: 211, episode reward: -77.762, mean reward: -0.755 [-100.000,  7.756], mean action: 0.864 [0.000, 3.000],  loss: 26.031799, mae: 77.516212, mean_q: 94.194969, mean_eps: 0.100000\n",
      " 877755/1000000: episode: 2622, duration: 0.546s, episode steps: 115, steps per second: 211, episode reward: -28.754, mean reward: -0.250 [-100.000,  8.352], mean action: 1.304 [0.000, 3.000],  loss: 20.206129, mae: 78.842636, mean_q: 96.322992, mean_eps: 0.100000\n",
      " 877897/1000000: episode: 2623, duration: 0.669s, episode steps: 142, steps per second: 212, episode reward: 44.806, mean reward:  0.316 [-100.000, 12.701], mean action: 1.486 [0.000, 3.000],  loss: 24.046102, mae: 78.250669, mean_q: 93.789807, mean_eps: 0.100000\n",
      " 877988/1000000: episode: 2624, duration: 0.433s, episode steps:  91, steps per second: 210, episode reward:  1.591, mean reward:  0.017 [-100.000, 17.929], mean action: 1.407 [0.000, 3.000],  loss: 28.213632, mae: 78.475743, mean_q: 95.501803, mean_eps: 0.100000\n",
      " 878411/1000000: episode: 2625, duration: 2.164s, episode steps: 423, steps per second: 196, episode reward: 204.315, mean reward:  0.483 [-17.288, 100.000], mean action: 1.258 [0.000, 3.000],  loss: 14.927997, mae: 70.046295, mean_q: 83.422272, mean_eps: 0.100000\n",
      " 878745/1000000: episode: 2626, duration: 1.672s, episode steps: 334, steps per second: 200, episode reward: 269.494, mean reward:  0.807 [-21.572, 100.000], mean action: 1.135 [0.000, 3.000],  loss: 16.918992, mae: 52.026625, mean_q: 60.517995, mean_eps: 0.100000\n",
      " 879719/1000000: episode: 2627, duration: 5.880s, episode steps: 974, steps per second: 166, episode reward: 108.630, mean reward:  0.112 [-17.355, 100.000], mean action: 1.737 [0.000, 3.000],  loss: 9.827260, mae: 31.140197, mean_q: 40.376620, mean_eps: 0.100000\n",
      " 880719/1000000: episode: 2628, duration: 5.350s, episode steps: 1000, steps per second: 187, episode reward: -14.901, mean reward: -0.015 [-20.840, 22.079], mean action: 1.436 [0.000, 3.000],  loss: 5.416743, mae: 25.445979, mean_q: 34.718888, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 880810/1000000: episode: 2629, duration: 0.439s, episode steps:  91, steps per second: 207, episode reward: -470.957, mean reward: -5.175 [-100.000, 94.141], mean action: 2.275 [0.000, 3.000],  loss: 7.233505, mae: 20.033173, mean_q: 23.810201, mean_eps: 0.100000\n",
      " 880916/1000000: episode: 2630, duration: 0.511s, episode steps: 106, steps per second: 207, episode reward: -37.190, mean reward: -0.351 [-100.000, 19.837], mean action: 2.057 [0.000, 3.000],  loss: 57.720032, mae: 22.395406, mean_q: 23.518144, mean_eps: 0.100000\n",
      " 881145/1000000: episode: 2631, duration: 1.109s, episode steps: 229, steps per second: 206, episode reward: 268.081, mean reward:  1.171 [-3.647, 100.000], mean action: 1.568 [0.000, 3.000],  loss: 24.402307, mae: 29.400855, mean_q: 33.042222, mean_eps: 0.100000\n",
      " 881926/1000000: episode: 2632, duration: 4.038s, episode steps: 781, steps per second: 193, episode reward: 209.241, mean reward:  0.268 [-20.183, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 20.783424, mae: 35.110575, mean_q: 46.626795, mean_eps: 0.100000\n",
      " 882035/1000000: episode: 2633, duration: 0.522s, episode steps: 109, steps per second: 209, episode reward: -330.325, mean reward: -3.031 [-100.000, 81.454], mean action: 1.606 [0.000, 3.000],  loss: 5.949593, mae: 25.012592, mean_q: 35.121944, mean_eps: 0.100000\n",
      " 882133/1000000: episode: 2634, duration: 0.470s, episode steps:  98, steps per second: 208, episode reward: -211.615, mean reward: -2.159 [-100.000, 45.259], mean action: 1.776 [0.000, 3.000],  loss: 37.703637, mae: 27.570267, mean_q: 29.287046, mean_eps: 0.100000\n",
      " 882434/1000000: episode: 2635, duration: 1.494s, episode steps: 301, steps per second: 201, episode reward: 238.504, mean reward:  0.792 [-6.149, 100.000], mean action: 1.508 [0.000, 3.000],  loss: 36.509366, mae: 27.114324, mean_q: 25.788641, mean_eps: 0.100000\n",
      " 882583/1000000: episode: 2636, duration: 0.718s, episode steps: 149, steps per second: 208, episode reward: -77.724, mean reward: -0.522 [-100.000,  7.635], mean action: 1.826 [0.000, 3.000],  loss: 47.690668, mae: 28.534082, mean_q: 26.614119, mean_eps: 0.100000\n",
      " 882647/1000000: episode: 2637, duration: 0.309s, episode steps:  64, steps per second: 207, episode reward: -68.879, mean reward: -1.076 [-100.000, 11.991], mean action: 1.891 [1.000, 3.000],  loss: 52.769806, mae: 36.072107, mean_q: 30.091795, mean_eps: 0.100000\n",
      " 882716/1000000: episode: 2638, duration: 0.336s, episode steps:  69, steps per second: 205, episode reward: -110.331, mean reward: -1.599 [-100.000, 29.589], mean action: 1.928 [0.000, 3.000],  loss: 17.159708, mae: 39.990954, mean_q: 36.171632, mean_eps: 0.100000\n",
      " 882811/1000000: episode: 2639, duration: 0.449s, episode steps:  95, steps per second: 211, episode reward: -31.867, mean reward: -0.335 [-100.000,  5.692], mean action: 1.179 [0.000, 3.000],  loss: 81.456866, mae: 47.042842, mean_q: 46.186363, mean_eps: 0.100000\n",
      " 882975/1000000: episode: 2640, duration: 0.788s, episode steps: 164, steps per second: 208, episode reward: 11.044, mean reward:  0.067 [-100.000, 14.994], mean action: 1.500 [0.000, 3.000],  loss: 50.554351, mae: 52.427541, mean_q: 51.915575, mean_eps: 0.100000\n",
      " 883420/1000000: episode: 2641, duration: 2.182s, episode steps: 445, steps per second: 204, episode reward: 305.947, mean reward:  0.688 [-10.598, 100.000], mean action: 1.088 [0.000, 3.000],  loss: 24.788804, mae: 46.727694, mean_q: 55.505061, mean_eps: 0.100000\n",
      " 883507/1000000: episode: 2642, duration: 0.420s, episode steps:  87, steps per second: 207, episode reward: -29.628, mean reward: -0.341 [-100.000, 16.906], mean action: 1.586 [0.000, 3.000],  loss: 14.856110, mae: 46.912586, mean_q: 58.955718, mean_eps: 0.100000\n",
      " 883689/1000000: episode: 2643, duration: 0.881s, episode steps: 182, steps per second: 206, episode reward: 28.499, mean reward:  0.157 [-100.000, 30.529], mean action: 1.714 [0.000, 3.000],  loss: 11.702911, mae: 46.244418, mean_q: 58.955361, mean_eps: 0.100000\n",
      " 884689/1000000: episode: 2644, duration: 5.518s, episode steps: 1000, steps per second: 181, episode reward: 58.202, mean reward:  0.058 [-22.163, 22.062], mean action: 1.903 [0.000, 3.000],  loss: 15.146861, mae: 31.060745, mean_q: 39.902860, mean_eps: 0.100000\n",
      " 885132/1000000: episode: 2645, duration: 2.246s, episode steps: 443, steps per second: 197, episode reward: 286.938, mean reward:  0.648 [-17.332, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 3.811756, mae: 24.525810, mean_q: 33.937009, mean_eps: 0.100000\n",
      " 885528/1000000: episode: 2646, duration: 2.033s, episode steps: 396, steps per second: 195, episode reward: 198.019, mean reward:  0.500 [-10.263, 100.000], mean action: 1.634 [0.000, 3.000],  loss: 4.019112, mae: 29.233888, mean_q: 40.600188, mean_eps: 0.100000\n",
      " 885758/1000000: episode: 2647, duration: 1.110s, episode steps: 230, steps per second: 207, episode reward: 287.308, mean reward:  1.249 [-11.181, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 5.724665, mae: 33.933998, mean_q: 46.638588, mean_eps: 0.100000\n",
      " 886222/1000000: episode: 2648, duration: 2.355s, episode steps: 464, steps per second: 197, episode reward: 247.449, mean reward:  0.533 [-17.198, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 6.531720, mae: 34.013471, mean_q: 46.294071, mean_eps: 0.100000\n",
      " 886404/1000000: episode: 2649, duration: 0.878s, episode steps: 182, steps per second: 207, episode reward: 274.433, mean reward:  1.508 [-5.813, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 9.174525, mae: 32.410581, mean_q: 44.251071, mean_eps: 0.100000\n",
      " 886754/1000000: episode: 2650, duration: 1.719s, episode steps: 350, steps per second: 204, episode reward: 274.816, mean reward:  0.785 [-17.666, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 9.635412, mae: 36.810540, mean_q: 50.047845, mean_eps: 0.100000\n",
      " 887046/1000000: episode: 2651, duration: 1.433s, episode steps: 292, steps per second: 204, episode reward: 260.911, mean reward:  0.894 [-18.234, 100.000], mean action: 1.171 [0.000, 3.000],  loss: 10.711967, mae: 39.284713, mean_q: 53.420982, mean_eps: 0.100000\n",
      " 887693/1000000: episode: 2652, duration: 3.495s, episode steps: 647, steps per second: 185, episode reward: 253.155, mean reward:  0.391 [-19.227, 100.000], mean action: 1.583 [0.000, 3.000],  loss: 7.355289, mae: 43.042185, mean_q: 58.284132, mean_eps: 0.100000\n",
      " 888062/1000000: episode: 2653, duration: 1.868s, episode steps: 369, steps per second: 198, episode reward: 213.446, mean reward:  0.578 [-14.347, 100.000], mean action: 1.488 [0.000, 3.000],  loss: 4.159890, mae: 37.256270, mean_q: 50.988555, mean_eps: 0.100000\n",
      " 888354/1000000: episode: 2654, duration: 1.431s, episode steps: 292, steps per second: 204, episode reward: 271.900, mean reward:  0.931 [-18.318, 100.000], mean action: 1.705 [0.000, 3.000],  loss: 6.539851, mae: 34.698061, mean_q: 48.089220, mean_eps: 0.100000\n",
      " 888596/1000000: episode: 2655, duration: 1.171s, episode steps: 242, steps per second: 207, episode reward: 286.325, mean reward:  1.183 [-9.731, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 11.299862, mae: 40.766060, mean_q: 56.298015, mean_eps: 0.100000\n",
      " 888792/1000000: episode: 2656, duration: 0.939s, episode steps: 196, steps per second: 209, episode reward: 223.293, mean reward:  1.139 [-8.809, 100.000], mean action: 1.362 [0.000, 3.000],  loss: 15.403818, mae: 44.048700, mean_q: 60.715967, mean_eps: 0.100000\n",
      " 888983/1000000: episode: 2657, duration: 0.916s, episode steps: 191, steps per second: 208, episode reward: 300.821, mean reward:  1.575 [-1.629, 100.000], mean action: 0.979 [0.000, 3.000],  loss: 12.094157, mae: 45.190371, mean_q: 62.237739, mean_eps: 0.100000\n",
      " 889211/1000000: episode: 2658, duration: 1.095s, episode steps: 228, steps per second: 208, episode reward: 244.167, mean reward:  1.071 [-9.545, 100.000], mean action: 0.956 [0.000, 3.000],  loss: 12.182643, mae: 52.019931, mean_q: 71.223407, mean_eps: 0.100000\n",
      " 889578/1000000: episode: 2659, duration: 1.887s, episode steps: 367, steps per second: 195, episode reward: 188.735, mean reward:  0.514 [-12.695, 100.000], mean action: 1.346 [0.000, 3.000],  loss: 8.205897, mae: 51.993662, mean_q: 71.071722, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 889935/1000000: episode: 2660, duration: 1.865s, episode steps: 357, steps per second: 191, episode reward: 208.446, mean reward:  0.584 [-20.280, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 6.502035, mae: 48.395965, mean_q: 66.130098, mean_eps: 0.100000\n",
      " 890174/1000000: episode: 2661, duration: 1.162s, episode steps: 239, steps per second: 206, episode reward: -427.006, mean reward: -1.787 [-100.000, 42.816], mean action: 1.678 [0.000, 3.000],  loss: 5.220248, mae: 46.959621, mean_q: 64.068907, mean_eps: 0.100000\n",
      " 890636/1000000: episode: 2662, duration: 2.296s, episode steps: 462, steps per second: 201, episode reward: 228.256, mean reward:  0.494 [-19.802, 100.000], mean action: 0.760 [0.000, 3.000],  loss: 7.049893, mae: 47.855607, mean_q: 60.857378, mean_eps: 0.100000\n",
      " 891015/1000000: episode: 2663, duration: 1.857s, episode steps: 379, steps per second: 204, episode reward: 258.783, mean reward:  0.683 [-21.450, 100.000], mean action: 0.660 [0.000, 3.000],  loss: 5.332904, mae: 47.571421, mean_q: 60.507163, mean_eps: 0.100000\n",
      " 891344/1000000: episode: 2664, duration: 1.645s, episode steps: 329, steps per second: 200, episode reward: 219.977, mean reward:  0.669 [-9.435, 100.000], mean action: 2.055 [0.000, 3.000],  loss: 3.864915, mae: 46.452871, mean_q: 61.921727, mean_eps: 0.100000\n",
      " 891583/1000000: episode: 2665, duration: 1.166s, episode steps: 239, steps per second: 205, episode reward: 265.445, mean reward:  1.111 [-18.123, 100.000], mean action: 1.159 [0.000, 3.000],  loss: 6.326287, mae: 49.170230, mean_q: 67.447111, mean_eps: 0.100000\n",
      " 891781/1000000: episode: 2666, duration: 0.958s, episode steps: 198, steps per second: 207, episode reward: 255.349, mean reward:  1.290 [-17.390, 100.000], mean action: 1.354 [0.000, 3.000],  loss: 4.850635, mae: 52.803727, mean_q: 72.300913, mean_eps: 0.100000\n",
      " 892297/1000000: episode: 2667, duration: 2.577s, episode steps: 516, steps per second: 200, episode reward: 263.013, mean reward:  0.510 [-19.046, 100.000], mean action: 1.246 [0.000, 3.000],  loss: 4.001550, mae: 53.740728, mean_q: 73.486453, mean_eps: 0.100000\n",
      " 892714/1000000: episode: 2668, duration: 2.168s, episode steps: 417, steps per second: 192, episode reward: 233.534, mean reward:  0.560 [-18.751, 100.000], mean action: 2.417 [0.000, 3.000],  loss: 3.286204, mae: 53.059748, mean_q: 72.375873, mean_eps: 0.100000\n",
      " 893197/1000000: episode: 2669, duration: 2.480s, episode steps: 483, steps per second: 195, episode reward: 260.518, mean reward:  0.539 [-17.418, 100.000], mean action: 1.337 [0.000, 3.000],  loss: 4.245893, mae: 49.351274, mean_q: 67.450843, mean_eps: 0.100000\n",
      " 893542/1000000: episode: 2670, duration: 1.710s, episode steps: 345, steps per second: 202, episode reward: 239.669, mean reward:  0.695 [-19.613, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 3.520120, mae: 51.273467, mean_q: 69.700418, mean_eps: 0.100000\n",
      " 893812/1000000: episode: 2671, duration: 1.308s, episode steps: 270, steps per second: 206, episode reward: 282.398, mean reward:  1.046 [-6.840, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 2.802639, mae: 53.967542, mean_q: 72.921803, mean_eps: 0.100000\n",
      " 894084/1000000: episode: 2672, duration: 1.337s, episode steps: 272, steps per second: 203, episode reward: 263.319, mean reward:  0.968 [-18.620, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 2.852743, mae: 55.996630, mean_q: 75.664948, mean_eps: 0.100000\n",
      " 894302/1000000: episode: 2673, duration: 1.057s, episode steps: 218, steps per second: 206, episode reward: 237.389, mean reward:  1.089 [-7.824, 100.000], mean action: 1.550 [0.000, 3.000],  loss: 4.459593, mae: 58.326656, mean_q: 78.815474, mean_eps: 0.100000\n",
      " 894840/1000000: episode: 2674, duration: 2.772s, episode steps: 538, steps per second: 194, episode reward: 256.713, mean reward:  0.477 [-19.455, 100.000], mean action: 1.379 [0.000, 3.000],  loss: 3.382784, mae: 56.953491, mean_q: 76.824379, mean_eps: 0.100000\n",
      " 895149/1000000: episode: 2675, duration: 1.523s, episode steps: 309, steps per second: 203, episode reward: 268.084, mean reward:  0.868 [-18.629, 100.000], mean action: 1.191 [0.000, 3.000],  loss: 2.713759, mae: 51.364503, mean_q: 69.239047, mean_eps: 0.100000\n",
      " 895400/1000000: episode: 2676, duration: 1.202s, episode steps: 251, steps per second: 209, episode reward: 295.712, mean reward:  1.178 [-19.451, 100.000], mean action: 1.502 [0.000, 3.000],  loss: 3.457558, mae: 51.929421, mean_q: 70.339069, mean_eps: 0.100000\n",
      " 895867/1000000: episode: 2677, duration: 2.328s, episode steps: 467, steps per second: 201, episode reward: 282.046, mean reward:  0.604 [-19.205, 100.000], mean action: 0.702 [0.000, 3.000],  loss: 2.907414, mae: 55.484178, mean_q: 75.308104, mean_eps: 0.100000\n",
      " 896295/1000000: episode: 2678, duration: 2.170s, episode steps: 428, steps per second: 197, episode reward: 268.677, mean reward:  0.628 [-19.964, 100.000], mean action: 1.164 [0.000, 3.000],  loss: 2.346942, mae: 57.186996, mean_q: 77.531248, mean_eps: 0.100000\n",
      " 896497/1000000: episode: 2679, duration: 0.967s, episode steps: 202, steps per second: 209, episode reward: 280.653, mean reward:  1.389 [-2.391, 100.000], mean action: 1.193 [0.000, 3.000],  loss: 2.750645, mae: 53.918954, mean_q: 72.821722, mean_eps: 0.100000\n",
      " 896760/1000000: episode: 2680, duration: 1.271s, episode steps: 263, steps per second: 207, episode reward: 274.704, mean reward:  1.045 [-17.540, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 2.642172, mae: 53.454796, mean_q: 72.155194, mean_eps: 0.100000\n",
      " 897136/1000000: episode: 2681, duration: 1.909s, episode steps: 376, steps per second: 197, episode reward: 229.292, mean reward:  0.610 [-18.307, 100.000], mean action: 1.213 [0.000, 3.000],  loss: 2.742165, mae: 55.022121, mean_q: 74.353647, mean_eps: 0.100000\n",
      " 897467/1000000: episode: 2682, duration: 1.641s, episode steps: 331, steps per second: 202, episode reward: 240.202, mean reward:  0.726 [-3.339, 100.000], mean action: 0.915 [0.000, 3.000],  loss: 3.319532, mae: 57.502309, mean_q: 77.765035, mean_eps: 0.100000\n",
      " 897743/1000000: episode: 2683, duration: 1.363s, episode steps: 276, steps per second: 202, episode reward: 277.733, mean reward:  1.006 [-17.361, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 2.850241, mae: 55.349760, mean_q: 74.806371, mean_eps: 0.100000\n",
      " 897849/1000000: episode: 2684, duration: 0.503s, episode steps: 106, steps per second: 211, episode reward: -143.554, mean reward: -1.354 [-100.000,  7.877], mean action: 1.387 [0.000, 3.000],  loss: 2.559994, mae: 55.376325, mean_q: 74.898417, mean_eps: 0.100000\n",
      " 898708/1000000: episode: 2685, duration: 4.351s, episode steps: 859, steps per second: 197, episode reward: 230.732, mean reward:  0.269 [-17.946, 100.000], mean action: 1.296 [0.000, 3.000],  loss: 11.779326, mae: 55.374621, mean_q: 75.158348, mean_eps: 0.100000\n",
      " 899708/1000000: episode: 2686, duration: 5.598s, episode steps: 1000, steps per second: 179, episode reward: 160.039, mean reward:  0.160 [-19.261, 23.557], mean action: 2.004 [0.000, 3.000],  loss: 1.421330, mae: 46.485131, mean_q: 62.846625, mean_eps: 0.100000\n",
      " 900055/1000000: episode: 2687, duration: 1.726s, episode steps: 347, steps per second: 201, episode reward: 295.878, mean reward:  0.853 [-9.737, 100.000], mean action: 1.723 [0.000, 3.000],  loss: 1.500984, mae: 39.526683, mean_q: 53.401758, mean_eps: 0.100000\n",
      " 900353/1000000: episode: 2688, duration: 1.438s, episode steps: 298, steps per second: 207, episode reward: 299.108, mean reward:  1.004 [-17.925, 100.000], mean action: 1.235 [0.000, 3.000],  loss: 4.221369, mae: 44.563194, mean_q: 60.574409, mean_eps: 0.100000\n",
      " 900505/1000000: episode: 2689, duration: 0.724s, episode steps: 152, steps per second: 210, episode reward: 285.229, mean reward:  1.877 [-1.697, 100.000], mean action: 1.461 [0.000, 3.000],  loss: 4.753148, mae: 51.092767, mean_q: 69.449037, mean_eps: 0.100000\n",
      " 900800/1000000: episode: 2690, duration: 1.430s, episode steps: 295, steps per second: 206, episode reward: 306.169, mean reward:  1.038 [-17.816, 100.000], mean action: 0.973 [0.000, 3.000],  loss: 4.882706, mae: 56.976222, mean_q: 77.523016, mean_eps: 0.100000\n",
      " 901007/1000000: episode: 2691, duration: 0.991s, episode steps: 207, steps per second: 209, episode reward: 237.556, mean reward:  1.148 [-10.944, 100.000], mean action: 1.295 [0.000, 3.000],  loss: 6.221527, mae: 58.997672, mean_q: 79.970710, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 901241/1000000: episode: 2692, duration: 1.127s, episode steps: 234, steps per second: 208, episode reward: 269.259, mean reward:  1.151 [-11.582, 100.000], mean action: 1.350 [0.000, 3.000],  loss: 5.404822, mae: 62.976457, mean_q: 85.236505, mean_eps: 0.100000\n",
      " 901488/1000000: episode: 2693, duration: 1.184s, episode steps: 247, steps per second: 209, episode reward: 288.547, mean reward:  1.168 [-9.707, 100.000], mean action: 1.332 [0.000, 3.000],  loss: 4.411572, mae: 64.928055, mean_q: 87.708022, mean_eps: 0.100000\n",
      " 901688/1000000: episode: 2694, duration: 0.962s, episode steps: 200, steps per second: 208, episode reward: 256.700, mean reward:  1.283 [-11.040, 100.000], mean action: 1.260 [0.000, 3.000],  loss: 3.522600, mae: 63.460385, mean_q: 85.680740, mean_eps: 0.100000\n",
      " 901808/1000000: episode: 2695, duration: 0.570s, episode steps: 120, steps per second: 210, episode reward: 16.932, mean reward:  0.141 [-100.000, 14.605], mean action: 1.492 [0.000, 3.000],  loss: 5.272665, mae: 64.413817, mean_q: 86.964435, mean_eps: 0.100000\n",
      " 902222/1000000: episode: 2696, duration: 2.105s, episode steps: 414, steps per second: 197, episode reward: 273.180, mean reward:  0.660 [-18.128, 100.000], mean action: 1.063 [0.000, 3.000],  loss: 13.639783, mae: 63.562573, mean_q: 86.402436, mean_eps: 0.100000\n",
      " 902528/1000000: episode: 2697, duration: 1.501s, episode steps: 306, steps per second: 204, episode reward: 268.355, mean reward:  0.877 [-17.376, 100.000], mean action: 0.990 [0.000, 3.000],  loss: 8.681056, mae: 61.004468, mean_q: 83.052617, mean_eps: 0.100000\n",
      " 902901/1000000: episode: 2698, duration: 1.863s, episode steps: 373, steps per second: 200, episode reward: 240.172, mean reward:  0.644 [-17.265, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 4.657510, mae: 58.707713, mean_q: 79.896285, mean_eps: 0.100000\n",
      " 903078/1000000: episode: 2699, duration: 0.843s, episode steps: 177, steps per second: 210, episode reward: 270.138, mean reward:  1.526 [-2.205, 100.000], mean action: 1.339 [0.000, 3.000],  loss: 2.537576, mae: 56.674059, mean_q: 77.043198, mean_eps: 0.100000\n",
      " 903378/1000000: episode: 2700, duration: 1.493s, episode steps: 300, steps per second: 201, episode reward: 242.147, mean reward:  0.807 [-12.315, 100.000], mean action: 1.510 [0.000, 3.000],  loss: 4.270745, mae: 58.417565, mean_q: 79.067611, mean_eps: 0.100000\n",
      " 903760/1000000: episode: 2701, duration: 1.918s, episode steps: 382, steps per second: 199, episode reward: 230.983, mean reward:  0.605 [-17.781, 100.000], mean action: 0.783 [0.000, 3.000],  loss: 3.420440, mae: 58.410554, mean_q: 79.011816, mean_eps: 0.100000\n",
      " 903985/1000000: episode: 2702, duration: 1.095s, episode steps: 225, steps per second: 205, episode reward: 289.665, mean reward:  1.287 [-10.575, 100.000], mean action: 1.484 [0.000, 3.000],  loss: 3.048584, mae: 59.628743, mean_q: 80.664154, mean_eps: 0.100000\n",
      " 904304/1000000: episode: 2703, duration: 1.604s, episode steps: 319, steps per second: 199, episode reward: 241.530, mean reward:  0.757 [-9.345, 100.000], mean action: 1.737 [0.000, 3.000],  loss: 3.898667, mae: 58.182229, mean_q: 78.635306, mean_eps: 0.100000\n",
      " 904520/1000000: episode: 2704, duration: 1.030s, episode steps: 216, steps per second: 210, episode reward: 257.248, mean reward:  1.191 [-2.571, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 2.941019, mae: 59.601890, mean_q: 80.541858, mean_eps: 0.100000\n",
      " 904735/1000000: episode: 2705, duration: 1.026s, episode steps: 215, steps per second: 210, episode reward: 263.588, mean reward:  1.226 [-9.387, 100.000], mean action: 1.340 [0.000, 3.000],  loss: 3.666196, mae: 60.517423, mean_q: 81.862033, mean_eps: 0.100000\n",
      " 904852/1000000: episode: 2706, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: 17.125, mean reward:  0.146 [-100.000, 13.307], mean action: 1.504 [0.000, 3.000],  loss: 3.653700, mae: 61.574830, mean_q: 83.450039, mean_eps: 0.100000\n",
      " 905049/1000000: episode: 2707, duration: 0.941s, episode steps: 197, steps per second: 209, episode reward: 264.557, mean reward:  1.343 [-10.055, 100.000], mean action: 1.010 [0.000, 3.000],  loss: 9.422253, mae: 62.942546, mean_q: 85.156576, mean_eps: 0.100000\n",
      " 905489/1000000: episode: 2708, duration: 2.240s, episode steps: 440, steps per second: 196, episode reward: 267.580, mean reward:  0.608 [-20.290, 100.000], mean action: 0.964 [0.000, 3.000],  loss: 6.989310, mae: 62.400491, mean_q: 84.160435, mean_eps: 0.100000\n",
      " 905584/1000000: episode: 2709, duration: 0.454s, episode steps:  95, steps per second: 209, episode reward: 11.544, mean reward:  0.122 [-100.000, 11.216], mean action: 1.432 [0.000, 3.000],  loss: 8.054868, mae: 61.217901, mean_q: 82.466880, mean_eps: 0.100000\n",
      " 906584/1000000: episode: 2710, duration: 5.407s, episode steps: 1000, steps per second: 185, episode reward: 153.894, mean reward:  0.154 [-21.092, 21.897], mean action: 1.027 [0.000, 3.000],  loss: 4.303628, mae: 51.502880, mean_q: 69.702403, mean_eps: 0.100000\n",
      " 906836/1000000: episode: 2711, duration: 1.205s, episode steps: 252, steps per second: 209, episode reward: 281.742, mean reward:  1.118 [-17.562, 100.000], mean action: 1.052 [0.000, 3.000],  loss: 1.709077, mae: 42.100707, mean_q: 57.218357, mean_eps: 0.100000\n",
      " 907021/1000000: episode: 2712, duration: 0.874s, episode steps: 185, steps per second: 212, episode reward: 257.130, mean reward:  1.390 [-10.648, 100.000], mean action: 1.130 [0.000, 3.000],  loss: 1.966494, mae: 47.376253, mean_q: 64.284924, mean_eps: 0.100000\n",
      " 907567/1000000: episode: 2713, duration: 2.661s, episode steps: 546, steps per second: 205, episode reward: 303.646, mean reward:  0.556 [-19.140, 100.000], mean action: 0.822 [0.000, 3.000],  loss: 5.294246, mae: 54.596600, mean_q: 74.468237, mean_eps: 0.100000\n",
      " 907952/1000000: episode: 2714, duration: 1.995s, episode steps: 385, steps per second: 193, episode reward: 266.017, mean reward:  0.691 [-17.423, 100.000], mean action: 1.366 [0.000, 3.000],  loss: 3.769677, mae: 56.483684, mean_q: 77.057344, mean_eps: 0.100000\n",
      " 908175/1000000: episode: 2715, duration: 1.066s, episode steps: 223, steps per second: 209, episode reward: 294.683, mean reward:  1.321 [-17.352, 100.000], mean action: 1.413 [0.000, 3.000],  loss: 2.484722, mae: 53.587977, mean_q: 73.139971, mean_eps: 0.100000\n",
      " 908432/1000000: episode: 2716, duration: 1.230s, episode steps: 257, steps per second: 209, episode reward: 242.177, mean reward:  0.942 [-18.767, 100.000], mean action: 0.860 [0.000, 3.000],  loss: 2.817913, mae: 53.572533, mean_q: 72.627587, mean_eps: 0.100000\n",
      " 908616/1000000: episode: 2717, duration: 0.883s, episode steps: 184, steps per second: 208, episode reward: 276.150, mean reward:  1.501 [-12.254, 100.000], mean action: 1.147 [0.000, 3.000],  loss: 5.499243, mae: 56.155586, mean_q: 76.099362, mean_eps: 0.100000\n",
      " 908802/1000000: episode: 2718, duration: 0.888s, episode steps: 186, steps per second: 209, episode reward: 244.256, mean reward:  1.313 [-8.762, 100.000], mean action: 0.941 [0.000, 3.000],  loss: 3.479920, mae: 56.843751, mean_q: 77.228753, mean_eps: 0.100000\n",
      " 909116/1000000: episode: 2719, duration: 1.532s, episode steps: 314, steps per second: 205, episode reward: 262.788, mean reward:  0.837 [-18.201, 100.000], mean action: 0.806 [0.000, 3.000],  loss: 5.436702, mae: 58.937062, mean_q: 80.072714, mean_eps: 0.100000\n",
      " 909366/1000000: episode: 2720, duration: 1.211s, episode steps: 250, steps per second: 207, episode reward: 306.783, mean reward:  1.227 [-7.613, 100.000], mean action: 1.428 [0.000, 3.000],  loss: 4.728997, mae: 58.130179, mean_q: 79.212336, mean_eps: 0.100000\n",
      " 910277/1000000: episode: 2721, duration: 4.735s, episode steps: 911, steps per second: 192, episode reward: 205.800, mean reward:  0.226 [-19.272, 100.000], mean action: 1.980 [0.000, 3.000],  loss: 3.053017, mae: 55.462544, mean_q: 75.555540, mean_eps: 0.100000\n",
      " 910590/1000000: episode: 2722, duration: 1.574s, episode steps: 313, steps per second: 199, episode reward: 257.216, mean reward:  0.822 [-20.935, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 1.654017, mae: 49.141176, mean_q: 66.588829, mean_eps: 0.100000\n",
      " 911206/1000000: episode: 2723, duration: 3.110s, episode steps: 616, steps per second: 198, episode reward: 254.028, mean reward:  0.412 [-18.037, 100.000], mean action: 0.692 [0.000, 3.000],  loss: 2.492099, mae: 53.624395, mean_q: 72.579841, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 911429/1000000: episode: 2724, duration: 1.087s, episode steps: 223, steps per second: 205, episode reward: -103.930, mean reward: -0.466 [-100.000, 13.777], mean action: 1.865 [0.000, 3.000],  loss: 2.951004, mae: 59.090052, mean_q: 79.910424, mean_eps: 0.100000\n",
      " 912194/1000000: episode: 2725, duration: 4.001s, episode steps: 765, steps per second: 191, episode reward: 189.136, mean reward:  0.247 [-19.451, 100.000], mean action: 1.919 [0.000, 3.000],  loss: 7.588031, mae: 55.922486, mean_q: 76.171832, mean_eps: 0.100000\n",
      " 912433/1000000: episode: 2726, duration: 1.169s, episode steps: 239, steps per second: 205, episode reward: 303.805, mean reward:  1.271 [-7.527, 100.000], mean action: 1.833 [0.000, 3.000],  loss: 6.861238, mae: 49.189321, mean_q: 66.971033, mean_eps: 0.100000\n",
      " 912731/1000000: episode: 2727, duration: 1.476s, episode steps: 298, steps per second: 202, episode reward: 265.706, mean reward:  0.892 [-17.170, 100.000], mean action: 1.285 [0.000, 3.000],  loss: 3.065631, mae: 50.278217, mean_q: 68.487255, mean_eps: 0.100000\n",
      " 912899/1000000: episode: 2728, duration: 0.804s, episode steps: 168, steps per second: 209, episode reward: -101.001, mean reward: -0.601 [-100.000,  6.780], mean action: 1.286 [0.000, 3.000],  loss: 3.542852, mae: 53.893161, mean_q: 73.338725, mean_eps: 0.100000\n",
      " 913370/1000000: episode: 2729, duration: 2.355s, episode steps: 471, steps per second: 200, episode reward: 303.147, mean reward:  0.644 [-17.337, 100.000], mean action: 1.081 [0.000, 3.000],  loss: 2.532059, mae: 59.240755, mean_q: 79.290396, mean_eps: 0.100000\n",
      " 913715/1000000: episode: 2730, duration: 1.798s, episode steps: 345, steps per second: 192, episode reward: 197.393, mean reward:  0.572 [-20.776, 100.000], mean action: 2.270 [0.000, 3.000],  loss: 3.252181, mae: 55.317737, mean_q: 73.075519, mean_eps: 0.100000\n",
      " 914230/1000000: episode: 2731, duration: 2.667s, episode steps: 515, steps per second: 193, episode reward: 244.681, mean reward:  0.475 [-17.499, 100.000], mean action: 1.072 [0.000, 3.000],  loss: 2.736078, mae: 52.560665, mean_q: 70.349723, mean_eps: 0.100000\n",
      " 914514/1000000: episode: 2732, duration: 1.377s, episode steps: 284, steps per second: 206, episode reward: 258.772, mean reward:  0.911 [-19.361, 100.000], mean action: 1.116 [0.000, 3.000],  loss: 3.543544, mae: 49.882974, mean_q: 67.531440, mean_eps: 0.100000\n",
      " 914762/1000000: episode: 2733, duration: 1.208s, episode steps: 248, steps per second: 205, episode reward: -219.316, mean reward: -0.884 [-100.000, 14.720], mean action: 1.637 [0.000, 3.000],  loss: 3.985397, mae: 50.355881, mean_q: 68.278482, mean_eps: 0.100000\n",
      " 915172/1000000: episode: 2734, duration: 2.053s, episode steps: 410, steps per second: 200, episode reward: 297.246, mean reward:  0.725 [-16.154, 100.000], mean action: 1.215 [0.000, 3.000],  loss: 6.933975, mae: 51.388943, mean_q: 69.930392, mean_eps: 0.100000\n",
      " 915417/1000000: episode: 2735, duration: 1.186s, episode steps: 245, steps per second: 207, episode reward: 272.736, mean reward:  1.113 [-21.078, 100.000], mean action: 0.980 [0.000, 3.000],  loss: 4.664111, mae: 55.331293, mean_q: 75.011621, mean_eps: 0.100000\n",
      " 915759/1000000: episode: 2736, duration: 1.714s, episode steps: 342, steps per second: 200, episode reward: 266.047, mean reward:  0.778 [-9.791, 100.000], mean action: 1.389 [0.000, 3.000],  loss: 4.737570, mae: 55.965848, mean_q: 75.098798, mean_eps: 0.100000\n",
      " 915899/1000000: episode: 2737, duration: 0.672s, episode steps: 140, steps per second: 208, episode reward: -1.121, mean reward: -0.008 [-100.000, 15.887], mean action: 1.907 [0.000, 3.000],  loss: 6.033335, mae: 56.087347, mean_q: 76.308155, mean_eps: 0.100000\n",
      " 916146/1000000: episode: 2738, duration: 1.201s, episode steps: 247, steps per second: 206, episode reward: 269.036, mean reward:  1.089 [-7.341, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 10.059999, mae: 59.593352, mean_q: 80.817752, mean_eps: 0.100000\n",
      " 916423/1000000: episode: 2739, duration: 1.341s, episode steps: 277, steps per second: 206, episode reward: 233.844, mean reward:  0.844 [-11.872, 100.000], mean action: 1.332 [0.000, 3.000],  loss: 11.983004, mae: 61.014204, mean_q: 82.635291, mean_eps: 0.100000\n",
      " 916602/1000000: episode: 2740, duration: 0.860s, episode steps: 179, steps per second: 208, episode reward: 247.996, mean reward:  1.385 [-8.520, 100.000], mean action: 1.447 [0.000, 3.000],  loss: 9.416374, mae: 62.008924, mean_q: 83.803515, mean_eps: 0.100000\n",
      " 917098/1000000: episode: 2741, duration: 2.566s, episode steps: 496, steps per second: 193, episode reward: 266.154, mean reward:  0.537 [-19.694, 100.000], mean action: 1.387 [0.000, 3.000],  loss: 6.794916, mae: 61.511693, mean_q: 83.161039, mean_eps: 0.100000\n",
      " 917342/1000000: episode: 2742, duration: 1.193s, episode steps: 244, steps per second: 204, episode reward: 241.872, mean reward:  0.991 [-14.357, 100.000], mean action: 2.316 [0.000, 3.000],  loss: 3.951036, mae: 57.244583, mean_q: 77.625700, mean_eps: 0.100000\n",
      " 917557/1000000: episode: 2743, duration: 1.030s, episode steps: 215, steps per second: 209, episode reward: 247.329, mean reward:  1.150 [-19.991, 100.000], mean action: 1.279 [0.000, 3.000],  loss: 4.886604, mae: 57.103096, mean_q: 77.596783, mean_eps: 0.100000\n",
      " 917953/1000000: episode: 2744, duration: 1.976s, episode steps: 396, steps per second: 200, episode reward: 286.284, mean reward:  0.723 [-9.760, 100.000], mean action: 1.035 [0.000, 3.000],  loss: 5.863517, mae: 55.581985, mean_q: 75.547836, mean_eps: 0.100000\n",
      " 918103/1000000: episode: 2745, duration: 0.723s, episode steps: 150, steps per second: 207, episode reward: -15.560, mean reward: -0.104 [-100.000, 14.945], mean action: 1.807 [0.000, 3.000],  loss: 6.345520, mae: 57.548439, mean_q: 77.991873, mean_eps: 0.100000\n",
      " 918552/1000000: episode: 2746, duration: 2.315s, episode steps: 449, steps per second: 194, episode reward: 263.637, mean reward:  0.587 [-20.446, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 13.003355, mae: 59.054757, mean_q: 79.923574, mean_eps: 0.100000\n",
      " 918959/1000000: episode: 2747, duration: 2.076s, episode steps: 407, steps per second: 196, episode reward: 227.879, mean reward:  0.560 [-21.848, 100.000], mean action: 1.074 [0.000, 3.000],  loss: 6.452610, mae: 57.830650, mean_q: 78.205858, mean_eps: 0.100000\n",
      " 919408/1000000: episode: 2748, duration: 2.244s, episode steps: 449, steps per second: 200, episode reward: 277.995, mean reward:  0.619 [-18.898, 100.000], mean action: 1.196 [0.000, 3.000],  loss: 4.424780, mae: 53.480286, mean_q: 72.573288, mean_eps: 0.100000\n",
      " 919754/1000000: episode: 2749, duration: 1.676s, episode steps: 346, steps per second: 207, episode reward: 280.263, mean reward:  0.810 [-18.868, 100.000], mean action: 1.136 [0.000, 3.000],  loss: 2.714169, mae: 53.883965, mean_q: 73.106150, mean_eps: 0.100000\n",
      " 920385/1000000: episode: 2750, duration: 3.174s, episode steps: 631, steps per second: 199, episode reward: 250.225, mean reward:  0.397 [-23.555, 100.000], mean action: 1.623 [0.000, 3.000],  loss: 2.203452, mae: 55.475669, mean_q: 75.107073, mean_eps: 0.100000\n",
      " 920622/1000000: episode: 2751, duration: 1.151s, episode steps: 237, steps per second: 206, episode reward: 302.549, mean reward:  1.277 [-9.396, 100.000], mean action: 1.367 [0.000, 3.000],  loss: 1.592122, mae: 56.203555, mean_q: 75.982228, mean_eps: 0.100000\n",
      " 921492/1000000: episode: 2752, duration: 4.424s, episode steps: 870, steps per second: 197, episode reward: 214.676, mean reward:  0.247 [-23.033, 100.000], mean action: 1.082 [0.000, 3.000],  loss: 2.194662, mae: 55.275374, mean_q: 74.612290, mean_eps: 0.100000\n",
      " 921912/1000000: episode: 2753, duration: 2.134s, episode steps: 420, steps per second: 197, episode reward: 187.189, mean reward:  0.446 [-17.585, 100.000], mean action: 1.888 [0.000, 3.000],  loss: 1.005989, mae: 50.511121, mean_q: 68.201114, mean_eps: 0.100000\n",
      " 922197/1000000: episode: 2754, duration: 1.385s, episode steps: 285, steps per second: 206, episode reward: -165.693, mean reward: -0.581 [-100.000, 16.398], mean action: 1.691 [0.000, 3.000],  loss: 3.165054, mae: 54.588636, mean_q: 73.987092, mean_eps: 0.100000\n",
      " 922988/1000000: episode: 2755, duration: 4.113s, episode steps: 791, steps per second: 192, episode reward: 258.822, mean reward:  0.327 [-19.855, 100.000], mean action: 1.191 [0.000, 3.000],  loss: 2.675982, mae: 58.671824, mean_q: 77.912232, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 923480/1000000: episode: 2756, duration: 2.584s, episode steps: 492, steps per second: 190, episode reward: 289.845, mean reward:  0.589 [-18.661, 100.000], mean action: 1.234 [0.000, 3.000],  loss: 2.477888, mae: 48.455171, mean_q: 64.120179, mean_eps: 0.100000\n",
      " 923685/1000000: episode: 2757, duration: 1.024s, episode steps: 205, steps per second: 200, episode reward: 302.297, mean reward:  1.475 [-10.166, 100.000], mean action: 1.395 [0.000, 3.000],  loss: 3.123058, mae: 49.790342, mean_q: 67.317709, mean_eps: 0.100000\n",
      " 924106/1000000: episode: 2758, duration: 2.311s, episode steps: 421, steps per second: 182, episode reward: 284.640, mean reward:  0.676 [-20.859, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 4.488134, mae: 56.327249, mean_q: 76.399280, mean_eps: 0.100000\n",
      " 924350/1000000: episode: 2759, duration: 1.440s, episode steps: 244, steps per second: 169, episode reward: 243.268, mean reward:  0.997 [-17.337, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 3.672286, mae: 54.549517, mean_q: 74.119150, mean_eps: 0.100000\n",
      " 924627/1000000: episode: 2760, duration: 1.621s, episode steps: 277, steps per second: 171, episode reward: 256.467, mean reward:  0.926 [-11.601, 100.000], mean action: 1.094 [0.000, 3.000],  loss: 5.705051, mae: 57.355976, mean_q: 78.110875, mean_eps: 0.100000\n",
      " 924852/1000000: episode: 2761, duration: 1.236s, episode steps: 225, steps per second: 182, episode reward: 274.760, mean reward:  1.221 [-10.337, 100.000], mean action: 1.276 [0.000, 3.000],  loss: 5.271169, mae: 58.476736, mean_q: 79.515040, mean_eps: 0.100000\n",
      " 925193/1000000: episode: 2762, duration: 1.745s, episode steps: 341, steps per second: 195, episode reward: 284.613, mean reward:  0.835 [-18.814, 100.000], mean action: 1.088 [0.000, 3.000],  loss: 5.645805, mae: 59.523628, mean_q: 80.970303, mean_eps: 0.100000\n",
      " 925423/1000000: episode: 2763, duration: 1.151s, episode steps: 230, steps per second: 200, episode reward: 313.139, mean reward:  1.361 [-12.319, 100.000], mean action: 1.565 [0.000, 3.000],  loss: 4.253886, mae: 63.044549, mean_q: 85.758113, mean_eps: 0.100000\n",
      " 925867/1000000: episode: 2764, duration: 2.353s, episode steps: 444, steps per second: 189, episode reward: 245.241, mean reward:  0.552 [-20.433, 100.000], mean action: 1.755 [0.000, 3.000],  loss: 6.522854, mae: 61.982396, mean_q: 84.517554, mean_eps: 0.100000\n",
      " 926372/1000000: episode: 2765, duration: 2.656s, episode steps: 505, steps per second: 190, episode reward: 192.943, mean reward:  0.382 [-18.747, 100.000], mean action: 2.269 [0.000, 3.000],  loss: 5.960708, mae: 55.579167, mean_q: 76.222267, mean_eps: 0.100000\n",
      " 926560/1000000: episode: 2766, duration: 1.030s, episode steps: 188, steps per second: 183, episode reward: 278.954, mean reward:  1.484 [-11.647, 100.000], mean action: 1.420 [0.000, 3.000],  loss: 5.657088, mae: 48.318295, mean_q: 66.844847, mean_eps: 0.100000\n",
      " 926941/1000000: episode: 2767, duration: 2.089s, episode steps: 381, steps per second: 182, episode reward: 262.833, mean reward:  0.690 [-11.575, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 5.498939, mae: 51.560319, mean_q: 71.049331, mean_eps: 0.100000\n",
      " 927115/1000000: episode: 2768, duration: 0.896s, episode steps: 174, steps per second: 194, episode reward: 263.822, mean reward:  1.516 [-1.870, 100.000], mean action: 1.115 [0.000, 3.000],  loss: 6.142045, mae: 51.789864, mean_q: 71.082663, mean_eps: 0.100000\n",
      " 927367/1000000: episode: 2769, duration: 1.346s, episode steps: 252, steps per second: 187, episode reward: 285.435, mean reward:  1.133 [-13.267, 100.000], mean action: 0.980 [0.000, 3.000],  loss: 6.475623, mae: 57.574648, mean_q: 78.648493, mean_eps: 0.100000\n",
      " 927801/1000000: episode: 2770, duration: 2.366s, episode steps: 434, steps per second: 183, episode reward: 255.354, mean reward:  0.588 [-20.535, 100.000], mean action: 0.855 [0.000, 3.000],  loss: 3.992672, mae: 59.008932, mean_q: 80.010477, mean_eps: 0.100000\n",
      " 928250/1000000: episode: 2771, duration: 2.461s, episode steps: 449, steps per second: 182, episode reward: 267.523, mean reward:  0.596 [-20.148, 100.000], mean action: 1.849 [0.000, 3.000],  loss: 2.981022, mae: 57.531372, mean_q: 77.712230, mean_eps: 0.100000\n",
      " 928486/1000000: episode: 2772, duration: 1.272s, episode steps: 236, steps per second: 186, episode reward: 275.561, mean reward:  1.168 [-8.486, 100.000], mean action: 1.331 [0.000, 3.000],  loss: 2.381802, mae: 54.934876, mean_q: 74.087647, mean_eps: 0.100000\n",
      " 928659/1000000: episode: 2773, duration: 0.850s, episode steps: 173, steps per second: 204, episode reward: 270.858, mean reward:  1.566 [-9.066, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 2.388874, mae: 57.095004, mean_q: 77.048051, mean_eps: 0.100000\n",
      " 928891/1000000: episode: 2774, duration: 1.266s, episode steps: 232, steps per second: 183, episode reward: 248.286, mean reward:  1.070 [-17.358, 100.000], mean action: 0.987 [0.000, 3.000],  loss: 4.091067, mae: 61.807980, mean_q: 83.291004, mean_eps: 0.100000\n",
      " 929046/1000000: episode: 2775, duration: 0.763s, episode steps: 155, steps per second: 203, episode reward: 281.648, mean reward:  1.817 [-8.695, 100.000], mean action: 1.303 [0.000, 3.000],  loss: 4.514894, mae: 63.162797, mean_q: 85.247276, mean_eps: 0.100000\n",
      " 929377/1000000: episode: 2776, duration: 1.706s, episode steps: 331, steps per second: 194, episode reward: 288.261, mean reward:  0.871 [-17.879, 100.000], mean action: 1.178 [0.000, 3.000],  loss: 3.621889, mae: 68.301816, mean_q: 92.135223, mean_eps: 0.100000\n",
      " 929590/1000000: episode: 2777, duration: 1.203s, episode steps: 213, steps per second: 177, episode reward: -91.635, mean reward: -0.430 [-100.000,  7.072], mean action: 1.296 [0.000, 3.000],  loss: 3.967162, mae: 66.681412, mean_q: 90.080271, mean_eps: 0.100000\n",
      " 929765/1000000: episode: 2778, duration: 0.860s, episode steps: 175, steps per second: 204, episode reward: 241.115, mean reward:  1.378 [-2.868, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 11.214337, mae: 69.144154, mean_q: 93.542945, mean_eps: 0.100000\n",
      " 930398/1000000: episode: 2779, duration: 3.549s, episode steps: 633, steps per second: 178, episode reward: 177.466, mean reward:  0.280 [-18.677, 100.000], mean action: 1.156 [0.000, 3.000],  loss: 8.279539, mae: 61.719403, mean_q: 84.435321, mean_eps: 0.100000\n",
      " 930506/1000000: episode: 2780, duration: 0.535s, episode steps: 108, steps per second: 202, episode reward: -70.505, mean reward: -0.653 [-100.000, 14.260], mean action: 1.898 [0.000, 3.000],  loss: 6.175726, mae: 53.654037, mean_q: 73.371967, mean_eps: 0.100000\n",
      " 930908/1000000: episode: 2781, duration: 2.034s, episode steps: 402, steps per second: 198, episode reward: 249.106, mean reward:  0.620 [-18.584, 100.000], mean action: 0.726 [0.000, 3.000],  loss: 11.937868, mae: 51.973890, mean_q: 71.207914, mean_eps: 0.100000\n",
      " 931213/1000000: episode: 2782, duration: 1.490s, episode steps: 305, steps per second: 205, episode reward: 288.674, mean reward:  0.946 [-18.913, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 7.481019, mae: 55.344347, mean_q: 75.090447, mean_eps: 0.100000\n",
      " 931461/1000000: episode: 2783, duration: 1.233s, episode steps: 248, steps per second: 201, episode reward: 164.704, mean reward:  0.664 [-15.121, 100.000], mean action: 2.343 [0.000, 3.000],  loss: 5.186374, mae: 61.238752, mean_q: 82.504705, mean_eps: 0.100000\n",
      " 931848/1000000: episode: 2784, duration: 1.931s, episode steps: 387, steps per second: 200, episode reward: 245.676, mean reward:  0.635 [-18.762, 100.000], mean action: 1.191 [0.000, 3.000],  loss: 3.288802, mae: 56.314550, mean_q: 76.666384, mean_eps: 0.100000\n",
      " 932475/1000000: episode: 2785, duration: 3.158s, episode steps: 627, steps per second: 199, episode reward: 264.604, mean reward:  0.422 [-20.103, 100.000], mean action: 1.166 [0.000, 3.000],  loss: 3.903424, mae: 54.270644, mean_q: 73.831416, mean_eps: 0.100000\n",
      " 932641/1000000: episode: 2786, duration: 0.835s, episode steps: 166, steps per second: 199, episode reward: 275.401, mean reward:  1.659 [-9.513, 100.000], mean action: 1.452 [0.000, 3.000],  loss: 2.650665, mae: 54.093936, mean_q: 72.928707, mean_eps: 0.100000\n",
      " 932886/1000000: episode: 2787, duration: 1.284s, episode steps: 245, steps per second: 191, episode reward: 286.766, mean reward:  1.170 [-19.121, 100.000], mean action: 1.576 [0.000, 3.000],  loss: 3.293475, mae: 57.808281, mean_q: 78.116046, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 933136/1000000: episode: 2788, duration: 1.262s, episode steps: 250, steps per second: 198, episode reward: 260.534, mean reward:  1.042 [-8.588, 100.000], mean action: 1.268 [0.000, 3.000],  loss: 2.912753, mae: 59.796200, mean_q: 80.821960, mean_eps: 0.100000\n",
      " 933258/1000000: episode: 2789, duration: 0.608s, episode steps: 122, steps per second: 201, episode reward: -17.635, mean reward: -0.145 [-100.000, 11.480], mean action: 1.910 [0.000, 3.000],  loss: 2.037305, mae: 64.113092, mean_q: 86.589548, mean_eps: 0.100000\n",
      " 933468/1000000: episode: 2790, duration: 1.026s, episode steps: 210, steps per second: 205, episode reward: 223.677, mean reward:  1.065 [-10.018, 100.000], mean action: 1.624 [0.000, 3.000],  loss: 9.176091, mae: 68.171805, mean_q: 92.139619, mean_eps: 0.100000\n",
      " 933739/1000000: episode: 2791, duration: 1.338s, episode steps: 271, steps per second: 202, episode reward: 298.186, mean reward:  1.100 [-18.686, 100.000], mean action: 1.572 [0.000, 3.000],  loss: 7.606514, mae: 65.981676, mean_q: 89.288713, mean_eps: 0.100000\n",
      " 934034/1000000: episode: 2792, duration: 1.472s, episode steps: 295, steps per second: 200, episode reward: 256.069, mean reward:  0.868 [-17.698, 100.000], mean action: 1.342 [0.000, 3.000],  loss: 5.765514, mae: 65.418311, mean_q: 88.018765, mean_eps: 0.100000\n",
      " 934158/1000000: episode: 2793, duration: 0.594s, episode steps: 124, steps per second: 209, episode reward:  6.059, mean reward:  0.049 [-100.000, 17.205], mean action: 1.540 [0.000, 3.000],  loss: 6.355123, mae: 64.249536, mean_q: 86.119569, mean_eps: 0.100000\n",
      " 934946/1000000: episode: 2794, duration: 4.591s, episode steps: 788, steps per second: 172, episode reward: 300.299, mean reward:  0.381 [-20.318, 100.000], mean action: 1.918 [0.000, 3.000],  loss: 3.272972, mae: 56.945069, mean_q: 76.148780, mean_eps: 0.100000\n",
      " 935157/1000000: episode: 2795, duration: 1.185s, episode steps: 211, steps per second: 178, episode reward: 269.757, mean reward:  1.278 [-20.401, 100.000], mean action: 1.626 [0.000, 3.000],  loss: 3.748167, mae: 47.702576, mean_q: 63.682101, mean_eps: 0.100000\n",
      " 935506/1000000: episode: 2796, duration: 1.844s, episode steps: 349, steps per second: 189, episode reward: 263.512, mean reward:  0.755 [-19.001, 100.000], mean action: 0.891 [0.000, 3.000],  loss: 3.199702, mae: 48.705916, mean_q: 66.013783, mean_eps: 0.100000\n",
      " 935730/1000000: episode: 2797, duration: 1.292s, episode steps: 224, steps per second: 173, episode reward: 305.558, mean reward:  1.364 [-2.437, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 3.255558, mae: 55.065982, mean_q: 74.599807, mean_eps: 0.100000\n",
      " 935944/1000000: episode: 2798, duration: 1.160s, episode steps: 214, steps per second: 184, episode reward: 228.911, mean reward:  1.070 [-10.073, 100.000], mean action: 1.229 [0.000, 3.000],  loss: 3.829002, mae: 61.045158, mean_q: 82.531499, mean_eps: 0.100000\n",
      " 936509/1000000: episode: 2799, duration: 2.991s, episode steps: 565, steps per second: 189, episode reward: 274.046, mean reward:  0.485 [-18.299, 100.000], mean action: 1.030 [0.000, 3.000],  loss: 2.938826, mae: 61.789781, mean_q: 83.613410, mean_eps: 0.100000\n",
      " 937509/1000000: episode: 2800, duration: 5.579s, episode steps: 1000, steps per second: 179, episode reward: 138.976, mean reward:  0.139 [-19.786, 13.504], mean action: 1.019 [0.000, 3.000],  loss: 1.744916, mae: 53.767908, mean_q: 72.731670, mean_eps: 0.100000\n",
      " 938430/1000000: episode: 2801, duration: 5.641s, episode steps: 921, steps per second: 163, episode reward: 105.282, mean reward:  0.114 [-20.649, 100.000], mean action: 1.577 [0.000, 3.000],  loss: 2.452985, mae: 41.894485, mean_q: 57.230138, mean_eps: 0.100000\n",
      " 939430/1000000: episode: 2802, duration: 5.609s, episode steps: 1000, steps per second: 178, episode reward: 121.871, mean reward:  0.122 [-18.680, 14.528], mean action: 1.951 [0.000, 3.000],  loss: 3.112808, mae: 40.048308, mean_q: 54.705038, mean_eps: 0.100000\n",
      " 940425/1000000: episode: 2803, duration: 5.639s, episode steps: 995, steps per second: 176, episode reward: 174.811, mean reward:  0.176 [-20.275, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 1.913058, mae: 36.469976, mean_q: 49.497828, mean_eps: 0.100000\n",
      " 940643/1000000: episode: 2804, duration: 1.052s, episode steps: 218, steps per second: 207, episode reward: -178.364, mean reward: -0.818 [-100.000,  5.932], mean action: 1.367 [0.000, 3.000],  loss: 4.255600, mae: 29.220580, mean_q: 39.907026, mean_eps: 0.100000\n",
      " 941643/1000000: episode: 2805, duration: 5.308s, episode steps: 1000, steps per second: 188, episode reward: 153.855, mean reward:  0.154 [-20.544, 22.452], mean action: 1.610 [0.000, 3.000],  loss: 5.247943, mae: 34.038798, mean_q: 43.857326, mean_eps: 0.100000\n",
      " 942125/1000000: episode: 2806, duration: 2.520s, episode steps: 482, steps per second: 191, episode reward: 181.968, mean reward:  0.378 [-18.670, 100.000], mean action: 2.199 [0.000, 3.000],  loss: 0.539142, mae: 25.417157, mean_q: 34.742200, mean_eps: 0.100000\n",
      " 942419/1000000: episode: 2807, duration: 1.497s, episode steps: 294, steps per second: 196, episode reward: 237.502, mean reward:  0.808 [-17.465, 100.000], mean action: 1.939 [0.000, 3.000],  loss: 1.541088, mae: 31.535114, mean_q: 43.363461, mean_eps: 0.100000\n",
      " 942830/1000000: episode: 2808, duration: 2.215s, episode steps: 411, steps per second: 186, episode reward: 230.326, mean reward:  0.560 [-20.183, 100.000], mean action: 2.265 [0.000, 3.000],  loss: 5.380016, mae: 38.786746, mean_q: 53.158090, mean_eps: 0.100000\n",
      " 943011/1000000: episode: 2809, duration: 0.898s, episode steps: 181, steps per second: 201, episode reward: 273.805, mean reward:  1.513 [-2.985, 100.000], mean action: 1.503 [0.000, 3.000],  loss: 9.004511, mae: 41.997042, mean_q: 57.350969, mean_eps: 0.100000\n",
      " 943290/1000000: episode: 2810, duration: 1.475s, episode steps: 279, steps per second: 189, episode reward: 249.392, mean reward:  0.894 [-9.432, 100.000], mean action: 1.315 [0.000, 3.000],  loss: 8.110540, mae: 47.241375, mean_q: 64.217957, mean_eps: 0.100000\n",
      " 944076/1000000: episode: 2811, duration: 4.581s, episode steps: 786, steps per second: 172, episode reward: 231.563, mean reward:  0.295 [-18.789, 100.000], mean action: 1.725 [0.000, 3.000],  loss: 8.212473, mae: 39.226585, mean_q: 52.010699, mean_eps: 0.100000\n",
      " 944302/1000000: episode: 2812, duration: 1.219s, episode steps: 226, steps per second: 185, episode reward: 284.707, mean reward:  1.260 [-11.058, 100.000], mean action: 1.434 [0.000, 3.000],  loss: 9.004711, mae: 26.101072, mean_q: 34.580599, mean_eps: 0.100000\n",
      " 944718/1000000: episode: 2813, duration: 2.485s, episode steps: 416, steps per second: 167, episode reward: 270.076, mean reward:  0.649 [-18.161, 100.000], mean action: 0.793 [0.000, 3.000],  loss: 8.741798, mae: 31.696721, mean_q: 42.985382, mean_eps: 0.100000\n",
      " 944972/1000000: episode: 2814, duration: 1.294s, episode steps: 254, steps per second: 196, episode reward: 258.080, mean reward:  1.016 [-17.925, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 7.930639, mae: 42.869869, mean_q: 58.546162, mean_eps: 0.100000\n",
      " 945059/1000000: episode: 2815, duration: 0.454s, episode steps:  87, steps per second: 191, episode reward: -188.347, mean reward: -2.165 [-100.000,  5.050], mean action: 1.448 [0.000, 3.000],  loss: 13.409988, mae: 49.674801, mean_q: 67.721198, mean_eps: 0.100000\n",
      " 945808/1000000: episode: 2816, duration: 4.434s, episode steps: 749, steps per second: 169, episode reward: 277.619, mean reward:  0.371 [-19.315, 100.000], mean action: 0.888 [0.000, 3.000],  loss: 10.120407, mae: 51.165093, mean_q: 68.226602, mean_eps: 0.100000\n",
      " 946202/1000000: episode: 2817, duration: 2.024s, episode steps: 394, steps per second: 195, episode reward: 255.232, mean reward:  0.648 [-19.152, 100.000], mean action: 1.165 [0.000, 3.000],  loss: 3.983855, mae: 50.000051, mean_q: 66.871833, mean_eps: 0.100000\n",
      " 946358/1000000: episode: 2818, duration: 0.748s, episode steps: 156, steps per second: 208, episode reward: -83.656, mean reward: -0.536 [-100.000, 18.876], mean action: 1.532 [0.000, 3.000],  loss: 2.863932, mae: 47.901987, mean_q: 65.302575, mean_eps: 0.100000\n",
      " 946590/1000000: episode: 2819, duration: 1.150s, episode steps: 232, steps per second: 202, episode reward: -184.469, mean reward: -0.795 [-100.000, 19.410], mean action: 1.685 [0.000, 3.000],  loss: 9.957404, mae: 49.845893, mean_q: 68.241315, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 947032/1000000: episode: 2820, duration: 2.497s, episode steps: 442, steps per second: 177, episode reward: 154.515, mean reward:  0.350 [-17.593, 100.000], mean action: 1.450 [0.000, 3.000],  loss: 9.612254, mae: 53.299715, mean_q: 71.717598, mean_eps: 0.100000\n",
      " 947385/1000000: episode: 2821, duration: 1.895s, episode steps: 353, steps per second: 186, episode reward: 215.085, mean reward:  0.609 [-19.584, 100.000], mean action: 1.595 [0.000, 3.000],  loss: 7.728107, mae: 54.496718, mean_q: 72.786907, mean_eps: 0.100000\n",
      " 947883/1000000: episode: 2822, duration: 2.628s, episode steps: 498, steps per second: 189, episode reward: 199.057, mean reward:  0.400 [-17.883, 100.000], mean action: 1.938 [0.000, 3.000],  loss: 4.657045, mae: 51.412331, mean_q: 69.200728, mean_eps: 0.100000\n",
      " 948606/1000000: episode: 2823, duration: 4.349s, episode steps: 723, steps per second: 166, episode reward: 150.029, mean reward:  0.208 [-17.869, 100.000], mean action: 1.628 [0.000, 3.000],  loss: 3.039072, mae: 50.658283, mean_q: 68.731099, mean_eps: 0.100000\n",
      " 948804/1000000: episode: 2824, duration: 1.035s, episode steps: 198, steps per second: 191, episode reward: 245.985, mean reward:  1.242 [-4.131, 100.000], mean action: 1.444 [0.000, 3.000],  loss: 3.010161, mae: 52.602109, mean_q: 71.540879, mean_eps: 0.100000\n",
      " 949804/1000000: episode: 2825, duration: 5.945s, episode steps: 1000, steps per second: 168, episode reward: -41.123, mean reward: -0.041 [-4.646,  4.236], mean action: 1.792 [0.000, 3.000],  loss: 2.103796, mae: 47.138873, mean_q: 64.008559, mean_eps: 0.100000\n",
      " 950140/1000000: episode: 2826, duration: 1.887s, episode steps: 336, steps per second: 178, episode reward: 202.516, mean reward:  0.603 [-19.252, 100.000], mean action: 1.435 [0.000, 3.000],  loss: 2.327656, mae: 41.878454, mean_q: 57.044153, mean_eps: 0.100000\n",
      " 950657/1000000: episode: 2827, duration: 2.661s, episode steps: 517, steps per second: 194, episode reward: 224.711, mean reward:  0.435 [-10.540, 100.000], mean action: 1.226 [0.000, 3.000],  loss: 2.796182, mae: 46.974388, mean_q: 64.365613, mean_eps: 0.100000\n",
      " 950900/1000000: episode: 2828, duration: 1.196s, episode steps: 243, steps per second: 203, episode reward: 218.788, mean reward:  0.900 [-11.250, 100.000], mean action: 2.004 [0.000, 3.000],  loss: 2.708877, mae: 51.267581, mean_q: 70.425947, mean_eps: 0.100000\n",
      " 951900/1000000: episode: 2829, duration: 5.647s, episode steps: 1000, steps per second: 177, episode reward: -44.972, mean reward: -0.045 [-15.194, 14.576], mean action: 1.879 [0.000, 3.000],  loss: 2.204019, mae: 44.079965, mean_q: 59.637729, mean_eps: 0.100000\n",
      " 952176/1000000: episode: 2830, duration: 1.387s, episode steps: 276, steps per second: 199, episode reward: 223.939, mean reward:  0.811 [-12.903, 100.000], mean action: 2.315 [0.000, 3.000],  loss: 3.357551, mae: 34.666764, mean_q: 46.983736, mean_eps: 0.100000\n",
      " 952807/1000000: episode: 2831, duration: 3.290s, episode steps: 631, steps per second: 192, episode reward: -171.925, mean reward: -0.272 [-100.000, 22.374], mean action: 1.388 [0.000, 3.000],  loss: 4.826167, mae: 40.125135, mean_q: 54.577680, mean_eps: 0.100000\n",
      " 953598/1000000: episode: 2832, duration: 4.203s, episode steps: 791, steps per second: 188, episode reward: 155.078, mean reward:  0.196 [-17.581, 100.000], mean action: 1.539 [0.000, 3.000],  loss: 9.490292, mae: 38.592888, mean_q: 52.945305, mean_eps: 0.100000\n",
      " 953875/1000000: episode: 2833, duration: 1.368s, episode steps: 277, steps per second: 202, episode reward: 255.738, mean reward:  0.923 [-12.517, 100.000], mean action: 0.993 [0.000, 3.000],  loss: 4.386844, mae: 38.977462, mean_q: 53.194650, mean_eps: 0.100000\n",
      " 953982/1000000: episode: 2834, duration: 0.521s, episode steps: 107, steps per second: 205, episode reward: -102.960, mean reward: -0.962 [-100.000, 22.108], mean action: 1.776 [0.000, 3.000],  loss: 4.079352, mae: 40.431408, mean_q: 54.959624, mean_eps: 0.100000\n",
      " 954982/1000000: episode: 2835, duration: 5.176s, episode steps: 1000, steps per second: 193, episode reward: -41.446, mean reward: -0.041 [-14.144, 11.700], mean action: 1.547 [0.000, 3.000],  loss: 6.156195, mae: 37.655124, mean_q: 50.716725, mean_eps: 0.100000\n",
      " 955913/1000000: episode: 2836, duration: 4.905s, episode steps: 931, steps per second: 190, episode reward: 113.554, mean reward:  0.122 [-9.748, 100.000], mean action: 1.511 [0.000, 3.000],  loss: 0.713654, mae: 27.717770, mean_q: 37.753540, mean_eps: 0.100000\n",
      " 956004/1000000: episode: 2837, duration: 0.437s, episode steps:  91, steps per second: 208, episode reward: -22.968, mean reward: -0.252 [-100.000, 24.725], mean action: 1.890 [0.000, 3.000],  loss: 2.267897, mae: 30.556895, mean_q: 41.355537, mean_eps: 0.100000\n",
      " 956127/1000000: episode: 2838, duration: 0.584s, episode steps: 123, steps per second: 211, episode reward: 40.408, mean reward:  0.329 [-100.000, 17.392], mean action: 1.886 [0.000, 3.000],  loss: 13.689012, mae: 33.278164, mean_q: 44.943122, mean_eps: 0.100000\n",
      " 956201/1000000: episode: 2839, duration: 0.355s, episode steps:  74, steps per second: 208, episode reward: -44.355, mean reward: -0.599 [-100.000, 12.083], mean action: 1.919 [0.000, 3.000],  loss: 20.449847, mae: 36.874665, mean_q: 50.156448, mean_eps: 0.100000\n",
      " 956332/1000000: episode: 2840, duration: 0.621s, episode steps: 131, steps per second: 211, episode reward:  7.875, mean reward:  0.060 [-100.000, 13.818], mean action: 1.580 [0.000, 3.000],  loss: 9.678255, mae: 40.544221, mean_q: 54.938535, mean_eps: 0.100000\n",
      " 956800/1000000: episode: 2841, duration: 2.420s, episode steps: 468, steps per second: 193, episode reward: 205.663, mean reward:  0.439 [-22.432, 100.000], mean action: 1.197 [0.000, 3.000],  loss: 14.468124, mae: 47.585926, mean_q: 64.371844, mean_eps: 0.100000\n",
      " 956883/1000000: episode: 2842, duration: 0.398s, episode steps:  83, steps per second: 209, episode reward: 19.635, mean reward:  0.237 [-100.000, 19.400], mean action: 1.928 [0.000, 3.000],  loss: 14.527529, mae: 51.813931, mean_q: 70.206281, mean_eps: 0.100000\n",
      " 957110/1000000: episode: 2843, duration: 1.090s, episode steps: 227, steps per second: 208, episode reward: 267.956, mean reward:  1.180 [-8.198, 100.000], mean action: 1.137 [0.000, 3.000],  loss: 8.011398, mae: 53.865726, mean_q: 71.527307, mean_eps: 0.100000\n",
      " 957186/1000000: episode: 2844, duration: 0.364s, episode steps:  76, steps per second: 209, episode reward: -411.675, mean reward: -5.417 [-100.000,  1.554], mean action: 1.618 [0.000, 3.000],  loss: 5.845334, mae: 50.291579, mean_q: 66.887310, mean_eps: 0.100000\n",
      " 957664/1000000: episode: 2845, duration: 2.374s, episode steps: 478, steps per second: 201, episode reward: 280.246, mean reward:  0.586 [-17.319, 100.000], mean action: 1.144 [0.000, 3.000],  loss: 5.687561, mae: 48.513310, mean_q: 65.111099, mean_eps: 0.100000\n",
      " 957914/1000000: episode: 2846, duration: 1.210s, episode steps: 250, steps per second: 207, episode reward: 242.630, mean reward:  0.971 [-17.462, 100.000], mean action: 1.192 [0.000, 3.000],  loss: 7.885415, mae: 44.630836, mean_q: 59.860877, mean_eps: 0.100000\n",
      " 958222/1000000: episode: 2847, duration: 1.521s, episode steps: 308, steps per second: 202, episode reward: 247.281, mean reward:  0.803 [-7.735, 100.000], mean action: 1.247 [0.000, 3.000],  loss: 8.106484, mae: 42.506091, mean_q: 57.393242, mean_eps: 0.100000\n",
      " 958866/1000000: episode: 2848, duration: 3.448s, episode steps: 644, steps per second: 187, episode reward: 165.490, mean reward:  0.257 [-19.810, 100.000], mean action: 1.837 [0.000, 3.000],  loss: 7.349293, mae: 42.454161, mean_q: 57.500645, mean_eps: 0.100000\n",
      " 959142/1000000: episode: 2849, duration: 1.367s, episode steps: 276, steps per second: 202, episode reward: 301.409, mean reward:  1.092 [-17.841, 100.000], mean action: 1.594 [0.000, 3.000],  loss: 6.729366, mae: 41.610744, mean_q: 57.067781, mean_eps: 0.100000\n",
      " 959937/1000000: episode: 2850, duration: 4.063s, episode steps: 795, steps per second: 196, episode reward: 280.134, mean reward:  0.352 [-18.134, 100.000], mean action: 0.752 [0.000, 3.000],  loss: 3.894139, mae: 43.718643, mean_q: 59.995115, mean_eps: 0.100000\n",
      " 960179/1000000: episode: 2851, duration: 1.325s, episode steps: 242, steps per second: 183, episode reward: 276.367, mean reward:  1.142 [-9.651, 100.000], mean action: 1.227 [0.000, 3.000],  loss: 3.150596, mae: 44.520309, mean_q: 60.547411, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 960483/1000000: episode: 2852, duration: 1.564s, episode steps: 304, steps per second: 194, episode reward: 249.843, mean reward:  0.822 [-10.418, 100.000], mean action: 1.102 [0.000, 3.000],  loss: 3.717019, mae: 45.272492, mean_q: 61.632898, mean_eps: 0.100000\n",
      " 960749/1000000: episode: 2853, duration: 1.464s, episode steps: 266, steps per second: 182, episode reward: 260.951, mean reward:  0.981 [-6.339, 100.000], mean action: 0.921 [0.000, 3.000],  loss: 4.160659, mae: 50.964862, mean_q: 69.260497, mean_eps: 0.100000\n",
      " 961014/1000000: episode: 2854, duration: 1.468s, episode steps: 265, steps per second: 180, episode reward: 304.909, mean reward:  1.151 [-20.003, 100.000], mean action: 1.087 [0.000, 3.000],  loss: 6.944752, mae: 54.957904, mean_q: 74.737297, mean_eps: 0.100000\n",
      " 961208/1000000: episode: 2855, duration: 1.135s, episode steps: 194, steps per second: 171, episode reward: 266.145, mean reward:  1.372 [-13.453, 100.000], mean action: 1.196 [0.000, 3.000],  loss: 6.284230, mae: 56.158572, mean_q: 76.273481, mean_eps: 0.100000\n",
      " 961558/1000000: episode: 2856, duration: 1.902s, episode steps: 350, steps per second: 184, episode reward: 251.847, mean reward:  0.720 [-19.217, 100.000], mean action: 0.914 [0.000, 3.000],  loss: 6.205770, mae: 56.293519, mean_q: 76.490636, mean_eps: 0.100000\n",
      " 961768/1000000: episode: 2857, duration: 1.063s, episode steps: 210, steps per second: 198, episode reward: 279.948, mean reward:  1.333 [-17.664, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 7.553927, mae: 56.828872, mean_q: 77.241023, mean_eps: 0.100000\n",
      " 962374/1000000: episode: 2858, duration: 3.106s, episode steps: 606, steps per second: 195, episode reward: 257.118, mean reward:  0.424 [-18.205, 100.000], mean action: 0.881 [0.000, 3.000],  loss: 5.600310, mae: 52.682185, mean_q: 71.396929, mean_eps: 0.100000\n",
      " 962549/1000000: episode: 2859, duration: 0.849s, episode steps: 175, steps per second: 206, episode reward: 277.784, mean reward:  1.587 [-17.974, 100.000], mean action: 1.269 [0.000, 3.000],  loss: 3.234013, mae: 47.790431, mean_q: 65.132993, mean_eps: 0.100000\n",
      " 962935/1000000: episode: 2860, duration: 2.008s, episode steps: 386, steps per second: 192, episode reward: 265.246, mean reward:  0.687 [-19.604, 100.000], mean action: 1.106 [0.000, 3.000],  loss: 4.528149, mae: 48.700435, mean_q: 66.420419, mean_eps: 0.100000\n",
      " 963358/1000000: episode: 2861, duration: 2.259s, episode steps: 423, steps per second: 187, episode reward: 238.339, mean reward:  0.563 [-20.254, 100.000], mean action: 1.433 [0.000, 3.000],  loss: 4.155383, mae: 51.441847, mean_q: 70.152356, mean_eps: 0.100000\n",
      " 963591/1000000: episode: 2862, duration: 1.196s, episode steps: 233, steps per second: 195, episode reward: 291.925, mean reward:  1.253 [-3.296, 100.000], mean action: 1.283 [0.000, 3.000],  loss: 3.745117, mae: 52.689946, mean_q: 71.806969, mean_eps: 0.100000\n",
      " 963823/1000000: episode: 2863, duration: 1.162s, episode steps: 232, steps per second: 200, episode reward: 273.057, mean reward:  1.177 [-17.924, 100.000], mean action: 0.944 [0.000, 3.000],  loss: 4.618270, mae: 52.441947, mean_q: 71.202533, mean_eps: 0.100000\n",
      " 964162/1000000: episode: 2864, duration: 1.721s, episode steps: 339, steps per second: 197, episode reward: 286.428, mean reward:  0.845 [-18.206, 100.000], mean action: 0.850 [0.000, 3.000],  loss: 6.780862, mae: 55.255963, mean_q: 74.962398, mean_eps: 0.100000\n",
      " 964457/1000000: episode: 2865, duration: 1.534s, episode steps: 295, steps per second: 192, episode reward: 228.591, mean reward:  0.775 [-17.394, 100.000], mean action: 1.353 [0.000, 3.000],  loss: 7.318754, mae: 58.237616, mean_q: 78.963924, mean_eps: 0.100000\n",
      " 964855/1000000: episode: 2866, duration: 2.157s, episode steps: 398, steps per second: 184, episode reward: 264.121, mean reward:  0.664 [-9.684, 100.000], mean action: 1.085 [0.000, 3.000],  loss: 5.259710, mae: 54.270845, mean_q: 73.816240, mean_eps: 0.100000\n",
      " 965506/1000000: episode: 2867, duration: 3.503s, episode steps: 651, steps per second: 186, episode reward: 261.863, mean reward:  0.402 [-18.892, 100.000], mean action: 0.848 [0.000, 3.000],  loss: 4.180506, mae: 48.476018, mean_q: 65.945927, mean_eps: 0.100000\n",
      " 966005/1000000: episode: 2868, duration: 2.720s, episode steps: 499, steps per second: 183, episode reward: 299.108, mean reward:  0.599 [-17.331, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 3.424633, mae: 44.384019, mean_q: 60.254254, mean_eps: 0.100000\n",
      " 966378/1000000: episode: 2869, duration: 1.920s, episode steps: 373, steps per second: 194, episode reward: 223.979, mean reward:  0.600 [-18.067, 100.000], mean action: 0.853 [0.000, 3.000],  loss: 4.012004, mae: 46.975603, mean_q: 63.662690, mean_eps: 0.100000\n",
      " 966904/1000000: episode: 2870, duration: 2.763s, episode steps: 526, steps per second: 190, episode reward: 268.179, mean reward:  0.510 [-19.024, 100.000], mean action: 2.101 [0.000, 3.000],  loss: 4.077407, mae: 48.228699, mean_q: 65.763424, mean_eps: 0.100000\n",
      " 967142/1000000: episode: 2871, duration: 1.146s, episode steps: 238, steps per second: 208, episode reward: 271.304, mean reward:  1.140 [-20.729, 100.000], mean action: 0.929 [0.000, 3.000],  loss: 5.087057, mae: 47.204224, mean_q: 64.690850, mean_eps: 0.100000\n",
      " 967388/1000000: episode: 2872, duration: 1.206s, episode steps: 246, steps per second: 204, episode reward: 290.432, mean reward:  1.181 [-11.515, 100.000], mean action: 1.098 [0.000, 3.000],  loss: 5.523228, mae: 52.183392, mean_q: 71.214465, mean_eps: 0.100000\n",
      " 967665/1000000: episode: 2873, duration: 1.389s, episode steps: 277, steps per second: 199, episode reward: 200.849, mean reward:  0.725 [-12.961, 100.000], mean action: 1.458 [0.000, 3.000],  loss: 5.285929, mae: 54.483079, mean_q: 73.958220, mean_eps: 0.100000\n",
      " 967972/1000000: episode: 2874, duration: 1.577s, episode steps: 307, steps per second: 195, episode reward: 276.225, mean reward:  0.900 [-11.224, 100.000], mean action: 1.893 [0.000, 3.000],  loss: 4.514602, mae: 59.606920, mean_q: 80.742248, mean_eps: 0.100000\n",
      " 968223/1000000: episode: 2875, duration: 1.345s, episode steps: 251, steps per second: 187, episode reward: 260.858, mean reward:  1.039 [-11.899, 100.000], mean action: 1.506 [0.000, 3.000],  loss: 5.107109, mae: 58.851015, mean_q: 79.633697, mean_eps: 0.100000\n",
      " 968447/1000000: episode: 2876, duration: 1.221s, episode steps: 224, steps per second: 183, episode reward: 280.739, mean reward:  1.253 [-9.097, 100.000], mean action: 1.379 [0.000, 3.000],  loss: 4.748693, mae: 56.232133, mean_q: 76.152891, mean_eps: 0.100000\n",
      " 969445/1000000: episode: 2877, duration: 5.336s, episode steps: 998, steps per second: 187, episode reward: 235.401, mean reward:  0.236 [-18.481, 100.000], mean action: 1.156 [0.000, 3.000],  loss: 3.069779, mae: 54.415422, mean_q: 73.791854, mean_eps: 0.100000\n",
      " 969673/1000000: episode: 2878, duration: 1.166s, episode steps: 228, steps per second: 196, episode reward: 262.868, mean reward:  1.153 [-17.473, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 1.555508, mae: 47.054078, mean_q: 63.968985, mean_eps: 0.100000\n",
      " 970011/1000000: episode: 2879, duration: 1.735s, episode steps: 338, steps per second: 195, episode reward: 261.320, mean reward:  0.773 [-8.033, 100.000], mean action: 1.175 [0.000, 3.000],  loss: 2.574508, mae: 49.391257, mean_q: 66.993905, mean_eps: 0.100000\n",
      " 970270/1000000: episode: 2880, duration: 1.297s, episode steps: 259, steps per second: 200, episode reward: 287.434, mean reward:  1.110 [-3.039, 100.000], mean action: 1.216 [0.000, 3.000],  loss: 3.387132, mae: 53.325348, mean_q: 72.422019, mean_eps: 0.100000\n",
      " 970524/1000000: episode: 2881, duration: 1.221s, episode steps: 254, steps per second: 208, episode reward: 289.561, mean reward:  1.140 [-2.309, 100.000], mean action: 1.035 [0.000, 3.000],  loss: 5.365625, mae: 57.571215, mean_q: 78.092773, mean_eps: 0.100000\n",
      " 971524/1000000: episode: 2882, duration: 5.085s, episode steps: 1000, steps per second: 197, episode reward: 162.882, mean reward:  0.163 [-20.355, 21.678], mean action: 0.982 [0.000, 3.000],  loss: 2.886144, mae: 52.728939, mean_q: 71.540255, mean_eps: 0.100000\n",
      " 971737/1000000: episode: 2883, duration: 1.043s, episode steps: 213, steps per second: 204, episode reward: 298.308, mean reward:  1.401 [-3.345, 100.000], mean action: 1.423 [0.000, 3.000],  loss: 1.997488, mae: 46.527390, mean_q: 62.985193, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 971954/1000000: episode: 2884, duration: 1.106s, episode steps: 217, steps per second: 196, episode reward: 285.462, mean reward:  1.315 [-9.989, 100.000], mean action: 1.373 [0.000, 3.000],  loss: 1.285546, mae: 50.269335, mean_q: 67.878714, mean_eps: 0.100000\n",
      " 972954/1000000: episode: 2885, duration: 5.470s, episode steps: 1000, steps per second: 183, episode reward: 110.190, mean reward:  0.110 [-23.095, 22.508], mean action: 0.997 [0.000, 3.000],  loss: 2.522868, mae: 52.512802, mean_q: 70.962910, mean_eps: 0.100000\n",
      " 973236/1000000: episode: 2886, duration: 1.519s, episode steps: 282, steps per second: 186, episode reward: 198.696, mean reward:  0.705 [-17.465, 100.000], mean action: 2.280 [0.000, 3.000],  loss: 2.507089, mae: 40.154886, mean_q: 54.306104, mean_eps: 0.100000\n",
      " 973403/1000000: episode: 2887, duration: 0.836s, episode steps: 167, steps per second: 200, episode reward: 258.120, mean reward:  1.546 [-9.511, 100.000], mean action: 1.204 [0.000, 3.000],  loss: 2.617631, mae: 44.440679, mean_q: 60.137654, mean_eps: 0.100000\n",
      " 973793/1000000: episode: 2888, duration: 2.160s, episode steps: 390, steps per second: 181, episode reward: 288.824, mean reward:  0.741 [-9.611, 100.000], mean action: 1.097 [0.000, 3.000],  loss: 4.854703, mae: 52.444431, mean_q: 70.933439, mean_eps: 0.100000\n",
      " 974793/1000000: episode: 2889, duration: 5.929s, episode steps: 1000, steps per second: 169, episode reward: -1.218, mean reward: -0.001 [-21.528, 19.634], mean action: 1.696 [0.000, 3.000],  loss: 2.829056, mae: 42.620568, mean_q: 58.485269, mean_eps: 0.100000\n",
      " 975053/1000000: episode: 2890, duration: 1.391s, episode steps: 260, steps per second: 187, episode reward: 278.707, mean reward:  1.072 [-3.316, 100.000], mean action: 1.446 [0.000, 3.000],  loss: 1.017364, mae: 24.048099, mean_q: 33.507458, mean_eps: 0.100000\n",
      " 975598/1000000: episode: 2891, duration: 2.900s, episode steps: 545, steps per second: 188, episode reward: 236.301, mean reward:  0.434 [-18.863, 100.000], mean action: 1.769 [0.000, 3.000],  loss: 2.288155, mae: 38.999855, mean_q: 53.320267, mean_eps: 0.100000\n",
      " 976026/1000000: episode: 2892, duration: 2.275s, episode steps: 428, steps per second: 188, episode reward: 303.215, mean reward:  0.708 [-19.980, 100.000], mean action: 0.706 [0.000, 3.000],  loss: 3.879675, mae: 53.069522, mean_q: 72.321070, mean_eps: 0.100000\n",
      " 976284/1000000: episode: 2893, duration: 1.333s, episode steps: 258, steps per second: 194, episode reward: 268.286, mean reward:  1.040 [-18.462, 100.000], mean action: 1.000 [0.000, 3.000],  loss: 1.763673, mae: 51.733571, mean_q: 70.553119, mean_eps: 0.100000\n",
      " 976658/1000000: episode: 2894, duration: 1.964s, episode steps: 374, steps per second: 190, episode reward: 253.122, mean reward:  0.677 [-17.758, 100.000], mean action: 0.821 [0.000, 3.000],  loss: 3.489498, mae: 54.886940, mean_q: 74.474733, mean_eps: 0.100000\n",
      " 977338/1000000: episode: 2895, duration: 3.683s, episode steps: 680, steps per second: 185, episode reward: 286.537, mean reward:  0.421 [-19.699, 100.000], mean action: 1.093 [0.000, 3.000],  loss: 3.823552, mae: 52.850949, mean_q: 71.611078, mean_eps: 0.100000\n",
      " 977512/1000000: episode: 2896, duration: 0.855s, episode steps: 174, steps per second: 203, episode reward: 45.171, mean reward:  0.260 [-100.000, 24.164], mean action: 1.994 [0.000, 3.000],  loss: 4.005683, mae: 48.148696, mean_q: 65.510796, mean_eps: 0.100000\n",
      " 977837/1000000: episode: 2897, duration: 1.717s, episode steps: 325, steps per second: 189, episode reward: 286.062, mean reward:  0.880 [-6.551, 100.000], mean action: 1.123 [0.000, 3.000],  loss: 15.645762, mae: 52.370018, mean_q: 71.390748, mean_eps: 0.100000\n",
      " 978256/1000000: episode: 2898, duration: 2.284s, episode steps: 419, steps per second: 183, episode reward: -93.777, mean reward: -0.224 [-100.000, 14.079], mean action: 1.570 [0.000, 3.000],  loss: 14.164050, mae: 52.583636, mean_q: 72.153076, mean_eps: 0.100000\n",
      " 978508/1000000: episode: 2899, duration: 1.252s, episode steps: 252, steps per second: 201, episode reward: 273.438, mean reward:  1.085 [-19.607, 100.000], mean action: 1.119 [0.000, 3.000],  loss: 14.755845, mae: 54.124381, mean_q: 74.095771, mean_eps: 0.100000\n",
      " 978790/1000000: episode: 2900, duration: 1.458s, episode steps: 282, steps per second: 193, episode reward: 277.808, mean reward:  0.985 [-19.477, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 9.871262, mae: 52.328184, mean_q: 71.513367, mean_eps: 0.100000\n",
      " 979013/1000000: episode: 2901, duration: 1.132s, episode steps: 223, steps per second: 197, episode reward: 277.906, mean reward:  1.246 [-10.780, 100.000], mean action: 1.040 [0.000, 3.000],  loss: 8.079088, mae: 53.476480, mean_q: 72.543648, mean_eps: 0.100000\n",
      " 979501/1000000: episode: 2902, duration: 2.691s, episode steps: 488, steps per second: 181, episode reward: 285.323, mean reward:  0.585 [-20.589, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 6.065288, mae: 59.697096, mean_q: 80.627533, mean_eps: 0.100000\n",
      " 979879/1000000: episode: 2903, duration: 1.932s, episode steps: 378, steps per second: 196, episode reward: 245.521, mean reward:  0.650 [-20.578, 100.000], mean action: 0.693 [0.000, 3.000],  loss: 3.751674, mae: 62.313202, mean_q: 84.297407, mean_eps: 0.100000\n",
      " 980056/1000000: episode: 2904, duration: 0.854s, episode steps: 177, steps per second: 207, episode reward: 244.153, mean reward:  1.379 [-2.745, 100.000], mean action: 1.079 [0.000, 3.000],  loss: 4.579468, mae: 58.576510, mean_q: 79.251673, mean_eps: 0.100000\n",
      " 980639/1000000: episode: 2905, duration: 2.904s, episode steps: 583, steps per second: 201, episode reward: 293.027, mean reward:  0.503 [-18.908, 100.000], mean action: 0.923 [0.000, 3.000],  loss: 5.032067, mae: 52.848905, mean_q: 71.603640, mean_eps: 0.100000\n",
      " 980882/1000000: episode: 2906, duration: 1.175s, episode steps: 243, steps per second: 207, episode reward: 268.324, mean reward:  1.104 [-4.184, 100.000], mean action: 0.934 [0.000, 3.000],  loss: 4.749196, mae: 50.911490, mean_q: 69.117556, mean_eps: 0.100000\n",
      " 981338/1000000: episode: 2907, duration: 2.395s, episode steps: 456, steps per second: 190, episode reward: 227.994, mean reward:  0.500 [-12.210, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 2.978730, mae: 51.297408, mean_q: 69.673444, mean_eps: 0.100000\n",
      " 981737/1000000: episode: 2908, duration: 2.200s, episode steps: 399, steps per second: 181, episode reward: 284.800, mean reward:  0.714 [-19.446, 100.000], mean action: 1.120 [0.000, 3.000],  loss: 3.835321, mae: 52.375560, mean_q: 71.171262, mean_eps: 0.100000\n",
      " 982028/1000000: episode: 2909, duration: 1.533s, episode steps: 291, steps per second: 190, episode reward: 272.610, mean reward:  0.937 [-9.588, 100.000], mean action: 1.388 [0.000, 3.000],  loss: 4.257171, mae: 50.242458, mean_q: 68.369259, mean_eps: 0.100000\n",
      " 982111/1000000: episode: 2910, duration: 0.404s, episode steps:  83, steps per second: 206, episode reward: 14.993, mean reward:  0.181 [-100.000, 22.322], mean action: 1.590 [0.000, 3.000],  loss: 4.305523, mae: 50.907858, mean_q: 69.272580, mean_eps: 0.100000\n",
      " 982536/1000000: episode: 2911, duration: 2.120s, episode steps: 425, steps per second: 200, episode reward: 265.036, mean reward:  0.624 [-17.680, 100.000], mean action: 0.734 [0.000, 3.000],  loss: 12.001456, mae: 53.338263, mean_q: 73.064615, mean_eps: 0.100000\n",
      " 982754/1000000: episode: 2912, duration: 1.188s, episode steps: 218, steps per second: 183, episode reward: 267.613, mean reward:  1.228 [-9.376, 100.000], mean action: 0.986 [0.000, 3.000],  loss: 4.171157, mae: 56.661349, mean_q: 77.042286, mean_eps: 0.100000\n",
      " 982936/1000000: episode: 2913, duration: 0.929s, episode steps: 182, steps per second: 196, episode reward: 276.969, mean reward:  1.522 [-9.303, 100.000], mean action: 0.989 [0.000, 3.000],  loss: 5.030597, mae: 58.236165, mean_q: 78.879777, mean_eps: 0.100000\n",
      " 983058/1000000: episode: 2914, duration: 0.644s, episode steps: 122, steps per second: 189, episode reward: 13.323, mean reward:  0.109 [-100.000, 14.347], mean action: 1.508 [0.000, 3.000],  loss: 4.419241, mae: 61.895685, mean_q: 83.778954, mean_eps: 0.100000\n",
      " 983300/1000000: episode: 2915, duration: 1.229s, episode steps: 242, steps per second: 197, episode reward: 275.088, mean reward:  1.137 [-10.627, 100.000], mean action: 0.760 [0.000, 3.000],  loss: 8.278933, mae: 63.233794, mean_q: 85.593156, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 983461/1000000: episode: 2916, duration: 0.878s, episode steps: 161, steps per second: 183, episode reward: 262.909, mean reward:  1.633 [-9.202, 100.000], mean action: 1.068 [0.000, 3.000],  loss: 5.986535, mae: 66.133813, mean_q: 89.471192, mean_eps: 0.100000\n",
      " 983738/1000000: episode: 2917, duration: 1.412s, episode steps: 277, steps per second: 196, episode reward: 281.399, mean reward:  1.016 [-10.793, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 8.647481, mae: 67.660162, mean_q: 91.520542, mean_eps: 0.100000\n",
      " 983954/1000000: episode: 2918, duration: 1.135s, episode steps: 216, steps per second: 190, episode reward: 282.560, mean reward:  1.308 [-6.622, 100.000], mean action: 0.829 [0.000, 3.000],  loss: 8.737578, mae: 66.255523, mean_q: 89.437714, mean_eps: 0.100000\n",
      " 984102/1000000: episode: 2919, duration: 0.721s, episode steps: 148, steps per second: 205, episode reward: 44.964, mean reward:  0.304 [-100.000, 21.394], mean action: 1.318 [0.000, 3.000],  loss: 7.173527, mae: 65.956207, mean_q: 88.937562, mean_eps: 0.100000\n",
      " 984726/1000000: episode: 2920, duration: 3.548s, episode steps: 624, steps per second: 176, episode reward: 287.205, mean reward:  0.460 [-23.986, 100.000], mean action: 1.367 [0.000, 3.000],  loss: 12.381945, mae: 64.850582, mean_q: 87.488250, mean_eps: 0.100000\n",
      " 984822/1000000: episode: 2921, duration: 0.523s, episode steps:  96, steps per second: 183, episode reward: -126.664, mean reward: -1.319 [-100.000, 15.321], mean action: 1.875 [0.000, 3.000],  loss: 14.680011, mae: 59.558604, mean_q: 80.107559, mean_eps: 0.100000\n",
      " 985279/1000000: episode: 2922, duration: 2.462s, episode steps: 457, steps per second: 186, episode reward: 268.090, mean reward:  0.587 [-19.638, 100.000], mean action: 1.604 [0.000, 3.000],  loss: 8.593792, mae: 55.351423, mean_q: 73.275369, mean_eps: 0.100000\n",
      " 985827/1000000: episode: 2923, duration: 2.863s, episode steps: 548, steps per second: 191, episode reward: 233.237, mean reward:  0.426 [-19.630, 100.000], mean action: 0.759 [0.000, 3.000],  loss: 7.577698, mae: 55.441380, mean_q: 73.227078, mean_eps: 0.100000\n",
      " 986031/1000000: episode: 2924, duration: 1.002s, episode steps: 204, steps per second: 204, episode reward: -245.719, mean reward: -1.205 [-100.000,  4.905], mean action: 1.725 [0.000, 3.000],  loss: 3.559994, mae: 53.862493, mean_q: 73.066279, mean_eps: 0.100000\n",
      " 986363/1000000: episode: 2925, duration: 1.700s, episode steps: 332, steps per second: 195, episode reward: 263.182, mean reward:  0.793 [-17.257, 100.000], mean action: 1.027 [0.000, 3.000],  loss: 10.077988, mae: 53.076145, mean_q: 71.216276, mean_eps: 0.100000\n",
      " 986699/1000000: episode: 2926, duration: 1.747s, episode steps: 336, steps per second: 192, episode reward: 246.133, mean reward:  0.733 [-7.443, 100.000], mean action: 1.080 [0.000, 3.000],  loss: 8.329257, mae: 49.425297, mean_q: 66.109903, mean_eps: 0.100000\n",
      " 987456/1000000: episode: 2927, duration: 4.180s, episode steps: 757, steps per second: 181, episode reward: 230.978, mean reward:  0.305 [-21.302, 100.000], mean action: 1.144 [0.000, 3.000],  loss: 6.442887, mae: 51.215239, mean_q: 69.186160, mean_eps: 0.100000\n",
      " 987536/1000000: episode: 2928, duration: 0.382s, episode steps:  80, steps per second: 209, episode reward: -14.688, mean reward: -0.184 [-100.000,  9.590], mean action: 1.312 [0.000, 3.000],  loss: 4.463902, mae: 47.991651, mean_q: 65.247518, mean_eps: 0.100000\n",
      " 987679/1000000: episode: 2929, duration: 0.683s, episode steps: 143, steps per second: 209, episode reward: -124.465, mean reward: -0.870 [-100.000, 13.163], mean action: 1.266 [0.000, 3.000],  loss: 15.014593, mae: 48.049074, mean_q: 65.458270, mean_eps: 0.100000\n",
      " 987761/1000000: episode: 2930, duration: 0.411s, episode steps:  82, steps per second: 200, episode reward: -19.545, mean reward: -0.238 [-100.000, 16.552], mean action: 1.329 [0.000, 3.000],  loss: 21.366716, mae: 47.074506, mean_q: 63.585263, mean_eps: 0.100000\n",
      " 988026/1000000: episode: 2931, duration: 1.458s, episode steps: 265, steps per second: 182, episode reward: 191.757, mean reward:  0.724 [-9.677, 100.000], mean action: 2.113 [0.000, 3.000],  loss: 13.891919, mae: 48.521887, mean_q: 65.450253, mean_eps: 0.100000\n",
      " 988167/1000000: episode: 2932, duration: 0.709s, episode steps: 141, steps per second: 199, episode reward: -105.333, mean reward: -0.747 [-100.000, 17.243], mean action: 1.723 [0.000, 3.000],  loss: 15.013735, mae: 46.148142, mean_q: 62.180596, mean_eps: 0.100000\n",
      " 988270/1000000: episode: 2933, duration: 0.495s, episode steps: 103, steps per second: 208, episode reward: -2.671, mean reward: -0.026 [-100.000, 46.587], mean action: 1.757 [0.000, 3.000],  loss: 19.131268, mae: 52.768067, mean_q: 70.949485, mean_eps: 0.100000\n",
      " 988871/1000000: episode: 2934, duration: 3.114s, episode steps: 601, steps per second: 193, episode reward: 182.315, mean reward:  0.303 [-10.825, 100.000], mean action: 1.770 [0.000, 3.000],  loss: 25.647862, mae: 53.310539, mean_q: 71.284291, mean_eps: 0.100000\n",
      " 988976/1000000: episode: 2935, duration: 0.501s, episode steps: 105, steps per second: 210, episode reward: -59.258, mean reward: -0.564 [-100.000, 26.848], mean action: 1.657 [0.000, 3.000],  loss: 11.870978, mae: 49.129975, mean_q: 66.421336, mean_eps: 0.100000\n",
      " 989510/1000000: episode: 2936, duration: 2.722s, episode steps: 534, steps per second: 196, episode reward: 234.134, mean reward:  0.438 [-9.866, 100.000], mean action: 1.242 [0.000, 3.000],  loss: 13.965304, mae: 49.289502, mean_q: 65.912489, mean_eps: 0.100000\n",
      " 989634/1000000: episode: 2937, duration: 0.636s, episode steps: 124, steps per second: 195, episode reward: -65.659, mean reward: -0.530 [-100.000,  9.322], mean action: 1.581 [0.000, 3.000],  loss: 15.048571, mae: 48.695887, mean_q: 65.702194, mean_eps: 0.100000\n",
      " 989742/1000000: episode: 2938, duration: 0.574s, episode steps: 108, steps per second: 188, episode reward: -26.612, mean reward: -0.246 [-100.000, 42.372], mean action: 1.769 [0.000, 3.000],  loss: 12.989645, mae: 55.718366, mean_q: 74.790378, mean_eps: 0.100000\n",
      " 989861/1000000: episode: 2939, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: -79.060, mean reward: -0.664 [-100.000, 15.732], mean action: 1.664 [0.000, 3.000],  loss: 18.795403, mae: 61.492995, mean_q: 81.893880, mean_eps: 0.100000\n",
      " 990861/1000000: episode: 2940, duration: 5.763s, episode steps: 1000, steps per second: 174, episode reward: -53.010, mean reward: -0.053 [-11.377, 12.011], mean action: 1.747 [0.000, 3.000],  loss: 11.873870, mae: 48.699314, mean_q: 59.207988, mean_eps: 0.100000\n",
      " 990952/1000000: episode: 2941, duration: 0.440s, episode steps:  91, steps per second: 207, episode reward: -109.795, mean reward: -1.207 [-100.000, 14.886], mean action: 1.835 [0.000, 3.000],  loss: 3.696050, mae: 28.094979, mean_q: 32.900600, mean_eps: 0.100000\n",
      " 991055/1000000: episode: 2942, duration: 0.511s, episode steps: 103, steps per second: 202, episode reward: -76.195, mean reward: -0.740 [-100.000, 43.771], mean action: 1.563 [0.000, 3.000],  loss: 7.314319, mae: 32.789163, mean_q: 37.948508, mean_eps: 0.100000\n",
      " 991154/1000000: episode: 2943, duration: 0.583s, episode steps:  99, steps per second: 170, episode reward: -138.483, mean reward: -1.399 [-100.000, 19.580], mean action: 0.970 [0.000, 3.000],  loss: 14.570316, mae: 36.447530, mean_q: 41.205164, mean_eps: 0.100000\n",
      " 991268/1000000: episode: 2944, duration: 0.636s, episode steps: 114, steps per second: 179, episode reward: -45.914, mean reward: -0.403 [-100.000,  7.662], mean action: 1.623 [0.000, 3.000],  loss: 11.897507, mae: 40.974096, mean_q: 47.996044, mean_eps: 0.100000\n",
      " 991379/1000000: episode: 2945, duration: 0.535s, episode steps: 111, steps per second: 208, episode reward: -80.874, mean reward: -0.729 [-100.000, 28.463], mean action: 1.468 [0.000, 3.000],  loss: 7.834255, mae: 42.861059, mean_q: 50.088777, mean_eps: 0.100000\n",
      " 991502/1000000: episode: 2946, duration: 0.588s, episode steps: 123, steps per second: 209, episode reward: -101.861, mean reward: -0.828 [-100.000, 12.023], mean action: 1.122 [0.000, 3.000],  loss: 11.808435, mae: 47.250353, mean_q: 52.467478, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 991677/1000000: episode: 2947, duration: 0.839s, episode steps: 175, steps per second: 209, episode reward: -227.189, mean reward: -1.298 [-100.000,  6.290], mean action: 1.754 [0.000, 3.000],  loss: 9.498402, mae: 48.878586, mean_q: 50.286017, mean_eps: 0.100000\n",
      " 991987/1000000: episode: 2948, duration: 1.513s, episode steps: 310, steps per second: 205, episode reward: -12.779, mean reward: -0.041 [-100.000, 16.507], mean action: 1.465 [0.000, 3.000],  loss: 11.087840, mae: 53.516646, mean_q: 54.858248, mean_eps: 0.100000\n",
      " 992090/1000000: episode: 2949, duration: 0.497s, episode steps: 103, steps per second: 207, episode reward: -45.126, mean reward: -0.438 [-100.000, 13.739], mean action: 1.660 [0.000, 3.000],  loss: 10.984884, mae: 57.880756, mean_q: 62.542133, mean_eps: 0.100000\n",
      " 993090/1000000: episode: 2950, duration: 5.632s, episode steps: 1000, steps per second: 178, episode reward: 50.914, mean reward:  0.051 [-19.197, 12.348], mean action: 1.294 [0.000, 3.000],  loss: 4.948386, mae: 40.870780, mean_q: 49.061695, mean_eps: 0.100000\n",
      " 993563/1000000: episode: 2951, duration: 2.534s, episode steps: 473, steps per second: 187, episode reward: 198.734, mean reward:  0.420 [-18.019, 100.000], mean action: 1.178 [0.000, 3.000],  loss: 2.373117, mae: 12.940414, mean_q: 18.516909, mean_eps: 0.100000\n",
      " 994132/1000000: episode: 2952, duration: 3.114s, episode steps: 569, steps per second: 183, episode reward: 232.500, mean reward:  0.409 [-16.872, 100.000], mean action: 1.575 [0.000, 3.000],  loss: 4.444879, mae: 23.724838, mean_q: 33.216355, mean_eps: 0.100000\n",
      " 994364/1000000: episode: 2953, duration: 1.142s, episode steps: 232, steps per second: 203, episode reward: 260.008, mean reward:  1.121 [-2.450, 100.000], mean action: 1.159 [0.000, 3.000],  loss: 7.637347, mae: 30.897805, mean_q: 42.705671, mean_eps: 0.100000\n",
      " 994771/1000000: episode: 2954, duration: 2.107s, episode steps: 407, steps per second: 193, episode reward: 231.835, mean reward:  0.570 [-14.239, 100.000], mean action: 1.300 [0.000, 3.000],  loss: 6.953457, mae: 37.894297, mean_q: 51.731040, mean_eps: 0.100000\n",
      " 995162/1000000: episode: 2955, duration: 2.138s, episode steps: 391, steps per second: 183, episode reward: 304.740, mean reward:  0.779 [-19.095, 100.000], mean action: 0.954 [0.000, 3.000],  loss: 7.425992, mae: 35.168714, mean_q: 48.020259, mean_eps: 0.100000\n",
      " 995771/1000000: episode: 2956, duration: 3.200s, episode steps: 609, steps per second: 190, episode reward: 252.920, mean reward:  0.415 [-21.748, 100.000], mean action: 0.670 [0.000, 3.000],  loss: 7.846182, mae: 32.868749, mean_q: 44.940884, mean_eps: 0.100000\n",
      " 996300/1000000: episode: 2957, duration: 2.961s, episode steps: 529, steps per second: 179, episode reward: 237.845, mean reward:  0.450 [-20.487, 100.000], mean action: 0.887 [0.000, 3.000],  loss: 3.261444, mae: 31.683140, mean_q: 43.478514, mean_eps: 0.100000\n",
      " 996818/1000000: episode: 2958, duration: 2.945s, episode steps: 518, steps per second: 176, episode reward: 264.202, mean reward:  0.510 [-19.800, 100.000], mean action: 0.880 [0.000, 3.000],  loss: 5.635800, mae: 31.824266, mean_q: 43.645609, mean_eps: 0.100000\n",
      " 997081/1000000: episode: 2959, duration: 1.363s, episode steps: 263, steps per second: 193, episode reward: 244.181, mean reward:  0.928 [-17.863, 100.000], mean action: 1.262 [0.000, 3.000],  loss: 5.050225, mae: 31.300424, mean_q: 42.903968, mean_eps: 0.100000\n",
      " 997330/1000000: episode: 2960, duration: 1.215s, episode steps: 249, steps per second: 205, episode reward: 285.045, mean reward:  1.145 [-18.014, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 6.860731, mae: 34.436515, mean_q: 47.017901, mean_eps: 0.100000\n",
      " 997629/1000000: episode: 2961, duration: 1.481s, episode steps: 299, steps per second: 202, episode reward: 259.410, mean reward:  0.868 [-11.272, 100.000], mean action: 1.538 [0.000, 3.000],  loss: 7.349699, mae: 34.383189, mean_q: 46.929183, mean_eps: 0.100000\n",
      " 998013/1000000: episode: 2962, duration: 1.919s, episode steps: 384, steps per second: 200, episode reward: 272.375, mean reward:  0.709 [-19.821, 100.000], mean action: 0.979 [0.000, 3.000],  loss: 10.473762, mae: 36.549456, mean_q: 49.697971, mean_eps: 0.100000\n",
      " 998715/1000000: episode: 2963, duration: 3.874s, episode steps: 702, steps per second: 181, episode reward: 196.221, mean reward:  0.280 [-19.516, 100.000], mean action: 1.097 [0.000, 3.000],  loss: 7.161584, mae: 34.024805, mean_q: 46.378929, mean_eps: 0.100000\n",
      " 999238/1000000: episode: 2964, duration: 2.923s, episode steps: 523, steps per second: 179, episode reward: 249.284, mean reward:  0.477 [-18.774, 100.000], mean action: 1.088 [0.000, 3.000],  loss: 5.155993, mae: 31.773901, mean_q: 43.607314, mean_eps: 0.100000\n",
      " 999772/1000000: episode: 2965, duration: 2.988s, episode steps: 534, steps per second: 179, episode reward: 263.731, mean reward:  0.494 [-17.480, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 4.661996, mae: 32.462694, mean_q: 44.422066, mean_eps: 0.100000\n",
      "done, took 5155.000 seconds\n",
      "CPU times: user 2h 19min 14s, sys: 25min 7s, total: 2h 44min 22s\n",
      "Wall time: 1h 25min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the build_agent function to traing the agent\n",
    "agent = build_agent(neural_network, actions)             # used build_agent to setup an agent\n",
    "\n",
    "agent.compile(Adam(learning_rate = 1e-4),                # use Adam optimisation with learning rate 0.0001\n",
    "            metrics = ['mae']                            # use mean absolute error to evaluate the metric\n",
    "           )      \n",
    "\n",
    "history = agent.fit(env, \n",
    "        nb_steps = 1000000,                              # number of timesteps \n",
    "        visualize = False,                               # visualize during the training\n",
    "        verbose = 2                                      # how to show the training output\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 277.172, steps: 179\n",
      "Episode 2: reward: 275.413, steps: 151\n",
      "Episode 3: reward: -10.307, steps: 237\n",
      "Episode 4: reward: 273.312, steps: 150\n",
      "Episode 5: reward: 275.036, steps: 343\n",
      "218.12528125606158\n"
     ]
    }
   ],
   "source": [
    "scores = agent.test(env,                             # pass our environment into the agent\n",
    "                  nb_episodes = 5,                   # number of episodes\n",
    "                  visualize = True                   # set True if we want to visualize it\n",
    "                 )\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- After 1 milliom timesteps of learning, the agent received score between -10 to 277 (average 218.0). ***The Lunar Lander landing on the ground 4 out of 5 times properly***\n",
    "- Time spend 1hr 25min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Epsilon-Greedy Policy\n",
    "\n",
    "Pass current state info into the function epsilon_greedy_policy, if random number is:\n",
    "- greater than epsilon: then the Agent will choice to take an action with maximum q_values.\n",
    "- less than epsilon: then the Agent will choice to take an random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")                       # Using Lunar Lander verion 2 in this notebook\n",
    "env.reset()                                            # reset the environment to initial default\n",
    "\n",
    "# Set the input_shape of the build_model function\n",
    "num_states = env.observation_space.shape               # return number of observation space\n",
    "\n",
    "# Set the output_shape of the build_model function\n",
    "num_actions = env.action_space.n                       # use .n to return number of action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(states, actions):\n",
    "    \n",
    "    network = Sequential()\n",
    "    \n",
    "    network.add(Flatten(input_shape = (num_states), name = 'States'))        # use Flatten layer as the input layer, input_shape = number of state \n",
    "    \n",
    "    network.add(Dense(128, activation = 'relu', name = 'Dense_1'))           # use Dense layer with 128 units of tensor with relu activation function\n",
    "    \n",
    "    network.add(Dense(64, activation = 'relu', name = 'Dense_2'))            # use another Dense layer with 64 units of tensor with relu activation function\n",
    "    \n",
    "    network.add(Dense(num_actions, activation = 'linear', name = 'Actions')) # the output layer with shape = number of actions with linear activation function\n",
    "                                                                              # Output the probability for each actions\n",
    "    \n",
    "    return network                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    \n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]              # q_values: the probability for each action, shape=[None]: size not fixed\n",
    "\n",
    "    action = np.random.choice(num_actions)                  # randomly pick an action\n",
    "    \n",
    "    # Choose the max of q values if a random number id greater than epsilon else action\n",
    "    if random.random() > epsilon:                           # if a random number > epsilon\n",
    "    \n",
    "        chosen_action = np.argmax(q_values)                 # output action with highest q_values\n",
    "    \n",
    "    else:                                                   # if a random number < epsilon\n",
    "    \n",
    "        chosen_action = action                              # output a random action\n",
    "        \n",
    "    return chosen_action                                    # output an action by epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Q-learning via Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + env.observation_space.shape)\n",
    "\n",
    "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "\n",
    "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "\n",
    "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + env.observation_space.shape)\n",
    "\n",
    "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "\n",
    "# run the build_network function\n",
    "network = build_network(states, actions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "States (Flatten)             (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Actions (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 9,668\n",
      "Trainable params: 9,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Q-Values in Current State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)                    # Output the probability for each actions from network\n",
    "\n",
    "# select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, env.action_space.n), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Q-Values for Next State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set reward discount factor gamma \n",
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "predicted_next_qvalues = network(next_states_ph)                 \n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues,1)         \n",
    "\n",
    "# compute \"target q-values\" for loss. Also call: TD Target\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values \n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)    # is_done_ph: to check is the last state or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Loss and Training Function\n",
    "\n",
    "when doing gradient descent, __we won't propagate gradients through it__ to make training more stable.\n",
    "\n",
    "To do so, we shall use `tf.stop_gradient` function which basically says ***\"consider this thing constant when doingbackprop\"***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss to minimize (predicted_qvalue - target_qvalue)\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "\n",
    "loss = tf.reduce_mean(loss)                                       # reduce_mean = sum of all loss / number of loss\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define session\n",
    "def generate_session(env, episode_max=500, epsilon=0, train=False):\n",
    "    \n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0                                               # initial reward = 0\n",
    "    \n",
    "    state = env.reset()                                            # initialize the environment\n",
    "    \n",
    "    for episode in range(episode_max):\n",
    "        \n",
    "        action = epsilon_greedy_policy(state, epsilon=epsilon)     # chose an action by epsilon_greedy_policy\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)             # the environment's feedback in terms of next_state, reward, is epsilon done\n",
    "        \n",
    "        if train:                                                  \n",
    "            sess.run(train_step,                                   # agent.update(state, action, reward, next_state) from tabular agent\n",
    "                     { \n",
    "                        states_ph: [state], \n",
    "                        actions_ph: [action], \n",
    "                        rewards_ph: [reward], \n",
    "                        next_states_ph: [next_state], \n",
    "                        is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += reward                                     # update total_reward\n",
    "        \n",
    "        state = next_state                                         # update state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = -162.786\tepsilon = 0.500\n",
      "epoch #1\tmean reward = -121.692\tepsilon = 0.495\n",
      "epoch #2\tmean reward = -274.428\tepsilon = 0.490\n",
      "epoch #3\tmean reward = -19.892\tepsilon = 0.485\n",
      "epoch #4\tmean reward = -129.700\tepsilon = 0.480\n",
      "epoch #5\tmean reward = -67.472\tepsilon = 0.475\n",
      "epoch #6\tmean reward = -259.282\tepsilon = 0.471\n",
      "epoch #7\tmean reward = -124.644\tepsilon = 0.466\n",
      "epoch #8\tmean reward = -132.942\tepsilon = 0.461\n",
      "epoch #9\tmean reward = -61.938\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 164.668\tepsilon = 0.452\n",
      "epoch #11\tmean reward = -55.738\tepsilon = 0.448\n",
      "epoch #12\tmean reward = -76.156\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 0.537\tepsilon = 0.439\n",
      "epoch #14\tmean reward = -1.613\tepsilon = 0.434\n",
      "epoch #15\tmean reward = -63.476\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 35.370\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 83.398\tepsilon = 0.421\n",
      "epoch #18\tmean reward = -2.538\tepsilon = 0.417\n",
      "epoch #19\tmean reward = -81.463\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 3.111\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 118.848\tepsilon = 0.405\n",
      "epoch #22\tmean reward = -2.770\tepsilon = 0.401\n",
      "epoch #23\tmean reward = -26.553\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 142.962\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 57.525\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 127.310\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 71.798\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 21.376\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 130.722\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 144.156\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 3.745\tepsilon = 0.366\n",
      "epoch #32\tmean reward = 40.746\tepsilon = 0.362\n",
      "epoch #33\tmean reward = 91.479\tepsilon = 0.359\n",
      "epoch #34\tmean reward = 116.333\tepsilon = 0.355\n",
      "epoch #35\tmean reward = -41.642\tepsilon = 0.352\n",
      "epoch #36\tmean reward = 150.966\tepsilon = 0.348\n",
      "epoch #37\tmean reward = 47.968\tepsilon = 0.345\n",
      "epoch #38\tmean reward = 20.797\tepsilon = 0.341\n",
      "epoch #39\tmean reward = 136.528\tepsilon = 0.338\n",
      "epoch #40\tmean reward = 120.470\tepsilon = 0.334\n",
      "epoch #41\tmean reward = 41.902\tepsilon = 0.331\n",
      "epoch #42\tmean reward = 114.215\tepsilon = 0.328\n",
      "epoch #43\tmean reward = -14.279\tepsilon = 0.325\n",
      "epoch #44\tmean reward = 32.570\tepsilon = 0.321\n",
      "epoch #45\tmean reward = 131.371\tepsilon = 0.318\n",
      "epoch #46\tmean reward = -13.354\tepsilon = 0.315\n",
      "epoch #47\tmean reward = -170.470\tepsilon = 0.312\n",
      "epoch #48\tmean reward = -145.179\tepsilon = 0.309\n",
      "epoch #49\tmean reward = 247.398\tepsilon = 0.306\n",
      "epoch #50\tmean reward = 62.140\tepsilon = 0.303\n",
      "epoch #51\tmean reward = 159.203\tepsilon = 0.299\n",
      "epoch #52\tmean reward = 152.454\tepsilon = 0.296\n",
      "epoch #53\tmean reward = 24.416\tepsilon = 0.294\n",
      "epoch #54\tmean reward = 142.253\tepsilon = 0.291\n",
      "epoch #55\tmean reward = 276.414\tepsilon = 0.288\n",
      "epoch #56\tmean reward = -47.878\tepsilon = 0.285\n",
      "epoch #57\tmean reward = 127.978\tepsilon = 0.282\n",
      "epoch #58\tmean reward = 282.597\tepsilon = 0.279\n",
      "epoch #59\tmean reward = 146.780\tepsilon = 0.276\n",
      "epoch #60\tmean reward = 136.971\tepsilon = 0.274\n",
      "epoch #61\tmean reward = 148.234\tepsilon = 0.271\n",
      "epoch #62\tmean reward = 148.757\tepsilon = 0.268\n",
      "epoch #63\tmean reward = -2.796\tepsilon = 0.265\n",
      "epoch #64\tmean reward = 119.781\tepsilon = 0.263\n",
      "epoch #65\tmean reward = 113.079\tepsilon = 0.260\n",
      "epoch #66\tmean reward = 172.389\tepsilon = 0.258\n",
      "epoch #67\tmean reward = 207.445\tepsilon = 0.255\n",
      "epoch #68\tmean reward = 54.715\tepsilon = 0.252\n",
      "epoch #69\tmean reward = 175.631\tepsilon = 0.250\n",
      "epoch #70\tmean reward = 135.422\tepsilon = 0.247\n",
      "epoch #71\tmean reward = 15.392\tepsilon = 0.245\n",
      "epoch #72\tmean reward = 240.148\tepsilon = 0.242\n",
      "epoch #73\tmean reward = 173.033\tepsilon = 0.240\n",
      "epoch #74\tmean reward = 134.651\tepsilon = 0.238\n",
      "epoch #75\tmean reward = 135.881\tepsilon = 0.235\n",
      "epoch #76\tmean reward = 22.399\tepsilon = 0.233\n",
      "epoch #77\tmean reward = 216.659\tepsilon = 0.231\n",
      "epoch #78\tmean reward = 157.296\tepsilon = 0.228\n",
      "epoch #79\tmean reward = 169.670\tepsilon = 0.226\n",
      "epoch #80\tmean reward = 18.641\tepsilon = 0.224\n",
      "epoch #81\tmean reward = 60.896\tepsilon = 0.222\n",
      "epoch #82\tmean reward = 135.555\tepsilon = 0.219\n",
      "epoch #83\tmean reward = 151.485\tepsilon = 0.217\n",
      "epoch #84\tmean reward = 135.980\tepsilon = 0.215\n",
      "epoch #85\tmean reward = 302.018\tepsilon = 0.213\n",
      "Learning Completed!\n",
      "CPU times: user 1h 53min 28s, sys: 31min 11s, total: 2h 24min 40s\n",
      "Wall time: 1h 19min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set epsilon: the probability for the Agent choice to take an random action when less than epsilon\n",
    "epsilon = 0.5                  # initialze epsilon\n",
    "\n",
    "for i in range(500):                                                                 # number of epoch to run\n",
    "    \n",
    "    for _ in range(100):                                                             # number of session to run\n",
    "        \n",
    "        session_rewards = generate_session(env, epsilon=epsilon, train=True)         # Train the agent\n",
    "        \n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99                                                                  # decrease epsilon per epoch mean less explortation over time\n",
    "    \n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"    # epsilon should be always nonzero \n",
    "    \n",
    "    if np.mean(session_rewards) > 300:                                               # session terminate if session_rewards > -85 \n",
    "        \n",
    "        print(\"Learning Completed!\")\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The ***Agent with Epsilon-Greedy Policy stop learning process at epoch #85 with average reward 302***\n",
    "- Time spend on receiving more than 300 average reward 1hr 19min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. PPO Model in Stable_baseline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Dependencies\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "DummyVecEnv = DummyVecEnv([lambda: env])\n",
    "\n",
    "log_path = os.path.join('Training', 'LunarLander-v2')\n",
    "\n",
    "PPO = PPO('MlpPolicy', DummyVecEnv, verbose=1, tensorboard_log = log_path)   # use multiple policy in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a callback to the training stage\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 300, verbose = 1)  # stop traing when achieve 300 reward\n",
    "\n",
    "eval_callback = EvalCallback(DummyVecEnv, \n",
    "                             callback_on_new_best = stop_callback,      # every time new best model, then run stop_callback. When stop_callback achieve 200 reward, then stop the training\n",
    "                             eval_freq = 10000,                         # set how often to run eval_callback \n",
    "                             best_model_save_path = save_path,\n",
    "                             verbose = 1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/LunarLander-v2/PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1507 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1251       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00528516 |\n",
      "|    clip_fraction        | 0.0728     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.548     |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.7       |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00253   |\n",
      "|    value_loss           | 101        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1119         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067898305 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.479       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.2         |\n",
      "|    n_updates            | 4910         |\n",
      "|    policy_gradient_loss | -0.00455     |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1074         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051056994 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.605        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.6         |\n",
      "|    n_updates            | 4920         |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 154          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8528, episode_reward=52.06 +/- 138.19\n",
      "Episode length: 438.60 +/- 282.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 439         |\n",
      "|    mean_reward          | 52.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8528        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015537521 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.38        |\n",
      "|    n_updates            | 4930        |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 25.5        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 898   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042043566 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.4         |\n",
      "|    n_updates            | 4940         |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045072017 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.39         |\n",
      "|    n_updates            | 4950         |\n",
      "|    policy_gradient_loss | -0.000924    |\n",
      "|    value_loss           | 32.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 889          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071815075 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.4         |\n",
      "|    n_updates            | 4960         |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046981946 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 4970         |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 71.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18528, episode_reward=97.85 +/- 133.26\n",
      "Episode length: 394.20 +/- 88.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 394          |\n",
      "|    mean_reward          | 97.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046565435 |\n",
      "|    clip_fraction        | 0.0741       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 4980         |\n",
      "|    policy_gradient_loss | -0.000712    |\n",
      "|    value_loss           | 47.5         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 827   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005227805 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.7        |\n",
      "|    n_updates            | 4990        |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 206         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 828          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074409675 |\n",
      "|    clip_fraction        | 0.0778       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.489       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 5000         |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 65.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 831         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006522388 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.5        |\n",
      "|    n_updates            | 5010        |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28528, episode_reward=39.68 +/- 121.44\n",
      "Episode length: 452.20 +/- 276.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 452          |\n",
      "|    mean_reward          | 39.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039450377 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 5020         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 53           |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 801   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068937256 |\n",
      "|    clip_fraction        | 0.0579       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 5030         |\n",
      "|    policy_gradient_loss | -0.000372    |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045092935 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 74           |\n",
      "|    n_updates            | 5040         |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007523385 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 80.3        |\n",
      "|    n_updates            | 5050        |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 840        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00445888 |\n",
      "|    clip_fraction        | 0.0501     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.449     |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 51.8       |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00292   |\n",
      "|    value_loss           | 99.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=38528, episode_reward=7.14 +/- 110.95\n",
      "Episode length: 450.20 +/- 275.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 450         |\n",
      "|    mean_reward          | 7.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013316297 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 5070        |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 819   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 38912 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008998814 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 5080        |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 834         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006536242 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 5090        |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 842          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034278163 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.512       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 5100         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 70.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029989716 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 5110         |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48528, episode_reward=-52.27 +/- 94.76\n",
      "Episode length: 482.20 +/- 184.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 482          |\n",
      "|    mean_reward          | -52.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048986296 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 83.7         |\n",
      "|    n_updates            | 5120         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 136          |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 823   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007551222 |\n",
      "|    clip_fraction        | 0.0634      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 5130        |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060472204 |\n",
      "|    clip_fraction        | 0.0661       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.86         |\n",
      "|    n_updates            | 5140         |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 70.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067388024 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.73         |\n",
      "|    n_updates            | 5150         |\n",
      "|    policy_gradient_loss | -0.000434    |\n",
      "|    value_loss           | 21.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051319655 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.2         |\n",
      "|    n_updates            | 5160         |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 94.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=58528, episode_reward=156.41 +/- 125.51\n",
      "Episode length: 484.60 +/- 261.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 485          |\n",
      "|    mean_reward          | 156          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068697617 |\n",
      "|    clip_fraction        | 0.0813       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 5170         |\n",
      "|    policy_gradient_loss | -0.0046      |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 801   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 59392 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00442769 |\n",
      "|    clip_fraction        | 0.0421     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.524     |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.7       |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.0038    |\n",
      "|    value_loss           | 74.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006105507 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28          |\n",
      "|    n_updates            | 5190        |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 76.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046864897 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 5200         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 44           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00646513 |\n",
      "|    clip_fraction        | 0.0604     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.522     |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.4       |\n",
      "|    n_updates            | 5210       |\n",
      "|    policy_gradient_loss | -0.0036    |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68528, episode_reward=93.74 +/- 146.01\n",
      "Episode length: 508.00 +/- 257.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 508          |\n",
      "|    mean_reward          | 93.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037811478 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 92.8         |\n",
      "|    n_updates            | 5220         |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 97.6         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 795   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005697978 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 5230        |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046752235 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 5240         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 93           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066881226 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.2         |\n",
      "|    n_updates            | 5250         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053888 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.84        |\n",
      "|    n_updates            | 5260        |\n",
      "|    policy_gradient_loss | -2.92e-05   |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=78528, episode_reward=84.09 +/- 145.66\n",
      "Episode length: 415.00 +/- 161.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 415          |\n",
      "|    mean_reward          | 84.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028300183 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.4         |\n",
      "|    n_updates            | 5270         |\n",
      "|    policy_gradient_loss | -0.000373    |\n",
      "|    value_loss           | 136          |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 802   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 79872 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042439066 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.5         |\n",
      "|    n_updates            | 5280         |\n",
      "|    policy_gradient_loss | -0.000536    |\n",
      "|    value_loss           | 49.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005814111 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 5290        |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    value_loss           | 54.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006978092 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.6        |\n",
      "|    n_updates            | 5300        |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    value_loss           | 81.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004798378 |\n",
      "|    clip_fraction        | 0.0527      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    value_loss           | 92.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=88528, episode_reward=101.43 +/- 132.72\n",
      "Episode length: 302.00 +/- 49.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 302         |\n",
      "|    mean_reward          | 101         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004767839 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 5320        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 84.7        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 812   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044922507 |\n",
      "|    clip_fraction        | 0.054        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 5330         |\n",
      "|    policy_gradient_loss | -0.000996    |\n",
      "|    value_loss           | 76.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004172177 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 5340        |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    value_loss           | 95.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037753312 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10           |\n",
      "|    n_updates            | 5350         |\n",
      "|    policy_gradient_loss | -0.000964    |\n",
      "|    value_loss           | 66.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005243559 |\n",
      "|    clip_fraction        | 0.0714      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71.6        |\n",
      "|    n_updates            | 5360        |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98528, episode_reward=85.92 +/- 63.32\n",
      "Episode length: 579.60 +/- 248.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 580          |\n",
      "|    mean_reward          | 85.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 98528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074661155 |\n",
      "|    clip_fraction        | 0.0766       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.52         |\n",
      "|    n_updates            | 5370         |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 67.4         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 808    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 124    |\n",
      "|    total_timesteps | 100352 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076254094 |\n",
      "|    clip_fraction        | 0.0855       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 5380         |\n",
      "|    policy_gradient_loss | 0.00227      |\n",
      "|    value_loss           | 32           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045333756 |\n",
      "|    clip_fraction        | 0.0676       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 5390         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 130          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042793145 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.8         |\n",
      "|    n_updates            | 5400         |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    value_loss           | 83.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=108528, episode_reward=88.79 +/- 104.59\n",
      "Episode length: 473.60 +/- 275.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 88.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 108528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004177634 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 80.9        |\n",
      "|    n_updates            | 5410        |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    value_loss           | 161         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 809    |\n",
      "|    iterations      | 53     |\n",
      "|    time_elapsed    | 134    |\n",
      "|    total_timesteps | 108544 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005518834 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 81.6        |\n",
      "|    n_updates            | 5420        |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074935183 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.7          |\n",
      "|    n_updates            | 5430         |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 87.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076640127 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 5440         |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    value_loss           | 78           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103006475 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90           |\n",
      "|    n_updates            | 5450         |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 57.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=118528, episode_reward=196.67 +/- 33.93\n",
      "Episode length: 442.20 +/- 98.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 442          |\n",
      "|    mean_reward          | 197          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 118528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050735422 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.654        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 5460         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 807    |\n",
      "|    iterations      | 58     |\n",
      "|    time_elapsed    | 147    |\n",
      "|    total_timesteps | 118784 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011769023 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.02        |\n",
      "|    n_updates            | 5470        |\n",
      "|    policy_gradient_loss | -0.000641   |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005380526 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 5480        |\n",
      "|    policy_gradient_loss | -0.000431   |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007473694 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.4        |\n",
      "|    n_updates            | 5490        |\n",
      "|    policy_gradient_loss | -0.00208    |\n",
      "|    value_loss           | 48.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051538264 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.26         |\n",
      "|    n_updates            | 5500         |\n",
      "|    policy_gradient_loss | -9.01e-05    |\n",
      "|    value_loss           | 73.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=128528, episode_reward=142.53 +/- 93.88\n",
      "Episode length: 373.00 +/- 96.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 373         |\n",
      "|    mean_reward          | 143         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 128528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004258511 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 5510        |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 66.1        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 806    |\n",
      "|    iterations      | 63     |\n",
      "|    time_elapsed    | 159    |\n",
      "|    total_timesteps | 129024 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065520364 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.6         |\n",
      "|    n_updates            | 5520         |\n",
      "|    policy_gradient_loss | 2.93e-05     |\n",
      "|    value_loss           | 43.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008073108 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 5530        |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062593273 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.41         |\n",
      "|    n_updates            | 5540         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 46.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048135202 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 5550         |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 66           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=138528, episode_reward=59.75 +/- 118.66\n",
      "Episode length: 365.80 +/- 166.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 366         |\n",
      "|    mean_reward          | 59.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 138528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012041902 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 5560        |\n",
      "|    policy_gradient_loss | -0.00452    |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 811    |\n",
      "|    iterations      | 68     |\n",
      "|    time_elapsed    | 171    |\n",
      "|    total_timesteps | 139264 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 173          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044173417 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.441       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.4         |\n",
      "|    n_updates            | 5570         |\n",
      "|    policy_gradient_loss | -0.000964    |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010616116 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 5580        |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006886986 |\n",
      "|    clip_fraction        | 0.0603      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.494      |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 5590        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042793527 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 5600         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 91.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=148528, episode_reward=121.41 +/- 112.93\n",
      "Episode length: 331.60 +/- 43.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | 121          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 148528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050609186 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.55        |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.57         |\n",
      "|    n_updates            | 5610         |\n",
      "|    policy_gradient_loss | -0.000985    |\n",
      "|    value_loss           | 82.7         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 815    |\n",
      "|    iterations      | 73     |\n",
      "|    time_elapsed    | 183    |\n",
      "|    total_timesteps | 149504 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005726497 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 5620        |\n",
      "|    policy_gradient_loss | -0.000279   |\n",
      "|    value_loss           | 98.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 187          |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029310205 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16           |\n",
      "|    n_updates            | 5630         |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 73.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030412953 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.9         |\n",
      "|    n_updates            | 5640         |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136962645 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.2         |\n",
      "|    n_updates            | 5650         |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=158528, episode_reward=148.41 +/- 70.16\n",
      "Episode length: 597.20 +/- 269.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 597          |\n",
      "|    mean_reward          | 148          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 158528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042647505 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.2         |\n",
      "|    n_updates            | 5660         |\n",
      "|    policy_gradient_loss | -0.00289     |\n",
      "|    value_loss           | 67.7         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 814    |\n",
      "|    iterations      | 78     |\n",
      "|    time_elapsed    | 196    |\n",
      "|    total_timesteps | 159744 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009892772 |\n",
      "|    clip_fraction        | 0.0767      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 5670        |\n",
      "|    policy_gradient_loss | -0.000363   |\n",
      "|    value_loss           | 41.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008334981 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 5680        |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    value_loss           | 83.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 202          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067981277 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 5690         |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 69.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050113783 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.5         |\n",
      "|    n_updates            | 5700         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 78.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=168528, episode_reward=173.88 +/- 58.18\n",
      "Episode length: 508.60 +/- 250.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 509         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009752463 |\n",
      "|    clip_fraction        | 0.086       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.79        |\n",
      "|    n_updates            | 5710        |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 54.7        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 812    |\n",
      "|    iterations      | 83     |\n",
      "|    time_elapsed    | 209    |\n",
      "|    total_timesteps | 169984 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010028964 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 5720        |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 213          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034281346 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | 0.687        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 118          |\n",
      "|    n_updates            | 5730         |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006620928 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 5740        |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 84.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006234535 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 5750        |\n",
      "|    policy_gradient_loss | 0.000667    |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=178528, episode_reward=103.51 +/- 89.21\n",
      "Episode length: 567.40 +/- 282.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 567          |\n",
      "|    mean_reward          | 104          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 178528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053813998 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.7         |\n",
      "|    n_updates            | 5760         |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 48.7         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 810    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 222    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 224          |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027412435 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.451       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 5770         |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 75.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006808081 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.44        |\n",
      "|    n_updates            | 5780        |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 42.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004076172 |\n",
      "|    clip_fraction        | 0.0604      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.78        |\n",
      "|    n_updates            | 5790        |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029864074 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.605        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.7         |\n",
      "|    n_updates            | 5800         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=188528, episode_reward=95.59 +/- 153.64\n",
      "Episode length: 352.00 +/- 58.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 188528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047654137 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 5810         |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 57.8         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 815    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 233    |\n",
      "|    total_timesteps | 190464 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056872116 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.444       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 5820         |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 91.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 237          |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042167557 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.441       |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 5830         |\n",
      "|    policy_gradient_loss | -0.000943    |\n",
      "|    value_loss           | 42           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 239          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047190287 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.44        |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 5840         |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 48.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=198528, episode_reward=113.34 +/- 101.24\n",
      "Episode length: 580.60 +/- 286.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 581          |\n",
      "|    mean_reward          | 113          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 198528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029355844 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10           |\n",
      "|    n_updates            | 5850         |\n",
      "|    policy_gradient_loss | -0.000866    |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 812    |\n",
      "|    iterations      | 97     |\n",
      "|    time_elapsed    | 244    |\n",
      "|    total_timesteps | 198656 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 246          |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073610665 |\n",
      "|    clip_fraction        | 0.0725       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.782        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.27         |\n",
      "|    n_updates            | 5860         |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 91.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006516653 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.36        |\n",
      "|    n_updates            | 5870        |\n",
      "|    policy_gradient_loss | -0.000494   |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003085921 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6           |\n",
      "|    n_updates            | 5880        |\n",
      "|    policy_gradient_loss | -0.000294   |\n",
      "|    value_loss           | 60          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 101          |\n",
      "|    time_elapsed         | 253          |\n",
      "|    total_timesteps      | 206848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036858874 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.2         |\n",
      "|    n_updates            | 5890         |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=208528, episode_reward=77.34 +/- 82.07\n",
      "Episode length: 479.60 +/- 197.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 480         |\n",
      "|    mean_reward          | 77.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 208528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004412886 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12        |\n",
      "|    n_updates            | 5900        |\n",
      "|    policy_gradient_loss | 0.00116     |\n",
      "|    value_loss           | 13.5        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 811    |\n",
      "|    iterations      | 102    |\n",
      "|    time_elapsed    | 257    |\n",
      "|    total_timesteps | 208896 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 210944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056476407 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.408       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 5910         |\n",
      "|    policy_gradient_loss | 0.000302     |\n",
      "|    value_loss           | 47.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005304205 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.01        |\n",
      "|    n_updates            | 5920        |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 80.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011204991 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.3        |\n",
      "|    n_updates            | 5930        |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 185         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 265          |\n",
      "|    total_timesteps      | 217088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052089873 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.726        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 5940         |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=218528, episode_reward=138.34 +/- 95.02\n",
      "Episode length: 437.20 +/- 164.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 437          |\n",
      "|    mean_reward          | 138          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 218528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051411935 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.75         |\n",
      "|    n_updates            | 5950         |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 45.1         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 812    |\n",
      "|    iterations      | 107    |\n",
      "|    time_elapsed    | 269    |\n",
      "|    total_timesteps | 219136 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 271          |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041453894 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.416       |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 5960         |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 93.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 223232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004161356 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008059207 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 5980        |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 227328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006341816 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 100         |\n",
      "|    n_updates            | 5990        |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 63.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=228528, episode_reward=115.84 +/- 104.72\n",
      "Episode length: 289.00 +/- 61.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 289          |\n",
      "|    mean_reward          | 116          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 228528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046346737 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.4         |\n",
      "|    n_updates            | 6000         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 815    |\n",
      "|    iterations      | 112    |\n",
      "|    time_elapsed    | 281    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 283          |\n",
      "|    total_timesteps      | 231424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036100065 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 76.5         |\n",
      "|    n_updates            | 6010         |\n",
      "|    policy_gradient_loss | -0.000989    |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009411721 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 6020        |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 52.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074448544 |\n",
      "|    clip_fraction        | 0.0931       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.91         |\n",
      "|    n_updates            | 6030         |\n",
      "|    policy_gradient_loss | -0.000783    |\n",
      "|    value_loss           | 42.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063418252 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.436       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.7         |\n",
      "|    n_updates            | 6040         |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 53.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=238528, episode_reward=151.94 +/- 80.02\n",
      "Episode length: 462.60 +/- 202.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 463        |\n",
      "|    mean_reward          | 152        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 238528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00345773 |\n",
      "|    clip_fraction        | 0.0558     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.437     |\n",
      "|    explained_variance   | 0.943      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.8       |\n",
      "|    n_updates            | 6050       |\n",
      "|    policy_gradient_loss | -0.00325   |\n",
      "|    value_loss           | 38.6       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 814    |\n",
      "|    iterations      | 117    |\n",
      "|    time_elapsed    | 294    |\n",
      "|    total_timesteps | 239616 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 241664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008671912 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 89.7        |\n",
      "|    n_updates            | 6060        |\n",
      "|    policy_gradient_loss | 0.000636    |\n",
      "|    value_loss           | 61.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 298          |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059904857 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.451       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 6070         |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    value_loss           | 63.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 300         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008236505 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.95        |\n",
      "|    n_updates            | 6080        |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003965038 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 6090        |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=248528, episode_reward=111.40 +/- 105.08\n",
      "Episode length: 293.20 +/- 56.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | 111         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 248528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008298176 |\n",
      "|    clip_fraction        | 0.0726      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 6100        |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    value_loss           | 33.6        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 122    |\n",
      "|    time_elapsed    | 305    |\n",
      "|    total_timesteps | 249856 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 308         |\n",
      "|    total_timesteps      | 251904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003801596 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.47        |\n",
      "|    n_updates            | 6110        |\n",
      "|    policy_gradient_loss | -9.02e-05   |\n",
      "|    value_loss           | 63.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 124          |\n",
      "|    time_elapsed         | 310          |\n",
      "|    total_timesteps      | 253952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052499166 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.68         |\n",
      "|    n_updates            | 6120         |\n",
      "|    policy_gradient_loss | -0.000411    |\n",
      "|    value_loss           | 12.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 125         |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004166862 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 6130        |\n",
      "|    policy_gradient_loss | -0.00222    |\n",
      "|    value_loss           | 56.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 315         |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005222497 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 6140        |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=258528, episode_reward=193.53 +/- 36.41\n",
      "Episode length: 349.40 +/- 62.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | 194          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 258528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064089764 |\n",
      "|    clip_fraction        | 0.0526       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.424       |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.6          |\n",
      "|    n_updates            | 6150         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 88           |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 318    |\n",
      "|    total_timesteps | 260096 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005189849 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.7        |\n",
      "|    n_updates            | 6160        |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 98.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 129          |\n",
      "|    time_elapsed         | 322          |\n",
      "|    total_timesteps      | 264192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027424877 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.2         |\n",
      "|    n_updates            | 6170         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 97.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 130          |\n",
      "|    time_elapsed         | 324          |\n",
      "|    total_timesteps      | 266240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067854403 |\n",
      "|    clip_fraction        | 0.0723       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71           |\n",
      "|    n_updates            | 6180         |\n",
      "|    policy_gradient_loss | -0.000282    |\n",
      "|    value_loss           | 99.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 326         |\n",
      "|    total_timesteps      | 268288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935921 |\n",
      "|    clip_fraction        | 0.0734      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.7        |\n",
      "|    n_updates            | 6190        |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=268528, episode_reward=136.22 +/- 100.56\n",
      "Episode length: 455.00 +/- 148.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 455          |\n",
      "|    mean_reward          | 136          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 268528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063878046 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.1         |\n",
      "|    n_updates            | 6200         |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 71.5         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 132    |\n",
      "|    time_elapsed    | 330    |\n",
      "|    total_timesteps | 270336 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 272384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005671775 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 6210        |\n",
      "|    policy_gradient_loss | -0.000801   |\n",
      "|    value_loss           | 85.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010158622 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.6        |\n",
      "|    n_updates            | 6220        |\n",
      "|    policy_gradient_loss | -0.000625   |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 135          |\n",
      "|    time_elapsed         | 337          |\n",
      "|    total_timesteps      | 276480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034702993 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 6230         |\n",
      "|    policy_gradient_loss | -5.25e-05    |\n",
      "|    value_loss           | 70.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=278528, episode_reward=37.23 +/- 113.55\n",
      "Episode length: 448.00 +/- 195.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 448          |\n",
      "|    mean_reward          | 37.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021766205 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.7         |\n",
      "|    n_updates            | 6240         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 136    |\n",
      "|    time_elapsed    | 340    |\n",
      "|    total_timesteps | 278528 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 817        |\n",
      "|    iterations           | 137        |\n",
      "|    time_elapsed         | 343        |\n",
      "|    total_timesteps      | 280576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00303072 |\n",
      "|    clip_fraction        | 0.0351     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.375     |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11         |\n",
      "|    n_updates            | 6250       |\n",
      "|    policy_gradient_loss | -0.00124   |\n",
      "|    value_loss           | 139        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 345         |\n",
      "|    total_timesteps      | 282624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012148168 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.56        |\n",
      "|    n_updates            | 6260        |\n",
      "|    policy_gradient_loss | 0.00282     |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009078953 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.6        |\n",
      "|    n_updates            | 6270        |\n",
      "|    policy_gradient_loss | -0.00278    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 349          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043864753 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.409       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 6280         |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 79.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=288528, episode_reward=100.14 +/- 104.37\n",
      "Episode length: 443.80 +/- 186.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 444         |\n",
      "|    mean_reward          | 100         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 288528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008127559 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 6290        |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 7.73        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 818    |\n",
      "|    iterations      | 141    |\n",
      "|    time_elapsed    | 352    |\n",
      "|    total_timesteps | 288768 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 290816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003988318 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.9        |\n",
      "|    n_updates            | 6300        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 85.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 357         |\n",
      "|    total_timesteps      | 292864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008144575 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38          |\n",
      "|    n_updates            | 6310        |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 360          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034277127 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.2         |\n",
      "|    n_updates            | 6320         |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 92.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018843696 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 6330        |\n",
      "|    policy_gradient_loss | 0.000272    |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=298528, episode_reward=139.58 +/- 116.76\n",
      "Episode length: 441.00 +/- 211.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 441          |\n",
      "|    mean_reward          | 140          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 298528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093582105 |\n",
      "|    clip_fraction        | 0.0764       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.6         |\n",
      "|    n_updates            | 6340         |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    value_loss           | 93.1         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 146    |\n",
      "|    time_elapsed    | 365    |\n",
      "|    total_timesteps | 299008 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 147          |\n",
      "|    time_elapsed         | 367          |\n",
      "|    total_timesteps      | 301056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047283187 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 6350         |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 41.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005404263 |\n",
      "|    clip_fraction        | 0.071       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 6360        |\n",
      "|    policy_gradient_loss | -0.00496    |\n",
      "|    value_loss           | 68.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 149          |\n",
      "|    time_elapsed         | 372          |\n",
      "|    total_timesteps      | 305152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048210034 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.462       |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 6370         |\n",
      "|    policy_gradient_loss | -0.00401     |\n",
      "|    value_loss           | 27.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 375          |\n",
      "|    total_timesteps      | 307200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031776135 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.423       |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 6380         |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=308528, episode_reward=39.32 +/- 92.49\n",
      "Episode length: 359.00 +/- 199.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 359         |\n",
      "|    mean_reward          | 39.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 308528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004218377 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 6390        |\n",
      "|    policy_gradient_loss | -0.000323   |\n",
      "|    value_loss           | 19.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 816    |\n",
      "|    iterations      | 151    |\n",
      "|    time_elapsed    | 378    |\n",
      "|    total_timesteps | 309248 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 381          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050829677 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 6400         |\n",
      "|    policy_gradient_loss | -0.000735    |\n",
      "|    value_loss           | 18.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008035349 |\n",
      "|    clip_fraction        | 0.0848      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.33        |\n",
      "|    n_updates            | 6410        |\n",
      "|    policy_gradient_loss | -0.000819   |\n",
      "|    value_loss           | 10.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 154        |\n",
      "|    time_elapsed         | 385        |\n",
      "|    total_timesteps      | 315392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00773839 |\n",
      "|    clip_fraction        | 0.0595     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.445     |\n",
      "|    explained_variance   | 0.826      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 57.7       |\n",
      "|    n_updates            | 6420       |\n",
      "|    policy_gradient_loss | -0.00106   |\n",
      "|    value_loss           | 93.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 317440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004559091 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 6430        |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=318528, episode_reward=34.25 +/- 126.45\n",
      "Episode length: 318.00 +/- 84.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 318        |\n",
      "|    mean_reward          | 34.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 318528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00628712 |\n",
      "|    clip_fraction        | 0.0408     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.403     |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.3       |\n",
      "|    n_updates            | 6440       |\n",
      "|    policy_gradient_loss | -0.00359   |\n",
      "|    value_loss           | 135        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 817    |\n",
      "|    iterations      | 156    |\n",
      "|    time_elapsed    | 390    |\n",
      "|    total_timesteps | 319488 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 157         |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 321536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004332315 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 6450        |\n",
      "|    policy_gradient_loss | -0.00058    |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 323584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003895924 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 6460        |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 64.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 159         |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 325632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011623092 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.27        |\n",
      "|    n_updates            | 6470        |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 399          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021123807 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.36         |\n",
      "|    n_updates            | 6480         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 67.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=328528, episode_reward=3.73 +/- 120.97\n",
      "Episode length: 479.60 +/- 266.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 480          |\n",
      "|    mean_reward          | 3.73         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 328528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057001766 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 6490         |\n",
      "|    policy_gradient_loss | -0.000395    |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 816    |\n",
      "|    iterations      | 161    |\n",
      "|    time_elapsed    | 404    |\n",
      "|    total_timesteps | 329728 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 162         |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 331776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006537012 |\n",
      "|    clip_fraction        | 0.0807      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21        |\n",
      "|    n_updates            | 6500        |\n",
      "|    policy_gradient_loss | 0.000825    |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 816        |\n",
      "|    iterations           | 163        |\n",
      "|    time_elapsed         | 408        |\n",
      "|    total_timesteps      | 333824     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00403414 |\n",
      "|    clip_fraction        | 0.0851     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.495     |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25         |\n",
      "|    n_updates            | 6510       |\n",
      "|    policy_gradient_loss | -0.00137   |\n",
      "|    value_loss           | 74.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 411         |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004201428 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.86        |\n",
      "|    n_updates            | 6520        |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 337920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007104198 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.67        |\n",
      "|    n_updates            | 6530        |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=338528, episode_reward=93.52 +/- 129.58\n",
      "Episode length: 413.80 +/- 295.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 414          |\n",
      "|    mean_reward          | 93.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 338528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052804886 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.413       |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.27         |\n",
      "|    n_updates            | 6540         |\n",
      "|    policy_gradient_loss | -0.000496    |\n",
      "|    value_loss           | 82.2         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 815    |\n",
      "|    iterations      | 166    |\n",
      "|    time_elapsed    | 416    |\n",
      "|    total_timesteps | 339968 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 167         |\n",
      "|    time_elapsed         | 419         |\n",
      "|    total_timesteps      | 342016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004830216 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 6550        |\n",
      "|    policy_gradient_loss | 0.00198     |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 421          |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061843535 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.1         |\n",
      "|    n_updates            | 6560         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 53.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 423          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032353927 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 6570         |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 54.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004908575 |\n",
      "|    clip_fraction        | 0.0593      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 6580        |\n",
      "|    policy_gradient_loss | -0.000116   |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=348528, episode_reward=76.56 +/- 91.29\n",
      "Episode length: 409.60 +/- 128.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 410          |\n",
      "|    mean_reward          | 76.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 348528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037935916 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.17         |\n",
      "|    n_updates            | 6590         |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 64.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 813    |\n",
      "|    iterations      | 171    |\n",
      "|    time_elapsed    | 430    |\n",
      "|    total_timesteps | 350208 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006001716 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.65        |\n",
      "|    n_updates            | 6600        |\n",
      "|    policy_gradient_loss | -0.000682   |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 434          |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072862036 |\n",
      "|    clip_fraction        | 0.0928       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.441       |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.44         |\n",
      "|    n_updates            | 6610         |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    value_loss           | 25.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 436          |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048860214 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.416       |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.22         |\n",
      "|    n_updates            | 6620         |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    value_loss           | 19.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283459 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.7        |\n",
      "|    n_updates            | 6630        |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=358528, episode_reward=7.97 +/- 135.71\n",
      "Episode length: 516.00 +/- 242.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 516         |\n",
      "|    mean_reward          | 7.97        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 358528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003706718 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.85        |\n",
      "|    n_updates            | 6640        |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 813    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 442    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 362496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006927643 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 6650        |\n",
      "|    policy_gradient_loss | 0.000103    |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 447          |\n",
      "|    total_timesteps      | 364544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047095837 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.9         |\n",
      "|    n_updates            | 6660         |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 57.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 366592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004618302 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.7        |\n",
      "|    n_updates            | 6670        |\n",
      "|    policy_gradient_loss | -0.000325   |\n",
      "|    value_loss           | 58.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=368528, episode_reward=22.20 +/- 106.00\n",
      "Episode length: 266.40 +/- 49.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 266         |\n",
      "|    mean_reward          | 22.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011058911 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.478      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 6680        |\n",
      "|    policy_gradient_loss | -0.00034    |\n",
      "|    value_loss           | 55.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 813    |\n",
      "|    iterations      | 180    |\n",
      "|    time_elapsed    | 452    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 455          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057624006 |\n",
      "|    clip_fraction        | 0.059        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 6690         |\n",
      "|    policy_gradient_loss | -0.00398     |\n",
      "|    value_loss           | 71.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 182          |\n",
      "|    time_elapsed         | 458          |\n",
      "|    total_timesteps      | 372736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074040894 |\n",
      "|    clip_fraction        | 0.0877       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.18         |\n",
      "|    n_updates            | 6700         |\n",
      "|    policy_gradient_loss | 0.000166     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 460          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042455015 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.406       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.4         |\n",
      "|    n_updates            | 6710         |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 70.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009593166 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.94        |\n",
      "|    n_updates            | 6720        |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=378528, episode_reward=249.93 +/- 11.94\n",
      "Episode length: 326.40 +/- 17.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 250          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 378528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024998235 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76         |\n",
      "|    n_updates            | 6730         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 70.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 812    |\n",
      "|    iterations      | 185    |\n",
      "|    time_elapsed    | 466    |\n",
      "|    total_timesteps | 378880 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 186         |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 380928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025090758 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 6740        |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 13.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 382976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005379577 |\n",
      "|    clip_fraction        | 0.0712      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 6750        |\n",
      "|    policy_gradient_loss | -0.000695   |\n",
      "|    value_loss           | 99.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009011091 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.68        |\n",
      "|    n_updates            | 6760        |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 476         |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009860185 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 6770        |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 39.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=388528, episode_reward=96.39 +/- 128.22\n",
      "Episode length: 341.20 +/- 74.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 341         |\n",
      "|    mean_reward          | 96.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 388528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007071998 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 6780        |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 31.1        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 811    |\n",
      "|    iterations      | 190    |\n",
      "|    time_elapsed    | 479    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 191         |\n",
      "|    time_elapsed         | 482         |\n",
      "|    total_timesteps      | 391168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011202645 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.91        |\n",
      "|    n_updates            | 6790        |\n",
      "|    policy_gradient_loss | -0.000249   |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900288 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 6800        |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 193          |\n",
      "|    time_elapsed         | 486          |\n",
      "|    total_timesteps      | 395264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031788987 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 6810         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 87.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 397312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004001531 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 6820        |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 71.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=398528, episode_reward=155.41 +/- 102.07\n",
      "Episode length: 400.00 +/- 163.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 400         |\n",
      "|    mean_reward          | 155         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 398528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008016596 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.43        |\n",
      "|    n_updates            | 6830        |\n",
      "|    policy_gradient_loss | 7.15e-05    |\n",
      "|    value_loss           | 49.2        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 810    |\n",
      "|    iterations      | 195    |\n",
      "|    time_elapsed    | 492    |\n",
      "|    total_timesteps | 399360 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 196          |\n",
      "|    time_elapsed         | 495          |\n",
      "|    total_timesteps      | 401408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113038495 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.535       |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 6840         |\n",
      "|    policy_gradient_loss | -0.000593    |\n",
      "|    value_loss           | 9.12         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027284883 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19           |\n",
      "|    n_updates            | 6850         |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 68.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 198          |\n",
      "|    time_elapsed         | 499          |\n",
      "|    total_timesteps      | 405504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025167982 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.99         |\n",
      "|    n_updates            | 6860         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 34.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 199          |\n",
      "|    time_elapsed         | 501          |\n",
      "|    total_timesteps      | 407552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049174167 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.5         |\n",
      "|    n_updates            | 6870         |\n",
      "|    policy_gradient_loss | -0.00055     |\n",
      "|    value_loss           | 80.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=408528, episode_reward=95.64 +/- 110.13\n",
      "Episode length: 590.40 +/- 263.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 590          |\n",
      "|    mean_reward          | 95.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 408528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069439253 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 6880         |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 808    |\n",
      "|    iterations      | 200    |\n",
      "|    time_elapsed    | 506    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 809         |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 508         |\n",
      "|    total_timesteps      | 411648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003814995 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 6890        |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 66.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 809          |\n",
      "|    iterations           | 202          |\n",
      "|    time_elapsed         | 510          |\n",
      "|    total_timesteps      | 413696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043603536 |\n",
      "|    clip_fraction        | 0.0864       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 6900         |\n",
      "|    policy_gradient_loss | 0.00195      |\n",
      "|    value_loss           | 51.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 513          |\n",
      "|    total_timesteps      | 415744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114619965 |\n",
      "|    clip_fraction        | 0.0968       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.35         |\n",
      "|    n_updates            | 6910         |\n",
      "|    policy_gradient_loss | -0.00489     |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 204          |\n",
      "|    time_elapsed         | 515          |\n",
      "|    total_timesteps      | 417792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065431944 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.7         |\n",
      "|    n_updates            | 6920         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 94.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=418528, episode_reward=151.80 +/- 86.72\n",
      "Episode length: 323.20 +/- 47.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 323          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 418528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031823958 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 6930         |\n",
      "|    policy_gradient_loss | -0.000552    |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 809    |\n",
      "|    iterations      | 205    |\n",
      "|    time_elapsed    | 518    |\n",
      "|    total_timesteps | 419840 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 206          |\n",
      "|    time_elapsed         | 520          |\n",
      "|    total_timesteps      | 421888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050872695 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.424       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 6940         |\n",
      "|    policy_gradient_loss | -0.000569    |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 522         |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005298923 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.14        |\n",
      "|    n_updates            | 6950        |\n",
      "|    policy_gradient_loss | -0.000572   |\n",
      "|    value_loss           | 74.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 525          |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052318047 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62         |\n",
      "|    n_updates            | 6960         |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 209          |\n",
      "|    time_elapsed         | 527          |\n",
      "|    total_timesteps      | 428032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047656735 |\n",
      "|    clip_fraction        | 0.0523       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.439       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87         |\n",
      "|    n_updates            | 6970         |\n",
      "|    policy_gradient_loss | -0.000515    |\n",
      "|    value_loss           | 59.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=428528, episode_reward=41.87 +/- 92.47\n",
      "Episode length: 542.20 +/- 344.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 542          |\n",
      "|    mean_reward          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 428528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050375564 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.61         |\n",
      "|    n_updates            | 6980         |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 807    |\n",
      "|    iterations      | 210    |\n",
      "|    time_elapsed    | 532    |\n",
      "|    total_timesteps | 430080 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 432128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008428331 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 6990        |\n",
      "|    policy_gradient_loss | 0.00119     |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 537         |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005756434 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 7000        |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 81          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 213         |\n",
      "|    time_elapsed         | 539         |\n",
      "|    total_timesteps      | 436224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004362854 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 7010        |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 53.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 809         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012175042 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 7020        |\n",
      "|    policy_gradient_loss | 0.000613    |\n",
      "|    value_loss           | 93.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=438528, episode_reward=57.71 +/- 128.40\n",
      "Episode length: 420.60 +/- 294.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 421         |\n",
      "|    mean_reward          | 57.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 438528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007228586 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 7030        |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 69.8        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 807    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 545    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 216          |\n",
      "|    time_elapsed         | 547          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043068817 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.392       |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 7040         |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 550         |\n",
      "|    total_timesteps      | 444416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007331509 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.789       |\n",
      "|    n_updates            | 7050        |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 552         |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016723005 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 7060        |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 448512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010736959 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 7070        |\n",
      "|    policy_gradient_loss | 0.00223     |\n",
      "|    value_loss           | 5.46        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=448528, episode_reward=51.02 +/- 115.29\n",
      "Episode length: 464.20 +/- 286.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 464        |\n",
      "|    mean_reward          | 51         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 448528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00691801 |\n",
      "|    clip_fraction        | 0.0778     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.382     |\n",
      "|    explained_variance   | 0.801      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.98       |\n",
      "|    n_updates            | 7080       |\n",
      "|    policy_gradient_loss | 0.000236   |\n",
      "|    value_loss           | 92         |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 806    |\n",
      "|    iterations      | 220    |\n",
      "|    time_elapsed    | 558    |\n",
      "|    total_timesteps | 450560 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 452608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004302617 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | 0.717       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 7090        |\n",
      "|    policy_gradient_loss | -0.0018     |\n",
      "|    value_loss           | 78.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848954 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 7100        |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 37.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 223          |\n",
      "|    time_elapsed         | 566          |\n",
      "|    total_timesteps      | 456704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068205073 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.409       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 7110         |\n",
      "|    policy_gradient_loss | -8.45e-05    |\n",
      "|    value_loss           | 70.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=458528, episode_reward=65.29 +/- 117.73\n",
      "Episode length: 424.00 +/- 292.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 424         |\n",
      "|    mean_reward          | 65.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 458528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004937011 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.66        |\n",
      "|    n_updates            | 7120        |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 32.8        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 805    |\n",
      "|    iterations      | 224    |\n",
      "|    time_elapsed    | 569    |\n",
      "|    total_timesteps | 458752 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 225         |\n",
      "|    time_elapsed         | 572         |\n",
      "|    total_timesteps      | 460800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004417742 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.11        |\n",
      "|    n_updates            | 7130        |\n",
      "|    policy_gradient_loss | -0.00229    |\n",
      "|    value_loss           | 52.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 574          |\n",
      "|    total_timesteps      | 462848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095604155 |\n",
      "|    clip_fraction        | 0.0819       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.469       |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.73         |\n",
      "|    n_updates            | 7140         |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009163624 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 7150        |\n",
      "|    policy_gradient_loss | -0.000419   |\n",
      "|    value_loss           | 7.65        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 228          |\n",
      "|    time_elapsed         | 579          |\n",
      "|    total_timesteps      | 466944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051585967 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.456       |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.94         |\n",
      "|    n_updates            | 7160         |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=468528, episode_reward=171.20 +/- 111.71\n",
      "Episode length: 288.00 +/- 37.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 288          |\n",
      "|    mean_reward          | 171          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 468528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057141855 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 7170         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 9.42         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 804    |\n",
      "|    iterations      | 229    |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 468992 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 584          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023057756 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.63         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.77         |\n",
      "|    n_updates            | 7180         |\n",
      "|    policy_gradient_loss | -0.000773    |\n",
      "|    value_loss           | 54.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 231          |\n",
      "|    time_elapsed         | 587          |\n",
      "|    total_timesteps      | 473088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027482035 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 7190         |\n",
      "|    policy_gradient_loss | -5.73e-05    |\n",
      "|    value_loss           | 62.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 589          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031360309 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.427       |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.1         |\n",
      "|    n_updates            | 7200         |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    value_loss           | 49.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 233          |\n",
      "|    time_elapsed         | 591          |\n",
      "|    total_timesteps      | 477184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029232372 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 7210         |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 68.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=478528, episode_reward=151.67 +/- 112.63\n",
      "Episode length: 321.60 +/- 105.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 478528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072762696 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.424       |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 7220         |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    value_loss           | 33.2         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 806    |\n",
      "|    iterations      | 234    |\n",
      "|    time_elapsed    | 594    |\n",
      "|    total_timesteps | 479232 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 235         |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 481280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003109456 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.9        |\n",
      "|    n_updates            | 7230        |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 598         |\n",
      "|    total_timesteps      | 483328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010517718 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.23        |\n",
      "|    n_updates            | 7240        |\n",
      "|    policy_gradient_loss | 0.000983    |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 237         |\n",
      "|    time_elapsed         | 601         |\n",
      "|    total_timesteps      | 485376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007222333 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.45        |\n",
      "|    n_updates            | 7250        |\n",
      "|    policy_gradient_loss | -0.00264    |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 603          |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070523275 |\n",
      "|    clip_fraction        | 0.0839       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.45         |\n",
      "|    n_updates            | 7260         |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 31.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=488528, episode_reward=175.59 +/- 100.23\n",
      "Episode length: 397.20 +/- 169.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 397         |\n",
      "|    mean_reward          | 176         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 488528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008043999 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.39        |\n",
      "|    n_updates            | 7270        |\n",
      "|    policy_gradient_loss | -0.000824   |\n",
      "|    value_loss           | 31.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 806    |\n",
      "|    iterations      | 239    |\n",
      "|    time_elapsed    | 606    |\n",
      "|    total_timesteps | 489472 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 609          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058638724 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.393       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.7         |\n",
      "|    n_updates            | 7280         |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 241          |\n",
      "|    time_elapsed         | 611          |\n",
      "|    total_timesteps      | 493568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073520294 |\n",
      "|    clip_fraction        | 0.0741       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.425       |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.58         |\n",
      "|    n_updates            | 7290         |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 21.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 613          |\n",
      "|    total_timesteps      | 495616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037897874 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.32         |\n",
      "|    n_updates            | 7300         |\n",
      "|    policy_gradient_loss | -0.000843    |\n",
      "|    value_loss           | 70.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 616         |\n",
      "|    total_timesteps      | 497664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006024532 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.87        |\n",
      "|    n_updates            | 7310        |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=498528, episode_reward=49.22 +/- 112.12\n",
      "Episode length: 451.00 +/- 280.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 451          |\n",
      "|    mean_reward          | 49.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 498528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035475707 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.686        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67         |\n",
      "|    n_updates            | 7320         |\n",
      "|    policy_gradient_loss | -0.000552    |\n",
      "|    value_loss           | 61.8         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 805    |\n",
      "|    iterations      | 244    |\n",
      "|    time_elapsed    | 620    |\n",
      "|    total_timesteps | 499712 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 245          |\n",
      "|    time_elapsed         | 622          |\n",
      "|    total_timesteps      | 501760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047680647 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.1         |\n",
      "|    n_updates            | 7330         |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 52.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 624         |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007176011 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 7340        |\n",
      "|    policy_gradient_loss | 0.00364     |\n",
      "|    value_loss           | 6.6         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 247          |\n",
      "|    time_elapsed         | 626          |\n",
      "|    total_timesteps      | 505856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025791489 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.7         |\n",
      "|    n_updates            | 7350         |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 54.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 248          |\n",
      "|    time_elapsed         | 628          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035318956 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 7360         |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=508528, episode_reward=12.31 +/- 111.28\n",
      "Episode length: 553.00 +/- 368.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 553          |\n",
      "|    mean_reward          | 12.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 508528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020695345 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.2         |\n",
      "|    n_updates            | 7370         |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 805    |\n",
      "|    iterations      | 249    |\n",
      "|    time_elapsed    | 633    |\n",
      "|    total_timesteps | 509952 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 635          |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028992416 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.444       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 86.4         |\n",
      "|    n_updates            | 7380         |\n",
      "|    policy_gradient_loss | 0.000753     |\n",
      "|    value_loss           | 71.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 638         |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008049913 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.16        |\n",
      "|    n_updates            | 7390        |\n",
      "|    policy_gradient_loss | -0.000765   |\n",
      "|    value_loss           | 46.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008942941 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 7400        |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 68.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 253         |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 518144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004455177 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 7410        |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 52.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=518528, episode_reward=101.34 +/- 129.42\n",
      "Episode length: 352.80 +/- 78.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 353          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 518528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067523164 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.14         |\n",
      "|    n_updates            | 7420         |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 804    |\n",
      "|    iterations      | 254    |\n",
      "|    time_elapsed    | 646    |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 648         |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005100757 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.376      |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.68        |\n",
      "|    n_updates            | 7430        |\n",
      "|    policy_gradient_loss | -0.000615   |\n",
      "|    value_loss           | 90.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009744847 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 7440        |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 257         |\n",
      "|    time_elapsed         | 653         |\n",
      "|    total_timesteps      | 526336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004141409 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.96        |\n",
      "|    n_updates            | 7450        |\n",
      "|    policy_gradient_loss | -0.000822   |\n",
      "|    value_loss           | 21.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 258          |\n",
      "|    time_elapsed         | 656          |\n",
      "|    total_timesteps      | 528384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078095784 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.88         |\n",
      "|    n_updates            | 7460         |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    value_loss           | 47           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=528528, episode_reward=67.56 +/- 108.76\n",
      "Episode length: 510.20 +/- 321.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 510         |\n",
      "|    mean_reward          | 67.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 528528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011616753 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.04        |\n",
      "|    n_updates            | 7470        |\n",
      "|    policy_gradient_loss | -0.000592   |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 803    |\n",
      "|    iterations      | 259    |\n",
      "|    time_elapsed    | 659    |\n",
      "|    total_timesteps | 530432 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 260          |\n",
      "|    time_elapsed         | 662          |\n",
      "|    total_timesteps      | 532480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059102876 |\n",
      "|    clip_fraction        | 0.0697       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 7480         |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 664          |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071624494 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 7490         |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 61.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 666         |\n",
      "|    total_timesteps      | 536576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005012455 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.4        |\n",
      "|    n_updates            | 7500        |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    value_loss           | 74.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=538528, episode_reward=-30.67 +/- 25.34\n",
      "Episode length: 246.40 +/- 28.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 246          |\n",
      "|    mean_reward          | -30.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 538528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073758746 |\n",
      "|    clip_fraction        | 0.0813       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.465       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46           |\n",
      "|    n_updates            | 7510         |\n",
      "|    policy_gradient_loss | 0.000926     |\n",
      "|    value_loss           | 39.5         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 804    |\n",
      "|    iterations      | 263    |\n",
      "|    time_elapsed    | 669    |\n",
      "|    total_timesteps | 538624 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 264         |\n",
      "|    time_elapsed         | 672         |\n",
      "|    total_timesteps      | 540672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009558983 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 7520        |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005525372 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 7530        |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 65.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 266         |\n",
      "|    time_elapsed         | 677         |\n",
      "|    total_timesteps      | 544768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007225342 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.33        |\n",
      "|    n_updates            | 7540        |\n",
      "|    policy_gradient_loss | 0.00149     |\n",
      "|    value_loss           | 19.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 679          |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029992508 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 7550         |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    value_loss           | 67.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=548528, episode_reward=44.22 +/- 151.62\n",
      "Episode length: 326.40 +/- 59.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 44.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 548528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056124404 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.3         |\n",
      "|    n_updates            | 7560         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 152          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 804    |\n",
      "|    iterations      | 268    |\n",
      "|    time_elapsed    | 682    |\n",
      "|    total_timesteps | 548864 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 269         |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 550912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013415236 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 7570        |\n",
      "|    policy_gradient_loss | -0.000465   |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 552960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007620584 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 7580        |\n",
      "|    policy_gradient_loss | -0.000954   |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 271         |\n",
      "|    time_elapsed         | 689         |\n",
      "|    total_timesteps      | 555008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004614715 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 7590        |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 54.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 272          |\n",
      "|    time_elapsed         | 691          |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026685195 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 7600         |\n",
      "|    policy_gradient_loss | -0.000261    |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=558528, episode_reward=90.65 +/- 93.11\n",
      "Episode length: 490.20 +/- 309.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | 90.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 558528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003581222 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.2        |\n",
      "|    n_updates            | 7610        |\n",
      "|    policy_gradient_loss | -0.000135   |\n",
      "|    value_loss           | 46.2        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 802    |\n",
      "|    iterations      | 273    |\n",
      "|    time_elapsed    | 696    |\n",
      "|    total_timesteps | 559104 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 803         |\n",
      "|    iterations           | 274         |\n",
      "|    time_elapsed         | 698         |\n",
      "|    total_timesteps      | 561152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987671 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.77        |\n",
      "|    n_updates            | 7620        |\n",
      "|    policy_gradient_loss | 0.000604    |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 700          |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070118704 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.408       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 7630         |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    value_loss           | 29.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 276         |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005319911 |\n",
      "|    clip_fraction        | 0.0566      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.5        |\n",
      "|    n_updates            | 7640        |\n",
      "|    policy_gradient_loss | -0.000449   |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 277          |\n",
      "|    time_elapsed         | 705          |\n",
      "|    total_timesteps      | 567296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055253967 |\n",
      "|    clip_fraction        | 0.0844       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 7650         |\n",
      "|    policy_gradient_loss | 0.00161      |\n",
      "|    value_loss           | 15.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=568528, episode_reward=54.77 +/- 115.39\n",
      "Episode length: 639.40 +/- 357.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 639         |\n",
      "|    mean_reward          | 54.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 568528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013311019 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.88        |\n",
      "|    n_updates            | 7660        |\n",
      "|    policy_gradient_loss | -0.000305   |\n",
      "|    value_loss           | 22          |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 801    |\n",
      "|    iterations      | 278    |\n",
      "|    time_elapsed    | 710    |\n",
      "|    total_timesteps | 569344 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 279         |\n",
      "|    time_elapsed         | 712         |\n",
      "|    total_timesteps      | 571392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008252153 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 7670        |\n",
      "|    policy_gradient_loss | -0.000508   |\n",
      "|    value_loss           | 89.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 280         |\n",
      "|    time_elapsed         | 715         |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062911 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.28        |\n",
      "|    n_updates            | 7680        |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 575488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007824933 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 7690        |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    value_loss           | 85.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 719         |\n",
      "|    total_timesteps      | 577536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009465804 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.19        |\n",
      "|    n_updates            | 7700        |\n",
      "|    policy_gradient_loss | -0.00209    |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=578528, episode_reward=-2.00 +/- 129.16\n",
      "Episode length: 387.60 +/- 312.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | -2          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 578528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006388868 |\n",
      "|    clip_fraction        | 0.0607      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 7710        |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 801    |\n",
      "|    iterations      | 283    |\n",
      "|    time_elapsed    | 723    |\n",
      "|    total_timesteps | 579584 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 284         |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 581632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004579802 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.2         |\n",
      "|    n_updates            | 7720        |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 65.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 802          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 727          |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029613753 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.02         |\n",
      "|    n_updates            | 7730         |\n",
      "|    policy_gradient_loss | 0.00145      |\n",
      "|    value_loss           | 18           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 802          |\n",
      "|    iterations           | 286          |\n",
      "|    time_elapsed         | 729          |\n",
      "|    total_timesteps      | 585728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030963845 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 7740         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 287         |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 587776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011191793 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.435      |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.74        |\n",
      "|    n_updates            | 7750        |\n",
      "|    policy_gradient_loss | 0.000355    |\n",
      "|    value_loss           | 51.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=588528, episode_reward=54.90 +/- 107.30\n",
      "Episode length: 482.80 +/- 307.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 54.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 588528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011304461 |\n",
      "|    clip_fraction        | 0.0762      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.52        |\n",
      "|    n_updates            | 7760        |\n",
      "|    policy_gradient_loss | -0.000685   |\n",
      "|    value_loss           | 38.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 800    |\n",
      "|    iterations      | 288    |\n",
      "|    time_elapsed    | 737    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 739          |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052392185 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 7770         |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 290          |\n",
      "|    time_elapsed         | 741          |\n",
      "|    total_timesteps      | 593920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073396442 |\n",
      "|    clip_fraction        | 0.0743       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.5         |\n",
      "|    n_updates            | 7780         |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 43.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007350634 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.11        |\n",
      "|    n_updates            | 7790        |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 26          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 292          |\n",
      "|    time_elapsed         | 746          |\n",
      "|    total_timesteps      | 598016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108514875 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 7800         |\n",
      "|    policy_gradient_loss | -0.00816     |\n",
      "|    value_loss           | 5.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=598528, episode_reward=8.28 +/- 115.78\n",
      "Episode length: 441.20 +/- 280.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 8.28        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 598528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003293143 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.97        |\n",
      "|    n_updates            | 7810        |\n",
      "|    policy_gradient_loss | -0.000152   |\n",
      "|    value_loss           | 50.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 799    |\n",
      "|    iterations      | 293    |\n",
      "|    time_elapsed    | 750    |\n",
      "|    total_timesteps | 600064 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 294          |\n",
      "|    time_elapsed         | 752          |\n",
      "|    total_timesteps      | 602112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024749306 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.5         |\n",
      "|    n_updates            | 7820         |\n",
      "|    policy_gradient_loss | -0.000852    |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 295         |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 604160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003505873 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 7830        |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 296         |\n",
      "|    time_elapsed         | 756         |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008947265 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 7840        |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 60.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 297         |\n",
      "|    time_elapsed         | 759         |\n",
      "|    total_timesteps      | 608256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004291522 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.55        |\n",
      "|    n_updates            | 7850        |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    value_loss           | 51.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=608528, episode_reward=39.10 +/- 70.15\n",
      "Episode length: 595.80 +/- 294.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 596          |\n",
      "|    mean_reward          | 39.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 608528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064468947 |\n",
      "|    clip_fraction        | 0.0786       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.86         |\n",
      "|    n_updates            | 7860         |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    value_loss           | 25.6         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 797    |\n",
      "|    iterations      | 298    |\n",
      "|    time_elapsed    | 765    |\n",
      "|    total_timesteps | 610304 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 767         |\n",
      "|    total_timesteps      | 612352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005244201 |\n",
      "|    clip_fraction        | 0.0635      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 7870        |\n",
      "|    policy_gradient_loss | 8.74e-05    |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 300         |\n",
      "|    time_elapsed         | 769         |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002697837 |\n",
      "|    clip_fraction        | 0.0266      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37          |\n",
      "|    n_updates            | 7880        |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 301        |\n",
      "|    time_elapsed         | 771        |\n",
      "|    total_timesteps      | 616448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00413727 |\n",
      "|    clip_fraction        | 0.0356     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.337     |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.6       |\n",
      "|    n_updates            | 7890       |\n",
      "|    policy_gradient_loss | -0.000926  |\n",
      "|    value_loss           | 77.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 618496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004058333 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.92        |\n",
      "|    n_updates            | 7900        |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    value_loss           | 84.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=618528, episode_reward=104.93 +/- 127.51\n",
      "Episode length: 467.00 +/- 210.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 467          |\n",
      "|    mean_reward          | 105          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 618528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024065152 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 7910         |\n",
      "|    policy_gradient_loss | -7.87e-05    |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 797    |\n",
      "|    iterations      | 303    |\n",
      "|    time_elapsed    | 778    |\n",
      "|    total_timesteps | 620544 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 304          |\n",
      "|    time_elapsed         | 781          |\n",
      "|    total_timesteps      | 622592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056710737 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.5         |\n",
      "|    n_updates            | 7920         |\n",
      "|    policy_gradient_loss | -0.000115    |\n",
      "|    value_loss           | 91.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 624640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012119626 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.994       |\n",
      "|    n_updates            | 7930        |\n",
      "|    policy_gradient_loss | 0.00241     |\n",
      "|    value_loss           | 6.31        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 786          |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033413812 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.7         |\n",
      "|    n_updates            | 7940         |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 70.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=628528, episode_reward=136.16 +/- 96.86\n",
      "Episode length: 543.20 +/- 248.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 543          |\n",
      "|    mean_reward          | 136          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 628528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036201933 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.66         |\n",
      "|    n_updates            | 7950         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 18.2         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 795    |\n",
      "|    iterations      | 307    |\n",
      "|    time_elapsed    | 790    |\n",
      "|    total_timesteps | 628736 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 308          |\n",
      "|    time_elapsed         | 792          |\n",
      "|    total_timesteps      | 630784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032743935 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.1          |\n",
      "|    n_updates            | 7960         |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    value_loss           | 70.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 309          |\n",
      "|    time_elapsed         | 794          |\n",
      "|    total_timesteps      | 632832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017574131 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.02         |\n",
      "|    n_updates            | 7970         |\n",
      "|    policy_gradient_loss | -9.13e-05    |\n",
      "|    value_loss           | 74.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 310          |\n",
      "|    time_elapsed         | 796          |\n",
      "|    total_timesteps      | 634880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065543195 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.723        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04         |\n",
      "|    n_updates            | 7980         |\n",
      "|    policy_gradient_loss | 0.000764     |\n",
      "|    value_loss           | 62.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 311         |\n",
      "|    time_elapsed         | 799         |\n",
      "|    total_timesteps      | 636928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003620442 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 7990        |\n",
      "|    policy_gradient_loss | -0.000673   |\n",
      "|    value_loss           | 60.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=638528, episode_reward=100.15 +/- 85.68\n",
      "Episode length: 653.40 +/- 247.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 653          |\n",
      "|    mean_reward          | 100          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 638528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050050737 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.393       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84         |\n",
      "|    n_updates            | 8000         |\n",
      "|    policy_gradient_loss | -0.000518    |\n",
      "|    value_loss           | 66.9         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 794    |\n",
      "|    iterations      | 312    |\n",
      "|    time_elapsed    | 804    |\n",
      "|    total_timesteps | 638976 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 313         |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 641024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012058989 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.68        |\n",
      "|    n_updates            | 8010        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 794          |\n",
      "|    iterations           | 314          |\n",
      "|    time_elapsed         | 808          |\n",
      "|    total_timesteps      | 643072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071357377 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.435       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 8020         |\n",
      "|    policy_gradient_loss | -0.000317    |\n",
      "|    value_loss           | 34.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 315         |\n",
      "|    time_elapsed         | 811         |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004841182 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.4        |\n",
      "|    n_updates            | 8030        |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 81.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 813         |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007097034 |\n",
      "|    clip_fraction        | 0.0621      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 8040        |\n",
      "|    policy_gradient_loss | -0.000912   |\n",
      "|    value_loss           | 71.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=648528, episode_reward=70.05 +/- 62.75\n",
      "Episode length: 745.40 +/- 273.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 745         |\n",
      "|    mean_reward          | 70          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 648528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008991814 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.76        |\n",
      "|    n_updates            | 8050        |\n",
      "|    policy_gradient_loss | -0.00335    |\n",
      "|    value_loss           | 53.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 792    |\n",
      "|    iterations      | 317    |\n",
      "|    time_elapsed    | 819    |\n",
      "|    total_timesteps | 649216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 318         |\n",
      "|    time_elapsed         | 822         |\n",
      "|    total_timesteps      | 651264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008437067 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.03        |\n",
      "|    n_updates            | 8060        |\n",
      "|    policy_gradient_loss | -0.000949   |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 319         |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 653312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518478 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.16        |\n",
      "|    n_updates            | 8070        |\n",
      "|    policy_gradient_loss | -2.53e-05   |\n",
      "|    value_loss           | 24.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 320          |\n",
      "|    time_elapsed         | 827          |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075481883 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 8080         |\n",
      "|    policy_gradient_loss | 0.00211      |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 793          |\n",
      "|    iterations           | 321          |\n",
      "|    time_elapsed         | 828          |\n",
      "|    total_timesteps      | 657408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071625733 |\n",
      "|    clip_fraction        | 0.0655       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 8090         |\n",
      "|    policy_gradient_loss | 0.000145     |\n",
      "|    value_loss           | 16.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=658528, episode_reward=28.67 +/- 90.02\n",
      "Episode length: 570.20 +/- 308.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 570          |\n",
      "|    mean_reward          | 28.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 658528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042164903 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.5         |\n",
      "|    n_updates            | 8100         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 791    |\n",
      "|    iterations      | 322    |\n",
      "|    time_elapsed    | 833    |\n",
      "|    total_timesteps | 659456 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 791          |\n",
      "|    iterations           | 323          |\n",
      "|    time_elapsed         | 835          |\n",
      "|    total_timesteps      | 661504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042200256 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.599        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.5         |\n",
      "|    n_updates            | 8110         |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 324         |\n",
      "|    time_elapsed         | 838         |\n",
      "|    total_timesteps      | 663552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004330727 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 8120        |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 81.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 325          |\n",
      "|    time_elapsed         | 840          |\n",
      "|    total_timesteps      | 665600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019082734 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.361       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.81         |\n",
      "|    n_updates            | 8130         |\n",
      "|    policy_gradient_loss | 0.00077      |\n",
      "|    value_loss           | 83.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 326         |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 667648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005025203 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 8140        |\n",
      "|    policy_gradient_loss | -0.000498   |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=668528, episode_reward=52.16 +/- 119.48\n",
      "Episode length: 667.80 +/- 284.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 668        |\n",
      "|    mean_reward          | 52.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 668528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00835753 |\n",
      "|    clip_fraction        | 0.0726     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.387     |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.75       |\n",
      "|    n_updates            | 8150       |\n",
      "|    policy_gradient_loss | 0.002      |\n",
      "|    value_loss           | 46.2       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 327    |\n",
      "|    time_elapsed    | 847    |\n",
      "|    total_timesteps | 669696 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 328          |\n",
      "|    time_elapsed         | 850          |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021974263 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.1         |\n",
      "|    n_updates            | 8160         |\n",
      "|    policy_gradient_loss | -0.00027     |\n",
      "|    value_loss           | 36.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 329          |\n",
      "|    time_elapsed         | 852          |\n",
      "|    total_timesteps      | 673792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055138734 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.954        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.13         |\n",
      "|    n_updates            | 8170         |\n",
      "|    policy_gradient_loss | 0.00289      |\n",
      "|    value_loss           | 11           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 855         |\n",
      "|    total_timesteps      | 675840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007086979 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 8180        |\n",
      "|    policy_gradient_loss | -0.000832   |\n",
      "|    value_loss           | 57.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 331          |\n",
      "|    time_elapsed         | 857          |\n",
      "|    total_timesteps      | 677888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056633553 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.35         |\n",
      "|    n_updates            | 8190         |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    value_loss           | 27.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=678528, episode_reward=143.25 +/- 104.96\n",
      "Episode length: 341.20 +/- 78.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | 143          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 678528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038628313 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.8         |\n",
      "|    n_updates            | 8200         |\n",
      "|    policy_gradient_loss | -0.000842    |\n",
      "|    value_loss           | 73           |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 332    |\n",
      "|    time_elapsed    | 860    |\n",
      "|    total_timesteps | 679936 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 333          |\n",
      "|    time_elapsed         | 862          |\n",
      "|    total_timesteps      | 681984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050817598 |\n",
      "|    clip_fraction        | 0.0922       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.67         |\n",
      "|    n_updates            | 8210         |\n",
      "|    policy_gradient_loss | 0.000316     |\n",
      "|    value_loss           | 10           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 334          |\n",
      "|    time_elapsed         | 865          |\n",
      "|    total_timesteps      | 684032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027205758 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.39         |\n",
      "|    n_updates            | 8220         |\n",
      "|    policy_gradient_loss | 0.000144     |\n",
      "|    value_loss           | 97.2         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 790        |\n",
      "|    iterations           | 335        |\n",
      "|    time_elapsed         | 868        |\n",
      "|    total_timesteps      | 686080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00812815 |\n",
      "|    clip_fraction        | 0.0587     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.384     |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.1       |\n",
      "|    n_updates            | 8230       |\n",
      "|    policy_gradient_loss | -0.0014    |\n",
      "|    value_loss           | 63.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 791          |\n",
      "|    iterations           | 336          |\n",
      "|    time_elapsed         | 869          |\n",
      "|    total_timesteps      | 688128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048106564 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.478       |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.63         |\n",
      "|    n_updates            | 8240         |\n",
      "|    policy_gradient_loss | 0.000961     |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=688528, episode_reward=81.61 +/- 102.54\n",
      "Episode length: 439.80 +/- 291.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 440        |\n",
      "|    mean_reward          | 81.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 688528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00629052 |\n",
      "|    clip_fraction        | 0.0393     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.353     |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26         |\n",
      "|    n_updates            | 8250       |\n",
      "|    policy_gradient_loss | -0.00388   |\n",
      "|    value_loss           | 76.9       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 337    |\n",
      "|    time_elapsed    | 874    |\n",
      "|    total_timesteps | 690176 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 876         |\n",
      "|    total_timesteps      | 692224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006391616 |\n",
      "|    clip_fraction        | 0.0671      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.23        |\n",
      "|    n_updates            | 8260        |\n",
      "|    policy_gradient_loss | 0.000128    |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 790        |\n",
      "|    iterations           | 339        |\n",
      "|    time_elapsed         | 878        |\n",
      "|    total_timesteps      | 694272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00785814 |\n",
      "|    clip_fraction        | 0.0703     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.366     |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.02       |\n",
      "|    n_updates            | 8270       |\n",
      "|    policy_gradient_loss | -0.00277   |\n",
      "|    value_loss           | 29.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 340          |\n",
      "|    time_elapsed         | 881          |\n",
      "|    total_timesteps      | 696320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044717276 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.358       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.5         |\n",
      "|    n_updates            | 8280         |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 341          |\n",
      "|    time_elapsed         | 883          |\n",
      "|    total_timesteps      | 698368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063343197 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.92         |\n",
      "|    n_updates            | 8290         |\n",
      "|    policy_gradient_loss | -0.000295    |\n",
      "|    value_loss           | 20           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=698528, episode_reward=92.63 +/- 142.56\n",
      "Episode length: 601.20 +/- 327.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 601         |\n",
      "|    mean_reward          | 92.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 698528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019426018 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 8300        |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 342    |\n",
      "|    time_elapsed    | 888    |\n",
      "|    total_timesteps | 700416 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 343          |\n",
      "|    time_elapsed         | 890          |\n",
      "|    total_timesteps      | 702464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067896163 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.957        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.9         |\n",
      "|    n_updates            | 8310         |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 52.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 344          |\n",
      "|    time_elapsed         | 892          |\n",
      "|    total_timesteps      | 704512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042137983 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.796        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.3         |\n",
      "|    n_updates            | 8320         |\n",
      "|    policy_gradient_loss | 0.000191     |\n",
      "|    value_loss           | 60           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 894         |\n",
      "|    total_timesteps      | 706560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006798654 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 100         |\n",
      "|    n_updates            | 8330        |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=708528, episode_reward=72.55 +/- 146.63\n",
      "Episode length: 408.60 +/- 297.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 409          |\n",
      "|    mean_reward          | 72.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 708528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032246753 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 8340         |\n",
      "|    policy_gradient_loss | -1.66e-05    |\n",
      "|    value_loss           | 52.1         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 346    |\n",
      "|    time_elapsed    | 897    |\n",
      "|    total_timesteps | 708608 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 347         |\n",
      "|    time_elapsed         | 900         |\n",
      "|    total_timesteps      | 710656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011125273 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 8350        |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    value_loss           | 24.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 348        |\n",
      "|    time_elapsed         | 902        |\n",
      "|    total_timesteps      | 712704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01664317 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.473     |\n",
      "|    explained_variance   | 0.73       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 42.5       |\n",
      "|    n_updates            | 8360       |\n",
      "|    policy_gradient_loss | -0.00485   |\n",
      "|    value_loss           | 57.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 904         |\n",
      "|    total_timesteps      | 714752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004815305 |\n",
      "|    clip_fraction        | 0.0721      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 8370        |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 350          |\n",
      "|    time_elapsed         | 907          |\n",
      "|    total_timesteps      | 716800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032584947 |\n",
      "|    clip_fraction        | 0.0744       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.56         |\n",
      "|    n_updates            | 8380         |\n",
      "|    policy_gradient_loss | 0.00158      |\n",
      "|    value_loss           | 7.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=718528, episode_reward=-61.76 +/- 20.36\n",
      "Episode length: 261.80 +/- 81.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 262          |\n",
      "|    mean_reward          | -61.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 718528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039512403 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 229          |\n",
      "|    n_updates            | 8390         |\n",
      "|    policy_gradient_loss | 4.79e-05     |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 790    |\n",
      "|    iterations      | 351    |\n",
      "|    time_elapsed    | 909    |\n",
      "|    total_timesteps | 718848 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 352         |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 720896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005189638 |\n",
      "|    clip_fraction        | 0.0575      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.39        |\n",
      "|    n_updates            | 8400        |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 59.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 914         |\n",
      "|    total_timesteps      | 722944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005240691 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.37       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33        |\n",
      "|    n_updates            | 8410        |\n",
      "|    policy_gradient_loss | -8.01e-05   |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 354         |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 724992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010853854 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.83        |\n",
      "|    n_updates            | 8420        |\n",
      "|    policy_gradient_loss | -0.000204   |\n",
      "|    value_loss           | 18.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 355         |\n",
      "|    time_elapsed         | 919         |\n",
      "|    total_timesteps      | 727040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004942055 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.91        |\n",
      "|    n_updates            | 8430        |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 8.29        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=728528, episode_reward=17.79 +/- 133.12\n",
      "Episode length: 454.00 +/- 195.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 454          |\n",
      "|    mean_reward          | 17.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 728528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034456989 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45           |\n",
      "|    n_updates            | 8440         |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    value_loss           | 57.7         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 356    |\n",
      "|    time_elapsed    | 923    |\n",
      "|    total_timesteps | 729088 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 357         |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 731136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018584378 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 8450        |\n",
      "|    policy_gradient_loss | -0.00294    |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 358          |\n",
      "|    time_elapsed         | 928          |\n",
      "|    total_timesteps      | 733184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049742516 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 8460         |\n",
      "|    policy_gradient_loss | -0.000609    |\n",
      "|    value_loss           | 50           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 359         |\n",
      "|    time_elapsed         | 930         |\n",
      "|    total_timesteps      | 735232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006219877 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 8470        |\n",
      "|    policy_gradient_loss | -2.62e-05   |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 360         |\n",
      "|    time_elapsed         | 933         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005654025 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.92        |\n",
      "|    n_updates            | 8480        |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=738528, episode_reward=48.11 +/- 127.56\n",
      "Episode length: 367.60 +/- 152.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 48.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 738528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005984095 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.28        |\n",
      "|    n_updates            | 8490        |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 361    |\n",
      "|    time_elapsed    | 936    |\n",
      "|    total_timesteps | 739328 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 362         |\n",
      "|    time_elapsed         | 938         |\n",
      "|    total_timesteps      | 741376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006332393 |\n",
      "|    clip_fraction        | 0.0755      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 8500        |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 363          |\n",
      "|    time_elapsed         | 940          |\n",
      "|    total_timesteps      | 743424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052797976 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 8510         |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    value_loss           | 73           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 364          |\n",
      "|    time_elapsed         | 943          |\n",
      "|    total_timesteps      | 745472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058564814 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 8520         |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 22.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 365          |\n",
      "|    time_elapsed         | 945          |\n",
      "|    total_timesteps      | 747520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044497834 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.76         |\n",
      "|    n_updates            | 8530         |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 22.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=748528, episode_reward=53.08 +/- 127.42\n",
      "Episode length: 584.20 +/- 347.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 584          |\n",
      "|    mean_reward          | 53.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 748528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062603564 |\n",
      "|    clip_fraction        | 0.0658       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.82         |\n",
      "|    n_updates            | 8540         |\n",
      "|    policy_gradient_loss | 0.000357     |\n",
      "|    value_loss           | 39.3         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 366    |\n",
      "|    time_elapsed    | 950    |\n",
      "|    total_timesteps | 749568 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 788          |\n",
      "|    iterations           | 367          |\n",
      "|    time_elapsed         | 952          |\n",
      "|    total_timesteps      | 751616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041362965 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 8550         |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 57.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 788          |\n",
      "|    iterations           | 368          |\n",
      "|    time_elapsed         | 955          |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061518345 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 8560         |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    value_loss           | 75.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 369          |\n",
      "|    time_elapsed         | 957          |\n",
      "|    total_timesteps      | 755712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053243763 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 8570         |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 36.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 370          |\n",
      "|    time_elapsed         | 960          |\n",
      "|    total_timesteps      | 757760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049435063 |\n",
      "|    clip_fraction        | 0.0519       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.6         |\n",
      "|    n_updates            | 8580         |\n",
      "|    policy_gradient_loss | -0.000595    |\n",
      "|    value_loss           | 59.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=758528, episode_reward=74.73 +/- 141.06\n",
      "Episode length: 282.00 +/- 55.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 74.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 758528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008222615 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 8590        |\n",
      "|    policy_gradient_loss | 0.00322     |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 371    |\n",
      "|    time_elapsed    | 962    |\n",
      "|    total_timesteps | 759808 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 372          |\n",
      "|    time_elapsed         | 965          |\n",
      "|    total_timesteps      | 761856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056778304 |\n",
      "|    clip_fraction        | 0.0745       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.4         |\n",
      "|    n_updates            | 8600         |\n",
      "|    policy_gradient_loss | 0.00289      |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 967         |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006186039 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 8610        |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 90.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 969         |\n",
      "|    total_timesteps      | 765952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009474897 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.92        |\n",
      "|    n_updates            | 8620        |\n",
      "|    policy_gradient_loss | 0.001       |\n",
      "|    value_loss           | 48.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 375          |\n",
      "|    time_elapsed         | 972          |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068562366 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 8630         |\n",
      "|    policy_gradient_loss | -0.00485     |\n",
      "|    value_loss           | 62.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=768528, episode_reward=79.03 +/- 119.92\n",
      "Episode length: 433.20 +/- 284.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 433         |\n",
      "|    mean_reward          | 79          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 768528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007822967 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.3        |\n",
      "|    n_updates            | 8640        |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 376    |\n",
      "|    time_elapsed    | 976    |\n",
      "|    total_timesteps | 770048 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 772096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004445293 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 8650        |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 981         |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007094285 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.765       |\n",
      "|    n_updates            | 8660        |\n",
      "|    policy_gradient_loss | 0.00172     |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 379         |\n",
      "|    time_elapsed         | 983         |\n",
      "|    total_timesteps      | 776192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002811 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.5         |\n",
      "|    n_updates            | 8670        |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    value_loss           | 49.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 380        |\n",
      "|    time_elapsed         | 986        |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00984726 |\n",
      "|    clip_fraction        | 0.0853     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.452     |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.36       |\n",
      "|    n_updates            | 8680       |\n",
      "|    policy_gradient_loss | -0.00162   |\n",
      "|    value_loss           | 46.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=778528, episode_reward=-17.01 +/- 99.68\n",
      "Episode length: 385.20 +/- 312.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 385         |\n",
      "|    mean_reward          | -17         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 778528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004275973 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.16        |\n",
      "|    n_updates            | 8690        |\n",
      "|    policy_gradient_loss | 0.000809    |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 787    |\n",
      "|    iterations      | 381    |\n",
      "|    time_elapsed    | 990    |\n",
      "|    total_timesteps | 780288 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 992         |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007153076 |\n",
      "|    clip_fraction        | 0.0506      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 8700        |\n",
      "|    policy_gradient_loss | -0.000501   |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 788          |\n",
      "|    iterations           | 383          |\n",
      "|    time_elapsed         | 994          |\n",
      "|    total_timesteps      | 784384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040813535 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.335       |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.55         |\n",
      "|    n_updates            | 8710         |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 95.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 384         |\n",
      "|    time_elapsed         | 997         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004624682 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.6        |\n",
      "|    n_updates            | 8720        |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 385          |\n",
      "|    time_elapsed         | 998          |\n",
      "|    total_timesteps      | 788480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049085687 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.79         |\n",
      "|    n_updates            | 8730         |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 36.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=788528, episode_reward=148.63 +/- 124.15\n",
      "Episode length: 404.60 +/- 133.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 405         |\n",
      "|    mean_reward          | 149         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 788528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004643808 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 8740        |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 24.8        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 386    |\n",
      "|    time_elapsed    | 1002   |\n",
      "|    total_timesteps | 790528 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 387         |\n",
      "|    time_elapsed         | 1004        |\n",
      "|    total_timesteps      | 792576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008218324 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28          |\n",
      "|    n_updates            | 8750        |\n",
      "|    policy_gradient_loss | -0.000833   |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 388          |\n",
      "|    time_elapsed         | 1006         |\n",
      "|    total_timesteps      | 794624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081798285 |\n",
      "|    clip_fraction        | 0.087        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.59         |\n",
      "|    n_updates            | 8760         |\n",
      "|    policy_gradient_loss | 0.00168      |\n",
      "|    value_loss           | 4.88         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 389          |\n",
      "|    time_elapsed         | 1008         |\n",
      "|    total_timesteps      | 796672       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022296961 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.6         |\n",
      "|    n_updates            | 8770         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=798528, episode_reward=140.44 +/- 85.11\n",
      "Episode length: 496.20 +/- 252.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | 140         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 798528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003141228 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.368      |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 8780        |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 390    |\n",
      "|    time_elapsed    | 1012   |\n",
      "|    total_timesteps | 798720 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 391          |\n",
      "|    time_elapsed         | 1014         |\n",
      "|    total_timesteps      | 800768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054211477 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81         |\n",
      "|    n_updates            | 8790         |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 1017        |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010036945 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.89        |\n",
      "|    n_updates            | 8800        |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    value_loss           | 29.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 393          |\n",
      "|    time_elapsed         | 1019         |\n",
      "|    total_timesteps      | 804864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054797707 |\n",
      "|    clip_fraction        | 0.0645       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.369       |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.46         |\n",
      "|    n_updates            | 8810         |\n",
      "|    policy_gradient_loss | 1.56e-05     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 394          |\n",
      "|    time_elapsed         | 1020         |\n",
      "|    total_timesteps      | 806912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050788904 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 8820         |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 82.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=808528, episode_reward=12.47 +/- 154.48\n",
      "Episode length: 349.40 +/- 164.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 349        |\n",
      "|    mean_reward          | 12.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 808528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00393881 |\n",
      "|    clip_fraction        | 0.0315     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.336     |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 32.5       |\n",
      "|    n_updates            | 8830       |\n",
      "|    policy_gradient_loss | -0.000985  |\n",
      "|    value_loss           | 129        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 395    |\n",
      "|    time_elapsed    | 1024   |\n",
      "|    total_timesteps | 808960 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 396          |\n",
      "|    time_elapsed         | 1026         |\n",
      "|    total_timesteps      | 811008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075070253 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.391       |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 8840         |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 51.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 397         |\n",
      "|    time_elapsed         | 1028        |\n",
      "|    total_timesteps      | 813056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007563876 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 8850        |\n",
      "|    policy_gradient_loss | -0.000529   |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 1030        |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005044147 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.2        |\n",
      "|    n_updates            | 8860        |\n",
      "|    policy_gradient_loss | -0.000525   |\n",
      "|    value_loss           | 25.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 791          |\n",
      "|    iterations           | 399          |\n",
      "|    time_elapsed         | 1032         |\n",
      "|    total_timesteps      | 817152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060995966 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 8870         |\n",
      "|    policy_gradient_loss | -0.00392     |\n",
      "|    value_loss           | 51           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=818528, episode_reward=82.80 +/- 121.00\n",
      "Episode length: 428.20 +/- 291.41\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 428          |\n",
      "|    mean_reward          | 82.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 818528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062567308 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.382       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.28         |\n",
      "|    n_updates            | 8880         |\n",
      "|    policy_gradient_loss | -0.000132    |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 790    |\n",
      "|    iterations      | 400    |\n",
      "|    time_elapsed    | 1036   |\n",
      "|    total_timesteps | 819200 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 401          |\n",
      "|    time_elapsed         | 1038         |\n",
      "|    total_timesteps      | 821248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057879873 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 8890         |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 76.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 402          |\n",
      "|    time_elapsed         | 1040         |\n",
      "|    total_timesteps      | 823296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066698277 |\n",
      "|    clip_fraction        | 0.059        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 8900         |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    value_loss           | 28.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 403          |\n",
      "|    time_elapsed         | 1043         |\n",
      "|    total_timesteps      | 825344       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019751946 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 8910         |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 181          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 404          |\n",
      "|    time_elapsed         | 1046         |\n",
      "|    total_timesteps      | 827392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043848827 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 8920         |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 54.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=828528, episode_reward=3.09 +/- 101.87\n",
      "Episode length: 440.60 +/- 298.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 3.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 828528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007554511 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 8930        |\n",
      "|    policy_gradient_loss | 0.00265     |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 405    |\n",
      "|    time_elapsed    | 1051   |\n",
      "|    total_timesteps | 829440 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 406         |\n",
      "|    time_elapsed         | 1053        |\n",
      "|    total_timesteps      | 831488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005026823 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.1        |\n",
      "|    n_updates            | 8940        |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 407          |\n",
      "|    time_elapsed         | 1055         |\n",
      "|    total_timesteps      | 833536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046386044 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 8950         |\n",
      "|    policy_gradient_loss | -0.00041     |\n",
      "|    value_loss           | 60.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 408         |\n",
      "|    time_elapsed         | 1058        |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004531093 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 8960        |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 409         |\n",
      "|    time_elapsed         | 1061        |\n",
      "|    total_timesteps      | 837632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004727987 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.4         |\n",
      "|    n_updates            | 8970        |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=838528, episode_reward=104.44 +/- 135.23\n",
      "Episode length: 302.20 +/- 61.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 302         |\n",
      "|    mean_reward          | 104         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 838528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007005834 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 8980        |\n",
      "|    policy_gradient_loss | -0.000378   |\n",
      "|    value_loss           | 27.2        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 788    |\n",
      "|    iterations      | 410    |\n",
      "|    time_elapsed    | 1064   |\n",
      "|    total_timesteps | 839680 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 411          |\n",
      "|    time_elapsed         | 1066         |\n",
      "|    total_timesteps      | 841728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018269954 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.341       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.1         |\n",
      "|    n_updates            | 8990         |\n",
      "|    policy_gradient_loss | -0.000963    |\n",
      "|    value_loss           | 87.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 1068        |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004570239 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.6        |\n",
      "|    n_updates            | 9000        |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 413         |\n",
      "|    time_elapsed         | 1070        |\n",
      "|    total_timesteps      | 845824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004941795 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 9010        |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 414         |\n",
      "|    time_elapsed         | 1072        |\n",
      "|    total_timesteps      | 847872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005115916 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 9020        |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 84.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=848528, episode_reward=155.90 +/- 107.76\n",
      "Episode length: 470.40 +/- 265.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 156         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 848528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005917631 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.57        |\n",
      "|    n_updates            | 9030        |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 415    |\n",
      "|    time_elapsed    | 1077   |\n",
      "|    total_timesteps | 849920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 416         |\n",
      "|    time_elapsed         | 1078        |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010650035 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.46        |\n",
      "|    n_updates            | 9040        |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 1080        |\n",
      "|    total_timesteps      | 854016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003085922 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 9050        |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 418          |\n",
      "|    time_elapsed         | 1083         |\n",
      "|    total_timesteps      | 856064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080378335 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.2         |\n",
      "|    n_updates            | 9060         |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 96.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 419         |\n",
      "|    time_elapsed         | 1086        |\n",
      "|    total_timesteps      | 858112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009680293 |\n",
      "|    clip_fraction        | 0.0832      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 9070        |\n",
      "|    policy_gradient_loss | -0.000311   |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=858528, episode_reward=220.04 +/- 43.33\n",
      "Episode length: 346.20 +/- 15.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | 220          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 858528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038918024 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 9080         |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 96.9         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 420    |\n",
      "|    time_elapsed    | 1089   |\n",
      "|    total_timesteps | 860160 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 421          |\n",
      "|    time_elapsed         | 1092         |\n",
      "|    total_timesteps      | 862208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074275555 |\n",
      "|    clip_fraction        | 0.071        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.1         |\n",
      "|    n_updates            | 9090         |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 56.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 422          |\n",
      "|    time_elapsed         | 1094         |\n",
      "|    total_timesteps      | 864256       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033797296 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.6         |\n",
      "|    n_updates            | 9100         |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 58.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 423          |\n",
      "|    time_elapsed         | 1096         |\n",
      "|    total_timesteps      | 866304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051481575 |\n",
      "|    clip_fraction        | 0.0659       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 9110         |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    value_loss           | 61.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 424         |\n",
      "|    time_elapsed         | 1099        |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007444762 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 9120        |\n",
      "|    policy_gradient_loss | 0.00101     |\n",
      "|    value_loss           | 25          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=868528, episode_reward=109.63 +/- 122.49\n",
      "Episode length: 454.80 +/- 275.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 455          |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 868528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047601247 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 9130         |\n",
      "|    policy_gradient_loss | -0.000934    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 425    |\n",
      "|    time_elapsed    | 1103   |\n",
      "|    total_timesteps | 870400 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 426          |\n",
      "|    time_elapsed         | 1105         |\n",
      "|    total_timesteps      | 872448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021234776 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.3         |\n",
      "|    n_updates            | 9140         |\n",
      "|    policy_gradient_loss | -0.000358    |\n",
      "|    value_loss           | 78.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 789          |\n",
      "|    iterations           | 427          |\n",
      "|    time_elapsed         | 1107         |\n",
      "|    total_timesteps      | 874496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036784878 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.94         |\n",
      "|    n_updates            | 9150         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 53.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 428          |\n",
      "|    time_elapsed         | 1109         |\n",
      "|    total_timesteps      | 876544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033065164 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 9160         |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 95.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=878528, episode_reward=128.32 +/- 108.82\n",
      "Episode length: 340.20 +/- 100.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | 128         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 878528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008559851 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 9170        |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    value_loss           | 47.6        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 789    |\n",
      "|    iterations      | 429    |\n",
      "|    time_elapsed    | 1112   |\n",
      "|    total_timesteps | 878592 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 430          |\n",
      "|    time_elapsed         | 1114         |\n",
      "|    total_timesteps      | 880640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035206845 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.427       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.8         |\n",
      "|    n_updates            | 9180         |\n",
      "|    policy_gradient_loss | 0.00212      |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 790        |\n",
      "|    iterations           | 431        |\n",
      "|    time_elapsed         | 1116       |\n",
      "|    total_timesteps      | 882688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00929701 |\n",
      "|    clip_fraction        | 0.08       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.44      |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.3       |\n",
      "|    n_updates            | 9190       |\n",
      "|    policy_gradient_loss | 0.00163    |\n",
      "|    value_loss           | 57.6       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 432         |\n",
      "|    time_elapsed         | 1118        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009126285 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 9200        |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    value_loss           | 80.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 433         |\n",
      "|    time_elapsed         | 1120        |\n",
      "|    total_timesteps      | 886784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006048143 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 9210        |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=888528, episode_reward=60.06 +/- 115.73\n",
      "Episode length: 578.20 +/- 276.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 888528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00641702 |\n",
      "|    clip_fraction        | 0.0506     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.336     |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.1       |\n",
      "|    n_updates            | 9220       |\n",
      "|    policy_gradient_loss | -0.0011    |\n",
      "|    value_loss           | 80.5       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 790    |\n",
      "|    iterations      | 434    |\n",
      "|    time_elapsed    | 1124   |\n",
      "|    total_timesteps | 888832 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 790          |\n",
      "|    iterations           | 435          |\n",
      "|    time_elapsed         | 1127         |\n",
      "|    total_timesteps      | 890880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062389555 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.3         |\n",
      "|    n_updates            | 9230         |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 436         |\n",
      "|    time_elapsed         | 1129        |\n",
      "|    total_timesteps      | 892928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008639336 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.18        |\n",
      "|    n_updates            | 9240        |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 48.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 791          |\n",
      "|    iterations           | 437          |\n",
      "|    time_elapsed         | 1130         |\n",
      "|    total_timesteps      | 894976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059346627 |\n",
      "|    clip_fraction        | 0.0798       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.91         |\n",
      "|    n_updates            | 9250         |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 38.1         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 438        |\n",
      "|    time_elapsed         | 1132       |\n",
      "|    total_timesteps      | 897024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00415911 |\n",
      "|    clip_fraction        | 0.055      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.375     |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 103        |\n",
      "|    n_updates            | 9260       |\n",
      "|    policy_gradient_loss | -0.00113   |\n",
      "|    value_loss           | 168        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=898528, episode_reward=-16.33 +/- 79.18\n",
      "Episode length: 263.20 +/- 62.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 263         |\n",
      "|    mean_reward          | -16.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 898528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264389 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 9270        |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 70.1        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 791    |\n",
      "|    iterations      | 439    |\n",
      "|    time_elapsed    | 1135   |\n",
      "|    total_timesteps | 899072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 440         |\n",
      "|    time_elapsed         | 1137        |\n",
      "|    total_timesteps      | 901120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005455678 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.7        |\n",
      "|    n_updates            | 9280        |\n",
      "|    policy_gradient_loss | -0.000582   |\n",
      "|    value_loss           | 97.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 441          |\n",
      "|    time_elapsed         | 1139         |\n",
      "|    total_timesteps      | 903168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055658575 |\n",
      "|    clip_fraction        | 0.0655       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.3         |\n",
      "|    n_updates            | 9290         |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 92.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 1141        |\n",
      "|    total_timesteps      | 905216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004502074 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 9300        |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 443          |\n",
      "|    time_elapsed         | 1144         |\n",
      "|    total_timesteps      | 907264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046686972 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.409       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.1         |\n",
      "|    n_updates            | 9310         |\n",
      "|    policy_gradient_loss | -0.000624    |\n",
      "|    value_loss           | 37.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=908528, episode_reward=30.17 +/- 139.80\n",
      "Episode length: 284.20 +/- 79.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 30.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 908528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010595765 |\n",
      "|    clip_fraction        | 0.0712      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.89        |\n",
      "|    n_updates            | 9320        |\n",
      "|    policy_gradient_loss | -0.000282   |\n",
      "|    value_loss           | 47.2        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 792    |\n",
      "|    iterations      | 444    |\n",
      "|    time_elapsed    | 1147   |\n",
      "|    total_timesteps | 909312 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 793          |\n",
      "|    iterations           | 445          |\n",
      "|    time_elapsed         | 1149         |\n",
      "|    total_timesteps      | 911360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029580514 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.8         |\n",
      "|    n_updates            | 9330         |\n",
      "|    policy_gradient_loss | -0.000905    |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 1151        |\n",
      "|    total_timesteps      | 913408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004983373 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 77.7        |\n",
      "|    n_updates            | 9340        |\n",
      "|    policy_gradient_loss | -0.00162    |\n",
      "|    value_loss           | 92.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 447        |\n",
      "|    time_elapsed         | 1153       |\n",
      "|    total_timesteps      | 915456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01012865 |\n",
      "|    clip_fraction        | 0.0968     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.443     |\n",
      "|    explained_variance   | 0.927      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.1        |\n",
      "|    n_updates            | 9350       |\n",
      "|    policy_gradient_loss | 0.00288    |\n",
      "|    value_loss           | 26.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 448         |\n",
      "|    time_elapsed         | 1155        |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005588684 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 9360        |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 74.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=918528, episode_reward=-63.16 +/- 28.70\n",
      "Episode length: 299.60 +/- 113.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 300         |\n",
      "|    mean_reward          | -63.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 918528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004116795 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.9         |\n",
      "|    n_updates            | 9370        |\n",
      "|    policy_gradient_loss | -0.000257   |\n",
      "|    value_loss           | 62.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 793    |\n",
      "|    iterations      | 449    |\n",
      "|    time_elapsed    | 1158   |\n",
      "|    total_timesteps | 919552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 450         |\n",
      "|    time_elapsed         | 1161        |\n",
      "|    total_timesteps      | 921600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008047569 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.57        |\n",
      "|    n_updates            | 9380        |\n",
      "|    policy_gradient_loss | 0.00032     |\n",
      "|    value_loss           | 32.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 451         |\n",
      "|    time_elapsed         | 1162        |\n",
      "|    total_timesteps      | 923648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019443363 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.422      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 9390        |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 59.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 452         |\n",
      "|    time_elapsed         | 1165        |\n",
      "|    total_timesteps      | 925696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005583772 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65.5        |\n",
      "|    n_updates            | 9400        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 794          |\n",
      "|    iterations           | 453          |\n",
      "|    time_elapsed         | 1167         |\n",
      "|    total_timesteps      | 927744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049984525 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 9410         |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    value_loss           | 58.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=928528, episode_reward=192.13 +/- 45.12\n",
      "Episode length: 446.40 +/- 147.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 446         |\n",
      "|    mean_reward          | 192         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 928528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008630741 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.73        |\n",
      "|    n_updates            | 9420        |\n",
      "|    policy_gradient_loss | -0.000398   |\n",
      "|    value_loss           | 26.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 794    |\n",
      "|    iterations      | 454    |\n",
      "|    time_elapsed    | 1170   |\n",
      "|    total_timesteps | 929792 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 794         |\n",
      "|    iterations           | 455         |\n",
      "|    time_elapsed         | 1172        |\n",
      "|    total_timesteps      | 931840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004133596 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.327      |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 88.7        |\n",
      "|    n_updates            | 9430        |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 201         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 456          |\n",
      "|    time_elapsed         | 1174         |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072276797 |\n",
      "|    clip_fraction        | 0.0597       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45         |\n",
      "|    n_updates            | 9440         |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    value_loss           | 74.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 457         |\n",
      "|    time_elapsed         | 1176        |\n",
      "|    total_timesteps      | 935936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005684411 |\n",
      "|    clip_fraction        | 0.0507      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 9450        |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 1178        |\n",
      "|    total_timesteps      | 937984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010623941 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.99        |\n",
      "|    n_updates            | 9460        |\n",
      "|    policy_gradient_loss | -0.00083    |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=938528, episode_reward=140.81 +/- 108.34\n",
      "Episode length: 439.80 +/- 162.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 440        |\n",
      "|    mean_reward          | 141        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 938528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00503412 |\n",
      "|    clip_fraction        | 0.051      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.332     |\n",
      "|    explained_variance   | 0.943      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 9470       |\n",
      "|    policy_gradient_loss | 0.000161   |\n",
      "|    value_loss           | 46.6       |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 795    |\n",
      "|    iterations      | 459    |\n",
      "|    time_elapsed    | 1181   |\n",
      "|    total_timesteps | 940032 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 460          |\n",
      "|    time_elapsed         | 1183         |\n",
      "|    total_timesteps      | 942080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039150245 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.4         |\n",
      "|    n_updates            | 9480         |\n",
      "|    policy_gradient_loss | -0.000354    |\n",
      "|    value_loss           | 64.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 461          |\n",
      "|    time_elapsed         | 1186         |\n",
      "|    total_timesteps      | 944128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065820185 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.346       |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.77         |\n",
      "|    n_updates            | 9490         |\n",
      "|    policy_gradient_loss | -0.000426    |\n",
      "|    value_loss           | 53.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 462          |\n",
      "|    time_elapsed         | 1188         |\n",
      "|    total_timesteps      | 946176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034914888 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.84         |\n",
      "|    n_updates            | 9500         |\n",
      "|    policy_gradient_loss | -0.000299    |\n",
      "|    value_loss           | 33.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 463          |\n",
      "|    time_elapsed         | 1190         |\n",
      "|    total_timesteps      | 948224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028060721 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.6         |\n",
      "|    n_updates            | 9510         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 77.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=948528, episode_reward=130.67 +/- 99.88\n",
      "Episode length: 558.80 +/- 269.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 559          |\n",
      "|    mean_reward          | 131          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 948528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041593206 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.6         |\n",
      "|    n_updates            | 9520         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 795    |\n",
      "|    iterations      | 464    |\n",
      "|    time_elapsed    | 1194   |\n",
      "|    total_timesteps | 950272 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 465         |\n",
      "|    time_elapsed         | 1196        |\n",
      "|    total_timesteps      | 952320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006520285 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 79.2        |\n",
      "|    n_updates            | 9530        |\n",
      "|    policy_gradient_loss | -0.00323    |\n",
      "|    value_loss           | 82          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 796          |\n",
      "|    iterations           | 466          |\n",
      "|    time_elapsed         | 1198         |\n",
      "|    total_timesteps      | 954368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021469544 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.5         |\n",
      "|    n_updates            | 9540         |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 467        |\n",
      "|    time_elapsed         | 1201       |\n",
      "|    total_timesteps      | 956416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00905278 |\n",
      "|    clip_fraction        | 0.0824     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.351     |\n",
      "|    explained_variance   | 0.868      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.5       |\n",
      "|    n_updates            | 9550       |\n",
      "|    policy_gradient_loss | -0.00292   |\n",
      "|    value_loss           | 92.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 468         |\n",
      "|    time_elapsed         | 1203        |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019470835 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.69        |\n",
      "|    n_updates            | 9560        |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=958528, episode_reward=16.31 +/- 77.27\n",
      "Episode length: 500.20 +/- 304.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 16.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 958528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031719343 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 9570         |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 65.5         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 795    |\n",
      "|    iterations      | 469    |\n",
      "|    time_elapsed    | 1207   |\n",
      "|    total_timesteps | 960512 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 470         |\n",
      "|    time_elapsed         | 1209        |\n",
      "|    total_timesteps      | 962560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007910589 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.04        |\n",
      "|    n_updates            | 9580        |\n",
      "|    policy_gradient_loss | -0.000804   |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 471         |\n",
      "|    time_elapsed         | 1211        |\n",
      "|    total_timesteps      | 964608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005396008 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 9590        |\n",
      "|    policy_gradient_loss | 0.00031     |\n",
      "|    value_loss           | 50          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 472         |\n",
      "|    time_elapsed         | 1213        |\n",
      "|    total_timesteps      | 966656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005084145 |\n",
      "|    clip_fraction        | 0.0619      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 9600        |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=968528, episode_reward=125.53 +/- 78.91\n",
      "Episode length: 345.20 +/- 75.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | 126          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 968528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045949966 |\n",
      "|    clip_fraction        | 0.0862       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 9610         |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 18.6         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 796    |\n",
      "|    iterations      | 473    |\n",
      "|    time_elapsed    | 1215   |\n",
      "|    total_timesteps | 968704 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 474         |\n",
      "|    time_elapsed         | 1217        |\n",
      "|    total_timesteps      | 970752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002833992 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 9620        |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 475         |\n",
      "|    time_elapsed         | 1219        |\n",
      "|    total_timesteps      | 972800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005953577 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.21        |\n",
      "|    n_updates            | 9630        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 84.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 476          |\n",
      "|    time_elapsed         | 1221         |\n",
      "|    total_timesteps      | 974848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074533485 |\n",
      "|    clip_fraction        | 0.0574       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.358       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.45         |\n",
      "|    n_updates            | 9640         |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 69           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 477         |\n",
      "|    time_elapsed         | 1223        |\n",
      "|    total_timesteps      | 976896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014222925 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 9650        |\n",
      "|    policy_gradient_loss | 0.0012      |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=978528, episode_reward=3.75 +/- 110.83\n",
      "Episode length: 245.00 +/- 74.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 245         |\n",
      "|    mean_reward          | 3.75        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 978528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004795186 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 9660        |\n",
      "|    policy_gradient_loss | 5.11e-05    |\n",
      "|    value_loss           | 31.6        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 798    |\n",
      "|    iterations      | 478    |\n",
      "|    time_elapsed    | 1226   |\n",
      "|    total_timesteps | 978944 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 479         |\n",
      "|    time_elapsed         | 1229        |\n",
      "|    total_timesteps      | 980992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008007141 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 9670        |\n",
      "|    policy_gradient_loss | -0.000928   |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 480          |\n",
      "|    time_elapsed         | 1230         |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042258245 |\n",
      "|    clip_fraction        | 0.0716       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 9680         |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 49.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 481          |\n",
      "|    time_elapsed         | 1232         |\n",
      "|    total_timesteps      | 985088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040330547 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19           |\n",
      "|    n_updates            | 9690         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 58.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 482         |\n",
      "|    time_elapsed         | 1234        |\n",
      "|    total_timesteps      | 987136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005443374 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.18        |\n",
      "|    n_updates            | 9700        |\n",
      "|    policy_gradient_loss | -0.000358   |\n",
      "|    value_loss           | 18.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=988528, episode_reward=116.49 +/- 151.62\n",
      "Episode length: 284.20 +/- 65.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 116         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 988528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009253489 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 9710        |\n",
      "|    policy_gradient_loss | -0.000155   |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 799    |\n",
      "|    iterations      | 483    |\n",
      "|    time_elapsed    | 1237   |\n",
      "|    total_timesteps | 989184 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 799          |\n",
      "|    iterations           | 484          |\n",
      "|    time_elapsed         | 1239         |\n",
      "|    total_timesteps      | 991232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017925214 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 9720         |\n",
      "|    policy_gradient_loss | -0.00058     |\n",
      "|    value_loss           | 93.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 485         |\n",
      "|    time_elapsed         | 1241        |\n",
      "|    total_timesteps      | 993280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007802331 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.27        |\n",
      "|    n_updates            | 9730        |\n",
      "|    policy_gradient_loss | -0.000143   |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 486         |\n",
      "|    time_elapsed         | 1243        |\n",
      "|    total_timesteps      | 995328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003347934 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54          |\n",
      "|    n_updates            | 9740        |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 487         |\n",
      "|    time_elapsed         | 1245        |\n",
      "|    total_timesteps      | 997376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013753052 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.74        |\n",
      "|    n_updates            | 9750        |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    value_loss           | 33          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=998528, episode_reward=139.29 +/- 101.32\n",
      "Episode length: 333.00 +/- 50.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 333          |\n",
      "|    mean_reward          | 139          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 998528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045372937 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 9760         |\n",
      "|    policy_gradient_loss | 2.56e-05     |\n",
      "|    value_loss           | 56.2         |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 800    |\n",
      "|    iterations      | 488    |\n",
      "|    time_elapsed    | 1248   |\n",
      "|    total_timesteps | 999424 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 489         |\n",
      "|    time_elapsed         | 1251        |\n",
      "|    total_timesteps      | 1001472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008357353 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.18        |\n",
      "|    n_updates            | 9770        |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 490          |\n",
      "|    time_elapsed         | 1252         |\n",
      "|    total_timesteps      | 1003520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034745773 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 9780         |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 491         |\n",
      "|    time_elapsed         | 1255        |\n",
      "|    total_timesteps      | 1005568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003125066 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.2        |\n",
      "|    n_updates            | 9790        |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 492         |\n",
      "|    time_elapsed         | 1257        |\n",
      "|    total_timesteps      | 1007616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005987869 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 9800        |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1008528, episode_reward=43.91 +/- 129.37\n",
      "Episode length: 259.00 +/- 68.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 259          |\n",
      "|    mean_reward          | 43.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1008528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066711083 |\n",
      "|    clip_fraction        | 0.0655       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.01         |\n",
      "|    n_updates            | 9810         |\n",
      "|    policy_gradient_loss | -0.000279    |\n",
      "|    value_loss           | 25.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 801     |\n",
      "|    iterations      | 493     |\n",
      "|    time_elapsed    | 1259    |\n",
      "|    total_timesteps | 1009664 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 494          |\n",
      "|    time_elapsed         | 1261         |\n",
      "|    total_timesteps      | 1011712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049538165 |\n",
      "|    clip_fraction        | 0.0866       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.23         |\n",
      "|    n_updates            | 9820         |\n",
      "|    policy_gradient_loss | 0.00162      |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 802          |\n",
      "|    iterations           | 495          |\n",
      "|    time_elapsed         | 1263         |\n",
      "|    total_timesteps      | 1013760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052690776 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 69.3         |\n",
      "|    n_updates            | 9830         |\n",
      "|    policy_gradient_loss | -0.000776    |\n",
      "|    value_loss           | 92           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 802          |\n",
      "|    iterations           | 496          |\n",
      "|    time_elapsed         | 1265         |\n",
      "|    total_timesteps      | 1015808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033168779 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 86.8         |\n",
      "|    n_updates            | 9840         |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 803         |\n",
      "|    iterations           | 497         |\n",
      "|    time_elapsed         | 1267        |\n",
      "|    total_timesteps      | 1017856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005070911 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.1        |\n",
      "|    n_updates            | 9850        |\n",
      "|    policy_gradient_loss | -0.000628   |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1018528, episode_reward=87.78 +/- 110.93\n",
      "Episode length: 420.20 +/- 293.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 420         |\n",
      "|    mean_reward          | 87.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1018528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008602214 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 77.1        |\n",
      "|    n_updates            | 9860        |\n",
      "|    policy_gradient_loss | -0.000293   |\n",
      "|    value_loss           | 65.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 802     |\n",
      "|    iterations      | 498     |\n",
      "|    time_elapsed    | 1271    |\n",
      "|    total_timesteps | 1019904 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 499        |\n",
      "|    time_elapsed         | 1273       |\n",
      "|    total_timesteps      | 1021952    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00556414 |\n",
      "|    clip_fraction        | 0.052      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.325     |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.3       |\n",
      "|    n_updates            | 9870       |\n",
      "|    policy_gradient_loss | -0.00238   |\n",
      "|    value_loss           | 71.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 803         |\n",
      "|    iterations           | 500         |\n",
      "|    time_elapsed         | 1274        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006855111 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 9880        |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    value_loss           | 28.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 803         |\n",
      "|    iterations           | 501         |\n",
      "|    time_elapsed         | 1276        |\n",
      "|    total_timesteps      | 1026048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003934251 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 9890        |\n",
      "|    policy_gradient_loss | -0.000743   |\n",
      "|    value_loss           | 84.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 502          |\n",
      "|    time_elapsed         | 1278         |\n",
      "|    total_timesteps      | 1028096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025282188 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.4         |\n",
      "|    n_updates            | 9900         |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1028528, episode_reward=36.24 +/- 119.10\n",
      "Episode length: 265.40 +/- 49.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 265          |\n",
      "|    mean_reward          | 36.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1028528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045278957 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.6         |\n",
      "|    n_updates            | 9910         |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    value_loss           | 83.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 803     |\n",
      "|    iterations      | 503     |\n",
      "|    time_elapsed    | 1281    |\n",
      "|    total_timesteps | 1030144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 504         |\n",
      "|    time_elapsed         | 1283        |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003561789 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.325      |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 9920        |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 505         |\n",
      "|    time_elapsed         | 1286        |\n",
      "|    total_timesteps      | 1034240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010091483 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 9930        |\n",
      "|    policy_gradient_loss | -1.66e-05   |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 506          |\n",
      "|    time_elapsed         | 1287         |\n",
      "|    total_timesteps      | 1036288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057524582 |\n",
      "|    clip_fraction        | 0.0526       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.486       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.24         |\n",
      "|    n_updates            | 9940         |\n",
      "|    policy_gradient_loss | -0.000803    |\n",
      "|    value_loss           | 57.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 804        |\n",
      "|    iterations           | 507        |\n",
      "|    time_elapsed         | 1290       |\n",
      "|    total_timesteps      | 1038336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00710516 |\n",
      "|    clip_fraction        | 0.0865     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.353     |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.5       |\n",
      "|    n_updates            | 9950       |\n",
      "|    policy_gradient_loss | -0.00608   |\n",
      "|    value_loss           | 52.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1038528, episode_reward=5.77 +/- 76.77\n",
      "Episode length: 411.20 +/- 312.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 411          |\n",
      "|    mean_reward          | 5.77         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1038528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040433863 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.31         |\n",
      "|    n_updates            | 9960         |\n",
      "|    policy_gradient_loss | 0.000236     |\n",
      "|    value_loss           | 64.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 804     |\n",
      "|    iterations      | 508     |\n",
      "|    time_elapsed    | 1293    |\n",
      "|    total_timesteps | 1040384 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 509          |\n",
      "|    time_elapsed         | 1295         |\n",
      "|    total_timesteps      | 1042432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034204687 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.98         |\n",
      "|    n_updates            | 9970         |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 86.7         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 805        |\n",
      "|    iterations           | 510        |\n",
      "|    time_elapsed         | 1296       |\n",
      "|    total_timesteps      | 1044480    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00780541 |\n",
      "|    clip_fraction        | 0.0723     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.345     |\n",
      "|    explained_variance   | 0.9        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.6       |\n",
      "|    n_updates            | 9980       |\n",
      "|    policy_gradient_loss | 0.00128    |\n",
      "|    value_loss           | 55.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 511          |\n",
      "|    time_elapsed         | 1299         |\n",
      "|    total_timesteps      | 1046528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046914956 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 9990         |\n",
      "|    policy_gradient_loss | 0.00091      |\n",
      "|    value_loss           | 61.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1048528, episode_reward=112.09 +/- 120.20\n",
      "Episode length: 278.00 +/- 48.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 278          |\n",
      "|    mean_reward          | 112          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1048528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048996834 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.406       |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.2         |\n",
      "|    n_updates            | 10000        |\n",
      "|    policy_gradient_loss | -0.000319    |\n",
      "|    value_loss           | 32.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 805     |\n",
      "|    iterations      | 512     |\n",
      "|    time_elapsed    | 1301    |\n",
      "|    total_timesteps | 1048576 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 805          |\n",
      "|    iterations           | 513          |\n",
      "|    time_elapsed         | 1303         |\n",
      "|    total_timesteps      | 1050624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067620105 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.3         |\n",
      "|    n_updates            | 10010        |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 514          |\n",
      "|    time_elapsed         | 1305         |\n",
      "|    total_timesteps      | 1052672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054534604 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.8         |\n",
      "|    n_updates            | 10020        |\n",
      "|    policy_gradient_loss | -0.000643    |\n",
      "|    value_loss           | 77.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 515          |\n",
      "|    time_elapsed         | 1307         |\n",
      "|    total_timesteps      | 1054720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040672673 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 10030        |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 806         |\n",
      "|    iterations           | 516         |\n",
      "|    time_elapsed         | 1309        |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023033097 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 10040       |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1058528, episode_reward=138.11 +/- 123.00\n",
      "Episode length: 281.60 +/- 34.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 138         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1058528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004154933 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 10050       |\n",
      "|    policy_gradient_loss | -0.000171   |\n",
      "|    value_loss           | 62.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 806     |\n",
      "|    iterations      | 517     |\n",
      "|    time_elapsed    | 1312    |\n",
      "|    total_timesteps | 1058816 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 807          |\n",
      "|    iterations           | 518          |\n",
      "|    time_elapsed         | 1314         |\n",
      "|    total_timesteps      | 1060864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048374436 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.3         |\n",
      "|    n_updates            | 10060        |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 519         |\n",
      "|    time_elapsed         | 1316        |\n",
      "|    total_timesteps      | 1062912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005068991 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.3        |\n",
      "|    n_updates            | 10070       |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 97.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 520         |\n",
      "|    time_elapsed         | 1318        |\n",
      "|    total_timesteps      | 1064960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011763461 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.4        |\n",
      "|    n_updates            | 10080       |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    value_loss           | 69.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 521          |\n",
      "|    time_elapsed         | 1320         |\n",
      "|    total_timesteps      | 1067008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028447467 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.2         |\n",
      "|    n_updates            | 10090        |\n",
      "|    policy_gradient_loss | -0.000992    |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1068528, episode_reward=77.00 +/- 143.22\n",
      "Episode length: 405.00 +/- 303.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 405        |\n",
      "|    mean_reward          | 77         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1068528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00640844 |\n",
      "|    clip_fraction        | 0.0453     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.315     |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 38.4       |\n",
      "|    n_updates            | 10100      |\n",
      "|    policy_gradient_loss | -0.0006    |\n",
      "|    value_loss           | 94.9       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 807     |\n",
      "|    iterations      | 522     |\n",
      "|    time_elapsed    | 1323    |\n",
      "|    total_timesteps | 1069056 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 523          |\n",
      "|    time_elapsed         | 1325         |\n",
      "|    total_timesteps      | 1071104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085734455 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.4         |\n",
      "|    n_updates            | 10110        |\n",
      "|    policy_gradient_loss | -0.00363     |\n",
      "|    value_loss           | 152          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 524         |\n",
      "|    time_elapsed         | 1327        |\n",
      "|    total_timesteps      | 1073152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007188707 |\n",
      "|    clip_fraction        | 0.0533      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.9        |\n",
      "|    n_updates            | 10120       |\n",
      "|    policy_gradient_loss | -0.000673   |\n",
      "|    value_loss           | 98.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 525          |\n",
      "|    time_elapsed         | 1329         |\n",
      "|    total_timesteps      | 1075200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071597956 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.3         |\n",
      "|    n_updates            | 10130        |\n",
      "|    policy_gradient_loss | -0.000962    |\n",
      "|    value_loss           | 91           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 809         |\n",
      "|    iterations           | 526         |\n",
      "|    time_elapsed         | 1331        |\n",
      "|    total_timesteps      | 1077248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014015765 |\n",
      "|    clip_fraction        | 0.058       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9           |\n",
      "|    n_updates            | 10140       |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1078528, episode_reward=-57.46 +/- 20.37\n",
      "Episode length: 222.60 +/- 34.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | -57.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1078528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009784253 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 10150       |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 809     |\n",
      "|    iterations      | 527     |\n",
      "|    time_elapsed    | 1333    |\n",
      "|    total_timesteps | 1079296 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 809          |\n",
      "|    iterations           | 528          |\n",
      "|    time_elapsed         | 1335         |\n",
      "|    total_timesteps      | 1081344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070512127 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.9          |\n",
      "|    n_updates            | 10160        |\n",
      "|    policy_gradient_loss | 0.000754     |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 809          |\n",
      "|    iterations           | 529          |\n",
      "|    time_elapsed         | 1337         |\n",
      "|    total_timesteps      | 1083392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058252346 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.1         |\n",
      "|    n_updates            | 10170        |\n",
      "|    policy_gradient_loss | 0.000328     |\n",
      "|    value_loss           | 69.9         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 810       |\n",
      "|    iterations           | 530       |\n",
      "|    time_elapsed         | 1339      |\n",
      "|    total_timesteps      | 1085440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0043693 |\n",
      "|    clip_fraction        | 0.0517    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.29     |\n",
      "|    explained_variance   | 0.884     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 34.1      |\n",
      "|    n_updates            | 10180     |\n",
      "|    policy_gradient_loss | -0.00226  |\n",
      "|    value_loss           | 124       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 531         |\n",
      "|    time_elapsed         | 1341        |\n",
      "|    total_timesteps      | 1087488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004162846 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 10190       |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1088528, episode_reward=100.83 +/- 155.78\n",
      "Episode length: 318.60 +/- 86.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1088528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025399337 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.2         |\n",
      "|    n_updates            | 10200        |\n",
      "|    policy_gradient_loss | -0.00044     |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 809     |\n",
      "|    iterations      | 532     |\n",
      "|    time_elapsed    | 1345    |\n",
      "|    total_timesteps | 1089536 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 809          |\n",
      "|    iterations           | 533          |\n",
      "|    time_elapsed         | 1347         |\n",
      "|    total_timesteps      | 1091584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069898795 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.39         |\n",
      "|    n_updates            | 10210        |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 38.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 809         |\n",
      "|    iterations           | 534         |\n",
      "|    time_elapsed         | 1350        |\n",
      "|    total_timesteps      | 1093632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010215579 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.7         |\n",
      "|    n_updates            | 10220       |\n",
      "|    policy_gradient_loss | 0.00058     |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 535         |\n",
      "|    time_elapsed         | 1352        |\n",
      "|    total_timesteps      | 1095680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002974564 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.3        |\n",
      "|    n_updates            | 10230       |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 810         |\n",
      "|    iterations           | 536         |\n",
      "|    time_elapsed         | 1354        |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027987368 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.17        |\n",
      "|    n_updates            | 10240       |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 30.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1098528, episode_reward=81.88 +/- 170.53\n",
      "Episode length: 362.60 +/- 70.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | 81.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1098528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015064708 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 10250       |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    value_loss           | 77.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 810     |\n",
      "|    iterations      | 537     |\n",
      "|    time_elapsed    | 1356    |\n",
      "|    total_timesteps | 1099776 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 538          |\n",
      "|    time_elapsed         | 1358         |\n",
      "|    total_timesteps      | 1101824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031231213 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.335       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.8         |\n",
      "|    n_updates            | 10260        |\n",
      "|    policy_gradient_loss | 0.000531     |\n",
      "|    value_loss           | 43.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 539         |\n",
      "|    time_elapsed         | 1360        |\n",
      "|    total_timesteps      | 1103872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003679338 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.4        |\n",
      "|    n_updates            | 10270       |\n",
      "|    policy_gradient_loss | -0.000492   |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 540         |\n",
      "|    time_elapsed         | 1363        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005749241 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 10280       |\n",
      "|    policy_gradient_loss | -0.00262    |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 541         |\n",
      "|    time_elapsed         | 1365        |\n",
      "|    total_timesteps      | 1107968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004460587 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 10290       |\n",
      "|    policy_gradient_loss | -0.000539   |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1108528, episode_reward=73.16 +/- 140.34\n",
      "Episode length: 262.40 +/- 53.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | 73.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1108528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006233134 |\n",
      "|    clip_fraction        | 0.0409      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.1        |\n",
      "|    n_updates            | 10300       |\n",
      "|    policy_gradient_loss | -0.00224    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 811     |\n",
      "|    iterations      | 542     |\n",
      "|    time_elapsed    | 1368    |\n",
      "|    total_timesteps | 1110016 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 543         |\n",
      "|    time_elapsed         | 1370        |\n",
      "|    total_timesteps      | 1112064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008595805 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.71        |\n",
      "|    n_updates            | 10310       |\n",
      "|    policy_gradient_loss | -7.91e-05   |\n",
      "|    value_loss           | 23.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 544         |\n",
      "|    time_elapsed         | 1372        |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013166638 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 10320       |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    value_loss           | 59.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 545          |\n",
      "|    time_elapsed         | 1374         |\n",
      "|    total_timesteps      | 1116160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053826007 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.308       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 56.5         |\n",
      "|    n_updates            | 10330        |\n",
      "|    policy_gradient_loss | -0.000553    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 546          |\n",
      "|    time_elapsed         | 1376         |\n",
      "|    total_timesteps      | 1118208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023300906 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 10340        |\n",
      "|    policy_gradient_loss | -0.00043     |\n",
      "|    value_loss           | 69           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1118528, episode_reward=-15.72 +/- 98.26\n",
      "Episode length: 268.80 +/- 80.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 269         |\n",
      "|    mean_reward          | -15.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1118528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007518764 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 10350       |\n",
      "|    policy_gradient_loss | 0.0025      |\n",
      "|    value_loss           | 60          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 812     |\n",
      "|    iterations      | 547     |\n",
      "|    time_elapsed    | 1378    |\n",
      "|    total_timesteps | 1120256 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 548         |\n",
      "|    time_elapsed         | 1380        |\n",
      "|    total_timesteps      | 1122304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005241164 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 10360       |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 549         |\n",
      "|    time_elapsed         | 1382        |\n",
      "|    total_timesteps      | 1124352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007112877 |\n",
      "|    clip_fraction        | 0.0677      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 10370       |\n",
      "|    policy_gradient_loss | -0.00106    |\n",
      "|    value_loss           | 19.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 550          |\n",
      "|    time_elapsed         | 1384         |\n",
      "|    total_timesteps      | 1126400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031098728 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.363       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.3         |\n",
      "|    n_updates            | 10380        |\n",
      "|    policy_gradient_loss | 0.000389     |\n",
      "|    value_loss           | 88.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 551         |\n",
      "|    time_elapsed         | 1386        |\n",
      "|    total_timesteps      | 1128448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013654998 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 134         |\n",
      "|    n_updates            | 10390       |\n",
      "|    policy_gradient_loss | -0.00486    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1128528, episode_reward=66.31 +/- 105.31\n",
      "Episode length: 450.00 +/- 285.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 450         |\n",
      "|    mean_reward          | 66.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1128528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002478876 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 112         |\n",
      "|    n_updates            | 10400       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 813     |\n",
      "|    iterations      | 552     |\n",
      "|    time_elapsed    | 1389    |\n",
      "|    total_timesteps | 1130496 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 553          |\n",
      "|    time_elapsed         | 1391         |\n",
      "|    total_timesteps      | 1132544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049373237 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67.8         |\n",
      "|    n_updates            | 10410        |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 554          |\n",
      "|    time_elapsed         | 1394         |\n",
      "|    total_timesteps      | 1134592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062941825 |\n",
      "|    clip_fraction        | 0.0769       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 10420        |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 67.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 555         |\n",
      "|    time_elapsed         | 1396        |\n",
      "|    total_timesteps      | 1136640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012913452 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.88        |\n",
      "|    n_updates            | 10430       |\n",
      "|    policy_gradient_loss | -0.000697   |\n",
      "|    value_loss           | 94.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1138528, episode_reward=132.86 +/- 94.17\n",
      "Episode length: 356.20 +/- 57.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | 133          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1138528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051920125 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.1         |\n",
      "|    n_updates            | 10440        |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 77.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 813     |\n",
      "|    iterations      | 556     |\n",
      "|    time_elapsed    | 1400    |\n",
      "|    total_timesteps | 1138688 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 557         |\n",
      "|    time_elapsed         | 1402        |\n",
      "|    total_timesteps      | 1140736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016773695 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.37        |\n",
      "|    n_updates            | 10450       |\n",
      "|    policy_gradient_loss | 0.000929    |\n",
      "|    value_loss           | 48.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 813         |\n",
      "|    iterations           | 558         |\n",
      "|    time_elapsed         | 1404        |\n",
      "|    total_timesteps      | 1142784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013026759 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.26        |\n",
      "|    n_updates            | 10460       |\n",
      "|    policy_gradient_loss | 0.000365    |\n",
      "|    value_loss           | 34.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 559          |\n",
      "|    time_elapsed         | 1405         |\n",
      "|    total_timesteps      | 1144832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054041757 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.9         |\n",
      "|    n_updates            | 10470        |\n",
      "|    policy_gradient_loss | -0.000879    |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 560         |\n",
      "|    time_elapsed         | 1407        |\n",
      "|    total_timesteps      | 1146880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003439455 |\n",
      "|    clip_fraction        | 0.0484      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.1        |\n",
      "|    n_updates            | 10480       |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    value_loss           | 81.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1148528, episode_reward=49.04 +/- 109.42\n",
      "Episode length: 534.40 +/- 383.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 534          |\n",
      "|    mean_reward          | 49           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1148528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066721896 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.3         |\n",
      "|    n_updates            | 10490        |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 813     |\n",
      "|    iterations      | 561     |\n",
      "|    time_elapsed    | 1411    |\n",
      "|    total_timesteps | 1148928 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 814          |\n",
      "|    iterations           | 562          |\n",
      "|    time_elapsed         | 1413         |\n",
      "|    total_timesteps      | 1150976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045572435 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 10500        |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 563         |\n",
      "|    time_elapsed         | 1415        |\n",
      "|    total_timesteps      | 1153024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011921501 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.2        |\n",
      "|    n_updates            | 10510       |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 61.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 564         |\n",
      "|    time_elapsed         | 1417        |\n",
      "|    total_timesteps      | 1155072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010830265 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.5        |\n",
      "|    n_updates            | 10520       |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 815          |\n",
      "|    iterations           | 565          |\n",
      "|    time_elapsed         | 1419         |\n",
      "|    total_timesteps      | 1157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034454982 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 10530        |\n",
      "|    policy_gradient_loss | 0.000439     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1158528, episode_reward=-5.58 +/- 121.66\n",
      "Episode length: 269.00 +/- 42.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | -5.58        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1158528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059972918 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.286       |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 10540        |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    value_loss           | 98           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 815     |\n",
      "|    iterations      | 566     |\n",
      "|    time_elapsed    | 1421    |\n",
      "|    total_timesteps | 1159168 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 567         |\n",
      "|    time_elapsed         | 1423        |\n",
      "|    total_timesteps      | 1161216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004782234 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 10550       |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 218         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 568          |\n",
      "|    time_elapsed         | 1425         |\n",
      "|    total_timesteps      | 1163264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077095544 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 10560        |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 86.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 569         |\n",
      "|    time_elapsed         | 1427        |\n",
      "|    total_timesteps      | 1165312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006446378 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 10570       |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    value_loss           | 52.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 570          |\n",
      "|    time_elapsed         | 1429         |\n",
      "|    total_timesteps      | 1167360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033377036 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.1         |\n",
      "|    n_updates            | 10580        |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    value_loss           | 84.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1168528, episode_reward=140.49 +/- 109.00\n",
      "Episode length: 306.20 +/- 65.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | 140          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1168528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034035058 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.9         |\n",
      "|    n_updates            | 10590        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 571     |\n",
      "|    time_elapsed    | 1432    |\n",
      "|    total_timesteps | 1169408 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 572         |\n",
      "|    time_elapsed         | 1434        |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740473 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.64        |\n",
      "|    n_updates            | 10600       |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 573          |\n",
      "|    time_elapsed         | 1436         |\n",
      "|    total_timesteps      | 1173504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028365068 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 10610        |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 574          |\n",
      "|    time_elapsed         | 1438         |\n",
      "|    total_timesteps      | 1175552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065792333 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 10620        |\n",
      "|    policy_gradient_loss | -0.00078     |\n",
      "|    value_loss           | 87.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 575         |\n",
      "|    time_elapsed         | 1440        |\n",
      "|    total_timesteps      | 1177600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005429759 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 10630       |\n",
      "|    policy_gradient_loss | 7.35e-05    |\n",
      "|    value_loss           | 56.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1178528, episode_reward=161.75 +/- 112.23\n",
      "Episode length: 446.00 +/- 277.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 446          |\n",
      "|    mean_reward          | 162          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1178528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038183802 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.322       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.7         |\n",
      "|    n_updates            | 10640        |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 175          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 576     |\n",
      "|    time_elapsed    | 1445    |\n",
      "|    total_timesteps | 1179648 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 577         |\n",
      "|    time_elapsed         | 1447        |\n",
      "|    total_timesteps      | 1181696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007994145 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.368      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 10650       |\n",
      "|    policy_gradient_loss | 7.43e-05    |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 578         |\n",
      "|    time_elapsed         | 1449        |\n",
      "|    total_timesteps      | 1183744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004702357 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.49        |\n",
      "|    n_updates            | 10660       |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 579          |\n",
      "|    time_elapsed         | 1451         |\n",
      "|    total_timesteps      | 1185792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065515027 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.422       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45           |\n",
      "|    n_updates            | 10670        |\n",
      "|    policy_gradient_loss | -0.000983    |\n",
      "|    value_loss           | 91.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 580          |\n",
      "|    time_elapsed         | 1453         |\n",
      "|    total_timesteps      | 1187840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075029833 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.9         |\n",
      "|    n_updates            | 10680        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 62.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1188528, episode_reward=208.85 +/- 27.71\n",
      "Episode length: 329.60 +/- 27.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 209         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1188528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010976871 |\n",
      "|    clip_fraction        | 0.0796      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 10690       |\n",
      "|    policy_gradient_loss | 0.00196     |\n",
      "|    value_loss           | 76.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 581     |\n",
      "|    time_elapsed    | 1456    |\n",
      "|    total_timesteps | 1189888 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 582          |\n",
      "|    time_elapsed         | 1458         |\n",
      "|    total_timesteps      | 1191936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020496235 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.3         |\n",
      "|    n_updates            | 10700        |\n",
      "|    policy_gradient_loss | 4.6e-05      |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 583          |\n",
      "|    time_elapsed         | 1461         |\n",
      "|    total_timesteps      | 1193984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024161255 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 10710        |\n",
      "|    policy_gradient_loss | -0.000392    |\n",
      "|    value_loss           | 75.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 584          |\n",
      "|    time_elapsed         | 1463         |\n",
      "|    total_timesteps      | 1196032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032025445 |\n",
      "|    clip_fraction        | 0.0345       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 10720        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 83.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 585         |\n",
      "|    time_elapsed         | 1465        |\n",
      "|    total_timesteps      | 1198080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008673573 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.328      |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 10730       |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1198528, episode_reward=50.96 +/- 115.78\n",
      "Episode length: 305.80 +/- 64.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | 51           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1198528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061199665 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.287       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.6         |\n",
      "|    n_updates            | 10740        |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 817     |\n",
      "|    iterations      | 586     |\n",
      "|    time_elapsed    | 1468    |\n",
      "|    total_timesteps | 1200128 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 587          |\n",
      "|    time_elapsed         | 1470         |\n",
      "|    total_timesteps      | 1202176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040364726 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 10750        |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 588          |\n",
      "|    time_elapsed         | 1473         |\n",
      "|    total_timesteps      | 1204224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060986043 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.6         |\n",
      "|    n_updates            | 10760        |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 209          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 589         |\n",
      "|    time_elapsed         | 1475        |\n",
      "|    total_timesteps      | 1206272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013360092 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.8        |\n",
      "|    n_updates            | 10770       |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 85.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 590         |\n",
      "|    time_elapsed         | 1478        |\n",
      "|    total_timesteps      | 1208320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008252987 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 10780       |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 78.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1208528, episode_reward=140.71 +/- 124.46\n",
      "Episode length: 429.20 +/- 293.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 429          |\n",
      "|    mean_reward          | 141          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1208528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070165796 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.8         |\n",
      "|    n_updates            | 10790        |\n",
      "|    policy_gradient_loss | -5.46e-05    |\n",
      "|    value_loss           | 65.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 591     |\n",
      "|    time_elapsed    | 1481    |\n",
      "|    total_timesteps | 1210368 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 592         |\n",
      "|    time_elapsed         | 1483        |\n",
      "|    total_timesteps      | 1212416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005433958 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 10800       |\n",
      "|    policy_gradient_loss | -8.65e-05   |\n",
      "|    value_loss           | 59.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 593          |\n",
      "|    time_elapsed         | 1486         |\n",
      "|    total_timesteps      | 1214464      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051700315 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 74.3         |\n",
      "|    n_updates            | 10810        |\n",
      "|    policy_gradient_loss | -0.000409    |\n",
      "|    value_loss           | 83.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 594          |\n",
      "|    time_elapsed         | 1488         |\n",
      "|    total_timesteps      | 1216512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042451867 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 10820        |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    value_loss           | 83.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1218528, episode_reward=133.08 +/- 120.57\n",
      "Episode length: 369.20 +/- 122.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 369         |\n",
      "|    mean_reward          | 133         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1218528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009040001 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50          |\n",
      "|    n_updates            | 10830       |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 595     |\n",
      "|    time_elapsed    | 1492    |\n",
      "|    total_timesteps | 1218560 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 816          |\n",
      "|    iterations           | 596          |\n",
      "|    time_elapsed         | 1494         |\n",
      "|    total_timesteps      | 1220608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093336925 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 10840        |\n",
      "|    policy_gradient_loss | 0.00316      |\n",
      "|    value_loss           | 26           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 816         |\n",
      "|    iterations           | 597         |\n",
      "|    time_elapsed         | 1496        |\n",
      "|    total_timesteps      | 1222656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005784031 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 10850       |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    value_loss           | 148         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 598         |\n",
      "|    time_elapsed         | 1498        |\n",
      "|    total_timesteps      | 1224704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003395745 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.9        |\n",
      "|    n_updates            | 10860       |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 81.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 599         |\n",
      "|    time_elapsed         | 1501        |\n",
      "|    total_timesteps      | 1226752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013284665 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.7        |\n",
      "|    n_updates            | 10870       |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1228528, episode_reward=160.51 +/- 106.55\n",
      "Episode length: 331.00 +/- 15.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 331          |\n",
      "|    mean_reward          | 161          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1228528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063343868 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 10880        |\n",
      "|    policy_gradient_loss | 0.000667     |\n",
      "|    value_loss           | 70.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 816     |\n",
      "|    iterations      | 600     |\n",
      "|    time_elapsed    | 1504    |\n",
      "|    total_timesteps | 1228800 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 601         |\n",
      "|    time_elapsed         | 1506        |\n",
      "|    total_timesteps      | 1230848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006327814 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.5        |\n",
      "|    n_updates            | 10890       |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 602          |\n",
      "|    time_elapsed         | 1507         |\n",
      "|    total_timesteps      | 1232896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029651124 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.8         |\n",
      "|    n_updates            | 10900        |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 172          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 603          |\n",
      "|    time_elapsed         | 1509         |\n",
      "|    total_timesteps      | 1234944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034931886 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 10910        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 604         |\n",
      "|    time_elapsed         | 1512        |\n",
      "|    total_timesteps      | 1236992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002412847 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 10920       |\n",
      "|    policy_gradient_loss | 0.000555    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1238528, episode_reward=153.89 +/- 92.46\n",
      "Episode length: 312.80 +/- 35.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 313          |\n",
      "|    mean_reward          | 154          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1238528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081622135 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 10930        |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    value_loss           | 56.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 817     |\n",
      "|    iterations      | 605     |\n",
      "|    time_elapsed    | 1515    |\n",
      "|    total_timesteps | 1239040 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 817          |\n",
      "|    iterations           | 606          |\n",
      "|    time_elapsed         | 1517         |\n",
      "|    total_timesteps      | 1241088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056434292 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.409       |\n",
      "|    explained_variance   | 0.726        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.8         |\n",
      "|    n_updates            | 10940        |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 607          |\n",
      "|    time_elapsed         | 1519         |\n",
      "|    total_timesteps      | 1243136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074430993 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.31        |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.3         |\n",
      "|    n_updates            | 10950        |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 55.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 608         |\n",
      "|    time_elapsed         | 1521        |\n",
      "|    total_timesteps      | 1245184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007943249 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 10960       |\n",
      "|    policy_gradient_loss | 0.000695    |\n",
      "|    value_loss           | 83.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 609          |\n",
      "|    time_elapsed         | 1524         |\n",
      "|    total_timesteps      | 1247232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064216806 |\n",
      "|    clip_fraction        | 0.0885       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.41        |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 10970        |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 30.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1248528, episode_reward=-35.56 +/- 17.03\n",
      "Episode length: 232.60 +/- 57.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | -35.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1248528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045905653 |\n",
      "|    clip_fraction        | 0.0313       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.269       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 10980        |\n",
      "|    policy_gradient_loss | 0.000995     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 818     |\n",
      "|    iterations      | 610     |\n",
      "|    time_elapsed    | 1526    |\n",
      "|    total_timesteps | 1249280 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 611          |\n",
      "|    time_elapsed         | 1528         |\n",
      "|    total_timesteps      | 1251328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050212033 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 10990        |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 84           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 612         |\n",
      "|    time_elapsed         | 1531        |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004669168 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.1        |\n",
      "|    n_updates            | 11000       |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 613          |\n",
      "|    time_elapsed         | 1533         |\n",
      "|    total_timesteps      | 1255424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072255703 |\n",
      "|    clip_fraction        | 0.0641       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.5         |\n",
      "|    n_updates            | 11010        |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 614          |\n",
      "|    time_elapsed         | 1535         |\n",
      "|    total_timesteps      | 1257472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062782066 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 11020        |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 59           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1258528, episode_reward=79.58 +/- 123.18\n",
      "Episode length: 287.80 +/- 72.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 288         |\n",
      "|    mean_reward          | 79.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1258528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062843 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.87        |\n",
      "|    n_updates            | 11030       |\n",
      "|    policy_gradient_loss | 0.000706    |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 818     |\n",
      "|    iterations      | 615     |\n",
      "|    time_elapsed    | 1538    |\n",
      "|    total_timesteps | 1259520 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 616          |\n",
      "|    time_elapsed         | 1540         |\n",
      "|    total_timesteps      | 1261568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030402488 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.669        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 137          |\n",
      "|    n_updates            | 11040        |\n",
      "|    policy_gradient_loss | -0.00232     |\n",
      "|    value_loss           | 199          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 617          |\n",
      "|    time_elapsed         | 1542         |\n",
      "|    total_timesteps      | 1263616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045872503 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.4         |\n",
      "|    n_updates            | 11050        |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 618         |\n",
      "|    time_elapsed         | 1544        |\n",
      "|    total_timesteps      | 1265664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009871768 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.58        |\n",
      "|    n_updates            | 11060       |\n",
      "|    policy_gradient_loss | -0.000373   |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 619          |\n",
      "|    time_elapsed         | 1546         |\n",
      "|    total_timesteps      | 1267712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038435943 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52           |\n",
      "|    n_updates            | 11070        |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1268528, episode_reward=25.85 +/- 90.69\n",
      "Episode length: 417.20 +/- 293.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 417         |\n",
      "|    mean_reward          | 25.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1268528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008091598 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 11080       |\n",
      "|    policy_gradient_loss | 3.51e-05    |\n",
      "|    value_loss           | 58.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 819     |\n",
      "|    iterations      | 620     |\n",
      "|    time_elapsed    | 1549    |\n",
      "|    total_timesteps | 1269760 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 621        |\n",
      "|    time_elapsed         | 1551       |\n",
      "|    total_timesteps      | 1271808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00506061 |\n",
      "|    clip_fraction        | 0.0415     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.233     |\n",
      "|    explained_variance   | 0.813      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21         |\n",
      "|    n_updates            | 11090      |\n",
      "|    policy_gradient_loss | -4.94e-05  |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 622          |\n",
      "|    time_elapsed         | 1554         |\n",
      "|    total_timesteps      | 1273856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039694775 |\n",
      "|    clip_fraction        | 0.0854       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.391       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 11100        |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 40           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 623          |\n",
      "|    time_elapsed         | 1556         |\n",
      "|    total_timesteps      | 1275904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125210425 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.83         |\n",
      "|    n_updates            | 11110        |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 64.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 624          |\n",
      "|    time_elapsed         | 1558         |\n",
      "|    total_timesteps      | 1277952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018891259 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.9         |\n",
      "|    n_updates            | 11120        |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 96           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1278528, episode_reward=99.28 +/- 119.98\n",
      "Episode length: 297.20 +/- 75.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | 99.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1278528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039594364 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.602        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.4         |\n",
      "|    n_updates            | 11130        |\n",
      "|    policy_gradient_loss | -9.86e-05    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 819     |\n",
      "|    iterations      | 625     |\n",
      "|    time_elapsed    | 1561    |\n",
      "|    total_timesteps | 1280000 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 626         |\n",
      "|    time_elapsed         | 1564        |\n",
      "|    total_timesteps      | 1282048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012821661 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.5        |\n",
      "|    n_updates            | 11140       |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 627          |\n",
      "|    time_elapsed         | 1566         |\n",
      "|    total_timesteps      | 1284096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077328985 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.73         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.09         |\n",
      "|    n_updates            | 11150        |\n",
      "|    policy_gradient_loss | 0.000222     |\n",
      "|    value_loss           | 74.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 628          |\n",
      "|    time_elapsed         | 1568         |\n",
      "|    total_timesteps      | 1286144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076553924 |\n",
      "|    clip_fraction        | 0.0917       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.75         |\n",
      "|    n_updates            | 11160        |\n",
      "|    policy_gradient_loss | 0.000249     |\n",
      "|    value_loss           | 27.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 629          |\n",
      "|    time_elapsed         | 1570         |\n",
      "|    total_timesteps      | 1288192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076021915 |\n",
      "|    clip_fraction        | 0.0705       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 11170        |\n",
      "|    policy_gradient_loss | -0.0028      |\n",
      "|    value_loss           | 67           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1288528, episode_reward=5.89 +/- 60.05\n",
      "Episode length: 271.40 +/- 49.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 271          |\n",
      "|    mean_reward          | 5.89         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1288528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058395676 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.962        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.1          |\n",
      "|    n_updates            | 11180        |\n",
      "|    policy_gradient_loss | -0.00083     |\n",
      "|    value_loss           | 26.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 820     |\n",
      "|    iterations      | 630     |\n",
      "|    time_elapsed    | 1573    |\n",
      "|    total_timesteps | 1290240 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 631          |\n",
      "|    time_elapsed         | 1574         |\n",
      "|    total_timesteps      | 1292288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062526916 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.423       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 11190        |\n",
      "|    policy_gradient_loss | 4e-05        |\n",
      "|    value_loss           | 53.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 820          |\n",
      "|    iterations           | 632          |\n",
      "|    time_elapsed         | 1577         |\n",
      "|    total_timesteps      | 1294336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042951577 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 11200        |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 820         |\n",
      "|    iterations           | 633         |\n",
      "|    time_elapsed         | 1579        |\n",
      "|    total_timesteps      | 1296384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004757593 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 11210       |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 821          |\n",
      "|    iterations           | 634          |\n",
      "|    time_elapsed         | 1581         |\n",
      "|    total_timesteps      | 1298432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029064806 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.713        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.3         |\n",
      "|    n_updates            | 11220        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1298528, episode_reward=22.90 +/- 178.45\n",
      "Episode length: 409.80 +/- 223.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 410         |\n",
      "|    mean_reward          | 22.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1298528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003437514 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 11230       |\n",
      "|    policy_gradient_loss | 0.000443    |\n",
      "|    value_loss           | 55.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 820     |\n",
      "|    iterations      | 635     |\n",
      "|    time_elapsed    | 1584    |\n",
      "|    total_timesteps | 1300480 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 820        |\n",
      "|    iterations           | 636        |\n",
      "|    time_elapsed         | 1586       |\n",
      "|    total_timesteps      | 1302528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03309246 |\n",
      "|    clip_fraction        | 0.0669     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 84.4       |\n",
      "|    n_updates            | 11240      |\n",
      "|    policy_gradient_loss | -0.00851   |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 637         |\n",
      "|    time_elapsed         | 1588        |\n",
      "|    total_timesteps      | 1304576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005046839 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 11250       |\n",
      "|    policy_gradient_loss | 0.000924    |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 638         |\n",
      "|    time_elapsed         | 1590        |\n",
      "|    total_timesteps      | 1306624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010238905 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44          |\n",
      "|    n_updates            | 11260       |\n",
      "|    policy_gradient_loss | -0.000628   |\n",
      "|    value_loss           | 42.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1308528, episode_reward=81.81 +/- 141.86\n",
      "Episode length: 272.80 +/- 39.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 273         |\n",
      "|    mean_reward          | 81.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1308528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002609739 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.5        |\n",
      "|    n_updates            | 11270       |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 94.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 821     |\n",
      "|    iterations      | 639     |\n",
      "|    time_elapsed    | 1593    |\n",
      "|    total_timesteps | 1308672 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 821          |\n",
      "|    iterations           | 640          |\n",
      "|    time_elapsed         | 1595         |\n",
      "|    total_timesteps      | 1310720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060396413 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 11280        |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    value_loss           | 63.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 641         |\n",
      "|    time_elapsed         | 1598        |\n",
      "|    total_timesteps      | 1312768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009752799 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 11290       |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 642         |\n",
      "|    time_elapsed         | 1600        |\n",
      "|    total_timesteps      | 1314816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020137463 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.51        |\n",
      "|    n_updates            | 11300       |\n",
      "|    policy_gradient_loss | -0.000693   |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 821          |\n",
      "|    iterations           | 643          |\n",
      "|    time_elapsed         | 1602         |\n",
      "|    total_timesteps      | 1316864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026835543 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 11310        |\n",
      "|    policy_gradient_loss | -0.000171    |\n",
      "|    value_loss           | 55.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1318528, episode_reward=161.32 +/- 123.03\n",
      "Episode length: 438.40 +/- 281.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 438          |\n",
      "|    mean_reward          | 161          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1318528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030836822 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.339       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.7         |\n",
      "|    n_updates            | 11320        |\n",
      "|    policy_gradient_loss | 0.000484     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 821     |\n",
      "|    iterations      | 644     |\n",
      "|    time_elapsed    | 1606    |\n",
      "|    total_timesteps | 1318912 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 645         |\n",
      "|    time_elapsed         | 1608        |\n",
      "|    total_timesteps      | 1320960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006169202 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 11330       |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 59.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 646         |\n",
      "|    time_elapsed         | 1610        |\n",
      "|    total_timesteps      | 1323008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010717222 |\n",
      "|    clip_fraction        | 0.0949      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.32        |\n",
      "|    n_updates            | 11340       |\n",
      "|    policy_gradient_loss | 0.00135     |\n",
      "|    value_loss           | 21.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 647         |\n",
      "|    time_elapsed         | 1612        |\n",
      "|    total_timesteps      | 1325056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005994335 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 11350       |\n",
      "|    policy_gradient_loss | -0.000931   |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 648          |\n",
      "|    time_elapsed         | 1614         |\n",
      "|    total_timesteps      | 1327104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056913504 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.3         |\n",
      "|    n_updates            | 11360        |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 56.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1328528, episode_reward=103.52 +/- 143.11\n",
      "Episode length: 272.60 +/- 65.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 273          |\n",
      "|    mean_reward          | 104          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1328528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071831075 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 11370        |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 821     |\n",
      "|    iterations      | 649     |\n",
      "|    time_elapsed    | 1617    |\n",
      "|    total_timesteps | 1329152 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 650         |\n",
      "|    time_elapsed         | 1619        |\n",
      "|    total_timesteps      | 1331200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008079652 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.1         |\n",
      "|    n_updates            | 11380       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 651          |\n",
      "|    time_elapsed         | 1621         |\n",
      "|    total_timesteps      | 1333248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067893267 |\n",
      "|    clip_fraction        | 0.061        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.19         |\n",
      "|    n_updates            | 11390        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 56.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 652          |\n",
      "|    time_elapsed         | 1623         |\n",
      "|    total_timesteps      | 1335296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033839606 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.261       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 11400        |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    value_loss           | 170          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 653          |\n",
      "|    time_elapsed         | 1625         |\n",
      "|    total_timesteps      | 1337344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035597403 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.3         |\n",
      "|    n_updates            | 11410        |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    value_loss           | 98.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1338528, episode_reward=151.63 +/- 112.91\n",
      "Episode length: 285.00 +/- 54.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1338528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056117615 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 119          |\n",
      "|    n_updates            | 11420        |\n",
      "|    policy_gradient_loss | 0.000456     |\n",
      "|    value_loss           | 67.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 822     |\n",
      "|    iterations      | 654     |\n",
      "|    time_elapsed    | 1628    |\n",
      "|    total_timesteps | 1339392 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 822        |\n",
      "|    iterations           | 655        |\n",
      "|    time_elapsed         | 1631       |\n",
      "|    total_timesteps      | 1341440    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00895378 |\n",
      "|    clip_fraction        | 0.0851     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.307     |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.2       |\n",
      "|    n_updates            | 11430      |\n",
      "|    policy_gradient_loss | -0.002     |\n",
      "|    value_loss           | 25         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 656         |\n",
      "|    time_elapsed         | 1633        |\n",
      "|    total_timesteps      | 1343488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008571392 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.88        |\n",
      "|    n_updates            | 11440       |\n",
      "|    policy_gradient_loss | 0.000758    |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 657         |\n",
      "|    time_elapsed         | 1635        |\n",
      "|    total_timesteps      | 1345536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012576645 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.9         |\n",
      "|    n_updates            | 11450       |\n",
      "|    policy_gradient_loss | 0.00223     |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 822         |\n",
      "|    iterations           | 658         |\n",
      "|    time_elapsed         | 1637        |\n",
      "|    total_timesteps      | 1347584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001786948 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 11460       |\n",
      "|    policy_gradient_loss | -0.000286   |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1348528, episode_reward=44.89 +/- 124.06\n",
      "Episode length: 275.60 +/- 62.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 276          |\n",
      "|    mean_reward          | 44.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1348528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064190654 |\n",
      "|    clip_fraction        | 0.0805       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.382       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.81         |\n",
      "|    n_updates            | 11470        |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 822     |\n",
      "|    iterations      | 659     |\n",
      "|    time_elapsed    | 1640    |\n",
      "|    total_timesteps | 1349632 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 822          |\n",
      "|    iterations           | 660          |\n",
      "|    time_elapsed         | 1642         |\n",
      "|    total_timesteps      | 1351680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051396526 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.375       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 11480        |\n",
      "|    policy_gradient_loss | -0.000546    |\n",
      "|    value_loss           | 52           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 661          |\n",
      "|    time_elapsed         | 1644         |\n",
      "|    total_timesteps      | 1353728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027851393 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.646        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.89         |\n",
      "|    n_updates            | 11490        |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 53.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 662         |\n",
      "|    time_elapsed         | 1646        |\n",
      "|    total_timesteps      | 1355776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009781575 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.376      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99.8        |\n",
      "|    n_updates            | 11500       |\n",
      "|    policy_gradient_loss | 0.00147     |\n",
      "|    value_loss           | 99.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 663          |\n",
      "|    time_elapsed         | 1648         |\n",
      "|    total_timesteps      | 1357824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039897263 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43           |\n",
      "|    n_updates            | 11510        |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1358528, episode_reward=151.30 +/- 107.70\n",
      "Episode length: 315.60 +/- 44.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1358528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037110494 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 11520        |\n",
      "|    policy_gradient_loss | 0.000769     |\n",
      "|    value_loss           | 23.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 823     |\n",
      "|    iterations      | 664     |\n",
      "|    time_elapsed    | 1652    |\n",
      "|    total_timesteps | 1359872 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 823        |\n",
      "|    iterations           | 665        |\n",
      "|    time_elapsed         | 1654       |\n",
      "|    total_timesteps      | 1361920    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00474733 |\n",
      "|    clip_fraction        | 0.0809     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.302     |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.1       |\n",
      "|    n_updates            | 11530      |\n",
      "|    policy_gradient_loss | 0.000343   |\n",
      "|    value_loss           | 52.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 666          |\n",
      "|    time_elapsed         | 1656         |\n",
      "|    total_timesteps      | 1363968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056256875 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 11540        |\n",
      "|    policy_gradient_loss | 0.000793     |\n",
      "|    value_loss           | 55.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 823        |\n",
      "|    iterations           | 667        |\n",
      "|    time_elapsed         | 1658       |\n",
      "|    total_timesteps      | 1366016    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00415419 |\n",
      "|    clip_fraction        | 0.0283     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.278     |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.4       |\n",
      "|    n_updates            | 11550      |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    value_loss           | 118        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 668         |\n",
      "|    time_elapsed         | 1660        |\n",
      "|    total_timesteps      | 1368064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004762577 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 11560       |\n",
      "|    policy_gradient_loss | -4.77e-05   |\n",
      "|    value_loss           | 74.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1368528, episode_reward=49.06 +/- 97.23\n",
      "Episode length: 326.40 +/- 141.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 49.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1368528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057542007 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.8         |\n",
      "|    n_updates            | 11570        |\n",
      "|    policy_gradient_loss | -0.000436    |\n",
      "|    value_loss           | 46.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 823     |\n",
      "|    iterations      | 669     |\n",
      "|    time_elapsed    | 1663    |\n",
      "|    total_timesteps | 1370112 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 670          |\n",
      "|    time_elapsed         | 1665         |\n",
      "|    total_timesteps      | 1372160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052022887 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 11580        |\n",
      "|    policy_gradient_loss | -0.000261    |\n",
      "|    value_loss           | 151          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 671         |\n",
      "|    time_elapsed         | 1667        |\n",
      "|    total_timesteps      | 1374208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010673593 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 11590       |\n",
      "|    policy_gradient_loss | 0.00392     |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 672         |\n",
      "|    time_elapsed         | 1670        |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003354402 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 11600       |\n",
      "|    policy_gradient_loss | -0.000829   |\n",
      "|    value_loss           | 66.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 673         |\n",
      "|    time_elapsed         | 1672        |\n",
      "|    total_timesteps      | 1378304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016611539 |\n",
      "|    clip_fraction        | 0.0556      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.7        |\n",
      "|    n_updates            | 11610       |\n",
      "|    policy_gradient_loss | 0.000442    |\n",
      "|    value_loss           | 34.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1378528, episode_reward=52.44 +/- 93.50\n",
      "Episode length: 426.40 +/- 292.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 426         |\n",
      "|    mean_reward          | 52.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1378528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004342547 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 11620       |\n",
      "|    policy_gradient_loss | 0.00447     |\n",
      "|    value_loss           | 44.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 823     |\n",
      "|    iterations      | 674     |\n",
      "|    time_elapsed    | 1676    |\n",
      "|    total_timesteps | 1380352 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 675         |\n",
      "|    time_elapsed         | 1678        |\n",
      "|    total_timesteps      | 1382400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004321091 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.9        |\n",
      "|    n_updates            | 11630       |\n",
      "|    policy_gradient_loss | -0.000645   |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 676          |\n",
      "|    time_elapsed         | 1680         |\n",
      "|    total_timesteps      | 1384448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025038477 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.9         |\n",
      "|    n_updates            | 11640        |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 73           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 677          |\n",
      "|    time_elapsed         | 1682         |\n",
      "|    total_timesteps      | 1386496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033114157 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.2         |\n",
      "|    n_updates            | 11650        |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 177          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1388528, episode_reward=65.21 +/- 83.22\n",
      "Episode length: 564.00 +/- 357.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 564         |\n",
      "|    mean_reward          | 65.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1388528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005581806 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 11660       |\n",
      "|    policy_gradient_loss | -0.000831   |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 823     |\n",
      "|    iterations      | 678     |\n",
      "|    time_elapsed    | 1686    |\n",
      "|    total_timesteps | 1388544 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 679          |\n",
      "|    time_elapsed         | 1688         |\n",
      "|    total_timesteps      | 1390592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052238433 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.9         |\n",
      "|    n_updates            | 11670        |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 94.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 680         |\n",
      "|    time_elapsed         | 1690        |\n",
      "|    total_timesteps      | 1392640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004596173 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.265      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 11680       |\n",
      "|    policy_gradient_loss | 0.000122    |\n",
      "|    value_loss           | 93.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 681          |\n",
      "|    time_elapsed         | 1692         |\n",
      "|    total_timesteps      | 1394688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039678817 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.226       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 64.2         |\n",
      "|    n_updates            | 11690        |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 682          |\n",
      "|    time_elapsed         | 1694         |\n",
      "|    total_timesteps      | 1396736      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042046458 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.9         |\n",
      "|    n_updates            | 11700        |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1398528, episode_reward=105.74 +/- 122.55\n",
      "Episode length: 319.00 +/- 61.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 106         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1398528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004486504 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.9        |\n",
      "|    n_updates            | 11710       |\n",
      "|    policy_gradient_loss | -9.08e-05   |\n",
      "|    value_loss           | 92          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 823     |\n",
      "|    iterations      | 683     |\n",
      "|    time_elapsed    | 1697    |\n",
      "|    total_timesteps | 1398784 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 684         |\n",
      "|    time_elapsed         | 1700        |\n",
      "|    total_timesteps      | 1400832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016460493 |\n",
      "|    clip_fraction        | 0.0945      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.7         |\n",
      "|    n_updates            | 11720       |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 685         |\n",
      "|    time_elapsed         | 1702        |\n",
      "|    total_timesteps      | 1402880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011354543 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 72.2        |\n",
      "|    n_updates            | 11730       |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 686          |\n",
      "|    time_elapsed         | 1703         |\n",
      "|    total_timesteps      | 1404928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033343663 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.232       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 11740        |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 824          |\n",
      "|    iterations           | 687          |\n",
      "|    time_elapsed         | 1705         |\n",
      "|    total_timesteps      | 1406976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034212046 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 11750        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1408528, episode_reward=156.04 +/- 104.75\n",
      "Episode length: 310.60 +/- 58.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 311          |\n",
      "|    mean_reward          | 156          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1408528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028599747 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 11760        |\n",
      "|    policy_gradient_loss | -0.00061     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 824     |\n",
      "|    iterations      | 688     |\n",
      "|    time_elapsed    | 1709    |\n",
      "|    total_timesteps | 1409024 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 824         |\n",
      "|    iterations           | 689         |\n",
      "|    time_elapsed         | 1710        |\n",
      "|    total_timesteps      | 1411072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026686324 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 11770       |\n",
      "|    policy_gradient_loss | 0.000119    |\n",
      "|    value_loss           | 98.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 690          |\n",
      "|    time_elapsed         | 1712         |\n",
      "|    total_timesteps      | 1413120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030949651 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 11780        |\n",
      "|    policy_gradient_loss | -0.000163    |\n",
      "|    value_loss           | 63.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 691          |\n",
      "|    time_elapsed         | 1714         |\n",
      "|    total_timesteps      | 1415168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020893533 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.5         |\n",
      "|    n_updates            | 11790        |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 692          |\n",
      "|    time_elapsed         | 1716         |\n",
      "|    total_timesteps      | 1417216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042226654 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 11800        |\n",
      "|    policy_gradient_loss | -0.000333    |\n",
      "|    value_loss           | 48.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1418528, episode_reward=130.29 +/- 131.28\n",
      "Episode length: 285.60 +/- 63.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | 130         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1418528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016208183 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 11810       |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 83.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 825     |\n",
      "|    iterations      | 693     |\n",
      "|    time_elapsed    | 1718    |\n",
      "|    total_timesteps | 1419264 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 694          |\n",
      "|    time_elapsed         | 1720         |\n",
      "|    total_timesteps      | 1421312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023945519 |\n",
      "|    clip_fraction        | 0.0441       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.5         |\n",
      "|    n_updates            | 11820        |\n",
      "|    policy_gradient_loss | -0.00087     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 695         |\n",
      "|    time_elapsed         | 1722        |\n",
      "|    total_timesteps      | 1423360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014046661 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 11830       |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 50.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 696          |\n",
      "|    time_elapsed         | 1724         |\n",
      "|    total_timesteps      | 1425408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047475654 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.6         |\n",
      "|    n_updates            | 11840        |\n",
      "|    policy_gradient_loss | -0.00371     |\n",
      "|    value_loss           | 85           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 697          |\n",
      "|    time_elapsed         | 1726         |\n",
      "|    total_timesteps      | 1427456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045363223 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.3         |\n",
      "|    n_updates            | 11850        |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 180          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1428528, episode_reward=46.21 +/- 114.27\n",
      "Episode length: 231.80 +/- 61.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 46.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1428528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030143168 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 11860        |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 94.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 826     |\n",
      "|    iterations      | 698     |\n",
      "|    time_elapsed    | 1729    |\n",
      "|    total_timesteps | 1429504 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 699         |\n",
      "|    time_elapsed         | 1731        |\n",
      "|    total_timesteps      | 1431552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004634106 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 11870       |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 158         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 700          |\n",
      "|    time_elapsed         | 1733         |\n",
      "|    total_timesteps      | 1433600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035758563 |\n",
      "|    clip_fraction        | 0.0617       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.64         |\n",
      "|    n_updates            | 11880        |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    value_loss           | 51.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 826         |\n",
      "|    iterations           | 701         |\n",
      "|    time_elapsed         | 1736        |\n",
      "|    total_timesteps      | 1435648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008188326 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.87        |\n",
      "|    n_updates            | 11890       |\n",
      "|    policy_gradient_loss | 0.00259     |\n",
      "|    value_loss           | 67.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 827        |\n",
      "|    iterations           | 702        |\n",
      "|    time_elapsed         | 1737       |\n",
      "|    total_timesteps      | 1437696    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00932003 |\n",
      "|    clip_fraction        | 0.0932     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.389     |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.07       |\n",
      "|    n_updates            | 11900      |\n",
      "|    policy_gradient_loss | 0.00295    |\n",
      "|    value_loss           | 16.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1438528, episode_reward=21.07 +/- 102.99\n",
      "Episode length: 272.40 +/- 54.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 272          |\n",
      "|    mean_reward          | 21.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1438528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052515636 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.9         |\n",
      "|    n_updates            | 11910        |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 827     |\n",
      "|    iterations      | 703     |\n",
      "|    time_elapsed    | 1740    |\n",
      "|    total_timesteps | 1439744 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 827          |\n",
      "|    iterations           | 704          |\n",
      "|    time_elapsed         | 1742         |\n",
      "|    total_timesteps      | 1441792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041177333 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.62         |\n",
      "|    n_updates            | 11920        |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 75.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 827         |\n",
      "|    iterations           | 705         |\n",
      "|    time_elapsed         | 1744        |\n",
      "|    total_timesteps      | 1443840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002181843 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.13        |\n",
      "|    n_updates            | 11930       |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 827          |\n",
      "|    iterations           | 706          |\n",
      "|    time_elapsed         | 1746         |\n",
      "|    total_timesteps      | 1445888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025259643 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.269       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.5         |\n",
      "|    n_updates            | 11940        |\n",
      "|    policy_gradient_loss | -0.000455    |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 828          |\n",
      "|    iterations           | 707          |\n",
      "|    time_elapsed         | 1748         |\n",
      "|    total_timesteps      | 1447936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067034354 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.4         |\n",
      "|    n_updates            | 11950        |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 76.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1448528, episode_reward=8.22 +/- 92.91\n",
      "Episode length: 238.40 +/- 53.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 238          |\n",
      "|    mean_reward          | 8.22         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1448528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073522353 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.9         |\n",
      "|    n_updates            | 11960        |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 827     |\n",
      "|    iterations      | 708     |\n",
      "|    time_elapsed    | 1751    |\n",
      "|    total_timesteps | 1449984 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 709         |\n",
      "|    time_elapsed         | 1753        |\n",
      "|    total_timesteps      | 1452032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005939452 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 11970       |\n",
      "|    policy_gradient_loss | -0.00079    |\n",
      "|    value_loss           | 92.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 828          |\n",
      "|    iterations           | 710          |\n",
      "|    time_elapsed         | 1755         |\n",
      "|    total_timesteps      | 1454080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042833714 |\n",
      "|    clip_fraction        | 0.0766       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08         |\n",
      "|    n_updates            | 11980        |\n",
      "|    policy_gradient_loss | 0.000182     |\n",
      "|    value_loss           | 12.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 711         |\n",
      "|    time_elapsed         | 1757        |\n",
      "|    total_timesteps      | 1456128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003922672 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 89          |\n",
      "|    n_updates            | 11990       |\n",
      "|    policy_gradient_loss | -0.000158   |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 828         |\n",
      "|    iterations           | 712         |\n",
      "|    time_elapsed         | 1759        |\n",
      "|    total_timesteps      | 1458176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005688743 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49          |\n",
      "|    n_updates            | 12000       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 95.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1458528, episode_reward=78.65 +/- 129.08\n",
      "Episode length: 261.00 +/- 75.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 261          |\n",
      "|    mean_reward          | 78.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1458528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011359968 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 12010        |\n",
      "|    policy_gradient_loss | -0.000618    |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 828     |\n",
      "|    iterations      | 713     |\n",
      "|    time_elapsed    | 1761    |\n",
      "|    total_timesteps | 1460224 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 714         |\n",
      "|    time_elapsed         | 1763        |\n",
      "|    total_timesteps      | 1462272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005339976 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 12020       |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 99.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 715         |\n",
      "|    time_elapsed         | 1765        |\n",
      "|    total_timesteps      | 1464320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006708585 |\n",
      "|    clip_fraction        | 0.059       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.77        |\n",
      "|    n_updates            | 12030       |\n",
      "|    policy_gradient_loss | 0.000226    |\n",
      "|    value_loss           | 14.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 716         |\n",
      "|    time_elapsed         | 1767        |\n",
      "|    total_timesteps      | 1466368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005655133 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.2        |\n",
      "|    n_updates            | 12040       |\n",
      "|    policy_gradient_loss | -0.00222    |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 829          |\n",
      "|    iterations           | 717          |\n",
      "|    time_elapsed         | 1769         |\n",
      "|    total_timesteps      | 1468416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022343998 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.233       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.3         |\n",
      "|    n_updates            | 12050        |\n",
      "|    policy_gradient_loss | -0.00097     |\n",
      "|    value_loss           | 176          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1468528, episode_reward=-7.62 +/- 130.37\n",
      "Episode length: 215.00 +/- 61.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 215          |\n",
      "|    mean_reward          | -7.62        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1468528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031398642 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.255       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 216          |\n",
      "|    n_updates            | 12060        |\n",
      "|    policy_gradient_loss | -9.58e-05    |\n",
      "|    value_loss           | 173          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 829     |\n",
      "|    iterations      | 718     |\n",
      "|    time_elapsed    | 1771    |\n",
      "|    total_timesteps | 1470464 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 719         |\n",
      "|    time_elapsed         | 1773        |\n",
      "|    total_timesteps      | 1472512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007470751 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.288      |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 12070       |\n",
      "|    policy_gradient_loss | -0.000709   |\n",
      "|    value_loss           | 75.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 720         |\n",
      "|    time_elapsed         | 1775        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016975252 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 12080       |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 830          |\n",
      "|    iterations           | 721          |\n",
      "|    time_elapsed         | 1778         |\n",
      "|    total_timesteps      | 1476608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023767613 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.209       |\n",
      "|    explained_variance   | 0.77         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.1         |\n",
      "|    n_updates            | 12090        |\n",
      "|    policy_gradient_loss | -0.000978    |\n",
      "|    value_loss           | 152          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1478528, episode_reward=142.71 +/- 119.46\n",
      "Episode length: 335.40 +/- 128.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 335         |\n",
      "|    mean_reward          | 143         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1478528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008033352 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 12100       |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 830     |\n",
      "|    iterations      | 722     |\n",
      "|    time_elapsed    | 1781    |\n",
      "|    total_timesteps | 1478656 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 723         |\n",
      "|    time_elapsed         | 1783        |\n",
      "|    total_timesteps      | 1480704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017782293 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 12110       |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 27.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 724         |\n",
      "|    time_elapsed         | 1785        |\n",
      "|    total_timesteps      | 1482752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004177057 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 12120       |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 830         |\n",
      "|    iterations           | 725         |\n",
      "|    time_elapsed         | 1787        |\n",
      "|    total_timesteps      | 1484800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018291175 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 12130       |\n",
      "|    policy_gradient_loss | 0.000159    |\n",
      "|    value_loss           | 45.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 830          |\n",
      "|    iterations           | 726          |\n",
      "|    time_elapsed         | 1789         |\n",
      "|    total_timesteps      | 1486848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060869907 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 12140        |\n",
      "|    policy_gradient_loss | 0.000677     |\n",
      "|    value_loss           | 68.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1488528, episode_reward=112.13 +/- 136.26\n",
      "Episode length: 271.40 +/- 80.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 271          |\n",
      "|    mean_reward          | 112          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1488528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026817797 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.309       |\n",
      "|    explained_variance   | 0.604        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 12150        |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    value_loss           | 189          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 830     |\n",
      "|    iterations      | 727     |\n",
      "|    time_elapsed    | 1791    |\n",
      "|    total_timesteps | 1488896 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 728          |\n",
      "|    time_elapsed         | 1794         |\n",
      "|    total_timesteps      | 1490944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035664132 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.4         |\n",
      "|    n_updates            | 12160        |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 729          |\n",
      "|    time_elapsed         | 1795         |\n",
      "|    total_timesteps      | 1492992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030604668 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.621        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 12170        |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 730          |\n",
      "|    time_elapsed         | 1797         |\n",
      "|    total_timesteps      | 1495040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050924765 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 12180        |\n",
      "|    policy_gradient_loss | 0.000455     |\n",
      "|    value_loss           | 65.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 731          |\n",
      "|    time_elapsed         | 1799         |\n",
      "|    total_timesteps      | 1497088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077844434 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.2         |\n",
      "|    n_updates            | 12190        |\n",
      "|    policy_gradient_loss | -0.000462    |\n",
      "|    value_loss           | 74.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1498528, episode_reward=44.17 +/- 102.78\n",
      "Episode length: 319.40 +/- 135.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 44.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1498528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008224783 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 12200       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 831     |\n",
      "|    iterations      | 732     |\n",
      "|    time_elapsed    | 1802    |\n",
      "|    total_timesteps | 1499136 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 733          |\n",
      "|    time_elapsed         | 1804         |\n",
      "|    total_timesteps      | 1501184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117452955 |\n",
      "|    clip_fraction        | 0.0714       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.305       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.5         |\n",
      "|    n_updates            | 12210        |\n",
      "|    policy_gradient_loss | 0.00148      |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 734         |\n",
      "|    time_elapsed         | 1806        |\n",
      "|    total_timesteps      | 1503232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008107578 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 12220       |\n",
      "|    policy_gradient_loss | -0.000957   |\n",
      "|    value_loss           | 43.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 735         |\n",
      "|    time_elapsed         | 1808        |\n",
      "|    total_timesteps      | 1505280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006588778 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 12230       |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 736         |\n",
      "|    time_elapsed         | 1810        |\n",
      "|    total_timesteps      | 1507328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009313724 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.7        |\n",
      "|    n_updates            | 12240       |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    value_loss           | 72.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1508528, episode_reward=106.19 +/- 131.40\n",
      "Episode length: 416.00 +/- 294.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 416          |\n",
      "|    mean_reward          | 106          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1508528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025301087 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.7         |\n",
      "|    n_updates            | 12250        |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 832     |\n",
      "|    iterations      | 737     |\n",
      "|    time_elapsed    | 1814    |\n",
      "|    total_timesteps | 1509376 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 832        |\n",
      "|    iterations           | 738        |\n",
      "|    time_elapsed         | 1816       |\n",
      "|    total_timesteps      | 1511424    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00741565 |\n",
      "|    clip_fraction        | 0.0554     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.258     |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.99       |\n",
      "|    n_updates            | 12260      |\n",
      "|    policy_gradient_loss | 0.000919   |\n",
      "|    value_loss           | 41.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 739         |\n",
      "|    time_elapsed         | 1817        |\n",
      "|    total_timesteps      | 1513472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005733421 |\n",
      "|    clip_fraction        | 0.0937      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 12270       |\n",
      "|    policy_gradient_loss | -0.000829   |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 832         |\n",
      "|    iterations           | 740         |\n",
      "|    time_elapsed         | 1819        |\n",
      "|    total_timesteps      | 1515520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615015 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 12280       |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 741          |\n",
      "|    time_elapsed         | 1821         |\n",
      "|    total_timesteps      | 1517568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021878334 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.4         |\n",
      "|    n_updates            | 12290        |\n",
      "|    policy_gradient_loss | -9.45e-05    |\n",
      "|    value_loss           | 87.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1518528, episode_reward=100.86 +/- 118.44\n",
      "Episode length: 272.00 +/- 65.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 272          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1518528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030743256 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 12300        |\n",
      "|    policy_gradient_loss | -0.000671    |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 833     |\n",
      "|    iterations      | 742     |\n",
      "|    time_elapsed    | 1824    |\n",
      "|    total_timesteps | 1519616 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 743         |\n",
      "|    time_elapsed         | 1825        |\n",
      "|    total_timesteps      | 1521664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006526747 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 12310       |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 744         |\n",
      "|    time_elapsed         | 1827        |\n",
      "|    total_timesteps      | 1523712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011866063 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55          |\n",
      "|    n_updates            | 12320       |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 833         |\n",
      "|    iterations           | 745         |\n",
      "|    time_elapsed         | 1829        |\n",
      "|    total_timesteps      | 1525760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005990379 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 12330       |\n",
      "|    policy_gradient_loss | 0.000219    |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 746          |\n",
      "|    time_elapsed         | 1831         |\n",
      "|    total_timesteps      | 1527808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062923455 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.8         |\n",
      "|    n_updates            | 12340        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1528528, episode_reward=50.72 +/- 122.61\n",
      "Episode length: 266.20 +/- 58.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 266         |\n",
      "|    mean_reward          | 50.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1528528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005130369 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.11        |\n",
      "|    n_updates            | 12350       |\n",
      "|    policy_gradient_loss | 0.00166     |\n",
      "|    value_loss           | 76.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 833     |\n",
      "|    iterations      | 747     |\n",
      "|    time_elapsed    | 1834    |\n",
      "|    total_timesteps | 1529856 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 834         |\n",
      "|    iterations           | 748         |\n",
      "|    time_elapsed         | 1836        |\n",
      "|    total_timesteps      | 1531904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006906824 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33        |\n",
      "|    n_updates            | 12360       |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 749          |\n",
      "|    time_elapsed         | 1838         |\n",
      "|    total_timesteps      | 1533952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024196147 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.2         |\n",
      "|    n_updates            | 12370        |\n",
      "|    policy_gradient_loss | 1.51e-06     |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 750          |\n",
      "|    time_elapsed         | 1840         |\n",
      "|    total_timesteps      | 1536000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029695542 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.8         |\n",
      "|    n_updates            | 12380        |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 165          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 834          |\n",
      "|    iterations           | 751          |\n",
      "|    time_elapsed         | 1842         |\n",
      "|    total_timesteps      | 1538048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036628293 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.4         |\n",
      "|    n_updates            | 12390        |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    value_loss           | 155          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1538528, episode_reward=104.02 +/- 102.43\n",
      "Episode length: 268.20 +/- 78.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 268         |\n",
      "|    mean_reward          | 104         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1538528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017685995 |\n",
      "|    clip_fraction        | 0.0663      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 12400       |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 834     |\n",
      "|    iterations      | 752     |\n",
      "|    time_elapsed    | 1844    |\n",
      "|    total_timesteps | 1540096 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 753          |\n",
      "|    time_elapsed         | 1846         |\n",
      "|    total_timesteps      | 1542144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040728534 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.264       |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 12410        |\n",
      "|    policy_gradient_loss | -0.000747    |\n",
      "|    value_loss           | 239          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 754          |\n",
      "|    time_elapsed         | 1848         |\n",
      "|    total_timesteps      | 1544192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069711395 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 12420        |\n",
      "|    policy_gradient_loss | -0.000828    |\n",
      "|    value_loss           | 74.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 835         |\n",
      "|    iterations           | 755         |\n",
      "|    time_elapsed         | 1850        |\n",
      "|    total_timesteps      | 1546240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014476282 |\n",
      "|    clip_fraction        | 0.0851      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 12430       |\n",
      "|    policy_gradient_loss | 0.000863    |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 756          |\n",
      "|    time_elapsed         | 1852         |\n",
      "|    total_timesteps      | 1548288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043327603 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.76         |\n",
      "|    n_updates            | 12440        |\n",
      "|    policy_gradient_loss | -0.000548    |\n",
      "|    value_loss           | 90.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1548528, episode_reward=64.06 +/- 106.74\n",
      "Episode length: 278.20 +/- 65.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 278         |\n",
      "|    mean_reward          | 64.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1548528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007814763 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 12450       |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 835     |\n",
      "|    iterations      | 757     |\n",
      "|    time_elapsed    | 1854    |\n",
      "|    total_timesteps | 1550336 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 758         |\n",
      "|    time_elapsed         | 1856        |\n",
      "|    total_timesteps      | 1552384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001980506 |\n",
      "|    clip_fraction        | 0.0267      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 12460       |\n",
      "|    policy_gradient_loss | -0.000297   |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 759          |\n",
      "|    time_elapsed         | 1858         |\n",
      "|    total_timesteps      | 1554432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061730742 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.6         |\n",
      "|    n_updates            | 12470        |\n",
      "|    policy_gradient_loss | -0.000227    |\n",
      "|    value_loss           | 45.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 760         |\n",
      "|    time_elapsed         | 1860        |\n",
      "|    total_timesteps      | 1556480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008877601 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 12480       |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1558528, episode_reward=15.35 +/- 133.48\n",
      "Episode length: 239.20 +/- 41.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 239          |\n",
      "|    mean_reward          | 15.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1558528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029717088 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.4         |\n",
      "|    n_updates            | 12490        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 79           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 836     |\n",
      "|    iterations      | 761     |\n",
      "|    time_elapsed    | 1863    |\n",
      "|    total_timesteps | 1558528 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 836         |\n",
      "|    iterations           | 762         |\n",
      "|    time_elapsed         | 1865        |\n",
      "|    total_timesteps      | 1560576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008120877 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 12500       |\n",
      "|    policy_gradient_loss | 0.00184     |\n",
      "|    value_loss           | 58.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 763          |\n",
      "|    time_elapsed         | 1867         |\n",
      "|    total_timesteps      | 1562624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069121784 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.35         |\n",
      "|    n_updates            | 12510        |\n",
      "|    policy_gradient_loss | -0.000358    |\n",
      "|    value_loss           | 21           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 764         |\n",
      "|    time_elapsed         | 1869        |\n",
      "|    total_timesteps      | 1564672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010605358 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65.2        |\n",
      "|    n_updates            | 12520       |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 765         |\n",
      "|    time_elapsed         | 1871        |\n",
      "|    total_timesteps      | 1566720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004360429 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 12530       |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1568528, episode_reward=128.31 +/- 120.31\n",
      "Episode length: 278.60 +/- 52.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 128          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1568528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066240523 |\n",
      "|    clip_fraction        | 0.054        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31           |\n",
      "|    n_updates            | 12540        |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 837     |\n",
      "|    iterations      | 766     |\n",
      "|    time_elapsed    | 1873    |\n",
      "|    total_timesteps | 1568768 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 767          |\n",
      "|    time_elapsed         | 1875         |\n",
      "|    total_timesteps      | 1570816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035857954 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 12550        |\n",
      "|    policy_gradient_loss | -0.000547    |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 768          |\n",
      "|    time_elapsed         | 1877         |\n",
      "|    total_timesteps      | 1572864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045291167 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.255       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.1         |\n",
      "|    n_updates            | 12560        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 769          |\n",
      "|    time_elapsed         | 1879         |\n",
      "|    total_timesteps      | 1574912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036099064 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 89.8         |\n",
      "|    n_updates            | 12570        |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 98           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 838          |\n",
      "|    iterations           | 770          |\n",
      "|    time_elapsed         | 1881         |\n",
      "|    total_timesteps      | 1576960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062037567 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 12580        |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1578528, episode_reward=59.81 +/- 96.21\n",
      "Episode length: 554.80 +/- 374.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 555        |\n",
      "|    mean_reward          | 59.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1578528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00652579 |\n",
      "|    clip_fraction        | 0.0318     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.819      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 93.6       |\n",
      "|    n_updates            | 12590      |\n",
      "|    policy_gradient_loss | 0.000281   |\n",
      "|    value_loss           | 152        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 837     |\n",
      "|    iterations      | 771     |\n",
      "|    time_elapsed    | 1884    |\n",
      "|    total_timesteps | 1579008 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 772          |\n",
      "|    time_elapsed         | 1887         |\n",
      "|    total_timesteps      | 1581056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038974725 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 12600        |\n",
      "|    policy_gradient_loss | -0.000924    |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 773         |\n",
      "|    time_elapsed         | 1889        |\n",
      "|    total_timesteps      | 1583104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010609125 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 12610       |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    value_loss           | 83.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 838          |\n",
      "|    iterations           | 774          |\n",
      "|    time_elapsed         | 1891         |\n",
      "|    total_timesteps      | 1585152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064577535 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 12620        |\n",
      "|    policy_gradient_loss | -0.000752    |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 775         |\n",
      "|    time_elapsed         | 1893        |\n",
      "|    total_timesteps      | 1587200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007306448 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 12630       |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1588528, episode_reward=47.78 +/- 87.54\n",
      "Episode length: 260.20 +/- 36.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 260          |\n",
      "|    mean_reward          | 47.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1588528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041126776 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.9         |\n",
      "|    n_updates            | 12640        |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    value_loss           | 128          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 838     |\n",
      "|    iterations      | 776     |\n",
      "|    time_elapsed    | 1895    |\n",
      "|    total_timesteps | 1589248 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 777         |\n",
      "|    time_elapsed         | 1897        |\n",
      "|    total_timesteps      | 1591296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008929611 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 12650       |\n",
      "|    policy_gradient_loss | 0.000742    |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 838         |\n",
      "|    iterations           | 778         |\n",
      "|    time_elapsed         | 1899        |\n",
      "|    total_timesteps      | 1593344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015746042 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 12660       |\n",
      "|    policy_gradient_loss | 0.00204     |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 838          |\n",
      "|    iterations           | 779          |\n",
      "|    time_elapsed         | 1901         |\n",
      "|    total_timesteps      | 1595392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044913488 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.8         |\n",
      "|    n_updates            | 12670        |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    value_loss           | 78.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 839          |\n",
      "|    iterations           | 780          |\n",
      "|    time_elapsed         | 1903         |\n",
      "|    total_timesteps      | 1597440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030306447 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.7         |\n",
      "|    n_updates            | 12680        |\n",
      "|    policy_gradient_loss | -0.000383    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1598528, episode_reward=147.94 +/- 98.87\n",
      "Episode length: 327.80 +/- 64.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | 148          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1598528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031207483 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.3         |\n",
      "|    n_updates            | 12690        |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    value_loss           | 97.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 839     |\n",
      "|    iterations      | 781     |\n",
      "|    time_elapsed    | 1906    |\n",
      "|    total_timesteps | 1599488 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 839          |\n",
      "|    iterations           | 782          |\n",
      "|    time_elapsed         | 1908         |\n",
      "|    total_timesteps      | 1601536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031080602 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.235       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.93         |\n",
      "|    n_updates            | 12700        |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    value_loss           | 89.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 839         |\n",
      "|    iterations           | 783         |\n",
      "|    time_elapsed         | 1910        |\n",
      "|    total_timesteps      | 1603584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004526963 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.96        |\n",
      "|    n_updates            | 12710       |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 49          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 839         |\n",
      "|    iterations           | 784         |\n",
      "|    time_elapsed         | 1912        |\n",
      "|    total_timesteps      | 1605632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003987076 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.6        |\n",
      "|    n_updates            | 12720       |\n",
      "|    policy_gradient_loss | -0.000279   |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 839         |\n",
      "|    iterations           | 785         |\n",
      "|    time_elapsed         | 1914        |\n",
      "|    total_timesteps      | 1607680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373662 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.3        |\n",
      "|    n_updates            | 12730       |\n",
      "|    policy_gradient_loss | -0.000571   |\n",
      "|    value_loss           | 89.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1608528, episode_reward=101.17 +/- 130.79\n",
      "Episode length: 286.20 +/- 43.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | 101          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1608528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066546462 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.2         |\n",
      "|    n_updates            | 12740        |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 839     |\n",
      "|    iterations      | 786     |\n",
      "|    time_elapsed    | 1916    |\n",
      "|    total_timesteps | 1609728 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 840          |\n",
      "|    iterations           | 787          |\n",
      "|    time_elapsed         | 1918         |\n",
      "|    total_timesteps      | 1611776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020031563 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 12750        |\n",
      "|    policy_gradient_loss | 0.000761     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 840          |\n",
      "|    iterations           | 788          |\n",
      "|    time_elapsed         | 1920         |\n",
      "|    total_timesteps      | 1613824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046946863 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 12760        |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 840         |\n",
      "|    iterations           | 789         |\n",
      "|    time_elapsed         | 1922        |\n",
      "|    total_timesteps      | 1615872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005457444 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 12770       |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 840          |\n",
      "|    iterations           | 790          |\n",
      "|    time_elapsed         | 1924         |\n",
      "|    total_timesteps      | 1617920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031304716 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.595        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.2         |\n",
      "|    n_updates            | 12780        |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1618528, episode_reward=41.88 +/- 119.99\n",
      "Episode length: 310.20 +/- 57.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 310         |\n",
      "|    mean_reward          | 41.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1618528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008708443 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.9        |\n",
      "|    n_updates            | 12790       |\n",
      "|    policy_gradient_loss | -0.000466   |\n",
      "|    value_loss           | 54.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 840     |\n",
      "|    iterations      | 791     |\n",
      "|    time_elapsed    | 1927    |\n",
      "|    total_timesteps | 1619968 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 840         |\n",
      "|    iterations           | 792         |\n",
      "|    time_elapsed         | 1929        |\n",
      "|    total_timesteps      | 1622016     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007551649 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 12800       |\n",
      "|    policy_gradient_loss | 0.000254    |\n",
      "|    value_loss           | 60.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 793         |\n",
      "|    time_elapsed         | 1930        |\n",
      "|    total_timesteps      | 1624064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010802843 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 12810       |\n",
      "|    policy_gradient_loss | -0.000384   |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 794         |\n",
      "|    time_elapsed         | 1933        |\n",
      "|    total_timesteps      | 1626112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005665556 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 12820       |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 841          |\n",
      "|    iterations           | 795          |\n",
      "|    time_elapsed         | 1935         |\n",
      "|    total_timesteps      | 1628160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052724835 |\n",
      "|    clip_fraction        | 0.0624       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 12830        |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1628528, episode_reward=134.93 +/- 88.04\n",
      "Episode length: 334.20 +/- 83.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | 135         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1628528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004834365 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 12840       |\n",
      "|    policy_gradient_loss | -0.000723   |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 841     |\n",
      "|    iterations      | 796     |\n",
      "|    time_elapsed    | 1937    |\n",
      "|    total_timesteps | 1630208 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 841          |\n",
      "|    iterations           | 797          |\n",
      "|    time_elapsed         | 1939         |\n",
      "|    total_timesteps      | 1632256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026179922 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.246       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 78.9         |\n",
      "|    n_updates            | 12850        |\n",
      "|    policy_gradient_loss | 0.000527     |\n",
      "|    value_loss           | 89.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 798         |\n",
      "|    time_elapsed         | 1941        |\n",
      "|    total_timesteps      | 1634304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004914608 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 12860       |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 799         |\n",
      "|    time_elapsed         | 1944        |\n",
      "|    total_timesteps      | 1636352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002657448 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.4        |\n",
      "|    n_updates            | 12870       |\n",
      "|    policy_gradient_loss | -0.000128   |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 800         |\n",
      "|    time_elapsed         | 1946        |\n",
      "|    total_timesteps      | 1638400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014320758 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.9         |\n",
      "|    n_updates            | 12880       |\n",
      "|    policy_gradient_loss | 0.000598    |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1638528, episode_reward=121.20 +/- 115.32\n",
      "Episode length: 314.60 +/- 86.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 315          |\n",
      "|    mean_reward          | 121          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1638528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067017945 |\n",
      "|    clip_fraction        | 0.073        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.384       |\n",
      "|    explained_variance   | 0.571        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 115          |\n",
      "|    n_updates            | 12890        |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 169          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 841     |\n",
      "|    iterations      | 801     |\n",
      "|    time_elapsed    | 1948    |\n",
      "|    total_timesteps | 1640448 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 802         |\n",
      "|    time_elapsed         | 1950        |\n",
      "|    total_timesteps      | 1642496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007099536 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 12900       |\n",
      "|    policy_gradient_loss | 3.12e-05    |\n",
      "|    value_loss           | 58.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 842          |\n",
      "|    iterations           | 803          |\n",
      "|    time_elapsed         | 1952         |\n",
      "|    total_timesteps      | 1644544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059771137 |\n",
      "|    clip_fraction        | 0.0619       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.2         |\n",
      "|    n_updates            | 12910        |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 842          |\n",
      "|    iterations           | 804          |\n",
      "|    time_elapsed         | 1954         |\n",
      "|    total_timesteps      | 1646592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026776784 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.1         |\n",
      "|    n_updates            | 12920        |\n",
      "|    policy_gradient_loss | 0.00126      |\n",
      "|    value_loss           | 61.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1648528, episode_reward=117.53 +/- 121.95\n",
      "Episode length: 256.20 +/- 59.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | 118          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1648528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053023277 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67.5         |\n",
      "|    n_updates            | 12930        |\n",
      "|    policy_gradient_loss | -2.69e-05    |\n",
      "|    value_loss           | 79.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 842     |\n",
      "|    iterations      | 805     |\n",
      "|    time_elapsed    | 1956    |\n",
      "|    total_timesteps | 1648640 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 842          |\n",
      "|    iterations           | 806          |\n",
      "|    time_elapsed         | 1958         |\n",
      "|    total_timesteps      | 1650688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059272265 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.26         |\n",
      "|    n_updates            | 12940        |\n",
      "|    policy_gradient_loss | -0.000526    |\n",
      "|    value_loss           | 70.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 807          |\n",
      "|    time_elapsed         | 1960         |\n",
      "|    total_timesteps      | 1652736      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033109626 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 12950        |\n",
      "|    policy_gradient_loss | -0.000667    |\n",
      "|    value_loss           | 84           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 843         |\n",
      "|    iterations           | 808         |\n",
      "|    time_elapsed         | 1962        |\n",
      "|    total_timesteps      | 1654784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004062813 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.02        |\n",
      "|    n_updates            | 12960       |\n",
      "|    policy_gradient_loss | -0.000156   |\n",
      "|    value_loss           | 75.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 809          |\n",
      "|    time_elapsed         | 1964         |\n",
      "|    total_timesteps      | 1656832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044836365 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 92.7         |\n",
      "|    n_updates            | 12970        |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 236          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1658528, episode_reward=97.80 +/- 115.43\n",
      "Episode length: 331.80 +/- 165.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 97.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1658528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005782121 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.9        |\n",
      "|    n_updates            | 12980       |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 843     |\n",
      "|    iterations      | 810     |\n",
      "|    time_elapsed    | 1967    |\n",
      "|    total_timesteps | 1658880 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 843         |\n",
      "|    iterations           | 811         |\n",
      "|    time_elapsed         | 1968        |\n",
      "|    total_timesteps      | 1660928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007114971 |\n",
      "|    clip_fraction        | 0.0518      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 12990       |\n",
      "|    policy_gradient_loss | 0.00251     |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 812          |\n",
      "|    time_elapsed         | 1970         |\n",
      "|    total_timesteps      | 1662976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059849657 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 76.1         |\n",
      "|    n_updates            | 13000        |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 82.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 843         |\n",
      "|    iterations           | 813         |\n",
      "|    time_elapsed         | 1972        |\n",
      "|    total_timesteps      | 1665024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010400003 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.1        |\n",
      "|    n_updates            | 13010       |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    value_loss           | 269         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 844          |\n",
      "|    iterations           | 814          |\n",
      "|    time_elapsed         | 1975         |\n",
      "|    total_timesteps      | 1667072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087204445 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.4         |\n",
      "|    n_updates            | 13020        |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    value_loss           | 47           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1668528, episode_reward=168.53 +/- 117.90\n",
      "Episode length: 293.40 +/- 65.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | 169         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1668528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004609548 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.41        |\n",
      "|    n_updates            | 13030       |\n",
      "|    policy_gradient_loss | -0.000494   |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 843     |\n",
      "|    iterations      | 815     |\n",
      "|    time_elapsed    | 1977    |\n",
      "|    total_timesteps | 1669120 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 844          |\n",
      "|    iterations           | 816          |\n",
      "|    time_elapsed         | 1979         |\n",
      "|    total_timesteps      | 1671168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048044845 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.95         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.58         |\n",
      "|    n_updates            | 13040        |\n",
      "|    policy_gradient_loss | -0.000727    |\n",
      "|    value_loss           | 47.2         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 844        |\n",
      "|    iterations           | 817        |\n",
      "|    time_elapsed         | 1981       |\n",
      "|    total_timesteps      | 1673216    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00402768 |\n",
      "|    clip_fraction        | 0.0307     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.362     |\n",
      "|    explained_variance   | 0.737      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40         |\n",
      "|    n_updates            | 13050      |\n",
      "|    policy_gradient_loss | -0.0015    |\n",
      "|    value_loss           | 107        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 844         |\n",
      "|    iterations           | 818         |\n",
      "|    time_elapsed         | 1983        |\n",
      "|    total_timesteps      | 1675264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008117784 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.3         |\n",
      "|    n_updates            | 13060       |\n",
      "|    policy_gradient_loss | 0.000173    |\n",
      "|    value_loss           | 34.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 844          |\n",
      "|    iterations           | 819          |\n",
      "|    time_elapsed         | 1986         |\n",
      "|    total_timesteps      | 1677312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040385867 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.7         |\n",
      "|    n_updates            | 13070        |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 96.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1678528, episode_reward=11.35 +/- 123.78\n",
      "Episode length: 229.00 +/- 63.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 229         |\n",
      "|    mean_reward          | 11.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1678528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004714873 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 13080       |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    value_loss           | 82.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 844     |\n",
      "|    iterations      | 820     |\n",
      "|    time_elapsed    | 1988    |\n",
      "|    total_timesteps | 1679360 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 844          |\n",
      "|    iterations           | 821          |\n",
      "|    time_elapsed         | 1990         |\n",
      "|    total_timesteps      | 1681408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060472004 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.266       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 13090        |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 845          |\n",
      "|    iterations           | 822          |\n",
      "|    time_elapsed         | 1992         |\n",
      "|    total_timesteps      | 1683456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036338998 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.3         |\n",
      "|    n_updates            | 13100        |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 845          |\n",
      "|    iterations           | 823          |\n",
      "|    time_elapsed         | 1994         |\n",
      "|    total_timesteps      | 1685504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034248638 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.233       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.49         |\n",
      "|    n_updates            | 13110        |\n",
      "|    policy_gradient_loss | 0.000275     |\n",
      "|    value_loss           | 47.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 845          |\n",
      "|    iterations           | 824          |\n",
      "|    time_elapsed         | 1995         |\n",
      "|    total_timesteps      | 1687552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037471936 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.36        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 13120        |\n",
      "|    policy_gradient_loss | -0.000555    |\n",
      "|    value_loss           | 31.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1688528, episode_reward=73.77 +/- 147.85\n",
      "Episode length: 234.40 +/- 63.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 234          |\n",
      "|    mean_reward          | 73.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1688528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019944482 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 13130        |\n",
      "|    policy_gradient_loss | -0.000397    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 845     |\n",
      "|    iterations      | 825     |\n",
      "|    time_elapsed    | 1998    |\n",
      "|    total_timesteps | 1689600 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 845         |\n",
      "|    iterations           | 826         |\n",
      "|    time_elapsed         | 2000        |\n",
      "|    total_timesteps      | 1691648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004149164 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8           |\n",
      "|    n_updates            | 13140       |\n",
      "|    policy_gradient_loss | -0.000676   |\n",
      "|    value_loss           | 90.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 845          |\n",
      "|    iterations           | 827          |\n",
      "|    time_elapsed         | 2002         |\n",
      "|    total_timesteps      | 1693696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060983477 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 13150        |\n",
      "|    policy_gradient_loss | -0.000252    |\n",
      "|    value_loss           | 53.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 828          |\n",
      "|    time_elapsed         | 2003         |\n",
      "|    total_timesteps      | 1695744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025354214 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.6         |\n",
      "|    n_updates            | 13160        |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    value_loss           | 91.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 829          |\n",
      "|    time_elapsed         | 2005         |\n",
      "|    total_timesteps      | 1697792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047906656 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.6         |\n",
      "|    n_updates            | 13170        |\n",
      "|    policy_gradient_loss | -0.000456    |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1698528, episode_reward=131.66 +/- 102.85\n",
      "Episode length: 440.00 +/- 283.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 440          |\n",
      "|    mean_reward          | 132          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1698528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037529538 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 13180        |\n",
      "|    policy_gradient_loss | 5.71e-05     |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 846     |\n",
      "|    iterations      | 830     |\n",
      "|    time_elapsed    | 2008    |\n",
      "|    total_timesteps | 1699840 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 831          |\n",
      "|    time_elapsed         | 2011         |\n",
      "|    total_timesteps      | 1701888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033275501 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.234       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.8         |\n",
      "|    n_updates            | 13190        |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 846         |\n",
      "|    iterations           | 832         |\n",
      "|    time_elapsed         | 2013        |\n",
      "|    total_timesteps      | 1703936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011223238 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.3        |\n",
      "|    n_updates            | 13200       |\n",
      "|    policy_gradient_loss | 0.000312    |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 846         |\n",
      "|    iterations           | 833         |\n",
      "|    time_elapsed         | 2015        |\n",
      "|    total_timesteps      | 1705984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012338475 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.63        |\n",
      "|    n_updates            | 13210       |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    value_loss           | 62.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 834          |\n",
      "|    time_elapsed         | 2017         |\n",
      "|    total_timesteps      | 1708032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069912774 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.4         |\n",
      "|    n_updates            | 13220        |\n",
      "|    policy_gradient_loss | 2.91e-05     |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1708528, episode_reward=21.30 +/- 104.16\n",
      "Episode length: 235.00 +/- 41.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 235          |\n",
      "|    mean_reward          | 21.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1708528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052095326 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 13230        |\n",
      "|    policy_gradient_loss | 0.00164      |\n",
      "|    value_loss           | 64.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 846     |\n",
      "|    iterations      | 835     |\n",
      "|    time_elapsed    | 2019    |\n",
      "|    total_timesteps | 1710080 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 836         |\n",
      "|    time_elapsed         | 2021        |\n",
      "|    total_timesteps      | 1712128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003076281 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.21       |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 13240       |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 847          |\n",
      "|    iterations           | 837          |\n",
      "|    time_elapsed         | 2022         |\n",
      "|    total_timesteps      | 1714176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062314533 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.8         |\n",
      "|    n_updates            | 13250        |\n",
      "|    policy_gradient_loss | -0.00559     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 838         |\n",
      "|    time_elapsed         | 2024        |\n",
      "|    total_timesteps      | 1716224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002462951 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 13260       |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 839         |\n",
      "|    time_elapsed         | 2026        |\n",
      "|    total_timesteps      | 1718272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013895435 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.371      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.77        |\n",
      "|    n_updates            | 13270       |\n",
      "|    policy_gradient_loss | -0.000828   |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1718528, episode_reward=27.11 +/- 112.85\n",
      "Episode length: 265.20 +/- 21.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 265          |\n",
      "|    mean_reward          | 27.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1718528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031892336 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 13280        |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 847     |\n",
      "|    iterations      | 840     |\n",
      "|    time_elapsed    | 2029    |\n",
      "|    total_timesteps | 1720320 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 847         |\n",
      "|    iterations           | 841         |\n",
      "|    time_elapsed         | 2031        |\n",
      "|    total_timesteps      | 1722368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004024186 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 13290       |\n",
      "|    policy_gradient_loss | -0.000267   |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 842         |\n",
      "|    time_elapsed         | 2033        |\n",
      "|    total_timesteps      | 1724416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003935761 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 13300       |\n",
      "|    policy_gradient_loss | -0.000961   |\n",
      "|    value_loss           | 74.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 843         |\n",
      "|    time_elapsed         | 2035        |\n",
      "|    total_timesteps      | 1726464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006777004 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 72.9        |\n",
      "|    n_updates            | 13310       |\n",
      "|    policy_gradient_loss | 0.000509    |\n",
      "|    value_loss           | 88.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 848          |\n",
      "|    iterations           | 844          |\n",
      "|    time_elapsed         | 2037         |\n",
      "|    total_timesteps      | 1728512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075691887 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.426       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 13320        |\n",
      "|    policy_gradient_loss | 0.00159      |\n",
      "|    value_loss           | 51.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1728528, episode_reward=118.19 +/- 117.54\n",
      "Episode length: 272.80 +/- 77.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 273          |\n",
      "|    mean_reward          | 118          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1728528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029638247 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 13330        |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 73.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 848     |\n",
      "|    iterations      | 845     |\n",
      "|    time_elapsed    | 2040    |\n",
      "|    total_timesteps | 1730560 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 846         |\n",
      "|    time_elapsed         | 2042        |\n",
      "|    total_timesteps      | 1732608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006226616 |\n",
      "|    clip_fraction        | 0.0773      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.8        |\n",
      "|    n_updates            | 13340       |\n",
      "|    policy_gradient_loss | -0.000829   |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 847         |\n",
      "|    time_elapsed         | 2044        |\n",
      "|    total_timesteps      | 1734656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008080449 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.7        |\n",
      "|    n_updates            | 13350       |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 848          |\n",
      "|    iterations           | 848          |\n",
      "|    time_elapsed         | 2045         |\n",
      "|    total_timesteps      | 1736704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031235078 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 155          |\n",
      "|    n_updates            | 13360        |\n",
      "|    policy_gradient_loss | -0.000527    |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1738528, episode_reward=68.42 +/- 133.50\n",
      "Episode length: 228.20 +/- 69.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 68.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1738528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068925885 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 13370        |\n",
      "|    policy_gradient_loss | 7.71e-05     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 848     |\n",
      "|    iterations      | 849     |\n",
      "|    time_elapsed    | 2048    |\n",
      "|    total_timesteps | 1738752 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 850          |\n",
      "|    time_elapsed         | 2050         |\n",
      "|    total_timesteps      | 1740800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054141404 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.2         |\n",
      "|    n_updates            | 13380        |\n",
      "|    policy_gradient_loss | -0.000636    |\n",
      "|    value_loss           | 78.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 851         |\n",
      "|    time_elapsed         | 2052        |\n",
      "|    total_timesteps      | 1742848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987898 |\n",
      "|    clip_fraction        | 0.0851      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 13390       |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    value_loss           | 31.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 852         |\n",
      "|    time_elapsed         | 2054        |\n",
      "|    total_timesteps      | 1744896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013713825 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.72        |\n",
      "|    n_updates            | 13400       |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 853          |\n",
      "|    time_elapsed         | 2056         |\n",
      "|    total_timesteps      | 1746944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077487146 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.07         |\n",
      "|    n_updates            | 13410        |\n",
      "|    policy_gradient_loss | 5.1e-05      |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1748528, episode_reward=104.61 +/- 115.16\n",
      "Episode length: 317.00 +/- 72.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | 105          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1748528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027746619 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.1         |\n",
      "|    n_updates            | 13420        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 849     |\n",
      "|    iterations      | 854     |\n",
      "|    time_elapsed    | 2059    |\n",
      "|    total_timesteps | 1748992 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 855          |\n",
      "|    time_elapsed         | 2061         |\n",
      "|    total_timesteps      | 1751040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070675323 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.9         |\n",
      "|    n_updates            | 13430        |\n",
      "|    policy_gradient_loss | -0.000837    |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 856          |\n",
      "|    time_elapsed         | 2063         |\n",
      "|    total_timesteps      | 1753088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031963768 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.674        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 13440        |\n",
      "|    policy_gradient_loss | -5.51e-05    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 857          |\n",
      "|    time_elapsed         | 2065         |\n",
      "|    total_timesteps      | 1755136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048645064 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.34         |\n",
      "|    n_updates            | 13450        |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 54.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 858          |\n",
      "|    time_elapsed         | 2067         |\n",
      "|    total_timesteps      | 1757184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054230858 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.84         |\n",
      "|    n_updates            | 13460        |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 52.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1758528, episode_reward=222.21 +/- 43.56\n",
      "Episode length: 446.80 +/- 245.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 447         |\n",
      "|    mean_reward          | 222         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1758528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008607631 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.22        |\n",
      "|    n_updates            | 13470       |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 849     |\n",
      "|    iterations      | 859     |\n",
      "|    time_elapsed    | 2071    |\n",
      "|    total_timesteps | 1759232 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 860         |\n",
      "|    time_elapsed         | 2073        |\n",
      "|    total_timesteps      | 1761280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005399038 |\n",
      "|    clip_fraction        | 0.0523      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.1        |\n",
      "|    n_updates            | 13480       |\n",
      "|    policy_gradient_loss | -0.000595   |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 861         |\n",
      "|    time_elapsed         | 2075        |\n",
      "|    total_timesteps      | 1763328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002663067 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 13490       |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 849         |\n",
      "|    iterations           | 862         |\n",
      "|    time_elapsed         | 2076        |\n",
      "|    total_timesteps      | 1765376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009043774 |\n",
      "|    clip_fraction        | 0.0984      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.53        |\n",
      "|    n_updates            | 13500       |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    value_loss           | 53.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 863         |\n",
      "|    time_elapsed         | 2078        |\n",
      "|    total_timesteps      | 1767424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003938498 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.7        |\n",
      "|    n_updates            | 13510       |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1768528, episode_reward=-2.20 +/- 89.75\n",
      "Episode length: 218.20 +/- 61.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 218        |\n",
      "|    mean_reward          | -2.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1768528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00997908 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.386     |\n",
      "|    explained_variance   | 0.839      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.18       |\n",
      "|    n_updates            | 13520      |\n",
      "|    policy_gradient_loss | 0.000558   |\n",
      "|    value_loss           | 49.6       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 850     |\n",
      "|    iterations      | 864     |\n",
      "|    time_elapsed    | 2081    |\n",
      "|    total_timesteps | 1769472 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 865         |\n",
      "|    time_elapsed         | 2083        |\n",
      "|    total_timesteps      | 1771520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004894454 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 13530       |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 866         |\n",
      "|    time_elapsed         | 2085        |\n",
      "|    total_timesteps      | 1773568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004253446 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 13540       |\n",
      "|    policy_gradient_loss | 0.000671    |\n",
      "|    value_loss           | 61.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 850          |\n",
      "|    iterations           | 867          |\n",
      "|    time_elapsed         | 2087         |\n",
      "|    total_timesteps      | 1775616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027675042 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.1         |\n",
      "|    n_updates            | 13550        |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 868         |\n",
      "|    time_elapsed         | 2089        |\n",
      "|    total_timesteps      | 1777664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004539863 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71.6        |\n",
      "|    n_updates            | 13560       |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1778528, episode_reward=-14.60 +/- 124.80\n",
      "Episode length: 226.40 +/- 54.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | -14.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1778528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049232286 |\n",
      "|    clip_fraction        | 0.0627       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 13570        |\n",
      "|    policy_gradient_loss | -0.000327    |\n",
      "|    value_loss           | 67.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 850     |\n",
      "|    iterations      | 869     |\n",
      "|    time_elapsed    | 2092    |\n",
      "|    total_timesteps | 1779712 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 870         |\n",
      "|    time_elapsed         | 2094        |\n",
      "|    total_timesteps      | 1781760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022872414 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 13580       |\n",
      "|    policy_gradient_loss | 0.003       |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 871         |\n",
      "|    time_elapsed         | 2096        |\n",
      "|    total_timesteps      | 1783808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004180521 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.7        |\n",
      "|    n_updates            | 13590       |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 872         |\n",
      "|    time_elapsed         | 2098        |\n",
      "|    total_timesteps      | 1785856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005509285 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 13600       |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 873         |\n",
      "|    time_elapsed         | 2100        |\n",
      "|    total_timesteps      | 1787904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020318046 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.87        |\n",
      "|    n_updates            | 13610       |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1788528, episode_reward=151.48 +/- 104.34\n",
      "Episode length: 290.20 +/- 32.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1788528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031036083 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.5         |\n",
      "|    n_updates            | 13620        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 84.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 851     |\n",
      "|    iterations      | 874     |\n",
      "|    time_elapsed    | 2102    |\n",
      "|    total_timesteps | 1789952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 875         |\n",
      "|    time_elapsed         | 2104        |\n",
      "|    total_timesteps      | 1792000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003519305 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.232      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45          |\n",
      "|    n_updates            | 13630       |\n",
      "|    policy_gradient_loss | -0.00245    |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 876         |\n",
      "|    time_elapsed         | 2106        |\n",
      "|    total_timesteps      | 1794048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008960227 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 82.9        |\n",
      "|    n_updates            | 13640       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 851          |\n",
      "|    iterations           | 877          |\n",
      "|    time_elapsed         | 2108         |\n",
      "|    total_timesteps      | 1796096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038309582 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.7         |\n",
      "|    n_updates            | 13650        |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 62.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 851         |\n",
      "|    iterations           | 878         |\n",
      "|    time_elapsed         | 2111        |\n",
      "|    total_timesteps      | 1798144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006166015 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.197      |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 13660       |\n",
      "|    policy_gradient_loss | 5.53e-05    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1798528, episode_reward=103.54 +/- 171.71\n",
      "Episode length: 262.00 +/- 47.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | 104         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1798528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008017737 |\n",
      "|    clip_fraction        | 0.08        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.92        |\n",
      "|    n_updates            | 13670       |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    value_loss           | 57.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 851     |\n",
      "|    iterations      | 879     |\n",
      "|    time_elapsed    | 2113    |\n",
      "|    total_timesteps | 1800192 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 851        |\n",
      "|    iterations           | 880        |\n",
      "|    time_elapsed         | 2115       |\n",
      "|    total_timesteps      | 1802240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00305829 |\n",
      "|    clip_fraction        | 0.0357     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.221     |\n",
      "|    explained_variance   | 0.629      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 18.5       |\n",
      "|    n_updates            | 13680      |\n",
      "|    policy_gradient_loss | -0.00197   |\n",
      "|    value_loss           | 152        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 851          |\n",
      "|    iterations           | 881          |\n",
      "|    time_elapsed         | 2117         |\n",
      "|    total_timesteps      | 1804288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059284703 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 13690        |\n",
      "|    policy_gradient_loss | 0.00033      |\n",
      "|    value_loss           | 57.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 882         |\n",
      "|    time_elapsed         | 2120        |\n",
      "|    total_timesteps      | 1806336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007920435 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.5        |\n",
      "|    n_updates            | 13700       |\n",
      "|    policy_gradient_loss | 0.00184     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 883         |\n",
      "|    time_elapsed         | 2121        |\n",
      "|    total_timesteps      | 1808384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009071682 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.98        |\n",
      "|    n_updates            | 13710       |\n",
      "|    policy_gradient_loss | -2.85e-05   |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1808528, episode_reward=152.44 +/- 123.26\n",
      "Episode length: 289.80 +/- 68.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1808528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055322424 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.5         |\n",
      "|    n_updates            | 13720        |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 852     |\n",
      "|    iterations      | 884     |\n",
      "|    time_elapsed    | 2124    |\n",
      "|    total_timesteps | 1810432 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 885         |\n",
      "|    time_elapsed         | 2127        |\n",
      "|    total_timesteps      | 1812480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007410845 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.93        |\n",
      "|    n_updates            | 13730       |\n",
      "|    policy_gradient_loss | -0.000901   |\n",
      "|    value_loss           | 39.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 886         |\n",
      "|    time_elapsed         | 2129        |\n",
      "|    total_timesteps      | 1814528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007439409 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 13740       |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 887          |\n",
      "|    time_elapsed         | 2131         |\n",
      "|    total_timesteps      | 1816576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048020547 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.8         |\n",
      "|    n_updates            | 13750        |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    value_loss           | 68.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1818528, episode_reward=136.29 +/- 130.57\n",
      "Episode length: 290.20 +/- 58.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 136          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1818528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053645205 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.444       |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.87         |\n",
      "|    n_updates            | 13760        |\n",
      "|    policy_gradient_loss | 0.00187      |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 852     |\n",
      "|    iterations      | 888     |\n",
      "|    time_elapsed    | 2133    |\n",
      "|    total_timesteps | 1818624 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 889          |\n",
      "|    time_elapsed         | 2135         |\n",
      "|    total_timesteps      | 1820672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067082443 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 13770        |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 890          |\n",
      "|    time_elapsed         | 2138         |\n",
      "|    total_timesteps      | 1822720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027070558 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.9         |\n",
      "|    n_updates            | 13780        |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    value_loss           | 80.9         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 852       |\n",
      "|    iterations           | 891       |\n",
      "|    time_elapsed         | 2140      |\n",
      "|    total_timesteps      | 1824768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0415019 |\n",
      "|    clip_fraction        | 0.114     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.378    |\n",
      "|    explained_variance   | 0.943     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.83      |\n",
      "|    n_updates            | 13790     |\n",
      "|    policy_gradient_loss | 0.00151   |\n",
      "|    value_loss           | 7.8       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 852         |\n",
      "|    iterations           | 892         |\n",
      "|    time_elapsed         | 2142        |\n",
      "|    total_timesteps      | 1826816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004797865 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 13800       |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1828528, episode_reward=6.61 +/- 91.64\n",
      "Episode length: 203.00 +/- 48.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 203         |\n",
      "|    mean_reward          | 6.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1828528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924699 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.98        |\n",
      "|    n_updates            | 13810       |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 75.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 852     |\n",
      "|    iterations      | 893     |\n",
      "|    time_elapsed    | 2144    |\n",
      "|    total_timesteps | 1828864 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 894          |\n",
      "|    time_elapsed         | 2146         |\n",
      "|    total_timesteps      | 1830912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023091666 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 13820        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 853         |\n",
      "|    iterations           | 895         |\n",
      "|    time_elapsed         | 2148        |\n",
      "|    total_timesteps      | 1832960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002324624 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 13830       |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 853          |\n",
      "|    iterations           | 896          |\n",
      "|    time_elapsed         | 2150         |\n",
      "|    total_timesteps      | 1835008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034534086 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.2         |\n",
      "|    n_updates            | 13840        |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 90.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 853         |\n",
      "|    iterations           | 897         |\n",
      "|    time_elapsed         | 2152        |\n",
      "|    total_timesteps      | 1837056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009449524 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 13850       |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1838528, episode_reward=92.82 +/- 126.24\n",
      "Episode length: 285.80 +/- 59.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | 92.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1838528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004557658 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 13860       |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    value_loss           | 80.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 853     |\n",
      "|    iterations      | 898     |\n",
      "|    time_elapsed    | 2155    |\n",
      "|    total_timesteps | 1839104 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 853          |\n",
      "|    iterations           | 899          |\n",
      "|    time_elapsed         | 2157         |\n",
      "|    total_timesteps      | 1841152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048355665 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.82         |\n",
      "|    n_updates            | 13870        |\n",
      "|    policy_gradient_loss | -0.000535    |\n",
      "|    value_loss           | 76.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 853         |\n",
      "|    iterations           | 900         |\n",
      "|    time_elapsed         | 2159        |\n",
      "|    total_timesteps      | 1843200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005204264 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 13880       |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 76          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 853         |\n",
      "|    iterations           | 901         |\n",
      "|    time_elapsed         | 2161        |\n",
      "|    total_timesteps      | 1845248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002257756 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.236      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 13890       |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 91.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 854          |\n",
      "|    iterations           | 902          |\n",
      "|    time_elapsed         | 2163         |\n",
      "|    total_timesteps      | 1847296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053102556 |\n",
      "|    clip_fraction        | 0.0911       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.36        |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.71         |\n",
      "|    n_updates            | 13900        |\n",
      "|    policy_gradient_loss | -0.000534    |\n",
      "|    value_loss           | 20.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1848528, episode_reward=73.36 +/- 129.17\n",
      "Episode length: 248.60 +/- 71.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 73.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1848528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004047055 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.84        |\n",
      "|    n_updates            | 13910       |\n",
      "|    policy_gradient_loss | -0.000909   |\n",
      "|    value_loss           | 86.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 854     |\n",
      "|    iterations      | 903     |\n",
      "|    time_elapsed    | 2165    |\n",
      "|    total_timesteps | 1849344 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 904         |\n",
      "|    time_elapsed         | 2167        |\n",
      "|    total_timesteps      | 1851392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004713027 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 13920       |\n",
      "|    policy_gradient_loss | -0.000546   |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 905         |\n",
      "|    time_elapsed         | 2169        |\n",
      "|    total_timesteps      | 1853440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002496436 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 13930       |\n",
      "|    policy_gradient_loss | -0.000514   |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 906         |\n",
      "|    time_elapsed         | 2171        |\n",
      "|    total_timesteps      | 1855488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004531148 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 13940       |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 87.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 854         |\n",
      "|    iterations           | 907         |\n",
      "|    time_elapsed         | 2173        |\n",
      "|    total_timesteps      | 1857536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012469919 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.9        |\n",
      "|    n_updates            | 13950       |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 69.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1858528, episode_reward=78.95 +/- 119.35\n",
      "Episode length: 239.60 +/- 46.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 240         |\n",
      "|    mean_reward          | 79          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1858528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004206962 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71.8        |\n",
      "|    n_updates            | 13960       |\n",
      "|    policy_gradient_loss | 2.81e-06    |\n",
      "|    value_loss           | 73.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 854     |\n",
      "|    iterations      | 908     |\n",
      "|    time_elapsed    | 2175    |\n",
      "|    total_timesteps | 1859584 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 854          |\n",
      "|    iterations           | 909          |\n",
      "|    time_elapsed         | 2177         |\n",
      "|    total_timesteps      | 1861632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032388968 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.1         |\n",
      "|    n_updates            | 13970        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 910         |\n",
      "|    time_elapsed         | 2179        |\n",
      "|    total_timesteps      | 1863680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007076309 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.51        |\n",
      "|    n_updates            | 13980       |\n",
      "|    policy_gradient_loss | 0.000145    |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 911         |\n",
      "|    time_elapsed         | 2181        |\n",
      "|    total_timesteps      | 1865728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006077462 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.5        |\n",
      "|    n_updates            | 13990       |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 912         |\n",
      "|    time_elapsed         | 2183        |\n",
      "|    total_timesteps      | 1867776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005886643 |\n",
      "|    clip_fraction        | 0.0461      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.8        |\n",
      "|    n_updates            | 14000       |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 85.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1868528, episode_reward=-40.08 +/- 52.32\n",
      "Episode length: 189.40 +/- 31.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 189        |\n",
      "|    mean_reward          | -40.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1868528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00426574 |\n",
      "|    clip_fraction        | 0.0383     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.189     |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.1       |\n",
      "|    n_updates            | 14010      |\n",
      "|    policy_gradient_loss | -0.000578  |\n",
      "|    value_loss           | 159        |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 855     |\n",
      "|    iterations      | 913     |\n",
      "|    time_elapsed    | 2185    |\n",
      "|    total_timesteps | 1869824 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 855          |\n",
      "|    iterations           | 914          |\n",
      "|    time_elapsed         | 2188         |\n",
      "|    total_timesteps      | 1871872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035309354 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.64         |\n",
      "|    n_updates            | 14020        |\n",
      "|    policy_gradient_loss | 0.00121      |\n",
      "|    value_loss           | 71.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 915         |\n",
      "|    time_elapsed         | 2190        |\n",
      "|    total_timesteps      | 1873920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005703142 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 14030       |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 85.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 855         |\n",
      "|    iterations           | 916         |\n",
      "|    time_elapsed         | 2191        |\n",
      "|    total_timesteps      | 1875968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005059203 |\n",
      "|    clip_fraction        | 0.0531      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 14040       |\n",
      "|    policy_gradient_loss | -0.00322    |\n",
      "|    value_loss           | 84.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 856          |\n",
      "|    iterations           | 917          |\n",
      "|    time_elapsed         | 2193         |\n",
      "|    total_timesteps      | 1878016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045276033 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 14050        |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1878528, episode_reward=-17.40 +/- 125.31\n",
      "Episode length: 208.20 +/- 56.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 208         |\n",
      "|    mean_reward          | -17.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1878528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005485348 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 14060       |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 91.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 856     |\n",
      "|    iterations      | 918     |\n",
      "|    time_elapsed    | 2196    |\n",
      "|    total_timesteps | 1880064 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 919         |\n",
      "|    time_elapsed         | 2197        |\n",
      "|    total_timesteps      | 1882112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005752283 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.5        |\n",
      "|    n_updates            | 14070       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 144         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 856          |\n",
      "|    iterations           | 920          |\n",
      "|    time_elapsed         | 2199         |\n",
      "|    total_timesteps      | 1884160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033594596 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 84.3         |\n",
      "|    n_updates            | 14080        |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 856          |\n",
      "|    iterations           | 921          |\n",
      "|    time_elapsed         | 2201         |\n",
      "|    total_timesteps      | 1886208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061310614 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.253       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 14090        |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 857          |\n",
      "|    iterations           | 922          |\n",
      "|    time_elapsed         | 2203         |\n",
      "|    total_timesteps      | 1888256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035358714 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 14100        |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 310          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1888528, episode_reward=105.73 +/- 83.99\n",
      "Episode length: 286.40 +/- 65.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | 106         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1888528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004576427 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 93.5        |\n",
      "|    n_updates            | 14110       |\n",
      "|    policy_gradient_loss | -0.000832   |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 856     |\n",
      "|    iterations      | 923     |\n",
      "|    time_elapsed    | 2205    |\n",
      "|    total_timesteps | 1890304 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 924         |\n",
      "|    time_elapsed         | 2207        |\n",
      "|    total_timesteps      | 1892352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003839083 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 129         |\n",
      "|    n_updates            | 14120       |\n",
      "|    policy_gradient_loss | 1.47e-05    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 925         |\n",
      "|    time_elapsed         | 2209        |\n",
      "|    total_timesteps      | 1894400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005188721 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.204      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 14130       |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 93.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 926         |\n",
      "|    time_elapsed         | 2211        |\n",
      "|    total_timesteps      | 1896448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018117901 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.8        |\n",
      "|    n_updates            | 14140       |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 857          |\n",
      "|    iterations           | 927          |\n",
      "|    time_elapsed         | 2213         |\n",
      "|    total_timesteps      | 1898496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074387714 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.238       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 82.1         |\n",
      "|    n_updates            | 14150        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 210          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1898528, episode_reward=138.05 +/- 120.62\n",
      "Episode length: 279.40 +/- 68.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 138          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1898528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029514763 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.9         |\n",
      "|    n_updates            | 14160        |\n",
      "|    policy_gradient_loss | -0.000441    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 857     |\n",
      "|    iterations      | 928     |\n",
      "|    time_elapsed    | 2215    |\n",
      "|    total_timesteps | 1900544 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 929         |\n",
      "|    time_elapsed         | 2217        |\n",
      "|    total_timesteps      | 1902592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013905039 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.236      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 14170       |\n",
      "|    policy_gradient_loss | 0.000668    |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 857          |\n",
      "|    iterations           | 930          |\n",
      "|    time_elapsed         | 2220         |\n",
      "|    total_timesteps      | 1904640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065918444 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.48         |\n",
      "|    n_updates            | 14180        |\n",
      "|    policy_gradient_loss | 0.000518     |\n",
      "|    value_loss           | 68.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 931         |\n",
      "|    time_elapsed         | 2221        |\n",
      "|    total_timesteps      | 1906688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006514323 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 14190       |\n",
      "|    policy_gradient_loss | 3.44e-05    |\n",
      "|    value_loss           | 88.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1908528, episode_reward=-24.88 +/- 223.11\n",
      "Episode length: 273.60 +/- 69.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | -24.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1908528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041463682 |\n",
      "|    clip_fraction        | 0.0514       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 14200        |\n",
      "|    policy_gradient_loss | -0.000704    |\n",
      "|    value_loss           | 64.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 858     |\n",
      "|    iterations      | 932     |\n",
      "|    time_elapsed    | 2224    |\n",
      "|    total_timesteps | 1908736 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 933         |\n",
      "|    time_elapsed         | 2226        |\n",
      "|    total_timesteps      | 1910784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003654097 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 14210       |\n",
      "|    policy_gradient_loss | -0.00036    |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 934         |\n",
      "|    time_elapsed         | 2227        |\n",
      "|    total_timesteps      | 1912832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003055991 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.2        |\n",
      "|    n_updates            | 14220       |\n",
      "|    policy_gradient_loss | -0.00337    |\n",
      "|    value_loss           | 202         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 858          |\n",
      "|    iterations           | 935          |\n",
      "|    time_elapsed         | 2229         |\n",
      "|    total_timesteps      | 1914880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018945988 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.3         |\n",
      "|    n_updates            | 14230        |\n",
      "|    policy_gradient_loss | 0.00113      |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 858          |\n",
      "|    iterations           | 936          |\n",
      "|    time_elapsed         | 2231         |\n",
      "|    total_timesteps      | 1916928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032596316 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.4         |\n",
      "|    n_updates            | 14240        |\n",
      "|    policy_gradient_loss | -0.000475    |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1918528, episode_reward=83.30 +/- 151.97\n",
      "Episode length: 287.40 +/- 76.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 287         |\n",
      "|    mean_reward          | 83.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1918528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006637799 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 106         |\n",
      "|    n_updates            | 14250       |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 858     |\n",
      "|    iterations      | 937     |\n",
      "|    time_elapsed    | 2234    |\n",
      "|    total_timesteps | 1918976 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 938         |\n",
      "|    time_elapsed         | 2236        |\n",
      "|    total_timesteps      | 1921024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009580173 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 14260       |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 65.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 939         |\n",
      "|    time_elapsed         | 2238        |\n",
      "|    total_timesteps      | 1923072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009774337 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 14270       |\n",
      "|    policy_gradient_loss | -4.32e-05   |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 940         |\n",
      "|    time_elapsed         | 2240        |\n",
      "|    total_timesteps      | 1925120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004830259 |\n",
      "|    clip_fraction        | 0.0363      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.54        |\n",
      "|    n_updates            | 14280       |\n",
      "|    policy_gradient_loss | 0.000247    |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 941         |\n",
      "|    time_elapsed         | 2242        |\n",
      "|    total_timesteps      | 1927168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005115321 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 14290       |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1928528, episode_reward=28.50 +/- 104.35\n",
      "Episode length: 393.00 +/- 305.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 393          |\n",
      "|    mean_reward          | 28.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1928528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025304512 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.5         |\n",
      "|    n_updates            | 14300        |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 859     |\n",
      "|    iterations      | 942     |\n",
      "|    time_elapsed    | 2245    |\n",
      "|    total_timesteps | 1929216 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 859          |\n",
      "|    iterations           | 943          |\n",
      "|    time_elapsed         | 2247         |\n",
      "|    total_timesteps      | 1931264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026041637 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.6         |\n",
      "|    n_updates            | 14310        |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 859         |\n",
      "|    iterations           | 944         |\n",
      "|    time_elapsed         | 2249        |\n",
      "|    total_timesteps      | 1933312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005237692 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.95        |\n",
      "|    n_updates            | 14320       |\n",
      "|    policy_gradient_loss | -0.000439   |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 859          |\n",
      "|    iterations           | 945          |\n",
      "|    time_elapsed         | 2251         |\n",
      "|    total_timesteps      | 1935360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043570423 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.8         |\n",
      "|    n_updates            | 14330        |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    value_loss           | 75           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 859          |\n",
      "|    iterations           | 946          |\n",
      "|    time_elapsed         | 2253         |\n",
      "|    total_timesteps      | 1937408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061608516 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.3         |\n",
      "|    n_updates            | 14340        |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1938528, episode_reward=47.61 +/- 132.57\n",
      "Episode length: 225.40 +/- 50.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 47.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1938528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014544677 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 14350        |\n",
      "|    policy_gradient_loss | -0.000371    |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 859     |\n",
      "|    iterations      | 947     |\n",
      "|    time_elapsed    | 2255    |\n",
      "|    total_timesteps | 1939456 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 948         |\n",
      "|    time_elapsed         | 2257        |\n",
      "|    total_timesteps      | 1941504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006426665 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 14360       |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 949          |\n",
      "|    time_elapsed         | 2259         |\n",
      "|    total_timesteps      | 1943552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045925565 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.287       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.1         |\n",
      "|    n_updates            | 14370        |\n",
      "|    policy_gradient_loss | 9.76e-05     |\n",
      "|    value_loss           | 168          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 950         |\n",
      "|    time_elapsed         | 2261        |\n",
      "|    total_timesteps      | 1945600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009476625 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 14380       |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 951         |\n",
      "|    time_elapsed         | 2263        |\n",
      "|    total_timesteps      | 1947648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005668344 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 14390       |\n",
      "|    policy_gradient_loss | 0.000869    |\n",
      "|    value_loss           | 61.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1948528, episode_reward=174.95 +/- 132.33\n",
      "Episode length: 299.40 +/- 56.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | 175          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1948528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032015312 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.207       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.3         |\n",
      "|    n_updates            | 14400        |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 84           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 860     |\n",
      "|    iterations      | 952     |\n",
      "|    time_elapsed    | 2266    |\n",
      "|    total_timesteps | 1949696 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 953         |\n",
      "|    time_elapsed         | 2268        |\n",
      "|    total_timesteps      | 1951744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003354975 |\n",
      "|    clip_fraction        | 0.0488      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56          |\n",
      "|    n_updates            | 14410       |\n",
      "|    policy_gradient_loss | 5.98e-05    |\n",
      "|    value_loss           | 93.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 954          |\n",
      "|    time_elapsed         | 2270         |\n",
      "|    total_timesteps      | 1953792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127498945 |\n",
      "|    clip_fraction        | 0.0859       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32           |\n",
      "|    n_updates            | 14420        |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 99.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 860         |\n",
      "|    iterations           | 955         |\n",
      "|    time_elapsed         | 2272        |\n",
      "|    total_timesteps      | 1955840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007521586 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 14430       |\n",
      "|    policy_gradient_loss | 0.000611    |\n",
      "|    value_loss           | 40.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 956          |\n",
      "|    time_elapsed         | 2274         |\n",
      "|    total_timesteps      | 1957888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037863639 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.6         |\n",
      "|    n_updates            | 14440        |\n",
      "|    policy_gradient_loss | 0.00057      |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1958528, episode_reward=142.64 +/- 107.01\n",
      "Episode length: 281.00 +/- 38.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 281         |\n",
      "|    mean_reward          | 143         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1958528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006359556 |\n",
      "|    clip_fraction        | 0.0611      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 14450       |\n",
      "|    policy_gradient_loss | -0.000129   |\n",
      "|    value_loss           | 65.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 860     |\n",
      "|    iterations      | 957     |\n",
      "|    time_elapsed    | 2276    |\n",
      "|    total_timesteps | 1959936 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 860        |\n",
      "|    iterations           | 958        |\n",
      "|    time_elapsed         | 2279       |\n",
      "|    total_timesteps      | 1961984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00692098 |\n",
      "|    clip_fraction        | 0.046      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.23      |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.16       |\n",
      "|    n_updates            | 14460      |\n",
      "|    policy_gradient_loss | 0.00096    |\n",
      "|    value_loss           | 70.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 959          |\n",
      "|    time_elapsed         | 2281         |\n",
      "|    total_timesteps      | 1964032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052553993 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.335       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.99         |\n",
      "|    n_updates            | 14470        |\n",
      "|    policy_gradient_loss | -0.000653    |\n",
      "|    value_loss           | 22.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 960          |\n",
      "|    time_elapsed         | 2283         |\n",
      "|    total_timesteps      | 1966080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034213015 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 14480        |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    value_loss           | 90           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 961         |\n",
      "|    time_elapsed         | 2285        |\n",
      "|    total_timesteps      | 1968128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008537283 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.3        |\n",
      "|    n_updates            | 14490       |\n",
      "|    policy_gradient_loss | -0.00054    |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1968528, episode_reward=59.39 +/- 134.58\n",
      "Episode length: 258.80 +/- 65.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 259          |\n",
      "|    mean_reward          | 59.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1968528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058039604 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.3          |\n",
      "|    n_updates            | 14500        |\n",
      "|    policy_gradient_loss | 4.63e-05     |\n",
      "|    value_loss           | 38.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 861     |\n",
      "|    iterations      | 962     |\n",
      "|    time_elapsed    | 2288    |\n",
      "|    total_timesteps | 1970176 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 963          |\n",
      "|    time_elapsed         | 2289         |\n",
      "|    total_timesteps      | 1972224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045874203 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.5         |\n",
      "|    n_updates            | 14510        |\n",
      "|    policy_gradient_loss | -0.000779    |\n",
      "|    value_loss           | 85.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 964         |\n",
      "|    time_elapsed         | 2292        |\n",
      "|    total_timesteps      | 1974272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006835157 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.92        |\n",
      "|    n_updates            | 14520       |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 965          |\n",
      "|    time_elapsed         | 2293         |\n",
      "|    total_timesteps      | 1976320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024435236 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.675        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 14530        |\n",
      "|    policy_gradient_loss | -0.000622    |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 966          |\n",
      "|    time_elapsed         | 2295         |\n",
      "|    total_timesteps      | 1978368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035027892 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.733        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.5         |\n",
      "|    n_updates            | 14540        |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 168          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1978528, episode_reward=122.61 +/- 144.90\n",
      "Episode length: 269.80 +/- 31.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 123         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1978528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004618154 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31          |\n",
      "|    n_updates            | 14550       |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 861     |\n",
      "|    iterations      | 967     |\n",
      "|    time_elapsed    | 2298    |\n",
      "|    total_timesteps | 1980416 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 968         |\n",
      "|    time_elapsed         | 2300        |\n",
      "|    total_timesteps      | 1982464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014414689 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 14560       |\n",
      "|    policy_gradient_loss | 4.69e-06    |\n",
      "|    value_loss           | 74.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 969         |\n",
      "|    time_elapsed         | 2302        |\n",
      "|    total_timesteps      | 1984512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004610532 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 14570       |\n",
      "|    policy_gradient_loss | -0.00071    |\n",
      "|    value_loss           | 84.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 970          |\n",
      "|    time_elapsed         | 2303         |\n",
      "|    total_timesteps      | 1986560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026170607 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 14580        |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1988528, episode_reward=47.29 +/- 113.72\n",
      "Episode length: 232.80 +/- 59.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 47.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1988528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066345683 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.5         |\n",
      "|    n_updates            | 14590        |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 149          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 862     |\n",
      "|    iterations      | 971     |\n",
      "|    time_elapsed    | 2306    |\n",
      "|    total_timesteps | 1988608 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 972         |\n",
      "|    time_elapsed         | 2308        |\n",
      "|    total_timesteps      | 1990656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002139099 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.84        |\n",
      "|    n_updates            | 14600       |\n",
      "|    policy_gradient_loss | -5.1e-05    |\n",
      "|    value_loss           | 69.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 973         |\n",
      "|    time_elapsed         | 2309        |\n",
      "|    total_timesteps      | 1992704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009316396 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.11        |\n",
      "|    n_updates            | 14610       |\n",
      "|    policy_gradient_loss | 0.000902    |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 974          |\n",
      "|    time_elapsed         | 2311         |\n",
      "|    total_timesteps      | 1994752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019881611 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 66.3         |\n",
      "|    n_updates            | 14620        |\n",
      "|    policy_gradient_loss | -0.000935    |\n",
      "|    value_loss           | 169          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 975          |\n",
      "|    time_elapsed         | 2313         |\n",
      "|    total_timesteps      | 1996800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036916537 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.2         |\n",
      "|    n_updates            | 14630        |\n",
      "|    policy_gradient_loss | -0.000775    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1998528, episode_reward=7.77 +/- 98.47\n",
      "Episode length: 239.80 +/- 34.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 240         |\n",
      "|    mean_reward          | 7.77        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1998528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005630154 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 14640       |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 863     |\n",
      "|    iterations      | 976     |\n",
      "|    time_elapsed    | 2315    |\n",
      "|    total_timesteps | 1998848 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 977          |\n",
      "|    time_elapsed         | 2317         |\n",
      "|    total_timesteps      | 2000896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052538286 |\n",
      "|    clip_fraction        | 0.0533       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 14650        |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 60.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 978          |\n",
      "|    time_elapsed         | 2319         |\n",
      "|    total_timesteps      | 2002944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034719578 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.232       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 14660        |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 979          |\n",
      "|    time_elapsed         | 2321         |\n",
      "|    total_timesteps      | 2004992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059702313 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 160          |\n",
      "|    n_updates            | 14670        |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 187          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 863         |\n",
      "|    iterations           | 980         |\n",
      "|    time_elapsed         | 2323        |\n",
      "|    total_timesteps      | 2007040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016547192 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.2        |\n",
      "|    n_updates            | 14680       |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2008528, episode_reward=85.28 +/- 121.82\n",
      "Episode length: 259.20 +/- 54.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 259          |\n",
      "|    mean_reward          | 85.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2008528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022758625 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.3         |\n",
      "|    n_updates            | 14690        |\n",
      "|    policy_gradient_loss | -0.000943    |\n",
      "|    value_loss           | 163          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 863     |\n",
      "|    iterations      | 981     |\n",
      "|    time_elapsed    | 2325    |\n",
      "|    total_timesteps | 2009088 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 864         |\n",
      "|    iterations           | 982         |\n",
      "|    time_elapsed         | 2327        |\n",
      "|    total_timesteps      | 2011136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003078304 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 14700       |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 983          |\n",
      "|    time_elapsed         | 2329         |\n",
      "|    total_timesteps      | 2013184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063943407 |\n",
      "|    clip_fraction        | 0.0771       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.7         |\n",
      "|    n_updates            | 14710        |\n",
      "|    policy_gradient_loss | 0.000711     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 984          |\n",
      "|    time_elapsed         | 2331         |\n",
      "|    total_timesteps      | 2015232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040392885 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.353       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47         |\n",
      "|    n_updates            | 14720        |\n",
      "|    policy_gradient_loss | -0.000283    |\n",
      "|    value_loss           | 94           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 985          |\n",
      "|    time_elapsed         | 2333         |\n",
      "|    total_timesteps      | 2017280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031987063 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.4         |\n",
      "|    n_updates            | 14730        |\n",
      "|    policy_gradient_loss | -0.000857    |\n",
      "|    value_loss           | 164          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2018528, episode_reward=189.06 +/- 87.30\n",
      "Episode length: 308.80 +/- 48.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 309          |\n",
      "|    mean_reward          | 189          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2018528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026649795 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.207       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.1         |\n",
      "|    n_updates            | 14740        |\n",
      "|    policy_gradient_loss | -0.000823    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 864     |\n",
      "|    iterations      | 986     |\n",
      "|    time_elapsed    | 2335    |\n",
      "|    total_timesteps | 2019328 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 987          |\n",
      "|    time_elapsed         | 2337         |\n",
      "|    total_timesteps      | 2021376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054784073 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.4         |\n",
      "|    n_updates            | 14750        |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 988          |\n",
      "|    time_elapsed         | 2339         |\n",
      "|    total_timesteps      | 2023424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064603565 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.205       |\n",
      "|    explained_variance   | 0.562        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 14760        |\n",
      "|    policy_gradient_loss | -0.000392    |\n",
      "|    value_loss           | 65.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 989          |\n",
      "|    time_elapsed         | 2341         |\n",
      "|    total_timesteps      | 2025472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019620345 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.8         |\n",
      "|    n_updates            | 14770        |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 865          |\n",
      "|    iterations           | 990          |\n",
      "|    time_elapsed         | 2343         |\n",
      "|    total_timesteps      | 2027520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028764177 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70.9         |\n",
      "|    n_updates            | 14780        |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2028528, episode_reward=152.34 +/- 107.35\n",
      "Episode length: 302.20 +/- 57.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 302          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2028528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034346944 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.9         |\n",
      "|    n_updates            | 14790        |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 218          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 865     |\n",
      "|    iterations      | 991     |\n",
      "|    time_elapsed    | 2346    |\n",
      "|    total_timesteps | 2029568 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 992         |\n",
      "|    time_elapsed         | 2348        |\n",
      "|    total_timesteps      | 2031616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004720366 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.3        |\n",
      "|    n_updates            | 14800       |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 865          |\n",
      "|    iterations           | 993          |\n",
      "|    time_elapsed         | 2350         |\n",
      "|    total_timesteps      | 2033664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052893492 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.233       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.8         |\n",
      "|    n_updates            | 14810        |\n",
      "|    policy_gradient_loss | -0.000569    |\n",
      "|    value_loss           | 92.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 994         |\n",
      "|    time_elapsed         | 2351        |\n",
      "|    total_timesteps      | 2035712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008037133 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 14820       |\n",
      "|    policy_gradient_loss | 0.00179     |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 995         |\n",
      "|    time_elapsed         | 2353        |\n",
      "|    total_timesteps      | 2037760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009423986 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.03        |\n",
      "|    n_updates            | 14830       |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2038528, episode_reward=122.08 +/- 153.57\n",
      "Episode length: 260.80 +/- 44.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 261          |\n",
      "|    mean_reward          | 122          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2038528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025771235 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.1         |\n",
      "|    n_updates            | 14840        |\n",
      "|    policy_gradient_loss | 0.000227     |\n",
      "|    value_loss           | 78.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 865     |\n",
      "|    iterations      | 996     |\n",
      "|    time_elapsed    | 2356    |\n",
      "|    total_timesteps | 2039808 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 997         |\n",
      "|    time_elapsed         | 2358        |\n",
      "|    total_timesteps      | 2041856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024850035 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15        |\n",
      "|    n_updates            | 14850       |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 59.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 998          |\n",
      "|    time_elapsed         | 2360         |\n",
      "|    total_timesteps      | 2043904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036011306 |\n",
      "|    clip_fraction        | 0.0731       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44         |\n",
      "|    n_updates            | 14860        |\n",
      "|    policy_gradient_loss | 0.00282      |\n",
      "|    value_loss           | 31.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 999          |\n",
      "|    time_elapsed         | 2362         |\n",
      "|    total_timesteps      | 2045952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034444546 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.235       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 14870        |\n",
      "|    policy_gradient_loss | -0.000819    |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 1000         |\n",
      "|    time_elapsed         | 2364         |\n",
      "|    total_timesteps      | 2048000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052180225 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.322       |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 14880        |\n",
      "|    policy_gradient_loss | 0.000775     |\n",
      "|    value_loss           | 53.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2048528, episode_reward=123.61 +/- 129.12\n",
      "Episode length: 292.60 +/- 46.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | 124         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2048528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010087743 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 14890       |\n",
      "|    policy_gradient_loss | -0.00245    |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 866     |\n",
      "|    iterations      | 1001    |\n",
      "|    time_elapsed    | 2366    |\n",
      "|    total_timesteps | 2050048 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 1002        |\n",
      "|    time_elapsed         | 2368        |\n",
      "|    total_timesteps      | 2052096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007112096 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.47        |\n",
      "|    n_updates            | 14900       |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    value_loss           | 29.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 1003         |\n",
      "|    time_elapsed         | 2370         |\n",
      "|    total_timesteps      | 2054144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023402185 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.3         |\n",
      "|    n_updates            | 14910        |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 1004         |\n",
      "|    time_elapsed         | 2372         |\n",
      "|    total_timesteps      | 2056192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050390074 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.209       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 14920        |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 1005         |\n",
      "|    time_elapsed         | 2374         |\n",
      "|    total_timesteps      | 2058240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059311916 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27           |\n",
      "|    n_updates            | 14930        |\n",
      "|    policy_gradient_loss | 0.00241      |\n",
      "|    value_loss           | 86.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2058528, episode_reward=101.09 +/- 148.79\n",
      "Episode length: 264.60 +/- 49.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 265        |\n",
      "|    mean_reward          | 101        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2058528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00254287 |\n",
      "|    clip_fraction        | 0.0382     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 41.2       |\n",
      "|    n_updates            | 14940      |\n",
      "|    policy_gradient_loss | -0.000564  |\n",
      "|    value_loss           | 81         |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 866     |\n",
      "|    iterations      | 1006    |\n",
      "|    time_elapsed    | 2377    |\n",
      "|    total_timesteps | 2060288 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 1007         |\n",
      "|    time_elapsed         | 2379         |\n",
      "|    total_timesteps      | 2062336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062468583 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 64.7         |\n",
      "|    n_updates            | 14950        |\n",
      "|    policy_gradient_loss | -0.000678    |\n",
      "|    value_loss           | 45.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 1008        |\n",
      "|    time_elapsed         | 2381        |\n",
      "|    total_timesteps      | 2064384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015593203 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 14960       |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1009         |\n",
      "|    time_elapsed         | 2383         |\n",
      "|    total_timesteps      | 2066432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065956395 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 14970        |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 201          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1010         |\n",
      "|    time_elapsed         | 2385         |\n",
      "|    total_timesteps      | 2068480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014637283 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 14980        |\n",
      "|    policy_gradient_loss | -0.000931    |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2068528, episode_reward=182.22 +/- 94.79\n",
      "Episode length: 282.20 +/- 32.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 182         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2068528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004351404 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.76        |\n",
      "|    n_updates            | 14990       |\n",
      "|    policy_gradient_loss | 1.88e-05    |\n",
      "|    value_loss           | 70.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 867     |\n",
      "|    iterations      | 1011    |\n",
      "|    time_elapsed    | 2387    |\n",
      "|    total_timesteps | 2070528 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1012         |\n",
      "|    time_elapsed         | 2389         |\n",
      "|    total_timesteps      | 2072576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036577913 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.664        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35           |\n",
      "|    n_updates            | 15000        |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 194          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 1013        |\n",
      "|    time_elapsed         | 2391        |\n",
      "|    total_timesteps      | 2074624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005400707 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 15010       |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 75.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 867        |\n",
      "|    iterations           | 1014       |\n",
      "|    time_elapsed         | 2393       |\n",
      "|    total_timesteps      | 2076672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01640797 |\n",
      "|    clip_fraction        | 0.0675     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.254     |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 15020      |\n",
      "|    policy_gradient_loss | 0.00121    |\n",
      "|    value_loss           | 69.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2078528, episode_reward=190.50 +/- 20.62\n",
      "Episode length: 342.80 +/- 72.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 343         |\n",
      "|    mean_reward          | 190         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2078528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006791103 |\n",
      "|    clip_fraction        | 0.06        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.95        |\n",
      "|    n_updates            | 15030       |\n",
      "|    policy_gradient_loss | -0.000901   |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 867     |\n",
      "|    iterations      | 1015    |\n",
      "|    time_elapsed    | 2396    |\n",
      "|    total_timesteps | 2078720 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 1016        |\n",
      "|    time_elapsed         | 2398        |\n",
      "|    total_timesteps      | 2080768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005526989 |\n",
      "|    clip_fraction        | 0.0483      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 15040       |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1017         |\n",
      "|    time_elapsed         | 2400         |\n",
      "|    total_timesteps      | 2082816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041981423 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.8         |\n",
      "|    n_updates            | 15050        |\n",
      "|    policy_gradient_loss | -0.000989    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1018         |\n",
      "|    time_elapsed         | 2402         |\n",
      "|    total_timesteps      | 2084864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036969935 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 15060        |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 66.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 1019        |\n",
      "|    time_elapsed         | 2404        |\n",
      "|    total_timesteps      | 2086912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004211371 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.4        |\n",
      "|    n_updates            | 15070       |\n",
      "|    policy_gradient_loss | 7.35e-05    |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2088528, episode_reward=84.17 +/- 156.06\n",
      "Episode length: 297.40 +/- 62.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 84.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2088528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011865515 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.14        |\n",
      "|    n_updates            | 15080       |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 867     |\n",
      "|    iterations      | 1020    |\n",
      "|    time_elapsed    | 2407    |\n",
      "|    total_timesteps | 2088960 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 1021         |\n",
      "|    time_elapsed         | 2409         |\n",
      "|    total_timesteps      | 2091008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108734965 |\n",
      "|    clip_fraction        | 0.0749       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 15090        |\n",
      "|    policy_gradient_loss | -0.00088     |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 868          |\n",
      "|    iterations           | 1022         |\n",
      "|    time_elapsed         | 2411         |\n",
      "|    total_timesteps      | 2093056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024612106 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 15100        |\n",
      "|    policy_gradient_loss | -0.000888    |\n",
      "|    value_loss           | 190          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 868          |\n",
      "|    iterations           | 1023         |\n",
      "|    time_elapsed         | 2412         |\n",
      "|    total_timesteps      | 2095104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067008436 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.344       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 15110        |\n",
      "|    policy_gradient_loss | 1.13e-05     |\n",
      "|    value_loss           | 99.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 868          |\n",
      "|    iterations           | 1024         |\n",
      "|    time_elapsed         | 2415         |\n",
      "|    total_timesteps      | 2097152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019362355 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 15120        |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2098528, episode_reward=143.96 +/- 104.80\n",
      "Episode length: 296.20 +/- 44.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 296         |\n",
      "|    mean_reward          | 144         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2098528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022832744 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 15130       |\n",
      "|    policy_gradient_loss | 0.00122     |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 868     |\n",
      "|    iterations      | 1025    |\n",
      "|    time_elapsed    | 2418    |\n",
      "|    total_timesteps | 2099200 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1026        |\n",
      "|    time_elapsed         | 2420        |\n",
      "|    total_timesteps      | 2101248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009261489 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.1        |\n",
      "|    n_updates            | 15140       |\n",
      "|    policy_gradient_loss | 0.000662    |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 868          |\n",
      "|    iterations           | 1027         |\n",
      "|    time_elapsed         | 2422         |\n",
      "|    total_timesteps      | 2103296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054161213 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.9         |\n",
      "|    n_updates            | 15150        |\n",
      "|    policy_gradient_loss | -0.000399    |\n",
      "|    value_loss           | 75.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1028        |\n",
      "|    time_elapsed         | 2424        |\n",
      "|    total_timesteps      | 2105344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007202393 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 15160       |\n",
      "|    policy_gradient_loss | 0.000105    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1029        |\n",
      "|    time_elapsed         | 2426        |\n",
      "|    total_timesteps      | 2107392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006442595 |\n",
      "|    clip_fraction        | 0.0677      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.337      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 15170       |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2108528, episode_reward=110.77 +/- 145.94\n",
      "Episode length: 350.40 +/- 196.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 350          |\n",
      "|    mean_reward          | 111          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2108528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062198173 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 85.1         |\n",
      "|    n_updates            | 15180        |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 868     |\n",
      "|    iterations      | 1030    |\n",
      "|    time_elapsed    | 2429    |\n",
      "|    total_timesteps | 2109440 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1031        |\n",
      "|    time_elapsed         | 2431        |\n",
      "|    total_timesteps      | 2111488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004825555 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.18        |\n",
      "|    n_updates            | 15190       |\n",
      "|    policy_gradient_loss | -0.000843   |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1032        |\n",
      "|    time_elapsed         | 2433        |\n",
      "|    total_timesteps      | 2113536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010084728 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 15200       |\n",
      "|    policy_gradient_loss | 0.000257    |\n",
      "|    value_loss           | 45.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 1033        |\n",
      "|    time_elapsed         | 2434        |\n",
      "|    total_timesteps      | 2115584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007114113 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 15210       |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1034         |\n",
      "|    time_elapsed         | 2436         |\n",
      "|    total_timesteps      | 2117632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028492883 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.4         |\n",
      "|    n_updates            | 15220        |\n",
      "|    policy_gradient_loss | -0.000292    |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2118528, episode_reward=31.69 +/- 119.00\n",
      "Episode length: 247.20 +/- 55.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 247          |\n",
      "|    mean_reward          | 31.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2118528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026291595 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.9         |\n",
      "|    n_updates            | 15230        |\n",
      "|    policy_gradient_loss | -0.000944    |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 868     |\n",
      "|    iterations      | 1035    |\n",
      "|    time_elapsed    | 2439    |\n",
      "|    total_timesteps | 2119680 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1036         |\n",
      "|    time_elapsed         | 2441         |\n",
      "|    total_timesteps      | 2121728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054687625 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 15240        |\n",
      "|    policy_gradient_loss | 0.000242     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1037         |\n",
      "|    time_elapsed         | 2443         |\n",
      "|    total_timesteps      | 2123776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050180834 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.4         |\n",
      "|    n_updates            | 15250        |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    value_loss           | 68.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1038        |\n",
      "|    time_elapsed         | 2445        |\n",
      "|    total_timesteps      | 2125824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017893098 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 74          |\n",
      "|    n_updates            | 15260       |\n",
      "|    policy_gradient_loss | 0.00129     |\n",
      "|    value_loss           | 96.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1039        |\n",
      "|    time_elapsed         | 2447        |\n",
      "|    total_timesteps      | 2127872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002609726 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.9        |\n",
      "|    n_updates            | 15270       |\n",
      "|    policy_gradient_loss | 0.000413    |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2128528, episode_reward=71.86 +/- 103.86\n",
      "Episode length: 538.80 +/- 328.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 539          |\n",
      "|    mean_reward          | 71.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2128528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068389713 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 15280        |\n",
      "|    policy_gradient_loss | 0.000995     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 868     |\n",
      "|    iterations      | 1040    |\n",
      "|    time_elapsed    | 2451    |\n",
      "|    total_timesteps | 2129920 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 868          |\n",
      "|    iterations           | 1041         |\n",
      "|    time_elapsed         | 2453         |\n",
      "|    total_timesteps      | 2131968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024892827 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.6         |\n",
      "|    n_updates            | 15290        |\n",
      "|    policy_gradient_loss | -0.000758    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1042         |\n",
      "|    time_elapsed         | 2455         |\n",
      "|    total_timesteps      | 2134016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023474195 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 80.5         |\n",
      "|    n_updates            | 15300        |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 85.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1043        |\n",
      "|    time_elapsed         | 2457        |\n",
      "|    total_timesteps      | 2136064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006049567 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 15310       |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1044        |\n",
      "|    time_elapsed         | 2459        |\n",
      "|    total_timesteps      | 2138112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004073281 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 15320       |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 67.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2138528, episode_reward=153.48 +/- 101.23\n",
      "Episode length: 432.60 +/- 285.46\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 433          |\n",
      "|    mean_reward          | 153          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2138528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050073685 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.1         |\n",
      "|    n_updates            | 15330        |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 97.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 868     |\n",
      "|    iterations      | 1045    |\n",
      "|    time_elapsed    | 2462    |\n",
      "|    total_timesteps | 2140160 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1046        |\n",
      "|    time_elapsed         | 2464        |\n",
      "|    total_timesteps      | 2142208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006803514 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 15340       |\n",
      "|    policy_gradient_loss | -0.000233   |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1047         |\n",
      "|    time_elapsed         | 2466         |\n",
      "|    total_timesteps      | 2144256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019080704 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.207       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33           |\n",
      "|    n_updates            | 15350        |\n",
      "|    policy_gradient_loss | -0.000955    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1048        |\n",
      "|    time_elapsed         | 2468        |\n",
      "|    total_timesteps      | 2146304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009959966 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.03        |\n",
      "|    n_updates            | 15360       |\n",
      "|    policy_gradient_loss | 0.000194    |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1049        |\n",
      "|    time_elapsed         | 2470        |\n",
      "|    total_timesteps      | 2148352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014820958 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.1        |\n",
      "|    n_updates            | 15370       |\n",
      "|    policy_gradient_loss | 2.59e-05    |\n",
      "|    value_loss           | 38.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2148528, episode_reward=115.01 +/- 106.05\n",
      "Episode length: 303.20 +/- 103.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | 115         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2148528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003032406 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65          |\n",
      "|    n_updates            | 15380       |\n",
      "|    policy_gradient_loss | -3.94e-05   |\n",
      "|    value_loss           | 91.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 869     |\n",
      "|    iterations      | 1050    |\n",
      "|    time_elapsed    | 2473    |\n",
      "|    total_timesteps | 2150400 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 1051         |\n",
      "|    time_elapsed         | 2475         |\n",
      "|    total_timesteps      | 2152448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034564193 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 85.9         |\n",
      "|    n_updates            | 15390        |\n",
      "|    policy_gradient_loss | -0.0008      |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1052        |\n",
      "|    time_elapsed         | 2477        |\n",
      "|    total_timesteps      | 2154496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005117382 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.7        |\n",
      "|    n_updates            | 15400       |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1053        |\n",
      "|    time_elapsed         | 2479        |\n",
      "|    total_timesteps      | 2156544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002501728 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 15410       |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 79.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2158528, episode_reward=94.07 +/- 112.70\n",
      "Episode length: 261.40 +/- 71.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 261          |\n",
      "|    mean_reward          | 94.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2158528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070800325 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 15420        |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    value_loss           | 79.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 869     |\n",
      "|    iterations      | 1054    |\n",
      "|    time_elapsed    | 2481    |\n",
      "|    total_timesteps | 2158592 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 869         |\n",
      "|    iterations           | 1055        |\n",
      "|    time_elapsed         | 2483        |\n",
      "|    total_timesteps      | 2160640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011641291 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 15430       |\n",
      "|    policy_gradient_loss | -0.00019    |\n",
      "|    value_loss           | 153         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1056        |\n",
      "|    time_elapsed         | 2485        |\n",
      "|    total_timesteps      | 2162688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016282622 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 15440       |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 41.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1057        |\n",
      "|    time_elapsed         | 2487        |\n",
      "|    total_timesteps      | 2164736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008035375 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.325      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.65        |\n",
      "|    n_updates            | 15450       |\n",
      "|    policy_gradient_loss | 0.0019      |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 870        |\n",
      "|    iterations           | 1058       |\n",
      "|    time_elapsed         | 2489       |\n",
      "|    total_timesteps      | 2166784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01070584 |\n",
      "|    clip_fraction        | 0.092      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.317     |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.8       |\n",
      "|    n_updates            | 15460      |\n",
      "|    policy_gradient_loss | -0.00154   |\n",
      "|    value_loss           | 35.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2168528, episode_reward=91.45 +/- 99.28\n",
      "Episode length: 446.00 +/- 303.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 446          |\n",
      "|    mean_reward          | 91.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2168528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061485153 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.5         |\n",
      "|    n_updates            | 15470        |\n",
      "|    policy_gradient_loss | -0.000435    |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 869     |\n",
      "|    iterations      | 1059    |\n",
      "|    time_elapsed    | 2493    |\n",
      "|    total_timesteps | 2168832 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 870          |\n",
      "|    iterations           | 1060         |\n",
      "|    time_elapsed         | 2494         |\n",
      "|    total_timesteps      | 2170880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044800993 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.196       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.59         |\n",
      "|    n_updates            | 15480        |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 81.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1061        |\n",
      "|    time_elapsed         | 2496        |\n",
      "|    total_timesteps      | 2172928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008187499 |\n",
      "|    clip_fraction        | 0.0588      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.5        |\n",
      "|    n_updates            | 15490       |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1062        |\n",
      "|    time_elapsed         | 2498        |\n",
      "|    total_timesteps      | 2174976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005751951 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.29        |\n",
      "|    n_updates            | 15500       |\n",
      "|    policy_gradient_loss | -0.000809   |\n",
      "|    value_loss           | 24.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 870          |\n",
      "|    iterations           | 1063         |\n",
      "|    time_elapsed         | 2500         |\n",
      "|    total_timesteps      | 2177024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064559462 |\n",
      "|    clip_fraction        | 0.0718       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.08         |\n",
      "|    n_updates            | 15510        |\n",
      "|    policy_gradient_loss | 0.00014      |\n",
      "|    value_loss           | 19.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2178528, episode_reward=59.57 +/- 139.69\n",
      "Episode length: 226.20 +/- 36.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | 59.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2178528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031677077 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.4         |\n",
      "|    n_updates            | 15520        |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 870     |\n",
      "|    iterations      | 1064    |\n",
      "|    time_elapsed    | 2503    |\n",
      "|    total_timesteps | 2179072 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1065        |\n",
      "|    time_elapsed         | 2505        |\n",
      "|    total_timesteps      | 2181120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004461759 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 15530       |\n",
      "|    policy_gradient_loss | -0.000759   |\n",
      "|    value_loss           | 91.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 870          |\n",
      "|    iterations           | 1066         |\n",
      "|    time_elapsed         | 2507         |\n",
      "|    total_timesteps      | 2183168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079129925 |\n",
      "|    clip_fraction        | 0.0789       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.4         |\n",
      "|    n_updates            | 15540        |\n",
      "|    policy_gradient_loss | 5.87e-05     |\n",
      "|    value_loss           | 49.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 1067        |\n",
      "|    time_elapsed         | 2509        |\n",
      "|    total_timesteps      | 2185216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004595991 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66          |\n",
      "|    n_updates            | 15550       |\n",
      "|    policy_gradient_loss | -0.000945   |\n",
      "|    value_loss           | 79.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1068         |\n",
      "|    time_elapsed         | 2511         |\n",
      "|    total_timesteps      | 2187264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020412917 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 111          |\n",
      "|    n_updates            | 15560        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 153          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2188528, episode_reward=40.51 +/- 91.29\n",
      "Episode length: 255.60 +/- 26.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2188528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020836564 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.1         |\n",
      "|    n_updates            | 15570        |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    value_loss           | 160          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 870     |\n",
      "|    iterations      | 1069    |\n",
      "|    time_elapsed    | 2513    |\n",
      "|    total_timesteps | 2189312 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1070         |\n",
      "|    time_elapsed         | 2515         |\n",
      "|    total_timesteps      | 2191360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028070458 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 56.5         |\n",
      "|    n_updates            | 15580        |\n",
      "|    policy_gradient_loss | -0.000894    |\n",
      "|    value_loss           | 149          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 1071        |\n",
      "|    time_elapsed         | 2517        |\n",
      "|    total_timesteps      | 2193408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003686058 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.06        |\n",
      "|    n_updates            | 15590       |\n",
      "|    policy_gradient_loss | 0.000744    |\n",
      "|    value_loss           | 18          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1072         |\n",
      "|    time_elapsed         | 2519         |\n",
      "|    total_timesteps      | 2195456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023780894 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.2         |\n",
      "|    n_updates            | 15600        |\n",
      "|    policy_gradient_loss | -0.000159    |\n",
      "|    value_loss           | 151          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1073         |\n",
      "|    time_elapsed         | 2521         |\n",
      "|    total_timesteps      | 2197504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046958015 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | 0.681        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.28         |\n",
      "|    n_updates            | 15610        |\n",
      "|    policy_gradient_loss | -0.000357    |\n",
      "|    value_loss           | 43           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2198528, episode_reward=61.28 +/- 117.89\n",
      "Episode length: 222.60 +/- 84.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 223          |\n",
      "|    mean_reward          | 61.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2198528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033548004 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.738        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.9         |\n",
      "|    n_updates            | 15620        |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 871     |\n",
      "|    iterations      | 1074    |\n",
      "|    time_elapsed    | 2524    |\n",
      "|    total_timesteps | 2199552 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 1075        |\n",
      "|    time_elapsed         | 2526        |\n",
      "|    total_timesteps      | 2201600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005134519 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 15630       |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1076         |\n",
      "|    time_elapsed         | 2528         |\n",
      "|    total_timesteps      | 2203648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049000587 |\n",
      "|    clip_fraction        | 0.05         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 15640        |\n",
      "|    policy_gradient_loss | -0.000509    |\n",
      "|    value_loss           | 39.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1077         |\n",
      "|    time_elapsed         | 2530         |\n",
      "|    total_timesteps      | 2205696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055477093 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.623        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.4         |\n",
      "|    n_updates            | 15650        |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1078         |\n",
      "|    time_elapsed         | 2532         |\n",
      "|    total_timesteps      | 2207744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060577067 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.342       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.5         |\n",
      "|    n_updates            | 15660        |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    value_loss           | 69.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2208528, episode_reward=131.12 +/- 133.84\n",
      "Episode length: 328.00 +/- 119.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | 131          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2208528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032084808 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 15670        |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 73.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 871     |\n",
      "|    iterations      | 1079    |\n",
      "|    time_elapsed    | 2535    |\n",
      "|    total_timesteps | 2209792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 1080        |\n",
      "|    time_elapsed         | 2537        |\n",
      "|    total_timesteps      | 2211840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005691232 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.197      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.6        |\n",
      "|    n_updates            | 15680       |\n",
      "|    policy_gradient_loss | 0.0012      |\n",
      "|    value_loss           | 69.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 871        |\n",
      "|    iterations           | 1081       |\n",
      "|    time_elapsed         | 2539       |\n",
      "|    total_timesteps      | 2213888    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00699063 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.913      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 15690      |\n",
      "|    policy_gradient_loss | 0.00185    |\n",
      "|    value_loss           | 8.33       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1082         |\n",
      "|    time_elapsed         | 2541         |\n",
      "|    total_timesteps      | 2215936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043410445 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.2         |\n",
      "|    n_updates            | 15700        |\n",
      "|    policy_gradient_loss | 0.000479     |\n",
      "|    value_loss           | 58.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 1083        |\n",
      "|    time_elapsed         | 2543        |\n",
      "|    total_timesteps      | 2217984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002867565 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.7        |\n",
      "|    n_updates            | 15710       |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2218528, episode_reward=107.58 +/- 120.13\n",
      "Episode length: 328.00 +/- 132.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 328         |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2218528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003509818 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 15720       |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 70.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 871     |\n",
      "|    iterations      | 1084    |\n",
      "|    time_elapsed    | 2546    |\n",
      "|    total_timesteps | 2220032 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 871          |\n",
      "|    iterations           | 1085         |\n",
      "|    time_elapsed         | 2548         |\n",
      "|    total_timesteps      | 2222080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071355337 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.303       |\n",
      "|    explained_variance   | 0.681        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 15730        |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1086        |\n",
      "|    time_elapsed         | 2550        |\n",
      "|    total_timesteps      | 2224128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014234642 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.1        |\n",
      "|    n_updates            | 15740       |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1087        |\n",
      "|    time_elapsed         | 2552        |\n",
      "|    total_timesteps      | 2226176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004988238 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 15750       |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1088        |\n",
      "|    time_elapsed         | 2554        |\n",
      "|    total_timesteps      | 2228224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007085969 |\n",
      "|    clip_fraction        | 0.0396      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.9        |\n",
      "|    n_updates            | 15760       |\n",
      "|    policy_gradient_loss | -0.000143   |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2228528, episode_reward=80.68 +/- 121.85\n",
      "Episode length: 262.80 +/- 52.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 263          |\n",
      "|    mean_reward          | 80.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2228528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035704314 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.209       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.2         |\n",
      "|    n_updates            | 15770        |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    value_loss           | 52.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1089    |\n",
      "|    time_elapsed    | 2556    |\n",
      "|    total_timesteps | 2230272 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1090         |\n",
      "|    time_elapsed         | 2559         |\n",
      "|    total_timesteps      | 2232320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046050716 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 15780        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1091         |\n",
      "|    time_elapsed         | 2560         |\n",
      "|    total_timesteps      | 2234368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056468877 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46         |\n",
      "|    n_updates            | 15790        |\n",
      "|    policy_gradient_loss | 0.000618     |\n",
      "|    value_loss           | 39.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1092         |\n",
      "|    time_elapsed         | 2563         |\n",
      "|    total_timesteps      | 2236416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050468873 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 15800        |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1093        |\n",
      "|    time_elapsed         | 2565        |\n",
      "|    total_timesteps      | 2238464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005329838 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.8         |\n",
      "|    n_updates            | 15810       |\n",
      "|    policy_gradient_loss | 0.000107    |\n",
      "|    value_loss           | 90.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2238528, episode_reward=179.59 +/- 104.64\n",
      "Episode length: 301.00 +/- 64.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 301          |\n",
      "|    mean_reward          | 180          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2238528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029160867 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.205       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.9         |\n",
      "|    n_updates            | 15820        |\n",
      "|    policy_gradient_loss | -8.86e-05    |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1094    |\n",
      "|    time_elapsed    | 2567    |\n",
      "|    total_timesteps | 2240512 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1095        |\n",
      "|    time_elapsed         | 2569        |\n",
      "|    total_timesteps      | 2242560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003648946 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 15830       |\n",
      "|    policy_gradient_loss | -0.000728   |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1096         |\n",
      "|    time_elapsed         | 2571         |\n",
      "|    total_timesteps      | 2244608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067059835 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 15840        |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 72.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1097        |\n",
      "|    time_elapsed         | 2573        |\n",
      "|    total_timesteps      | 2246656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007866862 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 15850       |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 87.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2248528, episode_reward=222.66 +/- 26.00\n",
      "Episode length: 285.20 +/- 17.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | 223         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2248528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003965504 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.7        |\n",
      "|    n_updates            | 15860       |\n",
      "|    policy_gradient_loss | -0.00236    |\n",
      "|    value_loss           | 187         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1098    |\n",
      "|    time_elapsed    | 2576    |\n",
      "|    total_timesteps | 2248704 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 872        |\n",
      "|    iterations           | 1099       |\n",
      "|    time_elapsed         | 2578       |\n",
      "|    total_timesteps      | 2250752    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02489753 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.473     |\n",
      "|    explained_variance   | 0.492      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.544      |\n",
      "|    n_updates            | 15870      |\n",
      "|    policy_gradient_loss | -0.00114   |\n",
      "|    value_loss           | 11.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1100        |\n",
      "|    time_elapsed         | 2580        |\n",
      "|    total_timesteps      | 2252800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007360477 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.186      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 15880       |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1101         |\n",
      "|    time_elapsed         | 2582         |\n",
      "|    total_timesteps      | 2254848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037116858 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 15890        |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 46.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1102        |\n",
      "|    time_elapsed         | 2584        |\n",
      "|    total_timesteps      | 2256896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006943176 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.18        |\n",
      "|    n_updates            | 15900       |\n",
      "|    policy_gradient_loss | 0.000984    |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2258528, episode_reward=156.25 +/- 92.86\n",
      "Episode length: 334.80 +/- 63.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 335         |\n",
      "|    mean_reward          | 156         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2258528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007540678 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.289      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 15910       |\n",
      "|    policy_gradient_loss | -0.000258   |\n",
      "|    value_loss           | 32.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1103    |\n",
      "|    time_elapsed    | 2587    |\n",
      "|    total_timesteps | 2258944 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1104         |\n",
      "|    time_elapsed         | 2589         |\n",
      "|    total_timesteps      | 2260992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032659075 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.5         |\n",
      "|    n_updates            | 15920        |\n",
      "|    policy_gradient_loss | -0.000574    |\n",
      "|    value_loss           | 75.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1105         |\n",
      "|    time_elapsed         | 2591         |\n",
      "|    total_timesteps      | 2263040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027515525 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 69.1         |\n",
      "|    n_updates            | 15930        |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1106         |\n",
      "|    time_elapsed         | 2593         |\n",
      "|    total_timesteps      | 2265088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019148793 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.1         |\n",
      "|    n_updates            | 15940        |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1107        |\n",
      "|    time_elapsed         | 2596        |\n",
      "|    total_timesteps      | 2267136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009190718 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 15950       |\n",
      "|    policy_gradient_loss | -0.00383    |\n",
      "|    value_loss           | 33.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2268528, episode_reward=214.11 +/- 45.28\n",
      "Episode length: 366.40 +/- 120.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 366          |\n",
      "|    mean_reward          | 214          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2268528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069294656 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.382       |\n",
      "|    explained_variance   | 0.503        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.56         |\n",
      "|    n_updates            | 15960        |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1108    |\n",
      "|    time_elapsed    | 2599    |\n",
      "|    total_timesteps | 2269184 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1109        |\n",
      "|    time_elapsed         | 2601        |\n",
      "|    total_timesteps      | 2271232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013199297 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.397      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 15970       |\n",
      "|    policy_gradient_loss | 0.0012      |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 873        |\n",
      "|    iterations           | 1110       |\n",
      "|    time_elapsed         | 2603       |\n",
      "|    total_timesteps      | 2273280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06166376 |\n",
      "|    clip_fraction        | 0.0738     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.363     |\n",
      "|    explained_variance   | 0.849      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.14       |\n",
      "|    n_updates            | 15980      |\n",
      "|    policy_gradient_loss | -0.00331   |\n",
      "|    value_loss           | 70.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1111        |\n",
      "|    time_elapsed         | 2605        |\n",
      "|    total_timesteps      | 2275328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004094512 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.1        |\n",
      "|    n_updates            | 15990       |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1112        |\n",
      "|    time_elapsed         | 2607        |\n",
      "|    total_timesteps      | 2277376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004222789 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.337      |\n",
      "|    explained_variance   | 0.617       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 16000       |\n",
      "|    policy_gradient_loss | -0.000154   |\n",
      "|    value_loss           | 42.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2278528, episode_reward=203.63 +/- 56.21\n",
      "Episode length: 456.00 +/- 272.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 456         |\n",
      "|    mean_reward          | 204         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2278528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009585108 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 16010       |\n",
      "|    policy_gradient_loss | -0.000221   |\n",
      "|    value_loss           | 60.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1113    |\n",
      "|    time_elapsed    | 2612    |\n",
      "|    total_timesteps | 2279424 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1114         |\n",
      "|    time_elapsed         | 2614         |\n",
      "|    total_timesteps      | 2281472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075878045 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.7         |\n",
      "|    n_updates            | 16020        |\n",
      "|    policy_gradient_loss | -1.57e-05    |\n",
      "|    value_loss           | 32           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1115         |\n",
      "|    time_elapsed         | 2616         |\n",
      "|    total_timesteps      | 2283520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031921382 |\n",
      "|    clip_fraction        | 0.0345       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 65.1         |\n",
      "|    n_updates            | 16030        |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1116        |\n",
      "|    time_elapsed         | 2618        |\n",
      "|    total_timesteps      | 2285568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003964305 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.11        |\n",
      "|    n_updates            | 16040       |\n",
      "|    policy_gradient_loss | 0.000844    |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1117        |\n",
      "|    time_elapsed         | 2619        |\n",
      "|    total_timesteps      | 2287616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003204818 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 16050       |\n",
      "|    policy_gradient_loss | -0.000557   |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2288528, episode_reward=76.80 +/- 95.96\n",
      "Episode length: 403.20 +/- 300.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 403          |\n",
      "|    mean_reward          | 76.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2288528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056156223 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 16060        |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 99.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1118    |\n",
      "|    time_elapsed    | 2623    |\n",
      "|    total_timesteps | 2289664 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1119        |\n",
      "|    time_elapsed         | 2624        |\n",
      "|    total_timesteps      | 2291712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006211594 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 16070       |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1120        |\n",
      "|    time_elapsed         | 2626        |\n",
      "|    total_timesteps      | 2293760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008671181 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 16080       |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    value_loss           | 91.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1121         |\n",
      "|    time_elapsed         | 2628         |\n",
      "|    total_timesteps      | 2295808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098945405 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.4         |\n",
      "|    n_updates            | 16090        |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 78.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1122         |\n",
      "|    time_elapsed         | 2631         |\n",
      "|    total_timesteps      | 2297856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071846894 |\n",
      "|    clip_fraction        | 0.0692       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.76         |\n",
      "|    n_updates            | 16100        |\n",
      "|    policy_gradient_loss | 0.000364     |\n",
      "|    value_loss           | 14.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2298528, episode_reward=188.03 +/- 36.04\n",
      "Episode length: 369.00 +/- 112.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2298528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046357843 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.554        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.4          |\n",
      "|    n_updates            | 16110        |\n",
      "|    policy_gradient_loss | 0.000266     |\n",
      "|    value_loss           | 63.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1123    |\n",
      "|    time_elapsed    | 2634    |\n",
      "|    total_timesteps | 2299904 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 873        |\n",
      "|    iterations           | 1124       |\n",
      "|    time_elapsed         | 2636       |\n",
      "|    total_timesteps      | 2301952    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00735622 |\n",
      "|    clip_fraction        | 0.0385     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.226     |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.7       |\n",
      "|    n_updates            | 16120      |\n",
      "|    policy_gradient_loss | 0.000471   |\n",
      "|    value_loss           | 47.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1125        |\n",
      "|    time_elapsed         | 2638        |\n",
      "|    total_timesteps      | 2304000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005056178 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 16130       |\n",
      "|    policy_gradient_loss | -0.000161   |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1126        |\n",
      "|    time_elapsed         | 2640        |\n",
      "|    total_timesteps      | 2306048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007914473 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.9         |\n",
      "|    n_updates            | 16140       |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1127         |\n",
      "|    time_elapsed         | 2642         |\n",
      "|    total_timesteps      | 2308096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030439869 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.716        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.7         |\n",
      "|    n_updates            | 16150        |\n",
      "|    policy_gradient_loss | 0.000639     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2308528, episode_reward=145.35 +/- 93.35\n",
      "Episode length: 536.20 +/- 275.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 536          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2308528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078106653 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.437       |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.3         |\n",
      "|    n_updates            | 16160        |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 96.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1128    |\n",
      "|    time_elapsed    | 2646    |\n",
      "|    total_timesteps | 2310144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 1129        |\n",
      "|    time_elapsed         | 2648        |\n",
      "|    total_timesteps      | 2312192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003721679 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 16170       |\n",
      "|    policy_gradient_loss | -0.000407   |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 872        |\n",
      "|    iterations           | 1130       |\n",
      "|    time_elapsed         | 2651       |\n",
      "|    total_timesteps      | 2314240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00316831 |\n",
      "|    clip_fraction        | 0.0604     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.34      |\n",
      "|    explained_variance   | 0.806      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.58       |\n",
      "|    n_updates            | 16180      |\n",
      "|    policy_gradient_loss | -0.00179   |\n",
      "|    value_loss           | 28.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1131        |\n",
      "|    time_elapsed         | 2653        |\n",
      "|    total_timesteps      | 2316288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602229 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.78        |\n",
      "|    n_updates            | 16190       |\n",
      "|    policy_gradient_loss | 0.00165     |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1132        |\n",
      "|    time_elapsed         | 2654        |\n",
      "|    total_timesteps      | 2318336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003632253 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.95        |\n",
      "|    n_updates            | 16200       |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 58          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2318528, episode_reward=190.84 +/- 27.47\n",
      "Episode length: 368.00 +/- 80.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 368          |\n",
      "|    mean_reward          | 191          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2318528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023324876 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.217       |\n",
      "|    explained_variance   | 0.73         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.1         |\n",
      "|    n_updates            | 16210        |\n",
      "|    policy_gradient_loss | -0.0006      |\n",
      "|    value_loss           | 169          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1133    |\n",
      "|    time_elapsed    | 2657    |\n",
      "|    total_timesteps | 2320384 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1134        |\n",
      "|    time_elapsed         | 2659        |\n",
      "|    total_timesteps      | 2322432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000989337 |\n",
      "|    clip_fraction        | 0.0158      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51          |\n",
      "|    n_updates            | 16220       |\n",
      "|    policy_gradient_loss | -0.000365   |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1135         |\n",
      "|    time_elapsed         | 2662         |\n",
      "|    total_timesteps      | 2324480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023491392 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.3         |\n",
      "|    n_updates            | 16230        |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1136         |\n",
      "|    time_elapsed         | 2665         |\n",
      "|    total_timesteps      | 2326528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055037737 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.97         |\n",
      "|    n_updates            | 16240        |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2328528, episode_reward=168.43 +/- 82.65\n",
      "Episode length: 279.60 +/- 58.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 280         |\n",
      "|    mean_reward          | 168         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2328528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021018207 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.837       |\n",
      "|    n_updates            | 16250       |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    value_loss           | 14.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1137    |\n",
      "|    time_elapsed    | 2668    |\n",
      "|    total_timesteps | 2328576 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 1138         |\n",
      "|    time_elapsed         | 2670         |\n",
      "|    total_timesteps      | 2330624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039688814 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 16260        |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 9.52         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1139         |\n",
      "|    time_elapsed         | 2671         |\n",
      "|    total_timesteps      | 2332672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064227805 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 16270        |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 78.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1140         |\n",
      "|    time_elapsed         | 2673         |\n",
      "|    total_timesteps      | 2334720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036125511 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 16280        |\n",
      "|    policy_gradient_loss | -0.000691    |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1141         |\n",
      "|    time_elapsed         | 2676         |\n",
      "|    total_timesteps      | 2336768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020740388 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.191       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 16290        |\n",
      "|    policy_gradient_loss | -0.000175    |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2338528, episode_reward=164.40 +/- 92.52\n",
      "Episode length: 347.40 +/- 90.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | 164         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2338528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012457091 |\n",
      "|    clip_fraction        | 0.0843      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.64        |\n",
      "|    n_updates            | 16300       |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1142    |\n",
      "|    time_elapsed    | 2679    |\n",
      "|    total_timesteps | 2338816 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1143        |\n",
      "|    time_elapsed         | 2681        |\n",
      "|    total_timesteps      | 2340864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004503685 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.85        |\n",
      "|    n_updates            | 16310       |\n",
      "|    policy_gradient_loss | -0.000389   |\n",
      "|    value_loss           | 24          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1144         |\n",
      "|    time_elapsed         | 2683         |\n",
      "|    total_timesteps      | 2342912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047771046 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.187       |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.9         |\n",
      "|    n_updates            | 16320        |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 80.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1145         |\n",
      "|    time_elapsed         | 2685         |\n",
      "|    total_timesteps      | 2344960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029040186 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.174       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 16330        |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 93.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1146        |\n",
      "|    time_elapsed         | 2687        |\n",
      "|    total_timesteps      | 2347008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008048544 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.04        |\n",
      "|    n_updates            | 16340       |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2348528, episode_reward=128.47 +/- 112.80\n",
      "Episode length: 329.20 +/- 85.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 329          |\n",
      "|    mean_reward          | 128          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2348528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040088855 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.243       |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.7         |\n",
      "|    n_updates            | 16350        |\n",
      "|    policy_gradient_loss | -0.000864    |\n",
      "|    value_loss           | 92.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1147    |\n",
      "|    time_elapsed    | 2690    |\n",
      "|    total_timesteps | 2349056 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1148         |\n",
      "|    time_elapsed         | 2692         |\n",
      "|    total_timesteps      | 2351104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032144007 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.33         |\n",
      "|    n_updates            | 16360        |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1149         |\n",
      "|    time_elapsed         | 2694         |\n",
      "|    total_timesteps      | 2353152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031664795 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.96         |\n",
      "|    n_updates            | 16370        |\n",
      "|    policy_gradient_loss | 0.0019       |\n",
      "|    value_loss           | 7.85         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1150        |\n",
      "|    time_elapsed         | 2696        |\n",
      "|    total_timesteps      | 2355200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004260697 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.46        |\n",
      "|    n_updates            | 16380       |\n",
      "|    policy_gradient_loss | 0.00177     |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 873        |\n",
      "|    iterations           | 1151       |\n",
      "|    time_elapsed         | 2698       |\n",
      "|    total_timesteps      | 2357248    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01811237 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.527     |\n",
      "|    explained_variance   | 0.859      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.75       |\n",
      "|    n_updates            | 16390      |\n",
      "|    policy_gradient_loss | 0.00531    |\n",
      "|    value_loss           | 6.55       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2358528, episode_reward=162.57 +/- 89.99\n",
      "Episode length: 416.60 +/- 75.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 417          |\n",
      "|    mean_reward          | 163          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2358528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016816845 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.246       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.24         |\n",
      "|    n_updates            | 16400        |\n",
      "|    policy_gradient_loss | -0.000199    |\n",
      "|    value_loss           | 26.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 872     |\n",
      "|    iterations      | 1152    |\n",
      "|    time_elapsed    | 2702    |\n",
      "|    total_timesteps | 2359296 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1153         |\n",
      "|    time_elapsed         | 2704         |\n",
      "|    total_timesteps      | 2361344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022412096 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.558        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56         |\n",
      "|    n_updates            | 16410        |\n",
      "|    policy_gradient_loss | -0.000448    |\n",
      "|    value_loss           | 12.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1154         |\n",
      "|    time_elapsed         | 2706         |\n",
      "|    total_timesteps      | 2363392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015614838 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 16420        |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 78.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1155         |\n",
      "|    time_elapsed         | 2708         |\n",
      "|    total_timesteps      | 2365440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033442848 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 16430        |\n",
      "|    policy_gradient_loss | -0.000957    |\n",
      "|    value_loss           | 13.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1156        |\n",
      "|    time_elapsed         | 2710        |\n",
      "|    total_timesteps      | 2367488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003295471 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.72        |\n",
      "|    n_updates            | 16440       |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 47.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2368528, episode_reward=126.82 +/- 120.69\n",
      "Episode length: 306.40 +/- 43.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | 127         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2368528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004997865 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 16450       |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1157    |\n",
      "|    time_elapsed    | 2713    |\n",
      "|    total_timesteps | 2369536 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1158         |\n",
      "|    time_elapsed         | 2715         |\n",
      "|    total_timesteps      | 2371584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029659085 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.3         |\n",
      "|    n_updates            | 16460        |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 81.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1159        |\n",
      "|    time_elapsed         | 2717        |\n",
      "|    total_timesteps      | 2373632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007394756 |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 16470       |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1160        |\n",
      "|    time_elapsed         | 2719        |\n",
      "|    total_timesteps      | 2375680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005201971 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 16480       |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    value_loss           | 48.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1161         |\n",
      "|    time_elapsed         | 2721         |\n",
      "|    total_timesteps      | 2377728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046620476 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.346       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 16490        |\n",
      "|    policy_gradient_loss | -0.000663    |\n",
      "|    value_loss           | 60.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2378528, episode_reward=223.88 +/- 29.70\n",
      "Episode length: 357.00 +/- 62.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 357         |\n",
      "|    mean_reward          | 224         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2378528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006832608 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 16500       |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1162    |\n",
      "|    time_elapsed    | 2724    |\n",
      "|    total_timesteps | 2379776 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1163         |\n",
      "|    time_elapsed         | 2726         |\n",
      "|    total_timesteps      | 2381824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054527745 |\n",
      "|    clip_fraction        | 0.0945       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.8         |\n",
      "|    n_updates            | 16510        |\n",
      "|    policy_gradient_loss | 0.000484     |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1164         |\n",
      "|    time_elapsed         | 2728         |\n",
      "|    total_timesteps      | 2383872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029730764 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 16520        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 60           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1165        |\n",
      "|    time_elapsed         | 2730        |\n",
      "|    total_timesteps      | 2385920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004451322 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.7        |\n",
      "|    n_updates            | 16530       |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1166        |\n",
      "|    time_elapsed         | 2732        |\n",
      "|    total_timesteps      | 2387968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004854252 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 16540       |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 60.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2388528, episode_reward=176.38 +/- 68.99\n",
      "Episode length: 553.00 +/- 260.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 553          |\n",
      "|    mean_reward          | 176          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2388528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045950813 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.286       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.9         |\n",
      "|    n_updates            | 16550        |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1167    |\n",
      "|    time_elapsed    | 2736    |\n",
      "|    total_timesteps | 2390016 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1168        |\n",
      "|    time_elapsed         | 2738        |\n",
      "|    total_timesteps      | 2392064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004310687 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 16560       |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    value_loss           | 87.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1169         |\n",
      "|    time_elapsed         | 2740         |\n",
      "|    total_timesteps      | 2394112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073730173 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.68         |\n",
      "|    n_updates            | 16570        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 72.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1170        |\n",
      "|    time_elapsed         | 2742        |\n",
      "|    total_timesteps      | 2396160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004829902 |\n",
      "|    clip_fraction        | 0.0552      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.97        |\n",
      "|    n_updates            | 16580       |\n",
      "|    policy_gradient_loss | 0.000537    |\n",
      "|    value_loss           | 38.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1171         |\n",
      "|    time_elapsed         | 2745         |\n",
      "|    total_timesteps      | 2398208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031191786 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.7         |\n",
      "|    n_updates            | 16590        |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 77.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2398528, episode_reward=235.67 +/- 17.17\n",
      "Episode length: 323.40 +/- 25.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 323         |\n",
      "|    mean_reward          | 236         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2398528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010287103 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.91        |\n",
      "|    n_updates            | 16600       |\n",
      "|    policy_gradient_loss | 0.00023     |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1172    |\n",
      "|    time_elapsed    | 2748    |\n",
      "|    total_timesteps | 2400256 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1173         |\n",
      "|    time_elapsed         | 2750         |\n",
      "|    total_timesteps      | 2402304      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043286067 |\n",
      "|    clip_fraction        | 0.0986       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.377       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.66         |\n",
      "|    n_updates            | 16610        |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 66.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1174         |\n",
      "|    time_elapsed         | 2752         |\n",
      "|    total_timesteps      | 2404352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017307474 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.3         |\n",
      "|    n_updates            | 16620        |\n",
      "|    policy_gradient_loss | 0.000127     |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1175         |\n",
      "|    time_elapsed         | 2754         |\n",
      "|    total_timesteps      | 2406400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046820077 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.5         |\n",
      "|    n_updates            | 16630        |\n",
      "|    policy_gradient_loss | -0.004       |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1176        |\n",
      "|    time_elapsed         | 2756        |\n",
      "|    total_timesteps      | 2408448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003600332 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.193      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.27        |\n",
      "|    n_updates            | 16640       |\n",
      "|    policy_gradient_loss | -0.000193   |\n",
      "|    value_loss           | 56.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2408528, episode_reward=204.96 +/- 87.45\n",
      "Episode length: 306.40 +/- 50.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | 205         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2408528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012672476 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.29        |\n",
      "|    n_updates            | 16650       |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1177    |\n",
      "|    time_elapsed    | 2759    |\n",
      "|    total_timesteps | 2410496 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1178        |\n",
      "|    time_elapsed         | 2761        |\n",
      "|    total_timesteps      | 2412544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013308585 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 16660       |\n",
      "|    policy_gradient_loss | -0.00052    |\n",
      "|    value_loss           | 55.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1179        |\n",
      "|    time_elapsed         | 2763        |\n",
      "|    total_timesteps      | 2414592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005817689 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 16670       |\n",
      "|    policy_gradient_loss | 0.000512    |\n",
      "|    value_loss           | 77          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1180         |\n",
      "|    time_elapsed         | 2765         |\n",
      "|    total_timesteps      | 2416640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020404384 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.196       |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 16680        |\n",
      "|    policy_gradient_loss | 0.000337     |\n",
      "|    value_loss           | 79.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2418528, episode_reward=195.15 +/- 54.10\n",
      "Episode length: 437.00 +/- 281.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 437          |\n",
      "|    mean_reward          | 195          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2418528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036472883 |\n",
      "|    clip_fraction        | 0.047        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.9         |\n",
      "|    n_updates            | 16690        |\n",
      "|    policy_gradient_loss | -0.000608    |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1181    |\n",
      "|    time_elapsed    | 2769    |\n",
      "|    total_timesteps | 2418688 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 873        |\n",
      "|    iterations           | 1182       |\n",
      "|    time_elapsed         | 2771       |\n",
      "|    total_timesteps      | 2420736    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00953616 |\n",
      "|    clip_fraction        | 0.0619     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.312     |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.85       |\n",
      "|    n_updates            | 16700      |\n",
      "|    policy_gradient_loss | -0.000793  |\n",
      "|    value_loss           | 14.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1183        |\n",
      "|    time_elapsed         | 2773        |\n",
      "|    total_timesteps      | 2422784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002430729 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 16710       |\n",
      "|    policy_gradient_loss | -0.000519   |\n",
      "|    value_loss           | 58.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1184        |\n",
      "|    time_elapsed         | 2775        |\n",
      "|    total_timesteps      | 2424832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005923504 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.3        |\n",
      "|    n_updates            | 16720       |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1185         |\n",
      "|    time_elapsed         | 2777         |\n",
      "|    total_timesteps      | 2426880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026277117 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.189       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 16730        |\n",
      "|    policy_gradient_loss | -0.000922    |\n",
      "|    value_loss           | 52.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2428528, episode_reward=131.94 +/- 120.11\n",
      "Episode length: 290.80 +/- 45.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 291         |\n",
      "|    mean_reward          | 132         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2428528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010539141 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 16740       |\n",
      "|    policy_gradient_loss | 0.00137     |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1186    |\n",
      "|    time_elapsed    | 2780    |\n",
      "|    total_timesteps | 2428928 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1187        |\n",
      "|    time_elapsed         | 2782        |\n",
      "|    total_timesteps      | 2430976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004790403 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 16750       |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1188         |\n",
      "|    time_elapsed         | 2784         |\n",
      "|    total_timesteps      | 2433024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031086928 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 16760        |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 33.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 1189        |\n",
      "|    time_elapsed         | 2786        |\n",
      "|    total_timesteps      | 2435072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002616832 |\n",
      "|    clip_fraction        | 0.0238      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 16770       |\n",
      "|    policy_gradient_loss | -0.000306   |\n",
      "|    value_loss           | 144         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1190         |\n",
      "|    time_elapsed         | 2788         |\n",
      "|    total_timesteps      | 2437120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080656875 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 16780        |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 49.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2438528, episode_reward=189.58 +/- 57.07\n",
      "Episode length: 482.60 +/- 264.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 483        |\n",
      "|    mean_reward          | 190        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2438528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00564198 |\n",
      "|    clip_fraction        | 0.0456     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.244     |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 62.5       |\n",
      "|    n_updates            | 16790      |\n",
      "|    policy_gradient_loss | -0.00314   |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1191    |\n",
      "|    time_elapsed    | 2791    |\n",
      "|    total_timesteps | 2439168 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1192         |\n",
      "|    time_elapsed         | 2793         |\n",
      "|    total_timesteps      | 2441216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045415107 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.54         |\n",
      "|    n_updates            | 16800        |\n",
      "|    policy_gradient_loss | -0.000457    |\n",
      "|    value_loss           | 52.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1193         |\n",
      "|    time_elapsed         | 2795         |\n",
      "|    total_timesteps      | 2443264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035584164 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.327       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19           |\n",
      "|    n_updates            | 16810        |\n",
      "|    policy_gradient_loss | 0.00074      |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1194        |\n",
      "|    time_elapsed         | 2797        |\n",
      "|    total_timesteps      | 2445312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005617527 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.8        |\n",
      "|    n_updates            | 16820       |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1195        |\n",
      "|    time_elapsed         | 2800        |\n",
      "|    total_timesteps      | 2447360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009930614 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 16830       |\n",
      "|    policy_gradient_loss | -0.000119   |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2448528, episode_reward=171.83 +/- 94.42\n",
      "Episode length: 346.80 +/- 68.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 347        |\n",
      "|    mean_reward          | 172        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2448528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02551864 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.474     |\n",
      "|    explained_variance   | 0.731      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.3       |\n",
      "|    n_updates            | 16840      |\n",
      "|    policy_gradient_loss | -2.79e-05  |\n",
      "|    value_loss           | 22.3       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 873     |\n",
      "|    iterations      | 1196    |\n",
      "|    time_elapsed    | 2803    |\n",
      "|    total_timesteps | 2449408 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 1197         |\n",
      "|    time_elapsed         | 2805         |\n",
      "|    total_timesteps      | 2451456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047707018 |\n",
      "|    clip_fraction        | 0.0636       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 16850        |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    value_loss           | 69.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1198         |\n",
      "|    time_elapsed         | 2807         |\n",
      "|    total_timesteps      | 2453504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029044533 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.93         |\n",
      "|    n_updates            | 16860        |\n",
      "|    policy_gradient_loss | 0.000278     |\n",
      "|    value_loss           | 38.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1199         |\n",
      "|    time_elapsed         | 2809         |\n",
      "|    total_timesteps      | 2455552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069485162 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 16870        |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    value_loss           | 51.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1200        |\n",
      "|    time_elapsed         | 2811        |\n",
      "|    total_timesteps      | 2457600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002967034 |\n",
      "|    clip_fraction        | 0.0274      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 16880       |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 77.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2458528, episode_reward=174.02 +/- 95.30\n",
      "Episode length: 301.20 +/- 30.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 301          |\n",
      "|    mean_reward          | 174          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2458528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037146285 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.222       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 16890        |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 55.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1201    |\n",
      "|    time_elapsed    | 2814    |\n",
      "|    total_timesteps | 2459648 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1202        |\n",
      "|    time_elapsed         | 2816        |\n",
      "|    total_timesteps      | 2461696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008437112 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.08        |\n",
      "|    n_updates            | 16900       |\n",
      "|    policy_gradient_loss | 2.14e-05    |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1203         |\n",
      "|    time_elapsed         | 2817         |\n",
      "|    total_timesteps      | 2463744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026635122 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 16910        |\n",
      "|    policy_gradient_loss | -0.000433    |\n",
      "|    value_loss           | 82.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1204         |\n",
      "|    time_elapsed         | 2819         |\n",
      "|    total_timesteps      | 2465792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031704288 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 16920        |\n",
      "|    policy_gradient_loss | -0.000496    |\n",
      "|    value_loss           | 176          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1205         |\n",
      "|    time_elapsed         | 2822         |\n",
      "|    total_timesteps      | 2467840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021359203 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.182       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.8         |\n",
      "|    n_updates            | 16930        |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2468528, episode_reward=213.97 +/- 23.11\n",
      "Episode length: 324.20 +/- 45.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 324        |\n",
      "|    mean_reward          | 214        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2468528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01350044 |\n",
      "|    clip_fraction        | 0.0962     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.348     |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 16940      |\n",
      "|    policy_gradient_loss | -0.00213   |\n",
      "|    value_loss           | 18.8       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1206    |\n",
      "|    time_elapsed    | 2825    |\n",
      "|    total_timesteps | 2469888 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1207        |\n",
      "|    time_elapsed         | 2827        |\n",
      "|    total_timesteps      | 2471936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010139448 |\n",
      "|    clip_fraction        | 0.0654      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 16950       |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 59.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 874        |\n",
      "|    iterations           | 1208       |\n",
      "|    time_elapsed         | 2829       |\n",
      "|    total_timesteps      | 2473984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00358039 |\n",
      "|    clip_fraction        | 0.0484     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.252     |\n",
      "|    explained_variance   | 0.775      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.93       |\n",
      "|    n_updates            | 16960      |\n",
      "|    policy_gradient_loss | -0.0027    |\n",
      "|    value_loss           | 18.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1209         |\n",
      "|    time_elapsed         | 2831         |\n",
      "|    total_timesteps      | 2476032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059279557 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 74.5         |\n",
      "|    n_updates            | 16970        |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1210         |\n",
      "|    time_elapsed         | 2833         |\n",
      "|    total_timesteps      | 2478080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031920252 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.3         |\n",
      "|    n_updates            | 16980        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 94.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2478528, episode_reward=187.60 +/- 100.21\n",
      "Episode length: 320.20 +/- 43.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2478528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023628166 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.8          |\n",
      "|    n_updates            | 16990        |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 71.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1211    |\n",
      "|    time_elapsed    | 2836    |\n",
      "|    total_timesteps | 2480128 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1212         |\n",
      "|    time_elapsed         | 2838         |\n",
      "|    total_timesteps      | 2482176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086592445 |\n",
      "|    clip_fraction        | 0.0679       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.6         |\n",
      "|    n_updates            | 17000        |\n",
      "|    policy_gradient_loss | 0.00202      |\n",
      "|    value_loss           | 40.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1213         |\n",
      "|    time_elapsed         | 2840         |\n",
      "|    total_timesteps      | 2484224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026904917 |\n",
      "|    clip_fraction        | 0.0326       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 17010        |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1214        |\n",
      "|    time_elapsed         | 2842        |\n",
      "|    total_timesteps      | 2486272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006575588 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.49        |\n",
      "|    n_updates            | 17020       |\n",
      "|    policy_gradient_loss | 0.000228    |\n",
      "|    value_loss           | 44.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1215         |\n",
      "|    time_elapsed         | 2845         |\n",
      "|    total_timesteps      | 2488320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090094395 |\n",
      "|    clip_fraction        | 0.0661       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.344       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 17030        |\n",
      "|    policy_gradient_loss | 0.000404     |\n",
      "|    value_loss           | 44           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2488528, episode_reward=220.35 +/- 74.88\n",
      "Episode length: 448.40 +/- 275.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 448          |\n",
      "|    mean_reward          | 220          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2488528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047546634 |\n",
      "|    clip_fraction        | 0.0981       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 80.7         |\n",
      "|    n_updates            | 17040        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1216    |\n",
      "|    time_elapsed    | 2848    |\n",
      "|    total_timesteps | 2490368 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1217        |\n",
      "|    time_elapsed         | 2850        |\n",
      "|    total_timesteps      | 2492416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004644636 |\n",
      "|    clip_fraction        | 0.0527      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.9         |\n",
      "|    n_updates            | 17050       |\n",
      "|    policy_gradient_loss | 0.00123     |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1218        |\n",
      "|    time_elapsed         | 2852        |\n",
      "|    total_timesteps      | 2494464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021916527 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 17060       |\n",
      "|    policy_gradient_loss | -0.000497   |\n",
      "|    value_loss           | 5.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1219        |\n",
      "|    time_elapsed         | 2854        |\n",
      "|    total_timesteps      | 2496512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014790826 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 17070       |\n",
      "|    policy_gradient_loss | -0.00258    |\n",
      "|    value_loss           | 43.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2498528, episode_reward=129.99 +/- 138.59\n",
      "Episode length: 345.00 +/- 122.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | 130          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2498528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034802933 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 17080        |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1220    |\n",
      "|    time_elapsed    | 2857    |\n",
      "|    total_timesteps | 2498560 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1221        |\n",
      "|    time_elapsed         | 2859        |\n",
      "|    total_timesteps      | 2500608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004475018 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 129         |\n",
      "|    n_updates            | 17090       |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1222         |\n",
      "|    time_elapsed         | 2861         |\n",
      "|    total_timesteps      | 2502656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102650635 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.339       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.7         |\n",
      "|    n_updates            | 17100        |\n",
      "|    policy_gradient_loss | -0.00473     |\n",
      "|    value_loss           | 38.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1223        |\n",
      "|    time_elapsed         | 2863        |\n",
      "|    total_timesteps      | 2504704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004323089 |\n",
      "|    clip_fraction        | 0.0317      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 17110       |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 74.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1224        |\n",
      "|    time_elapsed         | 2865        |\n",
      "|    total_timesteps      | 2506752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004105191 |\n",
      "|    clip_fraction        | 0.0363      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.08        |\n",
      "|    n_updates            | 17120       |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2508528, episode_reward=228.35 +/- 48.68\n",
      "Episode length: 364.00 +/- 42.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | 228          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2508528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104422625 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 17130        |\n",
      "|    policy_gradient_loss | 0.00328      |\n",
      "|    value_loss           | 16.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1225    |\n",
      "|    time_elapsed    | 2869    |\n",
      "|    total_timesteps | 2508800 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 874        |\n",
      "|    iterations           | 1226       |\n",
      "|    time_elapsed         | 2871       |\n",
      "|    total_timesteps      | 2510848    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01200193 |\n",
      "|    clip_fraction        | 0.0966     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.343     |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.67       |\n",
      "|    n_updates            | 17140      |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    value_loss           | 23.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1227        |\n",
      "|    time_elapsed         | 2873        |\n",
      "|    total_timesteps      | 2512896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002274673 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.5        |\n",
      "|    n_updates            | 17150       |\n",
      "|    policy_gradient_loss | -0.000994   |\n",
      "|    value_loss           | 97.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1228        |\n",
      "|    time_elapsed         | 2875        |\n",
      "|    total_timesteps      | 2514944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004680399 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 17160       |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1229         |\n",
      "|    time_elapsed         | 2877         |\n",
      "|    total_timesteps      | 2516992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035448815 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 17170        |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 85.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2518528, episode_reward=208.70 +/- 38.91\n",
      "Episode length: 347.40 +/- 39.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | 209          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2518528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014272344 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45           |\n",
      "|    n_updates            | 17180        |\n",
      "|    policy_gradient_loss | -8.93e-05    |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1230    |\n",
      "|    time_elapsed    | 2880    |\n",
      "|    total_timesteps | 2519040 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1231        |\n",
      "|    time_elapsed         | 2882        |\n",
      "|    total_timesteps      | 2521088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010831126 |\n",
      "|    clip_fraction        | 0.0613      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.45        |\n",
      "|    n_updates            | 17190       |\n",
      "|    policy_gradient_loss | -6.04e-05   |\n",
      "|    value_loss           | 57.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1232        |\n",
      "|    time_elapsed         | 2884        |\n",
      "|    total_timesteps      | 2523136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002721597 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 17200       |\n",
      "|    policy_gradient_loss | -0.000283   |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1233         |\n",
      "|    time_elapsed         | 2885         |\n",
      "|    total_timesteps      | 2525184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073698414 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 17210        |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 27.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1234         |\n",
      "|    time_elapsed         | 2888         |\n",
      "|    total_timesteps      | 2527232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033525918 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 17220        |\n",
      "|    policy_gradient_loss | 0.000247     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2528528, episode_reward=113.51 +/- 132.27\n",
      "Episode length: 314.20 +/- 32.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 314         |\n",
      "|    mean_reward          | 114         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2528528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007311958 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 17230       |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    value_loss           | 55.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1235    |\n",
      "|    time_elapsed    | 2891    |\n",
      "|    total_timesteps | 2529280 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1236         |\n",
      "|    time_elapsed         | 2893         |\n",
      "|    total_timesteps      | 2531328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032300705 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 17240        |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    value_loss           | 41           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1237         |\n",
      "|    time_elapsed         | 2896         |\n",
      "|    total_timesteps      | 2533376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040837484 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.226       |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 17250        |\n",
      "|    policy_gradient_loss | -0.000997    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1238        |\n",
      "|    time_elapsed         | 2898        |\n",
      "|    total_timesteps      | 2535424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012729829 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.16        |\n",
      "|    n_updates            | 17260       |\n",
      "|    policy_gradient_loss | 0.00181     |\n",
      "|    value_loss           | 25.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1239         |\n",
      "|    time_elapsed         | 2900         |\n",
      "|    total_timesteps      | 2537472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036842723 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.2         |\n",
      "|    n_updates            | 17270        |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2538528, episode_reward=122.37 +/- 141.39\n",
      "Episode length: 466.00 +/- 119.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 466          |\n",
      "|    mean_reward          | 122          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2538528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053232484 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.262       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 82.3         |\n",
      "|    n_updates            | 17280        |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 88.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1240    |\n",
      "|    time_elapsed    | 2903    |\n",
      "|    total_timesteps | 2539520 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1241         |\n",
      "|    time_elapsed         | 2905         |\n",
      "|    total_timesteps      | 2541568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017455831 |\n",
      "|    clip_fraction        | 0.0179       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.19         |\n",
      "|    n_updates            | 17290        |\n",
      "|    policy_gradient_loss | -0.000697    |\n",
      "|    value_loss           | 67           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1242         |\n",
      "|    time_elapsed         | 2907         |\n",
      "|    total_timesteps      | 2543616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050391695 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.189       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 17300        |\n",
      "|    policy_gradient_loss | -0.00054     |\n",
      "|    value_loss           | 79.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1243         |\n",
      "|    time_elapsed         | 2909         |\n",
      "|    total_timesteps      | 2545664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026836363 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.1         |\n",
      "|    n_updates            | 17310        |\n",
      "|    policy_gradient_loss | 0.000514     |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1244        |\n",
      "|    time_elapsed         | 2911        |\n",
      "|    total_timesteps      | 2547712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004638477 |\n",
      "|    clip_fraction        | 0.042       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.9        |\n",
      "|    n_updates            | 17320       |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 199         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2548528, episode_reward=146.18 +/- 144.54\n",
      "Episode length: 284.80 +/- 25.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | 146         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2548528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005415208 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 17330       |\n",
      "|    policy_gradient_loss | 0.000751    |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1245    |\n",
      "|    time_elapsed    | 2914    |\n",
      "|    total_timesteps | 2549760 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1246        |\n",
      "|    time_elapsed         | 2916        |\n",
      "|    total_timesteps      | 2551808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924183 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 17340       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1247         |\n",
      "|    time_elapsed         | 2918         |\n",
      "|    total_timesteps      | 2553856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023557113 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.1         |\n",
      "|    n_updates            | 17350        |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 163          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1248        |\n",
      "|    time_elapsed         | 2920        |\n",
      "|    total_timesteps      | 2555904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008004205 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.59        |\n",
      "|    n_updates            | 17360       |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 47.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1249        |\n",
      "|    time_elapsed         | 2922        |\n",
      "|    total_timesteps      | 2557952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003063072 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.46        |\n",
      "|    n_updates            | 17370       |\n",
      "|    policy_gradient_loss | -0.000947   |\n",
      "|    value_loss           | 50.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2558528, episode_reward=144.14 +/- 72.79\n",
      "Episode length: 375.60 +/- 172.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 376          |\n",
      "|    mean_reward          | 144          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2558528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049485336 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 17380        |\n",
      "|    policy_gradient_loss | 0.00279      |\n",
      "|    value_loss           | 15.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1250    |\n",
      "|    time_elapsed    | 2925    |\n",
      "|    total_timesteps | 2560000 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1251         |\n",
      "|    time_elapsed         | 2927         |\n",
      "|    total_timesteps      | 2562048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046021203 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.7         |\n",
      "|    n_updates            | 17390        |\n",
      "|    policy_gradient_loss | -0.000431    |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1252        |\n",
      "|    time_elapsed         | 2929        |\n",
      "|    total_timesteps      | 2564096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003810077 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 17400       |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 65.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1253         |\n",
      "|    time_elapsed         | 2932         |\n",
      "|    total_timesteps      | 2566144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030150576 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.2          |\n",
      "|    n_updates            | 17410        |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 26           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1254         |\n",
      "|    time_elapsed         | 2934         |\n",
      "|    total_timesteps      | 2568192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122620035 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.09         |\n",
      "|    n_updates            | 17420        |\n",
      "|    policy_gradient_loss | 0.00136      |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2568528, episode_reward=180.67 +/- 72.45\n",
      "Episode length: 470.40 +/- 261.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 470          |\n",
      "|    mean_reward          | 181          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2568528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069074947 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06         |\n",
      "|    n_updates            | 17430        |\n",
      "|    policy_gradient_loss | 2.14e-05     |\n",
      "|    value_loss           | 11.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1255    |\n",
      "|    time_elapsed    | 2938    |\n",
      "|    total_timesteps | 2570240 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1256        |\n",
      "|    time_elapsed         | 2940        |\n",
      "|    total_timesteps      | 2572288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008807554 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 17440       |\n",
      "|    policy_gradient_loss | 0.000837    |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1257        |\n",
      "|    time_elapsed         | 2942        |\n",
      "|    total_timesteps      | 2574336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007816312 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.35       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 17450       |\n",
      "|    policy_gradient_loss | -0.00309    |\n",
      "|    value_loss           | 35.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 1258         |\n",
      "|    time_elapsed         | 2944         |\n",
      "|    total_timesteps      | 2576384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018376035 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 72.9         |\n",
      "|    n_updates            | 17460        |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 188          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1259         |\n",
      "|    time_elapsed         | 2946         |\n",
      "|    total_timesteps      | 2578432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034912634 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.308       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 17470        |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    value_loss           | 53.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2578528, episode_reward=202.06 +/- 35.23\n",
      "Episode length: 339.20 +/- 112.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | 202          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2578528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050169593 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 68.7         |\n",
      "|    n_updates            | 17480        |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 88.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1260    |\n",
      "|    time_elapsed    | 2949    |\n",
      "|    total_timesteps | 2580480 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1261        |\n",
      "|    time_elapsed         | 2951        |\n",
      "|    total_timesteps      | 2582528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009206835 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.88        |\n",
      "|    n_updates            | 17490       |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1262         |\n",
      "|    time_elapsed         | 2953         |\n",
      "|    total_timesteps      | 2584576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046847844 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19           |\n",
      "|    n_updates            | 17500        |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 71.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1263         |\n",
      "|    time_elapsed         | 2955         |\n",
      "|    total_timesteps      | 2586624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011702345 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 17510        |\n",
      "|    policy_gradient_loss | -0.000794    |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2588528, episode_reward=194.35 +/- 41.84\n",
      "Episode length: 371.00 +/- 52.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 194         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2588528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006189281 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.9        |\n",
      "|    n_updates            | 17520       |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    value_loss           | 59.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 874     |\n",
      "|    iterations      | 1264    |\n",
      "|    time_elapsed    | 2959    |\n",
      "|    total_timesteps | 2588672 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 1265        |\n",
      "|    time_elapsed         | 2961        |\n",
      "|    total_timesteps      | 2590720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009887226 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 17530       |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 875        |\n",
      "|    iterations           | 1266       |\n",
      "|    time_elapsed         | 2962       |\n",
      "|    total_timesteps      | 2592768    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00337472 |\n",
      "|    clip_fraction        | 0.0396     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.207     |\n",
      "|    explained_variance   | 0.781      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19         |\n",
      "|    n_updates            | 17540      |\n",
      "|    policy_gradient_loss | -0.00166   |\n",
      "|    value_loss           | 56.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1267        |\n",
      "|    time_elapsed         | 2964        |\n",
      "|    total_timesteps      | 2594816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011745705 |\n",
      "|    clip_fraction        | 0.0375      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.84        |\n",
      "|    n_updates            | 17550       |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1268         |\n",
      "|    time_elapsed         | 2966         |\n",
      "|    total_timesteps      | 2596864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022180015 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 17560        |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 43.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2598528, episode_reward=196.69 +/- 99.30\n",
      "Episode length: 315.60 +/- 94.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 197         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2598528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005006655 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.14        |\n",
      "|    n_updates            | 17570       |\n",
      "|    policy_gradient_loss | 0.00163     |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1269    |\n",
      "|    time_elapsed    | 2969    |\n",
      "|    total_timesteps | 2598912 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1270         |\n",
      "|    time_elapsed         | 2971         |\n",
      "|    total_timesteps      | 2600960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013885874 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.678        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.6         |\n",
      "|    n_updates            | 17580        |\n",
      "|    policy_gradient_loss | -4.8e-05     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1271         |\n",
      "|    time_elapsed         | 2973         |\n",
      "|    total_timesteps      | 2603008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037021511 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.225       |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 17590        |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 96.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1272        |\n",
      "|    time_elapsed         | 2976        |\n",
      "|    total_timesteps      | 2605056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008981689 |\n",
      "|    clip_fraction        | 0.0492      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 17600       |\n",
      "|    policy_gradient_loss | -1.27e-05   |\n",
      "|    value_loss           | 43.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1273         |\n",
      "|    time_elapsed         | 2978         |\n",
      "|    total_timesteps      | 2607104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034872275 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.72         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 17610        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 80.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2608528, episode_reward=119.99 +/- 140.88\n",
      "Episode length: 335.40 +/- 98.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | 120          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2608528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058713364 |\n",
      "|    clip_fraction        | 0.0534       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.335       |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.04         |\n",
      "|    n_updates            | 17620        |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 51.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1274    |\n",
      "|    time_elapsed    | 2981    |\n",
      "|    total_timesteps | 2609152 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 875        |\n",
      "|    iterations           | 1275       |\n",
      "|    time_elapsed         | 2983       |\n",
      "|    total_timesteps      | 2611200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00859279 |\n",
      "|    clip_fraction        | 0.0695     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.425     |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.43       |\n",
      "|    n_updates            | 17630      |\n",
      "|    policy_gradient_loss | -0.000831  |\n",
      "|    value_loss           | 69.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1276         |\n",
      "|    time_elapsed         | 2986         |\n",
      "|    total_timesteps      | 2613248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028523528 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.32         |\n",
      "|    n_updates            | 17640        |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    value_loss           | 61           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1277        |\n",
      "|    time_elapsed         | 2988        |\n",
      "|    total_timesteps      | 2615296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004611628 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0982     |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 17650       |\n",
      "|    policy_gradient_loss | -0.000858   |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1278        |\n",
      "|    time_elapsed         | 2990        |\n",
      "|    total_timesteps      | 2617344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009488195 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.22        |\n",
      "|    n_updates            | 17660       |\n",
      "|    policy_gradient_loss | 0.00095     |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2618528, episode_reward=152.09 +/- 82.26\n",
      "Episode length: 308.20 +/- 68.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | 152         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2618528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004596578 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.73        |\n",
      "|    n_updates            | 17670       |\n",
      "|    policy_gradient_loss | 0.000897    |\n",
      "|    value_loss           | 11.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1279    |\n",
      "|    time_elapsed    | 2993    |\n",
      "|    total_timesteps | 2619392 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1280         |\n",
      "|    time_elapsed         | 2995         |\n",
      "|    total_timesteps      | 2621440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014281012 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8            |\n",
      "|    n_updates            | 17680        |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    value_loss           | 50.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1281         |\n",
      "|    time_elapsed         | 2996         |\n",
      "|    total_timesteps      | 2623488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026983968 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.4         |\n",
      "|    n_updates            | 17690        |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 39.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1282         |\n",
      "|    time_elapsed         | 2999         |\n",
      "|    total_timesteps      | 2625536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028852695 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 17700        |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 91.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1283        |\n",
      "|    time_elapsed         | 3001        |\n",
      "|    total_timesteps      | 2627584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008467209 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.04        |\n",
      "|    n_updates            | 17710       |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2628528, episode_reward=192.98 +/- 86.97\n",
      "Episode length: 298.80 +/- 22.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | 193          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2628528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035789502 |\n",
      "|    clip_fraction        | 0.0278       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42           |\n",
      "|    n_updates            | 17720        |\n",
      "|    policy_gradient_loss | -0.0009      |\n",
      "|    value_loss           | 78.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1284    |\n",
      "|    time_elapsed    | 3004    |\n",
      "|    total_timesteps | 2629632 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1285        |\n",
      "|    time_elapsed         | 3006        |\n",
      "|    total_timesteps      | 2631680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008608956 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 17730       |\n",
      "|    policy_gradient_loss | -0.000346   |\n",
      "|    value_loss           | 20.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1286         |\n",
      "|    time_elapsed         | 3008         |\n",
      "|    total_timesteps      | 2633728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037179913 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 17740        |\n",
      "|    policy_gradient_loss | -0.00413     |\n",
      "|    value_loss           | 48.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1287        |\n",
      "|    time_elapsed         | 3010        |\n",
      "|    total_timesteps      | 2635776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003278377 |\n",
      "|    clip_fraction        | 0.0199      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.158      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 17750       |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    value_loss           | 96.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1288         |\n",
      "|    time_elapsed         | 3012         |\n",
      "|    total_timesteps      | 2637824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044021844 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29           |\n",
      "|    n_updates            | 17760        |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2638528, episode_reward=230.71 +/- 27.92\n",
      "Episode length: 357.00 +/- 35.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 357         |\n",
      "|    mean_reward          | 231         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2638528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006044844 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 17770       |\n",
      "|    policy_gradient_loss | -0.000909   |\n",
      "|    value_loss           | 53.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1289    |\n",
      "|    time_elapsed    | 3014    |\n",
      "|    total_timesteps | 2639872 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1290         |\n",
      "|    time_elapsed         | 3016         |\n",
      "|    total_timesteps      | 2641920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079339035 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.11         |\n",
      "|    n_updates            | 17780        |\n",
      "|    policy_gradient_loss | 0.000223     |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1291        |\n",
      "|    time_elapsed         | 3018        |\n",
      "|    total_timesteps      | 2643968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003924677 |\n",
      "|    clip_fraction        | 0.0498      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.161      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 17790       |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 875        |\n",
      "|    iterations           | 1292       |\n",
      "|    time_elapsed         | 3020       |\n",
      "|    total_timesteps      | 2646016    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00299922 |\n",
      "|    clip_fraction        | 0.036      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.181     |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.3       |\n",
      "|    n_updates            | 17800      |\n",
      "|    policy_gradient_loss | -0.000351  |\n",
      "|    value_loss           | 108        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1293        |\n",
      "|    time_elapsed         | 3022        |\n",
      "|    total_timesteps      | 2648064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006887843 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.3        |\n",
      "|    n_updates            | 17810       |\n",
      "|    policy_gradient_loss | 0.000937    |\n",
      "|    value_loss           | 73.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2648528, episode_reward=170.16 +/- 106.19\n",
      "Episode length: 358.20 +/- 75.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | 170          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2648528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031425254 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 17820        |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1294    |\n",
      "|    time_elapsed    | 3025    |\n",
      "|    total_timesteps | 2650112 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1295        |\n",
      "|    time_elapsed         | 3027        |\n",
      "|    total_timesteps      | 2652160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003968381 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.192      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.2        |\n",
      "|    n_updates            | 17830       |\n",
      "|    policy_gradient_loss | 0.000387    |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1296         |\n",
      "|    time_elapsed         | 3030         |\n",
      "|    total_timesteps      | 2654208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066777314 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.639        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 17840        |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 94.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1297        |\n",
      "|    time_elapsed         | 3032        |\n",
      "|    total_timesteps      | 2656256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014546484 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.6         |\n",
      "|    n_updates            | 17850       |\n",
      "|    policy_gradient_loss | 0.00167     |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1298        |\n",
      "|    time_elapsed         | 3034        |\n",
      "|    total_timesteps      | 2658304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002619626 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 17860       |\n",
      "|    policy_gradient_loss | 0.00034     |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2658528, episode_reward=219.25 +/- 32.30\n",
      "Episode length: 303.00 +/- 23.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 303          |\n",
      "|    mean_reward          | 219          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2658528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038809348 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.707        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 17870        |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 91.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1299    |\n",
      "|    time_elapsed    | 3036    |\n",
      "|    total_timesteps | 2660352 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1300         |\n",
      "|    time_elapsed         | 3038         |\n",
      "|    total_timesteps      | 2662400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036532877 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.193       |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 17880        |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 81.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1301        |\n",
      "|    time_elapsed         | 3040        |\n",
      "|    total_timesteps      | 2664448     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005021065 |\n",
      "|    clip_fraction        | 0.0649      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.202      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.05        |\n",
      "|    n_updates            | 17890       |\n",
      "|    policy_gradient_loss | 0.00118     |\n",
      "|    value_loss           | 48.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1302         |\n",
      "|    time_elapsed         | 3042         |\n",
      "|    total_timesteps      | 2666496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044443104 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.635        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 17900        |\n",
      "|    policy_gradient_loss | -0.000877    |\n",
      "|    value_loss           | 69.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2668528, episode_reward=187.43 +/- 89.91\n",
      "Episode length: 308.20 +/- 29.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 308          |\n",
      "|    mean_reward          | 187          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2668528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059044873 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.714        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 17910        |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 56.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1303    |\n",
      "|    time_elapsed    | 3045    |\n",
      "|    total_timesteps | 2668544 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1304         |\n",
      "|    time_elapsed         | 3048         |\n",
      "|    total_timesteps      | 2670592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046053124 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.74         |\n",
      "|    n_updates            | 17920        |\n",
      "|    policy_gradient_loss | 0.000281     |\n",
      "|    value_loss           | 32.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1305         |\n",
      "|    time_elapsed         | 3050         |\n",
      "|    total_timesteps      | 2672640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053299074 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29         |\n",
      "|    n_updates            | 17930        |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    value_loss           | 16.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1306         |\n",
      "|    time_elapsed         | 3051         |\n",
      "|    total_timesteps      | 2674688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070684487 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.336       |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.28         |\n",
      "|    n_updates            | 17940        |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 37.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1307        |\n",
      "|    time_elapsed         | 3053        |\n",
      "|    total_timesteps      | 2676736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004048408 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 17950       |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2678528, episode_reward=130.24 +/- 139.52\n",
      "Episode length: 515.20 +/- 250.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 515          |\n",
      "|    mean_reward          | 130          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2678528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026165345 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 17960        |\n",
      "|    policy_gradient_loss | -0.000527    |\n",
      "|    value_loss           | 92.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1308    |\n",
      "|    time_elapsed    | 3058    |\n",
      "|    total_timesteps | 2678784 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1309         |\n",
      "|    time_elapsed         | 3060         |\n",
      "|    total_timesteps      | 2680832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044188136 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.2          |\n",
      "|    n_updates            | 17970        |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 44.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1310        |\n",
      "|    time_elapsed         | 3062        |\n",
      "|    total_timesteps      | 2682880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010577841 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.232      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 17980       |\n",
      "|    policy_gradient_loss | -0.000938   |\n",
      "|    value_loss           | 61.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1311        |\n",
      "|    time_elapsed         | 3065        |\n",
      "|    total_timesteps      | 2684928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004712049 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.32        |\n",
      "|    n_updates            | 17990       |\n",
      "|    policy_gradient_loss | 1.78e-06    |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1312         |\n",
      "|    time_elapsed         | 3067         |\n",
      "|    total_timesteps      | 2686976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045682946 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.269       |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.86         |\n",
      "|    n_updates            | 18000        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 45.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2688528, episode_reward=121.05 +/- 118.14\n",
      "Episode length: 283.40 +/- 79.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 121          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2688528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037109994 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 18010        |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 29.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1313    |\n",
      "|    time_elapsed    | 3069    |\n",
      "|    total_timesteps | 2689024 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1314         |\n",
      "|    time_elapsed         | 3071         |\n",
      "|    total_timesteps      | 2691072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047251587 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.2         |\n",
      "|    n_updates            | 18020        |\n",
      "|    policy_gradient_loss | -0.000204    |\n",
      "|    value_loss           | 94.1         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1315       |\n",
      "|    time_elapsed         | 3074       |\n",
      "|    total_timesteps      | 2693120    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00770086 |\n",
      "|    clip_fraction        | 0.0423     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.176     |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 30         |\n",
      "|    n_updates            | 18030      |\n",
      "|    policy_gradient_loss | -0.00168   |\n",
      "|    value_loss           | 75.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1316        |\n",
      "|    time_elapsed         | 3075        |\n",
      "|    total_timesteps      | 2695168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011807721 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.84        |\n",
      "|    n_updates            | 18040       |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    value_loss           | 41.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1317         |\n",
      "|    time_elapsed         | 3077         |\n",
      "|    total_timesteps      | 2697216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032515205 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.9         |\n",
      "|    n_updates            | 18050        |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2698528, episode_reward=186.02 +/- 120.67\n",
      "Episode length: 318.60 +/- 66.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | 186          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2698528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077106366 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.95         |\n",
      "|    n_updates            | 18060        |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1318    |\n",
      "|    time_elapsed    | 3080    |\n",
      "|    total_timesteps | 2699264 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1319         |\n",
      "|    time_elapsed         | 3082         |\n",
      "|    total_timesteps      | 2701312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051134145 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.36         |\n",
      "|    n_updates            | 18070        |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 32.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1320        |\n",
      "|    time_elapsed         | 3084        |\n",
      "|    total_timesteps      | 2703360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005162185 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.54        |\n",
      "|    n_updates            | 18080       |\n",
      "|    policy_gradient_loss | -0.000295   |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1321         |\n",
      "|    time_elapsed         | 3086         |\n",
      "|    total_timesteps      | 2705408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043262243 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 18090        |\n",
      "|    policy_gradient_loss | 8.24e-05     |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1322         |\n",
      "|    time_elapsed         | 3088         |\n",
      "|    total_timesteps      | 2707456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099986615 |\n",
      "|    clip_fraction        | 0.0763       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 18100        |\n",
      "|    policy_gradient_loss | 0.000932     |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2708528, episode_reward=161.02 +/- 83.89\n",
      "Episode length: 325.60 +/- 58.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 161          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2708528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032152685 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 18110        |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 136          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1323    |\n",
      "|    time_elapsed    | 3091    |\n",
      "|    total_timesteps | 2709504 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1324        |\n",
      "|    time_elapsed         | 3093        |\n",
      "|    total_timesteps      | 2711552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004147995 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21        |\n",
      "|    n_updates            | 18120       |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 31.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1325         |\n",
      "|    time_elapsed         | 3095         |\n",
      "|    total_timesteps      | 2713600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071008764 |\n",
      "|    clip_fraction        | 0.047        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 84.1         |\n",
      "|    n_updates            | 18130        |\n",
      "|    policy_gradient_loss | 0.00094      |\n",
      "|    value_loss           | 72.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1326        |\n",
      "|    time_elapsed         | 3097        |\n",
      "|    total_timesteps      | 2715648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004079035 |\n",
      "|    clip_fraction        | 0.0544      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 18140       |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1327        |\n",
      "|    time_elapsed         | 3099        |\n",
      "|    total_timesteps      | 2717696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007752965 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.89        |\n",
      "|    n_updates            | 18150       |\n",
      "|    policy_gradient_loss | -0.000521   |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2718528, episode_reward=225.10 +/- 39.44\n",
      "Episode length: 315.80 +/- 35.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 316          |\n",
      "|    mean_reward          | 225          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2718528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028970593 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.182       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 18160        |\n",
      "|    policy_gradient_loss | 1.36e-05     |\n",
      "|    value_loss           | 84.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1328    |\n",
      "|    time_elapsed    | 3102    |\n",
      "|    total_timesteps | 2719744 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1329         |\n",
      "|    time_elapsed         | 3104         |\n",
      "|    total_timesteps      | 2721792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069222115 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 18170        |\n",
      "|    policy_gradient_loss | -0.000599    |\n",
      "|    value_loss           | 56           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1330         |\n",
      "|    time_elapsed         | 3106         |\n",
      "|    total_timesteps      | 2723840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033520006 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.71         |\n",
      "|    n_updates            | 18180        |\n",
      "|    policy_gradient_loss | 4.04e-05     |\n",
      "|    value_loss           | 54.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1331         |\n",
      "|    time_elapsed         | 3108         |\n",
      "|    total_timesteps      | 2725888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042192135 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.695        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.07         |\n",
      "|    n_updates            | 18190        |\n",
      "|    policy_gradient_loss | 0.00325      |\n",
      "|    value_loss           | 20.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1332         |\n",
      "|    time_elapsed         | 3110         |\n",
      "|    total_timesteps      | 2727936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035099005 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 18200        |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    value_loss           | 49.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2728528, episode_reward=109.10 +/- 99.23\n",
      "Episode length: 407.20 +/- 298.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 407         |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2728528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007907002 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.13        |\n",
      "|    n_updates            | 18210       |\n",
      "|    policy_gradient_loss | -0.000122   |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1333    |\n",
      "|    time_elapsed    | 3114    |\n",
      "|    total_timesteps | 2729984 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1334         |\n",
      "|    time_elapsed         | 3116         |\n",
      "|    total_timesteps      | 2732032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039142123 |\n",
      "|    clip_fraction        | 0.0664       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.1          |\n",
      "|    n_updates            | 18220        |\n",
      "|    policy_gradient_loss | -0.000706    |\n",
      "|    value_loss           | 28.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1335        |\n",
      "|    time_elapsed         | 3118        |\n",
      "|    total_timesteps      | 2734080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014726365 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.85        |\n",
      "|    n_updates            | 18230       |\n",
      "|    policy_gradient_loss | -0.00857    |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1336         |\n",
      "|    time_elapsed         | 3120         |\n",
      "|    total_timesteps      | 2736128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052088215 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 18240        |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1337        |\n",
      "|    time_elapsed         | 3122        |\n",
      "|    total_timesteps      | 2738176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007827008 |\n",
      "|    clip_fraction        | 0.044       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 18250       |\n",
      "|    policy_gradient_loss | -0.00145    |\n",
      "|    value_loss           | 23.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2738528, episode_reward=203.23 +/- 30.35\n",
      "Episode length: 454.40 +/- 180.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 454          |\n",
      "|    mean_reward          | 203          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2738528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031119478 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 18260        |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    value_loss           | 71.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1338    |\n",
      "|    time_elapsed    | 3126    |\n",
      "|    total_timesteps | 2740224 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1339        |\n",
      "|    time_elapsed         | 3128        |\n",
      "|    total_timesteps      | 2742272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005872056 |\n",
      "|    clip_fraction        | 0.0442      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 18270       |\n",
      "|    policy_gradient_loss | -0.000123   |\n",
      "|    value_loss           | 26.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1340         |\n",
      "|    time_elapsed         | 3130         |\n",
      "|    total_timesteps      | 2744320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046135327 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.73         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.4          |\n",
      "|    n_updates            | 18280        |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1341         |\n",
      "|    time_elapsed         | 3132         |\n",
      "|    total_timesteps      | 2746368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034487555 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 18290        |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 73.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1342        |\n",
      "|    time_elapsed         | 3134        |\n",
      "|    total_timesteps      | 2748416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005578079 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 18300       |\n",
      "|    policy_gradient_loss | -0.00208    |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2748528, episode_reward=220.77 +/- 37.19\n",
      "Episode length: 322.20 +/- 48.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | 221          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2748528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024556196 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.3         |\n",
      "|    n_updates            | 18310        |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    value_loss           | 54.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1343    |\n",
      "|    time_elapsed    | 3138    |\n",
      "|    total_timesteps | 2750464 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1344        |\n",
      "|    time_elapsed         | 3140        |\n",
      "|    total_timesteps      | 2752512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007964848 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 18320       |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 93          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1345         |\n",
      "|    time_elapsed         | 3142         |\n",
      "|    total_timesteps      | 2754560      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047512674 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.389       |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04         |\n",
      "|    n_updates            | 18330        |\n",
      "|    policy_gradient_loss | -0.000801    |\n",
      "|    value_loss           | 50.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1346        |\n",
      "|    time_elapsed         | 3144        |\n",
      "|    total_timesteps      | 2756608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009586966 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 18340       |\n",
      "|    policy_gradient_loss | 0.000824    |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2758528, episode_reward=191.25 +/- 116.19\n",
      "Episode length: 291.80 +/- 37.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 292         |\n",
      "|    mean_reward          | 191         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2758528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011988966 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.39        |\n",
      "|    n_updates            | 18350       |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 32.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1347    |\n",
      "|    time_elapsed    | 3147    |\n",
      "|    total_timesteps | 2758656 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1348         |\n",
      "|    time_elapsed         | 3149         |\n",
      "|    total_timesteps      | 2760704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053729485 |\n",
      "|    clip_fraction        | 0.0663       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 116          |\n",
      "|    n_updates            | 18360        |\n",
      "|    policy_gradient_loss | -0.00684     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1349        |\n",
      "|    time_elapsed         | 3152        |\n",
      "|    total_timesteps      | 2762752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014041087 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.46        |\n",
      "|    n_updates            | 18370       |\n",
      "|    policy_gradient_loss | 0.00081     |\n",
      "|    value_loss           | 12.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1350         |\n",
      "|    time_elapsed         | 3154         |\n",
      "|    total_timesteps      | 2764800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124902455 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.2         |\n",
      "|    n_updates            | 18380        |\n",
      "|    policy_gradient_loss | 0.000446     |\n",
      "|    value_loss           | 56.4         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1351       |\n",
      "|    time_elapsed         | 3156       |\n",
      "|    total_timesteps      | 2766848    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01159616 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.311     |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.47       |\n",
      "|    n_updates            | 18390      |\n",
      "|    policy_gradient_loss | -0.00132   |\n",
      "|    value_loss           | 45         |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2768528, episode_reward=212.56 +/- 35.17\n",
      "Episode length: 322.00 +/- 27.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2768528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076174038 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.346       |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.73         |\n",
      "|    n_updates            | 18400        |\n",
      "|    policy_gradient_loss | 0.000736     |\n",
      "|    value_loss           | 22.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1352    |\n",
      "|    time_elapsed    | 3159    |\n",
      "|    total_timesteps | 2768896 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1353         |\n",
      "|    time_elapsed         | 3161         |\n",
      "|    total_timesteps      | 2770944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031653673 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.6         |\n",
      "|    n_updates            | 18410        |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 93.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1354        |\n",
      "|    time_elapsed         | 3163        |\n",
      "|    total_timesteps      | 2772992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004621245 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 18420       |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 77.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1355         |\n",
      "|    time_elapsed         | 3165         |\n",
      "|    total_timesteps      | 2775040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039307233 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 18430        |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1356        |\n",
      "|    time_elapsed         | 3168        |\n",
      "|    total_timesteps      | 2777088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008051716 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.16        |\n",
      "|    n_updates            | 18440       |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 39.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2778528, episode_reward=56.49 +/- 106.93\n",
      "Episode length: 440.80 +/- 289.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 56.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2778528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008057574 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.42        |\n",
      "|    n_updates            | 18450       |\n",
      "|    policy_gradient_loss | -0.000534   |\n",
      "|    value_loss           | 34.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1357    |\n",
      "|    time_elapsed    | 3171    |\n",
      "|    total_timesteps | 2779136 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1358         |\n",
      "|    time_elapsed         | 3174         |\n",
      "|    total_timesteps      | 2781184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064121746 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 18460        |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 32.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1359        |\n",
      "|    time_elapsed         | 3176        |\n",
      "|    total_timesteps      | 2783232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008596977 |\n",
      "|    clip_fraction        | 0.066       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.77        |\n",
      "|    n_updates            | 18470       |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1360        |\n",
      "|    time_elapsed         | 3177        |\n",
      "|    total_timesteps      | 2785280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003962978 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.183      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 18480       |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1361         |\n",
      "|    time_elapsed         | 3180         |\n",
      "|    total_timesteps      | 2787328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034252265 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.623        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 18490        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2788528, episode_reward=121.01 +/- 125.77\n",
      "Episode length: 468.80 +/- 267.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | 121         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2788528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016683169 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.93        |\n",
      "|    n_updates            | 18500       |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1362    |\n",
      "|    time_elapsed    | 3183    |\n",
      "|    total_timesteps | 2789376 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1363        |\n",
      "|    time_elapsed         | 3186        |\n",
      "|    total_timesteps      | 2791424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003879202 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.46        |\n",
      "|    n_updates            | 18510       |\n",
      "|    policy_gradient_loss | -0.000409   |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1364        |\n",
      "|    time_elapsed         | 3188        |\n",
      "|    total_timesteps      | 2793472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003440773 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 18520       |\n",
      "|    policy_gradient_loss | -0.000131   |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1365         |\n",
      "|    time_elapsed         | 3190         |\n",
      "|    total_timesteps      | 2795520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034562112 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14         |\n",
      "|    n_updates            | 18530        |\n",
      "|    policy_gradient_loss | -0.000829    |\n",
      "|    value_loss           | 65.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1366        |\n",
      "|    time_elapsed         | 3192        |\n",
      "|    total_timesteps      | 2797568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005273723 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.6        |\n",
      "|    n_updates            | 18540       |\n",
      "|    policy_gradient_loss | -0.000889   |\n",
      "|    value_loss           | 79.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2798528, episode_reward=182.23 +/- 101.77\n",
      "Episode length: 294.00 +/- 46.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 294         |\n",
      "|    mean_reward          | 182         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2798528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006838939 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 18550       |\n",
      "|    policy_gradient_loss | 9.46e-05    |\n",
      "|    value_loss           | 48.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1367    |\n",
      "|    time_elapsed    | 3195    |\n",
      "|    total_timesteps | 2799616 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1368         |\n",
      "|    time_elapsed         | 3196         |\n",
      "|    total_timesteps      | 2801664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101664625 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.53         |\n",
      "|    n_updates            | 18560        |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 23           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1369        |\n",
      "|    time_elapsed         | 3198        |\n",
      "|    total_timesteps      | 2803712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004822785 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 80.3        |\n",
      "|    n_updates            | 18570       |\n",
      "|    policy_gradient_loss | -0.000978   |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1370         |\n",
      "|    time_elapsed         | 3201         |\n",
      "|    total_timesteps      | 2805760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067814747 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 18580        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 51.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1371        |\n",
      "|    time_elapsed         | 3203        |\n",
      "|    total_timesteps      | 2807808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017841917 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.4         |\n",
      "|    n_updates            | 18590       |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2808528, episode_reward=176.85 +/- 115.88\n",
      "Episode length: 278.20 +/- 37.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 278         |\n",
      "|    mean_reward          | 177         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2808528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004104164 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.6        |\n",
      "|    n_updates            | 18600       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1372    |\n",
      "|    time_elapsed    | 3206    |\n",
      "|    total_timesteps | 2809856 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1373         |\n",
      "|    time_elapsed         | 3208         |\n",
      "|    total_timesteps      | 2811904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054089325 |\n",
      "|    clip_fraction        | 0.059        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.1         |\n",
      "|    n_updates            | 18610        |\n",
      "|    policy_gradient_loss | 0.000807     |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1374       |\n",
      "|    time_elapsed         | 3210       |\n",
      "|    total_timesteps      | 2813952    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00671073 |\n",
      "|    clip_fraction        | 0.0943     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.375     |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.777      |\n",
      "|    n_updates            | 18620      |\n",
      "|    policy_gradient_loss | 0.00137    |\n",
      "|    value_loss           | 15.1       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1375         |\n",
      "|    time_elapsed         | 3212         |\n",
      "|    total_timesteps      | 2816000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042420933 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.91         |\n",
      "|    n_updates            | 18630        |\n",
      "|    policy_gradient_loss | -0.000732    |\n",
      "|    value_loss           | 134          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1376        |\n",
      "|    time_elapsed         | 3214        |\n",
      "|    total_timesteps      | 2818048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007311894 |\n",
      "|    clip_fraction        | 0.0411      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.53        |\n",
      "|    n_updates            | 18640       |\n",
      "|    policy_gradient_loss | 0.000291    |\n",
      "|    value_loss           | 70.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2818528, episode_reward=215.97 +/- 60.12\n",
      "Episode length: 481.20 +/- 260.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 481         |\n",
      "|    mean_reward          | 216         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2818528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008860424 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.7         |\n",
      "|    n_updates            | 18650       |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1377    |\n",
      "|    time_elapsed    | 3218    |\n",
      "|    total_timesteps | 2820096 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1378        |\n",
      "|    time_elapsed         | 3219        |\n",
      "|    total_timesteps      | 2822144     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012485413 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.54        |\n",
      "|    n_updates            | 18660       |\n",
      "|    policy_gradient_loss | -0.000454   |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1379         |\n",
      "|    time_elapsed         | 3221         |\n",
      "|    total_timesteps      | 2824192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042850105 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.241       |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 18670        |\n",
      "|    policy_gradient_loss | -0.000377    |\n",
      "|    value_loss           | 91           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1380         |\n",
      "|    time_elapsed         | 3224         |\n",
      "|    total_timesteps      | 2826240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026891986 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.142       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 18680        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1381        |\n",
      "|    time_elapsed         | 3226        |\n",
      "|    total_timesteps      | 2828288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021401387 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.18        |\n",
      "|    n_updates            | 18690       |\n",
      "|    policy_gradient_loss | -0.000765   |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2828528, episode_reward=147.41 +/- 97.85\n",
      "Episode length: 374.80 +/- 122.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 375          |\n",
      "|    mean_reward          | 147          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2828528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050955266 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.357       |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.73         |\n",
      "|    n_updates            | 18700        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 30.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1382    |\n",
      "|    time_elapsed    | 3229    |\n",
      "|    total_timesteps | 2830336 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1383        |\n",
      "|    time_elapsed         | 3232        |\n",
      "|    total_timesteps      | 2832384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004445766 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.22        |\n",
      "|    n_updates            | 18710       |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1384         |\n",
      "|    time_elapsed         | 3234         |\n",
      "|    total_timesteps      | 2834432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035554832 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | 0.606        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 18720        |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 35.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1385         |\n",
      "|    time_elapsed         | 3237         |\n",
      "|    total_timesteps      | 2836480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026390622 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.462        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.55         |\n",
      "|    n_updates            | 18730        |\n",
      "|    policy_gradient_loss | -0.00075     |\n",
      "|    value_loss           | 57.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2838528, episode_reward=96.23 +/- 81.54\n",
      "Episode length: 339.80 +/- 159.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | 96.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2838528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004996989 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 18740       |\n",
      "|    policy_gradient_loss | 0.00086     |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1386    |\n",
      "|    time_elapsed    | 3239    |\n",
      "|    total_timesteps | 2838528 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1387        |\n",
      "|    time_elapsed         | 3241        |\n",
      "|    total_timesteps      | 2840576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004639378 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 18750       |\n",
      "|    policy_gradient_loss | -0.000467   |\n",
      "|    value_loss           | 44.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1388         |\n",
      "|    time_elapsed         | 3243         |\n",
      "|    total_timesteps      | 2842624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032008914 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 18760        |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1389         |\n",
      "|    time_elapsed         | 3245         |\n",
      "|    total_timesteps      | 2844672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038958234 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 18770        |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 96           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1390         |\n",
      "|    time_elapsed         | 3247         |\n",
      "|    total_timesteps      | 2846720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036588379 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 18780        |\n",
      "|    policy_gradient_loss | 0.000114     |\n",
      "|    value_loss           | 41.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2848528, episode_reward=211.71 +/- 13.87\n",
      "Episode length: 307.20 +/- 8.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 307          |\n",
      "|    mean_reward          | 212          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2848528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026449617 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.2         |\n",
      "|    n_updates            | 18790        |\n",
      "|    policy_gradient_loss | -0.000515    |\n",
      "|    value_loss           | 56.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1391    |\n",
      "|    time_elapsed    | 3250    |\n",
      "|    total_timesteps | 2848768 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1392        |\n",
      "|    time_elapsed         | 3252        |\n",
      "|    total_timesteps      | 2850816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012195832 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 18800       |\n",
      "|    policy_gradient_loss | 0.003       |\n",
      "|    value_loss           | 6.33        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1393         |\n",
      "|    time_elapsed         | 3254         |\n",
      "|    total_timesteps      | 2852864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013492185 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.9         |\n",
      "|    n_updates            | 18810        |\n",
      "|    policy_gradient_loss | -0.000244    |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1394        |\n",
      "|    time_elapsed         | 3256        |\n",
      "|    total_timesteps      | 2854912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004314934 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 18820       |\n",
      "|    policy_gradient_loss | -0.000188   |\n",
      "|    value_loss           | 41.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1395       |\n",
      "|    time_elapsed         | 3258       |\n",
      "|    total_timesteps      | 2856960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00272474 |\n",
      "|    clip_fraction        | 0.0302     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.16      |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.15       |\n",
      "|    n_updates            | 18830      |\n",
      "|    policy_gradient_loss | -0.00193   |\n",
      "|    value_loss           | 66.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2858528, episode_reward=147.77 +/- 111.11\n",
      "Episode length: 466.40 +/- 294.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 466         |\n",
      "|    mean_reward          | 148         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2858528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006147068 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.328      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 18840       |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1396    |\n",
      "|    time_elapsed    | 3262    |\n",
      "|    total_timesteps | 2859008 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1397         |\n",
      "|    time_elapsed         | 3264         |\n",
      "|    total_timesteps      | 2861056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048189526 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 18850        |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1398         |\n",
      "|    time_elapsed         | 3266         |\n",
      "|    total_timesteps      | 2863104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038119731 |\n",
      "|    clip_fraction        | 0.0484       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70.3         |\n",
      "|    n_updates            | 18860        |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 96.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1399         |\n",
      "|    time_elapsed         | 3268         |\n",
      "|    total_timesteps      | 2865152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042604357 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29           |\n",
      "|    n_updates            | 18870        |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 63.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1400         |\n",
      "|    time_elapsed         | 3270         |\n",
      "|    total_timesteps      | 2867200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049744323 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.73         |\n",
      "|    n_updates            | 18880        |\n",
      "|    policy_gradient_loss | 0.00221      |\n",
      "|    value_loss           | 24.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2868528, episode_reward=168.51 +/- 96.94\n",
      "Episode length: 632.80 +/- 331.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 633          |\n",
      "|    mean_reward          | 169          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2868528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059398953 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.83         |\n",
      "|    n_updates            | 18890        |\n",
      "|    policy_gradient_loss | 0.00164      |\n",
      "|    value_loss           | 39.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1401    |\n",
      "|    time_elapsed    | 3275    |\n",
      "|    total_timesteps | 2869248 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1402        |\n",
      "|    time_elapsed         | 3277        |\n",
      "|    total_timesteps      | 2871296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004373745 |\n",
      "|    clip_fraction        | 0.0522      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 18900       |\n",
      "|    policy_gradient_loss | 8.74e-05    |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1403        |\n",
      "|    time_elapsed         | 3279        |\n",
      "|    total_timesteps      | 2873344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002980426 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.218      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 18910       |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 32.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1404         |\n",
      "|    time_elapsed         | 3281         |\n",
      "|    total_timesteps      | 2875392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037322126 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.248       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58         |\n",
      "|    n_updates            | 18920        |\n",
      "|    policy_gradient_loss | -0.000654    |\n",
      "|    value_loss           | 46.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1405         |\n",
      "|    time_elapsed         | 3283         |\n",
      "|    total_timesteps      | 2877440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045482665 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.279       |\n",
      "|    explained_variance   | 0.584        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.8         |\n",
      "|    n_updates            | 18930        |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 54.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2878528, episode_reward=161.27 +/- 96.87\n",
      "Episode length: 377.60 +/- 180.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 161         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2878528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018684112 |\n",
      "|    clip_fraction        | 0.0894      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 18940       |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    value_loss           | 48.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1406    |\n",
      "|    time_elapsed    | 3287    |\n",
      "|    total_timesteps | 2879488 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1407        |\n",
      "|    time_elapsed         | 3289        |\n",
      "|    total_timesteps      | 2881536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005032971 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41          |\n",
      "|    n_updates            | 18950       |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1408        |\n",
      "|    time_elapsed         | 3291        |\n",
      "|    total_timesteps      | 2883584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006768738 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 18960       |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 70          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1409         |\n",
      "|    time_elapsed         | 3293         |\n",
      "|    total_timesteps      | 2885632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030038618 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.6          |\n",
      "|    n_updates            | 18970        |\n",
      "|    policy_gradient_loss | 0.000101     |\n",
      "|    value_loss           | 39.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1410        |\n",
      "|    time_elapsed         | 3295        |\n",
      "|    total_timesteps      | 2887680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005142061 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 18980       |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 56.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2888528, episode_reward=229.53 +/- 29.57\n",
      "Episode length: 316.60 +/- 21.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | 230          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2888528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035555393 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 18990        |\n",
      "|    policy_gradient_loss | -0.000703    |\n",
      "|    value_loss           | 72.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1411    |\n",
      "|    time_elapsed    | 3298    |\n",
      "|    total_timesteps | 2889728 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1412        |\n",
      "|    time_elapsed         | 3300        |\n",
      "|    total_timesteps      | 2891776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010925399 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.29        |\n",
      "|    n_updates            | 19000       |\n",
      "|    policy_gradient_loss | -0.000876   |\n",
      "|    value_loss           | 45.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1413         |\n",
      "|    time_elapsed         | 3301         |\n",
      "|    total_timesteps      | 2893824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040661297 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.3         |\n",
      "|    n_updates            | 19010        |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1414       |\n",
      "|    time_elapsed         | 3303       |\n",
      "|    total_timesteps      | 2895872    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00279612 |\n",
      "|    clip_fraction        | 0.0385     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.192     |\n",
      "|    explained_variance   | 0.905      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.87       |\n",
      "|    n_updates            | 19020      |\n",
      "|    policy_gradient_loss | -0.00155   |\n",
      "|    value_loss           | 44.3       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1415         |\n",
      "|    time_elapsed         | 3305         |\n",
      "|    total_timesteps      | 2897920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036467828 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.44         |\n",
      "|    n_updates            | 19030        |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 46           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2898528, episode_reward=183.25 +/- 112.93\n",
      "Episode length: 328.40 +/- 77.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 328          |\n",
      "|    mean_reward          | 183          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2898528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053074895 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 88.4         |\n",
      "|    n_updates            | 19040        |\n",
      "|    policy_gradient_loss | -0.00346     |\n",
      "|    value_loss           | 72.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1416    |\n",
      "|    time_elapsed    | 3308    |\n",
      "|    total_timesteps | 2899968 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1417         |\n",
      "|    time_elapsed         | 3310         |\n",
      "|    total_timesteps      | 2902016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042178724 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 19050        |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1418        |\n",
      "|    time_elapsed         | 3313        |\n",
      "|    total_timesteps      | 2904064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003922255 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.55        |\n",
      "|    n_updates            | 19060       |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 63          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1419        |\n",
      "|    time_elapsed         | 3315        |\n",
      "|    total_timesteps      | 2906112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003370841 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 19070       |\n",
      "|    policy_gradient_loss | -0.000243   |\n",
      "|    value_loss           | 46.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1420        |\n",
      "|    time_elapsed         | 3316        |\n",
      "|    total_timesteps      | 2908160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005349093 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.19        |\n",
      "|    n_updates            | 19080       |\n",
      "|    policy_gradient_loss | -0.00047    |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2908528, episode_reward=142.91 +/- 74.14\n",
      "Episode length: 337.60 +/- 95.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 338        |\n",
      "|    mean_reward          | 143        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2908528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00381862 |\n",
      "|    clip_fraction        | 0.0535     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0.859      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 24.5       |\n",
      "|    n_updates            | 19090      |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    value_loss           | 63.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1421    |\n",
      "|    time_elapsed    | 3319    |\n",
      "|    total_timesteps | 2910208 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1422        |\n",
      "|    time_elapsed         | 3321        |\n",
      "|    total_timesteps      | 2912256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011163093 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 19100       |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    value_loss           | 39.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1423         |\n",
      "|    time_elapsed         | 3323         |\n",
      "|    total_timesteps      | 2914304      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036474876 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.74         |\n",
      "|    n_updates            | 19110        |\n",
      "|    policy_gradient_loss | -0.000823    |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1424         |\n",
      "|    time_elapsed         | 3326         |\n",
      "|    total_timesteps      | 2916352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042158496 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.273        |\n",
      "|    n_updates            | 19120        |\n",
      "|    policy_gradient_loss | 0.000538     |\n",
      "|    value_loss           | 18.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1425         |\n",
      "|    time_elapsed         | 3328         |\n",
      "|    total_timesteps      | 2918400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074848663 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.382       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.7          |\n",
      "|    n_updates            | 19130        |\n",
      "|    policy_gradient_loss | -7.58e-05    |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2918528, episode_reward=172.13 +/- 109.52\n",
      "Episode length: 337.80 +/- 76.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 338          |\n",
      "|    mean_reward          | 172          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2918528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037966415 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.2         |\n",
      "|    n_updates            | 19140        |\n",
      "|    policy_gradient_loss | -0.000829    |\n",
      "|    value_loss           | 22.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1426    |\n",
      "|    time_elapsed    | 3331    |\n",
      "|    total_timesteps | 2920448 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1427         |\n",
      "|    time_elapsed         | 3333         |\n",
      "|    total_timesteps      | 2922496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037004559 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24         |\n",
      "|    n_updates            | 19150        |\n",
      "|    policy_gradient_loss | 6.26e-05     |\n",
      "|    value_loss           | 17.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1428         |\n",
      "|    time_elapsed         | 3335         |\n",
      "|    total_timesteps      | 2924544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076314574 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 19160        |\n",
      "|    policy_gradient_loss | -0.000987    |\n",
      "|    value_loss           | 68           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1429         |\n",
      "|    time_elapsed         | 3337         |\n",
      "|    total_timesteps      | 2926592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050758133 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.7         |\n",
      "|    n_updates            | 19170        |\n",
      "|    policy_gradient_loss | -0.000777    |\n",
      "|    value_loss           | 49.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2928528, episode_reward=244.68 +/- 22.51\n",
      "Episode length: 306.80 +/- 28.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | 245         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2928528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019410241 |\n",
      "|    clip_fraction        | 0.0807      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 19180       |\n",
      "|    policy_gradient_loss | 0.00179     |\n",
      "|    value_loss           | 5.69        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1430    |\n",
      "|    time_elapsed    | 3340    |\n",
      "|    total_timesteps | 2928640 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1431         |\n",
      "|    time_elapsed         | 3342         |\n",
      "|    total_timesteps      | 2930688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020222815 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.232       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 19190        |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 30.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1432        |\n",
      "|    time_elapsed         | 3344        |\n",
      "|    total_timesteps      | 2932736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004063105 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 19200       |\n",
      "|    policy_gradient_loss | -0.00309    |\n",
      "|    value_loss           | 93.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1433        |\n",
      "|    time_elapsed         | 3346        |\n",
      "|    total_timesteps      | 2934784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002175091 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 19210       |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 59.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1434        |\n",
      "|    time_elapsed         | 3348        |\n",
      "|    total_timesteps      | 2936832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003961622 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 19220       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2938528, episode_reward=203.97 +/- 131.95\n",
      "Episode length: 269.80 +/- 17.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 270          |\n",
      "|    mean_reward          | 204          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2938528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035672607 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.7         |\n",
      "|    n_updates            | 19230        |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 73.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1435    |\n",
      "|    time_elapsed    | 3351    |\n",
      "|    total_timesteps | 2938880 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1436        |\n",
      "|    time_elapsed         | 3354        |\n",
      "|    total_timesteps      | 2940928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015980063 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.92        |\n",
      "|    n_updates            | 19240       |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1437         |\n",
      "|    time_elapsed         | 3356         |\n",
      "|    total_timesteps      | 2942976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055767493 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.583        |\n",
      "|    n_updates            | 19250        |\n",
      "|    policy_gradient_loss | 0.00195      |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1438         |\n",
      "|    time_elapsed         | 3358         |\n",
      "|    total_timesteps      | 2945024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037040953 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 19260        |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 46.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1439         |\n",
      "|    time_elapsed         | 3360         |\n",
      "|    total_timesteps      | 2947072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077760527 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.61         |\n",
      "|    n_updates            | 19270        |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    value_loss           | 25.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2948528, episode_reward=227.80 +/- 46.11\n",
      "Episode length: 335.00 +/- 67.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 335         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2948528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010596628 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.13        |\n",
      "|    n_updates            | 19280       |\n",
      "|    policy_gradient_loss | -0.00234    |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1440    |\n",
      "|    time_elapsed    | 3363    |\n",
      "|    total_timesteps | 2949120 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1441         |\n",
      "|    time_elapsed         | 3365         |\n",
      "|    total_timesteps      | 2951168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063828425 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.99         |\n",
      "|    n_updates            | 19290        |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1442         |\n",
      "|    time_elapsed         | 3367         |\n",
      "|    total_timesteps      | 2953216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073969085 |\n",
      "|    clip_fraction        | 0.0847       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.343       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.72         |\n",
      "|    n_updates            | 19300        |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 22.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1443         |\n",
      "|    time_elapsed         | 3369         |\n",
      "|    total_timesteps      | 2955264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018943162 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.731        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.55         |\n",
      "|    n_updates            | 19310        |\n",
      "|    policy_gradient_loss | -0.000411    |\n",
      "|    value_loss           | 97.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1444         |\n",
      "|    time_elapsed         | 3371         |\n",
      "|    total_timesteps      | 2957312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020566855 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58           |\n",
      "|    n_updates            | 19320        |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 203          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2958528, episode_reward=187.08 +/- 49.72\n",
      "Episode length: 463.80 +/- 281.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 464          |\n",
      "|    mean_reward          | 187          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2958528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026958922 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.717        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.8         |\n",
      "|    n_updates            | 19330        |\n",
      "|    policy_gradient_loss | -0.000716    |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1445    |\n",
      "|    time_elapsed    | 3375    |\n",
      "|    total_timesteps | 2959360 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1446        |\n",
      "|    time_elapsed         | 3377        |\n",
      "|    total_timesteps      | 2961408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003535947 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 19340       |\n",
      "|    policy_gradient_loss | -0.000466   |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1447        |\n",
      "|    time_elapsed         | 3379        |\n",
      "|    total_timesteps      | 2963456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004820548 |\n",
      "|    clip_fraction        | 0.0491      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 19350       |\n",
      "|    policy_gradient_loss | -0.000232   |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1448         |\n",
      "|    time_elapsed         | 3381         |\n",
      "|    total_timesteps      | 2965504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054042283 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.46         |\n",
      "|    n_updates            | 19360        |\n",
      "|    policy_gradient_loss | -0.00034     |\n",
      "|    value_loss           | 66.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1449        |\n",
      "|    time_elapsed         | 3383        |\n",
      "|    total_timesteps      | 2967552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005152514 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 19370       |\n",
      "|    policy_gradient_loss | -0.000814   |\n",
      "|    value_loss           | 40.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2968528, episode_reward=194.01 +/- 32.94\n",
      "Episode length: 555.40 +/- 307.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 555         |\n",
      "|    mean_reward          | 194         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2968528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003744747 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 19380       |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 90.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1450    |\n",
      "|    time_elapsed    | 3387    |\n",
      "|    total_timesteps | 2969600 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1451       |\n",
      "|    time_elapsed         | 3389       |\n",
      "|    total_timesteps      | 2971648    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00527056 |\n",
      "|    clip_fraction        | 0.0612     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.192     |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.39       |\n",
      "|    n_updates            | 19390      |\n",
      "|    policy_gradient_loss | -0.00348   |\n",
      "|    value_loss           | 34.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1452         |\n",
      "|    time_elapsed         | 3390         |\n",
      "|    total_timesteps      | 2973696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025884483 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.6         |\n",
      "|    n_updates            | 19400        |\n",
      "|    policy_gradient_loss | -0.00072     |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1453         |\n",
      "|    time_elapsed         | 3393         |\n",
      "|    total_timesteps      | 2975744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025842264 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.15        |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75         |\n",
      "|    n_updates            | 19410        |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1454        |\n",
      "|    time_elapsed         | 3394        |\n",
      "|    total_timesteps      | 2977792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005586449 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.3         |\n",
      "|    n_updates            | 19420       |\n",
      "|    policy_gradient_loss | 0.00082     |\n",
      "|    value_loss           | 14.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2978528, episode_reward=212.66 +/- 56.42\n",
      "Episode length: 439.80 +/- 280.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 440         |\n",
      "|    mean_reward          | 213         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2978528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005332769 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.188      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 19430       |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1455    |\n",
      "|    time_elapsed    | 3399    |\n",
      "|    total_timesteps | 2979840 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1456        |\n",
      "|    time_elapsed         | 3401        |\n",
      "|    total_timesteps      | 2981888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009323913 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 19440       |\n",
      "|    policy_gradient_loss | -0.000958   |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1457         |\n",
      "|    time_elapsed         | 3403         |\n",
      "|    total_timesteps      | 2983936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031326725 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.286       |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 102          |\n",
      "|    n_updates            | 19450        |\n",
      "|    policy_gradient_loss | -0.000607    |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1458         |\n",
      "|    time_elapsed         | 3405         |\n",
      "|    total_timesteps      | 2985984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031677298 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 19460        |\n",
      "|    policy_gradient_loss | 0.00139      |\n",
      "|    value_loss           | 42.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1459        |\n",
      "|    time_elapsed         | 3407        |\n",
      "|    total_timesteps      | 2988032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010204973 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.4         |\n",
      "|    n_updates            | 19470       |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 9.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2988528, episode_reward=158.81 +/- 106.89\n",
      "Episode length: 331.20 +/- 104.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 331         |\n",
      "|    mean_reward          | 159         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2988528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004293698 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 19480       |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 79.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1460    |\n",
      "|    time_elapsed    | 3410    |\n",
      "|    total_timesteps | 2990080 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1461        |\n",
      "|    time_elapsed         | 3412        |\n",
      "|    total_timesteps      | 2992128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002278428 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.192      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.98        |\n",
      "|    n_updates            | 19490       |\n",
      "|    policy_gradient_loss | 0.00047     |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1462         |\n",
      "|    time_elapsed         | 3414         |\n",
      "|    total_timesteps      | 2994176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045581707 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.14         |\n",
      "|    n_updates            | 19500        |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1463        |\n",
      "|    time_elapsed         | 3417        |\n",
      "|    total_timesteps      | 2996224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002481504 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.23        |\n",
      "|    n_updates            | 19510       |\n",
      "|    policy_gradient_loss | -0.000587   |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1464         |\n",
      "|    time_elapsed         | 3419         |\n",
      "|    total_timesteps      | 2998272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023591395 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.266       |\n",
      "|    explained_variance   | 0.583        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11         |\n",
      "|    n_updates            | 19520        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2998528, episode_reward=219.38 +/- 21.02\n",
      "Episode length: 335.60 +/- 42.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | 219         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2998528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011728677 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.68        |\n",
      "|    n_updates            | 19530       |\n",
      "|    policy_gradient_loss | -5.07e-05   |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1465    |\n",
      "|    time_elapsed    | 3422    |\n",
      "|    total_timesteps | 3000320 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1466        |\n",
      "|    time_elapsed         | 3424        |\n",
      "|    total_timesteps      | 3002368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003921671 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.785       |\n",
      "|    n_updates            | 19540       |\n",
      "|    policy_gradient_loss | 0.000621    |\n",
      "|    value_loss           | 8.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1467        |\n",
      "|    time_elapsed         | 3426        |\n",
      "|    total_timesteps      | 3004416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011446184 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.158      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.45        |\n",
      "|    n_updates            | 19550       |\n",
      "|    policy_gradient_loss | -0.00364    |\n",
      "|    value_loss           | 50.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1468        |\n",
      "|    time_elapsed         | 3429        |\n",
      "|    total_timesteps      | 3006464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001729485 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 19560       |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 96.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1469         |\n",
      "|    time_elapsed         | 3431         |\n",
      "|    total_timesteps      | 3008512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057768156 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.48         |\n",
      "|    n_updates            | 19570        |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3008528, episode_reward=165.24 +/- 90.68\n",
      "Episode length: 441.00 +/- 282.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 441          |\n",
      "|    mean_reward          | 165          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3008528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043512005 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25         |\n",
      "|    n_updates            | 19580        |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 18.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1470    |\n",
      "|    time_elapsed    | 3435    |\n",
      "|    total_timesteps | 3010560 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1471         |\n",
      "|    time_elapsed         | 3437         |\n",
      "|    total_timesteps      | 3012608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032182876 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.27         |\n",
      "|    n_updates            | 19590        |\n",
      "|    policy_gradient_loss | -0.0002      |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1472        |\n",
      "|    time_elapsed         | 3440        |\n",
      "|    total_timesteps      | 3014656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010777647 |\n",
      "|    clip_fraction        | 0.0832      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.79        |\n",
      "|    n_updates            | 19600       |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1473        |\n",
      "|    time_elapsed         | 3442        |\n",
      "|    total_timesteps      | 3016704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009018767 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.755       |\n",
      "|    n_updates            | 19610       |\n",
      "|    policy_gradient_loss | 0.000223    |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3018528, episode_reward=197.27 +/- 67.22\n",
      "Episode length: 575.20 +/- 347.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 575          |\n",
      "|    mean_reward          | 197          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3018528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022985514 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.02         |\n",
      "|    n_updates            | 19620        |\n",
      "|    policy_gradient_loss | 0.000804     |\n",
      "|    value_loss           | 43           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1474    |\n",
      "|    time_elapsed    | 3446    |\n",
      "|    total_timesteps | 3018752 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1475         |\n",
      "|    time_elapsed         | 3448         |\n",
      "|    total_timesteps      | 3020800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059225652 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.62         |\n",
      "|    n_updates            | 19630        |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    value_loss           | 43.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1476        |\n",
      "|    time_elapsed         | 3450        |\n",
      "|    total_timesteps      | 3022848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004421684 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 19640       |\n",
      "|    policy_gradient_loss | -0.00299    |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 1477         |\n",
      "|    time_elapsed         | 3453         |\n",
      "|    total_timesteps      | 3024896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021160524 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 19650        |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1478        |\n",
      "|    time_elapsed         | 3455        |\n",
      "|    total_timesteps      | 3026944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012542827 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 19660       |\n",
      "|    policy_gradient_loss | -0.000351   |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3028528, episode_reward=195.83 +/- 83.67\n",
      "Episode length: 269.20 +/- 17.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | 196          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3028528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059481887 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.9          |\n",
      "|    n_updates            | 19670        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 71           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1479    |\n",
      "|    time_elapsed    | 3458    |\n",
      "|    total_timesteps | 3028992 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1480        |\n",
      "|    time_elapsed         | 3460        |\n",
      "|    total_timesteps      | 3031040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008457545 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.07        |\n",
      "|    n_updates            | 19680       |\n",
      "|    policy_gradient_loss | -0.000614   |\n",
      "|    value_loss           | 9.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1481        |\n",
      "|    time_elapsed         | 3462        |\n",
      "|    total_timesteps      | 3033088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002412992 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 19690       |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 80.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1482        |\n",
      "|    time_elapsed         | 3464        |\n",
      "|    total_timesteps      | 3035136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012110713 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 19700       |\n",
      "|    policy_gradient_loss | 0.000254    |\n",
      "|    value_loss           | 55.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1483        |\n",
      "|    time_elapsed         | 3466        |\n",
      "|    total_timesteps      | 3037184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011273453 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.27        |\n",
      "|    n_updates            | 19710       |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3038528, episode_reward=234.82 +/- 15.46\n",
      "Episode length: 319.40 +/- 44.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 319          |\n",
      "|    mean_reward          | 235          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3038528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065919645 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.68         |\n",
      "|    n_updates            | 19720        |\n",
      "|    policy_gradient_loss | -0.000955    |\n",
      "|    value_loss           | 33.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1484    |\n",
      "|    time_elapsed    | 3469    |\n",
      "|    total_timesteps | 3039232 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1485        |\n",
      "|    time_elapsed         | 3471        |\n",
      "|    total_timesteps      | 3041280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008473421 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.64        |\n",
      "|    n_updates            | 19730       |\n",
      "|    policy_gradient_loss | -0.000151   |\n",
      "|    value_loss           | 40.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1486        |\n",
      "|    time_elapsed         | 3473        |\n",
      "|    total_timesteps      | 3043328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002606682 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.65        |\n",
      "|    n_updates            | 19740       |\n",
      "|    policy_gradient_loss | 0.000639    |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1487        |\n",
      "|    time_elapsed         | 3476        |\n",
      "|    total_timesteps      | 3045376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002642617 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.158      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.03        |\n",
      "|    n_updates            | 19750       |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    value_loss           | 50.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1488         |\n",
      "|    time_elapsed         | 3478         |\n",
      "|    total_timesteps      | 3047424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036974722 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.479        |\n",
      "|    n_updates            | 19760        |\n",
      "|    policy_gradient_loss | 0.000662     |\n",
      "|    value_loss           | 7.58         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3048528, episode_reward=137.15 +/- 136.57\n",
      "Episode length: 257.20 +/- 30.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 257         |\n",
      "|    mean_reward          | 137         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3048528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003157239 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.92        |\n",
      "|    n_updates            | 19770       |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1489    |\n",
      "|    time_elapsed    | 3481    |\n",
      "|    total_timesteps | 3049472 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1490         |\n",
      "|    time_elapsed         | 3483         |\n",
      "|    total_timesteps      | 3051520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029753977 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.91         |\n",
      "|    n_updates            | 19780        |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 71.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1491         |\n",
      "|    time_elapsed         | 3485         |\n",
      "|    total_timesteps      | 3053568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053693857 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54           |\n",
      "|    n_updates            | 19790        |\n",
      "|    policy_gradient_loss | -0.00326     |\n",
      "|    value_loss           | 92           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1492        |\n",
      "|    time_elapsed         | 3487        |\n",
      "|    total_timesteps      | 3055616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007384303 |\n",
      "|    clip_fraction        | 0.0355      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.78        |\n",
      "|    n_updates            | 19800       |\n",
      "|    policy_gradient_loss | -0.000725   |\n",
      "|    value_loss           | 22.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1493        |\n",
      "|    time_elapsed         | 3489        |\n",
      "|    total_timesteps      | 3057664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011560325 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.5         |\n",
      "|    n_updates            | 19810       |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 17.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3058528, episode_reward=88.65 +/- 122.69\n",
      "Episode length: 267.60 +/- 33.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 268         |\n",
      "|    mean_reward          | 88.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3058528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001638998 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 19820       |\n",
      "|    policy_gradient_loss | 0.00131     |\n",
      "|    value_loss           | 25.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1494    |\n",
      "|    time_elapsed    | 3492    |\n",
      "|    total_timesteps | 3059712 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1495       |\n",
      "|    time_elapsed         | 3494       |\n",
      "|    total_timesteps      | 3061760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01082458 |\n",
      "|    clip_fraction        | 0.0373     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.185     |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.41       |\n",
      "|    n_updates            | 19830      |\n",
      "|    policy_gradient_loss | -0.000873  |\n",
      "|    value_loss           | 85         |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1496         |\n",
      "|    time_elapsed         | 3497         |\n",
      "|    total_timesteps      | 3063808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066564847 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.824        |\n",
      "|    n_updates            | 19840        |\n",
      "|    policy_gradient_loss | 0.000211     |\n",
      "|    value_loss           | 9.16         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1497        |\n",
      "|    time_elapsed         | 3499        |\n",
      "|    total_timesteps      | 3065856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004025054 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.29        |\n",
      "|    n_updates            | 19850       |\n",
      "|    policy_gradient_loss | -0.000724   |\n",
      "|    value_loss           | 22.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1498        |\n",
      "|    time_elapsed         | 3502        |\n",
      "|    total_timesteps      | 3067904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490985 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 19860       |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 30.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3068528, episode_reward=175.16 +/- 110.04\n",
      "Episode length: 281.80 +/- 28.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 175         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3068528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005165483 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 19870       |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 31.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1499    |\n",
      "|    time_elapsed    | 3504    |\n",
      "|    total_timesteps | 3069952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1500        |\n",
      "|    time_elapsed         | 3507        |\n",
      "|    total_timesteps      | 3072000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006883066 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 19880       |\n",
      "|    policy_gradient_loss | -0.000717   |\n",
      "|    value_loss           | 31.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1501       |\n",
      "|    time_elapsed         | 3509       |\n",
      "|    total_timesteps      | 3074048    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00972253 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.375     |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.14       |\n",
      "|    n_updates            | 19890      |\n",
      "|    policy_gradient_loss | -0.00115   |\n",
      "|    value_loss           | 15.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1502        |\n",
      "|    time_elapsed         | 3511        |\n",
      "|    total_timesteps      | 3076096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005870762 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 19900       |\n",
      "|    policy_gradient_loss | 0.00121     |\n",
      "|    value_loss           | 25.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1503         |\n",
      "|    time_elapsed         | 3513         |\n",
      "|    total_timesteps      | 3078144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042450866 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.71         |\n",
      "|    n_updates            | 19910        |\n",
      "|    policy_gradient_loss | -0.000526    |\n",
      "|    value_loss           | 17.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3078528, episode_reward=223.50 +/- 25.12\n",
      "Episode length: 328.60 +/- 35.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | 223         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3078528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004296612 |\n",
      "|    clip_fraction        | 0.0392      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.2        |\n",
      "|    n_updates            | 19920       |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1504    |\n",
      "|    time_elapsed    | 3516    |\n",
      "|    total_timesteps | 3080192 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1505         |\n",
      "|    time_elapsed         | 3518         |\n",
      "|    total_timesteps      | 3082240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039661527 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.4         |\n",
      "|    n_updates            | 19930        |\n",
      "|    policy_gradient_loss | -0.000435    |\n",
      "|    value_loss           | 60.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 1506        |\n",
      "|    time_elapsed         | 3520        |\n",
      "|    total_timesteps      | 3084288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003986786 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.85        |\n",
      "|    n_updates            | 19940       |\n",
      "|    policy_gradient_loss | -0.00092    |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1507       |\n",
      "|    time_elapsed         | 3522       |\n",
      "|    total_timesteps      | 3086336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542899 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.7       |\n",
      "|    n_updates            | 19950      |\n",
      "|    policy_gradient_loss | 0.000602   |\n",
      "|    value_loss           | 53.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1508        |\n",
      "|    time_elapsed         | 3525        |\n",
      "|    total_timesteps      | 3088384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010189426 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.18        |\n",
      "|    n_updates            | 19960       |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 27.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3088528, episode_reward=228.64 +/- 24.79\n",
      "Episode length: 341.20 +/- 40.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 341         |\n",
      "|    mean_reward          | 229         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3088528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011080619 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 19970       |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 60.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1509    |\n",
      "|    time_elapsed    | 3527    |\n",
      "|    total_timesteps | 3090432 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1510         |\n",
      "|    time_elapsed         | 3529         |\n",
      "|    total_timesteps      | 3092480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037132876 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 19980        |\n",
      "|    policy_gradient_loss | 0.00118      |\n",
      "|    value_loss           | 36.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1511        |\n",
      "|    time_elapsed         | 3531        |\n",
      "|    total_timesteps      | 3094528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001319 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.43        |\n",
      "|    n_updates            | 19990       |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1512         |\n",
      "|    time_elapsed         | 3533         |\n",
      "|    total_timesteps      | 3096576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077200136 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 20000        |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 45.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3098528, episode_reward=62.07 +/- 106.95\n",
      "Episode length: 267.00 +/- 68.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 267          |\n",
      "|    mean_reward          | 62.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3098528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041891346 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.5         |\n",
      "|    n_updates            | 20010        |\n",
      "|    policy_gradient_loss | -0.000668    |\n",
      "|    value_loss           | 31.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1513    |\n",
      "|    time_elapsed    | 3536    |\n",
      "|    total_timesteps | 3098624 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1514         |\n",
      "|    time_elapsed         | 3538         |\n",
      "|    total_timesteps      | 3100672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043633566 |\n",
      "|    clip_fraction        | 0.044        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 20020        |\n",
      "|    policy_gradient_loss | -0.00288     |\n",
      "|    value_loss           | 78.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1515        |\n",
      "|    time_elapsed         | 3539        |\n",
      "|    total_timesteps      | 3102720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010128321 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 20030       |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1516         |\n",
      "|    time_elapsed         | 3541         |\n",
      "|    total_timesteps      | 3104768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034758495 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.163       |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 56.2         |\n",
      "|    n_updates            | 20040        |\n",
      "|    policy_gradient_loss | -0.000642    |\n",
      "|    value_loss           | 67.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1517        |\n",
      "|    time_elapsed         | 3543        |\n",
      "|    total_timesteps      | 3106816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009092948 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 20050       |\n",
      "|    policy_gradient_loss | 0.000338    |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3108528, episode_reward=205.30 +/- 21.33\n",
      "Episode length: 324.80 +/- 43.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 205          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3108528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034478432 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.162       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.87         |\n",
      "|    n_updates            | 20060        |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    value_loss           | 48.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1518    |\n",
      "|    time_elapsed    | 3546    |\n",
      "|    total_timesteps | 3108864 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1519         |\n",
      "|    time_elapsed         | 3548         |\n",
      "|    total_timesteps      | 3110912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039529437 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.4          |\n",
      "|    n_updates            | 20070        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 73           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1520         |\n",
      "|    time_elapsed         | 3550         |\n",
      "|    total_timesteps      | 3112960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106503125 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.27         |\n",
      "|    n_updates            | 20080        |\n",
      "|    policy_gradient_loss | -0.000192    |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1521        |\n",
      "|    time_elapsed         | 3552        |\n",
      "|    total_timesteps      | 3115008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012088468 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.05        |\n",
      "|    n_updates            | 20090       |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1522         |\n",
      "|    time_elapsed         | 3555         |\n",
      "|    total_timesteps      | 3117056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122292545 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 20100        |\n",
      "|    policy_gradient_loss | -0.000928    |\n",
      "|    value_loss           | 9.33         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3118528, episode_reward=166.96 +/- 86.36\n",
      "Episode length: 427.40 +/- 287.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 427         |\n",
      "|    mean_reward          | 167         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3118528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021170344 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 20110       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1523    |\n",
      "|    time_elapsed    | 3558    |\n",
      "|    total_timesteps | 3119104 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1524         |\n",
      "|    time_elapsed         | 3560         |\n",
      "|    total_timesteps      | 3121152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049944497 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.198       |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.1         |\n",
      "|    n_updates            | 20120        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 58           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1525       |\n",
      "|    time_elapsed         | 3562       |\n",
      "|    total_timesteps      | 3123200    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00345224 |\n",
      "|    clip_fraction        | 0.0294     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.278     |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.44       |\n",
      "|    n_updates            | 20130      |\n",
      "|    policy_gradient_loss | -0.000488  |\n",
      "|    value_loss           | 40.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1526       |\n",
      "|    time_elapsed         | 3565       |\n",
      "|    total_timesteps      | 3125248    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01461871 |\n",
      "|    clip_fraction        | 0.048      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.92       |\n",
      "|    n_updates            | 20140      |\n",
      "|    policy_gradient_loss | -0.000144  |\n",
      "|    value_loss           | 11.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1527        |\n",
      "|    time_elapsed         | 3567        |\n",
      "|    total_timesteps      | 3127296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009196197 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 20150       |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3128528, episode_reward=215.73 +/- 22.10\n",
      "Episode length: 354.60 +/- 68.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | 216          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3128528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045128074 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.806        |\n",
      "|    n_updates            | 20160        |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 12.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1528    |\n",
      "|    time_elapsed    | 3570    |\n",
      "|    total_timesteps | 3129344 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1529        |\n",
      "|    time_elapsed         | 3572        |\n",
      "|    total_timesteps      | 3131392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002671259 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.62        |\n",
      "|    n_updates            | 20170       |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 73.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1530        |\n",
      "|    time_elapsed         | 3574        |\n",
      "|    total_timesteps      | 3133440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001712203 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.24        |\n",
      "|    n_updates            | 20180       |\n",
      "|    policy_gradient_loss | 0.000305    |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1531         |\n",
      "|    time_elapsed         | 3576         |\n",
      "|    total_timesteps      | 3135488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039249156 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 20190        |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 35.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1532         |\n",
      "|    time_elapsed         | 3578         |\n",
      "|    total_timesteps      | 3137536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014741565 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.93         |\n",
      "|    n_updates            | 20200        |\n",
      "|    policy_gradient_loss | -2.59e-05    |\n",
      "|    value_loss           | 48.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3138528, episode_reward=129.14 +/- 126.30\n",
      "Episode length: 304.20 +/- 72.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | 129         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3138528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004323992 |\n",
      "|    clip_fraction        | 0.0421      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.15       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3           |\n",
      "|    n_updates            | 20210       |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1533    |\n",
      "|    time_elapsed    | 3581    |\n",
      "|    total_timesteps | 3139584 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1534        |\n",
      "|    time_elapsed         | 3584        |\n",
      "|    total_timesteps      | 3141632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005037041 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.44        |\n",
      "|    n_updates            | 20220       |\n",
      "|    policy_gradient_loss | 0.00024     |\n",
      "|    value_loss           | 22.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1535        |\n",
      "|    time_elapsed         | 3586        |\n",
      "|    total_timesteps      | 3143680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005801663 |\n",
      "|    clip_fraction        | 0.0721      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.96        |\n",
      "|    n_updates            | 20230       |\n",
      "|    policy_gradient_loss | -0.000112   |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1536        |\n",
      "|    time_elapsed         | 3588        |\n",
      "|    total_timesteps      | 3145728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009332721 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 20240       |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1537         |\n",
      "|    time_elapsed         | 3590         |\n",
      "|    total_timesteps      | 3147776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026055365 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.272       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.22         |\n",
      "|    n_updates            | 20250        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 23.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3148528, episode_reward=236.22 +/- 21.56\n",
      "Episode length: 333.00 +/- 31.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | 236         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3148528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006466196 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.42        |\n",
      "|    n_updates            | 20260       |\n",
      "|    policy_gradient_loss | 0.00119     |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1538    |\n",
      "|    time_elapsed    | 3593    |\n",
      "|    total_timesteps | 3149824 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1539         |\n",
      "|    time_elapsed         | 3596         |\n",
      "|    total_timesteps      | 3151872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069038095 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.707        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.12         |\n",
      "|    n_updates            | 20270        |\n",
      "|    policy_gradient_loss | 9.72e-05     |\n",
      "|    value_loss           | 32.3         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 876       |\n",
      "|    iterations           | 1540      |\n",
      "|    time_elapsed         | 3598      |\n",
      "|    total_timesteps      | 3153920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0049051 |\n",
      "|    clip_fraction        | 0.0284    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.235    |\n",
      "|    explained_variance   | 0.693     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 20.3      |\n",
      "|    n_updates            | 20280     |\n",
      "|    policy_gradient_loss | -3.81e-05 |\n",
      "|    value_loss           | 53.3      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1541        |\n",
      "|    time_elapsed         | 3600        |\n",
      "|    total_timesteps      | 3155968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008055725 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.87        |\n",
      "|    n_updates            | 20290       |\n",
      "|    policy_gradient_loss | -0.000947   |\n",
      "|    value_loss           | 50.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1542         |\n",
      "|    time_elapsed         | 3602         |\n",
      "|    total_timesteps      | 3158016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062002605 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.153       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.9         |\n",
      "|    n_updates            | 20300        |\n",
      "|    policy_gradient_loss | -0.00425     |\n",
      "|    value_loss           | 97.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3158528, episode_reward=144.67 +/- 116.84\n",
      "Episode length: 242.80 +/- 47.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 243          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3158528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040035425 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 84.1         |\n",
      "|    n_updates            | 20310        |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1543    |\n",
      "|    time_elapsed    | 3605    |\n",
      "|    total_timesteps | 3160064 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1544         |\n",
      "|    time_elapsed         | 3607         |\n",
      "|    total_timesteps      | 3162112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016632637 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.603        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.1         |\n",
      "|    n_updates            | 20320        |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 195          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1545        |\n",
      "|    time_elapsed         | 3609        |\n",
      "|    total_timesteps      | 3164160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008912113 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 20330       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1546         |\n",
      "|    time_elapsed         | 3611         |\n",
      "|    total_timesteps      | 3166208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081705395 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 20340        |\n",
      "|    policy_gradient_loss | -0.000887    |\n",
      "|    value_loss           | 69.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1547         |\n",
      "|    time_elapsed         | 3613         |\n",
      "|    total_timesteps      | 3168256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050348556 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 20350        |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 77.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3168528, episode_reward=112.50 +/- 92.91\n",
      "Episode length: 335.60 +/- 53.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | 113          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3168528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048984005 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.31        |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.65         |\n",
      "|    n_updates            | 20360        |\n",
      "|    policy_gradient_loss | 0.000731     |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1548    |\n",
      "|    time_elapsed    | 3616    |\n",
      "|    total_timesteps | 3170304 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1549        |\n",
      "|    time_elapsed         | 3618        |\n",
      "|    total_timesteps      | 3172352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005888335 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 20370       |\n",
      "|    policy_gradient_loss | -0.000842   |\n",
      "|    value_loss           | 50          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1550         |\n",
      "|    time_elapsed         | 3620         |\n",
      "|    total_timesteps      | 3174400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039713997 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 20380        |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1551         |\n",
      "|    time_elapsed         | 3622         |\n",
      "|    total_timesteps      | 3176448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050706137 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.3         |\n",
      "|    n_updates            | 20390        |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1552        |\n",
      "|    time_elapsed         | 3624        |\n",
      "|    total_timesteps      | 3178496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007406976 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 20400       |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 94.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3178528, episode_reward=63.89 +/- 103.91\n",
      "Episode length: 381.60 +/- 311.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 382          |\n",
      "|    mean_reward          | 63.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3178528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021406754 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.589        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 20410        |\n",
      "|    policy_gradient_loss | -0.000279    |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1553    |\n",
      "|    time_elapsed    | 3627    |\n",
      "|    total_timesteps | 3180544 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1554         |\n",
      "|    time_elapsed         | 3629         |\n",
      "|    total_timesteps      | 3182592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069289003 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23         |\n",
      "|    n_updates            | 20420        |\n",
      "|    policy_gradient_loss | 0.00251      |\n",
      "|    value_loss           | 42.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1555        |\n",
      "|    time_elapsed         | 3632        |\n",
      "|    total_timesteps      | 3184640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007690727 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 20430       |\n",
      "|    policy_gradient_loss | -0.00016    |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1556         |\n",
      "|    time_elapsed         | 3633         |\n",
      "|    total_timesteps      | 3186688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030051535 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.85         |\n",
      "|    n_updates            | 20440        |\n",
      "|    policy_gradient_loss | 0.000608     |\n",
      "|    value_loss           | 31.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3188528, episode_reward=177.72 +/- 106.70\n",
      "Episode length: 309.60 +/- 38.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 310         |\n",
      "|    mean_reward          | 178         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3188528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004889857 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 20450       |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 52          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1557    |\n",
      "|    time_elapsed    | 3636    |\n",
      "|    total_timesteps | 3188736 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1558        |\n",
      "|    time_elapsed         | 3638        |\n",
      "|    total_timesteps      | 3190784     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009771549 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.72        |\n",
      "|    n_updates            | 20460       |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 95.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1559        |\n",
      "|    time_elapsed         | 3640        |\n",
      "|    total_timesteps      | 3192832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008744841 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.3        |\n",
      "|    n_updates            | 20470       |\n",
      "|    policy_gradient_loss | 0.000112    |\n",
      "|    value_loss           | 63          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1560         |\n",
      "|    time_elapsed         | 3642         |\n",
      "|    total_timesteps      | 3194880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027596448 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.68         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.9         |\n",
      "|    n_updates            | 20480        |\n",
      "|    policy_gradient_loss | 0.000383     |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1561        |\n",
      "|    time_elapsed         | 3644        |\n",
      "|    total_timesteps      | 3196928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008616926 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.66        |\n",
      "|    n_updates            | 20490       |\n",
      "|    policy_gradient_loss | 0.000687    |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3198528, episode_reward=166.43 +/- 107.85\n",
      "Episode length: 434.40 +/- 283.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 434         |\n",
      "|    mean_reward          | 166         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3198528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007332611 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.9        |\n",
      "|    n_updates            | 20500       |\n",
      "|    policy_gradient_loss | -0.000283   |\n",
      "|    value_loss           | 95.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1562    |\n",
      "|    time_elapsed    | 3647    |\n",
      "|    total_timesteps | 3198976 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1563        |\n",
      "|    time_elapsed         | 3650        |\n",
      "|    total_timesteps      | 3201024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007085597 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 20510       |\n",
      "|    policy_gradient_loss | -0.000737   |\n",
      "|    value_loss           | 32          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1564        |\n",
      "|    time_elapsed         | 3652        |\n",
      "|    total_timesteps      | 3203072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008313218 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.667       |\n",
      "|    n_updates            | 20520       |\n",
      "|    policy_gradient_loss | 0.00453     |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1565         |\n",
      "|    time_elapsed         | 3653         |\n",
      "|    total_timesteps      | 3205120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019182127 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.739        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.9         |\n",
      "|    n_updates            | 20530        |\n",
      "|    policy_gradient_loss | -0.000287    |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1566         |\n",
      "|    time_elapsed         | 3655         |\n",
      "|    total_timesteps      | 3207168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020973869 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.6         |\n",
      "|    n_updates            | 20540        |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3208528, episode_reward=192.13 +/- 107.14\n",
      "Episode length: 277.60 +/- 23.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 278          |\n",
      "|    mean_reward          | 192          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3208528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039721765 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.5         |\n",
      "|    n_updates            | 20550        |\n",
      "|    policy_gradient_loss | -0.000699    |\n",
      "|    value_loss           | 51.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1567    |\n",
      "|    time_elapsed    | 3658    |\n",
      "|    total_timesteps | 3209216 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 877        |\n",
      "|    iterations           | 1568       |\n",
      "|    time_elapsed         | 3660       |\n",
      "|    total_timesteps      | 3211264    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02344605 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.292     |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.2       |\n",
      "|    n_updates            | 20560      |\n",
      "|    policy_gradient_loss | -0.00301   |\n",
      "|    value_loss           | 53.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1569        |\n",
      "|    time_elapsed         | 3662        |\n",
      "|    total_timesteps      | 3213312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007266146 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 20570       |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 42.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1570         |\n",
      "|    time_elapsed         | 3665         |\n",
      "|    total_timesteps      | 3215360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035174368 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.174       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.1         |\n",
      "|    n_updates            | 20580        |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 81.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1571        |\n",
      "|    time_elapsed         | 3666        |\n",
      "|    total_timesteps      | 3217408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008851263 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.14        |\n",
      "|    n_updates            | 20590       |\n",
      "|    policy_gradient_loss | 0.000108    |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3218528, episode_reward=172.96 +/- 89.41\n",
      "Episode length: 341.80 +/- 64.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | 173          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3218528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033969036 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.4         |\n",
      "|    n_updates            | 20600        |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1572    |\n",
      "|    time_elapsed    | 3670    |\n",
      "|    total_timesteps | 3219456 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1573        |\n",
      "|    time_elapsed         | 3672        |\n",
      "|    total_timesteps      | 3221504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003443584 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.94        |\n",
      "|    n_updates            | 20610       |\n",
      "|    policy_gradient_loss | 5.1e-05     |\n",
      "|    value_loss           | 33.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1574        |\n",
      "|    time_elapsed         | 3674        |\n",
      "|    total_timesteps      | 3223552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008481195 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 20620       |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1575         |\n",
      "|    time_elapsed         | 3677         |\n",
      "|    total_timesteps      | 3225600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054333974 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.322       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36         |\n",
      "|    n_updates            | 20630        |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 20.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1576         |\n",
      "|    time_elapsed         | 3679         |\n",
      "|    total_timesteps      | 3227648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065860073 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.601        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35         |\n",
      "|    n_updates            | 20640        |\n",
      "|    policy_gradient_loss | 0.000348     |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3228528, episode_reward=208.58 +/- 53.43\n",
      "Episode length: 437.80 +/- 281.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 438         |\n",
      "|    mean_reward          | 209         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3228528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005374761 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.243      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.49        |\n",
      "|    n_updates            | 20650       |\n",
      "|    policy_gradient_loss | 0.000215    |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1577    |\n",
      "|    time_elapsed    | 3683    |\n",
      "|    total_timesteps | 3229696 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1578         |\n",
      "|    time_elapsed         | 3685         |\n",
      "|    total_timesteps      | 3231744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035205004 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 20660        |\n",
      "|    policy_gradient_loss | 0.00119      |\n",
      "|    value_loss           | 10.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1579        |\n",
      "|    time_elapsed         | 3687        |\n",
      "|    total_timesteps      | 3233792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789234 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 20670       |\n",
      "|    policy_gradient_loss | -0.000993   |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1580        |\n",
      "|    time_elapsed         | 3689        |\n",
      "|    total_timesteps      | 3235840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009528031 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.24        |\n",
      "|    n_updates            | 20680       |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 27.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1581        |\n",
      "|    time_elapsed         | 3691        |\n",
      "|    total_timesteps      | 3237888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004158863 |\n",
      "|    clip_fraction        | 0.0236      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.17        |\n",
      "|    n_updates            | 20690       |\n",
      "|    policy_gradient_loss | -7.52e-05   |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3238528, episode_reward=237.60 +/- 23.21\n",
      "Episode length: 309.40 +/- 15.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 309         |\n",
      "|    mean_reward          | 238         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3238528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004712097 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 20700       |\n",
      "|    policy_gradient_loss | -0.00074    |\n",
      "|    value_loss           | 24.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1582    |\n",
      "|    time_elapsed    | 3694    |\n",
      "|    total_timesteps | 3239936 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1583        |\n",
      "|    time_elapsed         | 3697        |\n",
      "|    total_timesteps      | 3241984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003916076 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 20710       |\n",
      "|    policy_gradient_loss | -0.000913   |\n",
      "|    value_loss           | 83.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1584        |\n",
      "|    time_elapsed         | 3699        |\n",
      "|    total_timesteps      | 3244032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009596867 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92        |\n",
      "|    n_updates            | 20720       |\n",
      "|    policy_gradient_loss | 0.000413    |\n",
      "|    value_loss           | 6.52        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1585         |\n",
      "|    time_elapsed         | 3701         |\n",
      "|    total_timesteps      | 3246080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063119885 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.33        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89         |\n",
      "|    n_updates            | 20730        |\n",
      "|    policy_gradient_loss | 0.000442     |\n",
      "|    value_loss           | 15.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1586         |\n",
      "|    time_elapsed         | 3703         |\n",
      "|    total_timesteps      | 3248128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035382418 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.4         |\n",
      "|    n_updates            | 20740        |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 75.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3248528, episode_reward=219.19 +/- 55.28\n",
      "Episode length: 444.60 +/- 278.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 445          |\n",
      "|    mean_reward          | 219          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3248528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040700277 |\n",
      "|    clip_fraction        | 0.0716       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.63         |\n",
      "|    n_updates            | 20750        |\n",
      "|    policy_gradient_loss | -0.000528    |\n",
      "|    value_loss           | 14.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1587    |\n",
      "|    time_elapsed    | 3707    |\n",
      "|    total_timesteps | 3250176 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1588         |\n",
      "|    time_elapsed         | 3709         |\n",
      "|    total_timesteps      | 3252224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032005464 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.662        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 20760        |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1589         |\n",
      "|    time_elapsed         | 3711         |\n",
      "|    total_timesteps      | 3254272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065407585 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 20770        |\n",
      "|    policy_gradient_loss | -0.000416    |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1590         |\n",
      "|    time_elapsed         | 3714         |\n",
      "|    total_timesteps      | 3256320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044395374 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73         |\n",
      "|    n_updates            | 20780        |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 13           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1591        |\n",
      "|    time_elapsed         | 3716        |\n",
      "|    total_timesteps      | 3258368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008549904 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.982       |\n",
      "|    n_updates            | 20790       |\n",
      "|    policy_gradient_loss | 0.00105     |\n",
      "|    value_loss           | 9.65        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3258528, episode_reward=119.23 +/- 113.84\n",
      "Episode length: 436.20 +/- 291.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 436         |\n",
      "|    mean_reward          | 119         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3258528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008330624 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44        |\n",
      "|    n_updates            | 20800       |\n",
      "|    policy_gradient_loss | -4.16e-05   |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1592    |\n",
      "|    time_elapsed    | 3720    |\n",
      "|    total_timesteps | 3260416 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1593         |\n",
      "|    time_elapsed         | 3722         |\n",
      "|    total_timesteps      | 3262464      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143937655 |\n",
      "|    clip_fraction        | 0.0782       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.273       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 20810        |\n",
      "|    policy_gradient_loss | -0.000631    |\n",
      "|    value_loss           | 54.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1594        |\n",
      "|    time_elapsed         | 3724        |\n",
      "|    total_timesteps      | 3264512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100075 |\n",
      "|    clip_fraction        | 0.0504      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.13        |\n",
      "|    n_updates            | 20820       |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1595        |\n",
      "|    time_elapsed         | 3726        |\n",
      "|    total_timesteps      | 3266560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009053519 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 20830       |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    value_loss           | 20.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3268528, episode_reward=174.46 +/- 93.91\n",
      "Episode length: 317.60 +/- 63.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | 174          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3268528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070991023 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 82           |\n",
      "|    n_updates            | 20840        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1596    |\n",
      "|    time_elapsed    | 3730    |\n",
      "|    total_timesteps | 3268608 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1597        |\n",
      "|    time_elapsed         | 3731        |\n",
      "|    total_timesteps      | 3270656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005756044 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.332       |\n",
      "|    n_updates            | 20850       |\n",
      "|    policy_gradient_loss | -0.000443   |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1598         |\n",
      "|    time_elapsed         | 3734         |\n",
      "|    total_timesteps      | 3272704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044543445 |\n",
      "|    clip_fraction        | 0.0496       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.54         |\n",
      "|    n_updates            | 20860        |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 31.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1599         |\n",
      "|    time_elapsed         | 3736         |\n",
      "|    total_timesteps      | 3274752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057007843 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 20870        |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 31.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1600         |\n",
      "|    time_elapsed         | 3738         |\n",
      "|    total_timesteps      | 3276800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060980897 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.344       |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.9         |\n",
      "|    n_updates            | 20880        |\n",
      "|    policy_gradient_loss | 0.000186     |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3278528, episode_reward=197.62 +/- 71.38\n",
      "Episode length: 578.60 +/- 344.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 579          |\n",
      "|    mean_reward          | 198          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3278528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034574685 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.2         |\n",
      "|    n_updates            | 20890        |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 875     |\n",
      "|    iterations      | 1601    |\n",
      "|    time_elapsed    | 3743    |\n",
      "|    total_timesteps | 3278848 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1602        |\n",
      "|    time_elapsed         | 3744        |\n",
      "|    total_timesteps      | 3280896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015924815 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.421       |\n",
      "|    n_updates            | 20900       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 2.1         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1603         |\n",
      "|    time_elapsed         | 3746         |\n",
      "|    total_timesteps      | 3282944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026240093 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.9         |\n",
      "|    n_updates            | 20910        |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 92.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1604         |\n",
      "|    time_elapsed         | 3749         |\n",
      "|    total_timesteps      | 3284992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046876073 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 20920        |\n",
      "|    policy_gradient_loss | -0.00067     |\n",
      "|    value_loss           | 85           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1605         |\n",
      "|    time_elapsed         | 3751         |\n",
      "|    total_timesteps      | 3287040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046043606 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93         |\n",
      "|    n_updates            | 20930        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3288528, episode_reward=226.32 +/- 21.92\n",
      "Episode length: 297.20 +/- 20.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3288528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004495715 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.336      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.33        |\n",
      "|    n_updates            | 20940       |\n",
      "|    policy_gradient_loss | -0.000396   |\n",
      "|    value_loss           | 27.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1606    |\n",
      "|    time_elapsed    | 3753    |\n",
      "|    total_timesteps | 3289088 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1607         |\n",
      "|    time_elapsed         | 3755         |\n",
      "|    total_timesteps      | 3291136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026725829 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.6         |\n",
      "|    n_updates            | 20950        |\n",
      "|    policy_gradient_loss | 9.47e-05     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1608         |\n",
      "|    time_elapsed         | 3757         |\n",
      "|    total_timesteps      | 3293184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032776957 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.76         |\n",
      "|    n_updates            | 20960        |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1609        |\n",
      "|    time_elapsed         | 3759        |\n",
      "|    total_timesteps      | 3295232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012121811 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 20970       |\n",
      "|    policy_gradient_loss | -0.0007     |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1610        |\n",
      "|    time_elapsed         | 3761        |\n",
      "|    total_timesteps      | 3297280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014619661 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 20980       |\n",
      "|    policy_gradient_loss | -0.000822   |\n",
      "|    value_loss           | 36.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3298528, episode_reward=226.34 +/- 15.12\n",
      "Episode length: 307.60 +/- 30.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3298528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004480987 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20990       |\n",
      "|    policy_gradient_loss | -0.000411   |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1611    |\n",
      "|    time_elapsed    | 3764    |\n",
      "|    total_timesteps | 3299328 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1612         |\n",
      "|    time_elapsed         | 3766         |\n",
      "|    total_timesteps      | 3301376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064019486 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 21000        |\n",
      "|    policy_gradient_loss | 0.00014      |\n",
      "|    value_loss           | 25.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1613         |\n",
      "|    time_elapsed         | 3769         |\n",
      "|    total_timesteps      | 3303424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026861224 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 21010        |\n",
      "|    policy_gradient_loss | 0.000258     |\n",
      "|    value_loss           | 37.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1614         |\n",
      "|    time_elapsed         | 3770         |\n",
      "|    total_timesteps      | 3305472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063405973 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.352       |\n",
      "|    explained_variance   | 0.615        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 21020        |\n",
      "|    policy_gradient_loss | -0.000563    |\n",
      "|    value_loss           | 85           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1615         |\n",
      "|    time_elapsed         | 3772         |\n",
      "|    total_timesteps      | 3307520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036983355 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.35         |\n",
      "|    n_updates            | 21030        |\n",
      "|    policy_gradient_loss | -0.000682    |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3308528, episode_reward=166.13 +/- 73.29\n",
      "Episode length: 327.80 +/- 62.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 328         |\n",
      "|    mean_reward          | 166         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3308528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008659171 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 21040       |\n",
      "|    policy_gradient_loss | -0.000179   |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1616    |\n",
      "|    time_elapsed    | 3775    |\n",
      "|    total_timesteps | 3309568 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1617         |\n",
      "|    time_elapsed         | 3777         |\n",
      "|    total_timesteps      | 3311616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055958503 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.9         |\n",
      "|    n_updates            | 21050        |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 99           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1618        |\n",
      "|    time_elapsed         | 3780        |\n",
      "|    total_timesteps      | 3313664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006247737 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 21060       |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1619         |\n",
      "|    time_elapsed         | 3782         |\n",
      "|    total_timesteps      | 3315712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079679135 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 21070        |\n",
      "|    policy_gradient_loss | 0.00361      |\n",
      "|    value_loss           | 5.26         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1620         |\n",
      "|    time_elapsed         | 3784         |\n",
      "|    total_timesteps      | 3317760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034737373 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 21080        |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 45.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3318528, episode_reward=188.38 +/- 105.54\n",
      "Episode length: 292.40 +/- 39.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 292        |\n",
      "|    mean_reward          | 188        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3318528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00811669 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.344     |\n",
      "|    explained_variance   | 0.815      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.82       |\n",
      "|    n_updates            | 21090      |\n",
      "|    policy_gradient_loss | -0.000607  |\n",
      "|    value_loss           | 81.1       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1621    |\n",
      "|    time_elapsed    | 3787    |\n",
      "|    total_timesteps | 3319808 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1622        |\n",
      "|    time_elapsed         | 3789        |\n",
      "|    total_timesteps      | 3321856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005392613 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.98        |\n",
      "|    n_updates            | 21100       |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1623         |\n",
      "|    time_elapsed         | 3791         |\n",
      "|    total_timesteps      | 3323904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059955707 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.51         |\n",
      "|    n_updates            | 21110        |\n",
      "|    policy_gradient_loss | -0.000731    |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1624        |\n",
      "|    time_elapsed         | 3793        |\n",
      "|    total_timesteps      | 3325952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005796704 |\n",
      "|    clip_fraction        | 0.0368      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 21120       |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 56.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1625         |\n",
      "|    time_elapsed         | 3795         |\n",
      "|    total_timesteps      | 3328000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030906687 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 21130        |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3328528, episode_reward=91.05 +/- 85.05\n",
      "Episode length: 282.80 +/- 75.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 91           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3328528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074654035 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 21140        |\n",
      "|    policy_gradient_loss | -6.65e-05    |\n",
      "|    value_loss           | 24.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1626    |\n",
      "|    time_elapsed    | 3798    |\n",
      "|    total_timesteps | 3330048 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1627         |\n",
      "|    time_elapsed         | 3800         |\n",
      "|    total_timesteps      | 3332096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034939828 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 21150        |\n",
      "|    policy_gradient_loss | -0.000235    |\n",
      "|    value_loss           | 71.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1628         |\n",
      "|    time_elapsed         | 3802         |\n",
      "|    total_timesteps      | 3334144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033575185 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.81         |\n",
      "|    n_updates            | 21160        |\n",
      "|    policy_gradient_loss | -0.000962    |\n",
      "|    value_loss           | 77           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1629        |\n",
      "|    time_elapsed         | 3804        |\n",
      "|    total_timesteps      | 3336192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012487334 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.119      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.6        |\n",
      "|    n_updates            | 21170       |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1630         |\n",
      "|    time_elapsed         | 3806         |\n",
      "|    total_timesteps      | 3338240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032018344 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26           |\n",
      "|    n_updates            | 21180        |\n",
      "|    policy_gradient_loss | -0.000395    |\n",
      "|    value_loss           | 80.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3338528, episode_reward=191.14 +/- 62.78\n",
      "Episode length: 572.00 +/- 349.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 572         |\n",
      "|    mean_reward          | 191         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3338528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005067985 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 21190       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1631    |\n",
      "|    time_elapsed    | 3809    |\n",
      "|    total_timesteps | 3340288 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1632        |\n",
      "|    time_elapsed         | 3811        |\n",
      "|    total_timesteps      | 3342336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010205904 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.1        |\n",
      "|    n_updates            | 21200       |\n",
      "|    policy_gradient_loss | -0.000626   |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1633        |\n",
      "|    time_elapsed         | 3814        |\n",
      "|    total_timesteps      | 3344384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006262811 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 185         |\n",
      "|    n_updates            | 21210       |\n",
      "|    policy_gradient_loss | -0.000365   |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1634        |\n",
      "|    time_elapsed         | 3816        |\n",
      "|    total_timesteps      | 3346432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010696596 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 21220       |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1635        |\n",
      "|    time_elapsed         | 3818        |\n",
      "|    total_timesteps      | 3348480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012064628 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 21230       |\n",
      "|    policy_gradient_loss | 0.00177     |\n",
      "|    value_loss           | 32.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3348528, episode_reward=238.83 +/- 23.43\n",
      "Episode length: 413.40 +/- 228.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 413         |\n",
      "|    mean_reward          | 239         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3348528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007453076 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.134      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.9        |\n",
      "|    n_updates            | 21240       |\n",
      "|    policy_gradient_loss | 0.0016      |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1636    |\n",
      "|    time_elapsed    | 3821    |\n",
      "|    total_timesteps | 3350528 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1637         |\n",
      "|    time_elapsed         | 3823         |\n",
      "|    total_timesteps      | 3352576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028138938 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.96         |\n",
      "|    n_updates            | 21250        |\n",
      "|    policy_gradient_loss | 0.000682     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1638         |\n",
      "|    time_elapsed         | 3825         |\n",
      "|    total_timesteps      | 3354624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031130612 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 21260        |\n",
      "|    policy_gradient_loss | -0.000598    |\n",
      "|    value_loss           | 92.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 876        |\n",
      "|    iterations           | 1639       |\n",
      "|    time_elapsed         | 3828       |\n",
      "|    total_timesteps      | 3356672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01513092 |\n",
      "|    clip_fraction        | 0.0924     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.313     |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.1       |\n",
      "|    n_updates            | 21270      |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    value_loss           | 61.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3358528, episode_reward=179.77 +/- 90.40\n",
      "Episode length: 303.80 +/- 53.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | 180         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3358528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014092895 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 21280       |\n",
      "|    policy_gradient_loss | -0.000942   |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1640    |\n",
      "|    time_elapsed    | 3831    |\n",
      "|    total_timesteps | 3358720 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1641        |\n",
      "|    time_elapsed         | 3833        |\n",
      "|    total_timesteps      | 3360768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005057311 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 21290       |\n",
      "|    policy_gradient_loss | -0.000231   |\n",
      "|    value_loss           | 64.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1642        |\n",
      "|    time_elapsed         | 3835        |\n",
      "|    total_timesteps      | 3362816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002392086 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.3        |\n",
      "|    n_updates            | 21300       |\n",
      "|    policy_gradient_loss | -0.000891   |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1643         |\n",
      "|    time_elapsed         | 3836         |\n",
      "|    total_timesteps      | 3364864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032575359 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.7         |\n",
      "|    n_updates            | 21310        |\n",
      "|    policy_gradient_loss | -0.000776    |\n",
      "|    value_loss           | 94.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1644        |\n",
      "|    time_elapsed         | 3839        |\n",
      "|    total_timesteps      | 3366912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005450937 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 21320       |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3368528, episode_reward=98.12 +/- 127.82\n",
      "Episode length: 256.80 +/- 40.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 257         |\n",
      "|    mean_reward          | 98.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3368528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012728075 |\n",
      "|    clip_fraction        | 0.0621      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.71        |\n",
      "|    n_updates            | 21330       |\n",
      "|    policy_gradient_loss | 0.00396     |\n",
      "|    value_loss           | 28.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1645    |\n",
      "|    time_elapsed    | 3841    |\n",
      "|    total_timesteps | 3368960 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 1646         |\n",
      "|    time_elapsed         | 3843         |\n",
      "|    total_timesteps      | 3371008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021892847 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | 0.731        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.42         |\n",
      "|    n_updates            | 21340        |\n",
      "|    policy_gradient_loss | 2.99e-05     |\n",
      "|    value_loss           | 76.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1647        |\n",
      "|    time_elapsed         | 3845        |\n",
      "|    total_timesteps      | 3373056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009435589 |\n",
      "|    clip_fraction        | 0.0815      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 21350       |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    value_loss           | 34.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1648        |\n",
      "|    time_elapsed         | 3847        |\n",
      "|    total_timesteps      | 3375104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017142761 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 21360       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 96.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1649         |\n",
      "|    time_elapsed         | 3849         |\n",
      "|    total_timesteps      | 3377152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029625162 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.188       |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.1         |\n",
      "|    n_updates            | 21370        |\n",
      "|    policy_gradient_loss | -0.000305    |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3378528, episode_reward=209.20 +/- 58.07\n",
      "Episode length: 454.60 +/- 273.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 455          |\n",
      "|    mean_reward          | 209          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3378528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095043965 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 21380        |\n",
      "|    policy_gradient_loss | -0.00268     |\n",
      "|    value_loss           | 84           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1650    |\n",
      "|    time_elapsed    | 3853    |\n",
      "|    total_timesteps | 3379200 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 1651        |\n",
      "|    time_elapsed         | 3855        |\n",
      "|    total_timesteps      | 3381248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010117115 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.99        |\n",
      "|    n_updates            | 21390       |\n",
      "|    policy_gradient_loss | 1.94e-05    |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 877        |\n",
      "|    iterations           | 1652       |\n",
      "|    time_elapsed         | 3857       |\n",
      "|    total_timesteps      | 3383296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01775708 |\n",
      "|    clip_fraction        | 0.0735     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.306     |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20         |\n",
      "|    n_updates            | 21400      |\n",
      "|    policy_gradient_loss | -0.00346   |\n",
      "|    value_loss           | 44.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1653        |\n",
      "|    time_elapsed         | 3859        |\n",
      "|    total_timesteps      | 3385344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014827023 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 21410       |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 69.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1654         |\n",
      "|    time_elapsed         | 3861         |\n",
      "|    total_timesteps      | 3387392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066770045 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.552        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 21420        |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    value_loss           | 62.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3388528, episode_reward=162.31 +/- 85.28\n",
      "Episode length: 298.40 +/- 37.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 298         |\n",
      "|    mean_reward          | 162         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3388528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013205985 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.2        |\n",
      "|    n_updates            | 21430       |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1655    |\n",
      "|    time_elapsed    | 3864    |\n",
      "|    total_timesteps | 3389440 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1656        |\n",
      "|    time_elapsed         | 3866        |\n",
      "|    total_timesteps      | 3391488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004453101 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.5         |\n",
      "|    n_updates            | 21440       |\n",
      "|    policy_gradient_loss | -0.000408   |\n",
      "|    value_loss           | 27.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1657        |\n",
      "|    time_elapsed         | 3868        |\n",
      "|    total_timesteps      | 3393536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008513559 |\n",
      "|    clip_fraction        | 0.0602      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.21        |\n",
      "|    n_updates            | 21450       |\n",
      "|    policy_gradient_loss | 0.00232     |\n",
      "|    value_loss           | 7.25        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1658         |\n",
      "|    time_elapsed         | 3870         |\n",
      "|    total_timesteps      | 3395584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017450326 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.7         |\n",
      "|    n_updates            | 21460        |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 75.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1659         |\n",
      "|    time_elapsed         | 3873         |\n",
      "|    total_timesteps      | 3397632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027335724 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 21470        |\n",
      "|    policy_gradient_loss | 6.74e-05     |\n",
      "|    value_loss           | 48.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3398528, episode_reward=147.86 +/- 94.12\n",
      "Episode length: 560.80 +/- 351.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 561          |\n",
      "|    mean_reward          | 148          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3398528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067512174 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.287       |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.04         |\n",
      "|    n_updates            | 21480        |\n",
      "|    policy_gradient_loss | -0.00232     |\n",
      "|    value_loss           | 99           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 876     |\n",
      "|    iterations      | 1660    |\n",
      "|    time_elapsed    | 3876    |\n",
      "|    total_timesteps | 3399680 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1661         |\n",
      "|    time_elapsed         | 3878         |\n",
      "|    total_timesteps      | 3401728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024345806 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.697        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.6         |\n",
      "|    n_updates            | 21490        |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 81.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1662         |\n",
      "|    time_elapsed         | 3880         |\n",
      "|    total_timesteps      | 3403776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022777468 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 21500        |\n",
      "|    policy_gradient_loss | -0.000124    |\n",
      "|    value_loss           | 80.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1663         |\n",
      "|    time_elapsed         | 3882         |\n",
      "|    total_timesteps      | 3405824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048823226 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.3         |\n",
      "|    n_updates            | 21510        |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1664        |\n",
      "|    time_elapsed         | 3884        |\n",
      "|    total_timesteps      | 3407872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004184429 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 21520       |\n",
      "|    policy_gradient_loss | -0.000545   |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3408528, episode_reward=218.62 +/- 22.59\n",
      "Episode length: 368.80 +/- 100.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | 219          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3408528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023136782 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.535        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 92.7         |\n",
      "|    n_updates            | 21530        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1665    |\n",
      "|    time_elapsed    | 3887    |\n",
      "|    total_timesteps | 3409920 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1666        |\n",
      "|    time_elapsed         | 3889        |\n",
      "|    total_timesteps      | 3411968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009438994 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.93        |\n",
      "|    n_updates            | 21540       |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1667         |\n",
      "|    time_elapsed         | 3891         |\n",
      "|    total_timesteps      | 3414016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056048343 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.1         |\n",
      "|    n_updates            | 21550        |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 94.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1668        |\n",
      "|    time_elapsed         | 3893        |\n",
      "|    total_timesteps      | 3416064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008102473 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.4        |\n",
      "|    n_updates            | 21560       |\n",
      "|    policy_gradient_loss | 0.00237     |\n",
      "|    value_loss           | 72.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1669        |\n",
      "|    time_elapsed         | 3895        |\n",
      "|    total_timesteps      | 3418112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006088447 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.4         |\n",
      "|    n_updates            | 21570       |\n",
      "|    policy_gradient_loss | 6.06e-05    |\n",
      "|    value_loss           | 63          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3418528, episode_reward=175.98 +/- 79.04\n",
      "Episode length: 299.80 +/- 45.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 300         |\n",
      "|    mean_reward          | 176         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3418528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004696863 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 21580       |\n",
      "|    policy_gradient_loss | 8.46e-05    |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1670    |\n",
      "|    time_elapsed    | 3898    |\n",
      "|    total_timesteps | 3420160 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 877        |\n",
      "|    iterations           | 1671       |\n",
      "|    time_elapsed         | 3900       |\n",
      "|    total_timesteps      | 3422208    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00411601 |\n",
      "|    clip_fraction        | 0.0368     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 42.8       |\n",
      "|    n_updates            | 21590      |\n",
      "|    policy_gradient_loss | -0.00236   |\n",
      "|    value_loss           | 148        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1672        |\n",
      "|    time_elapsed         | 3902        |\n",
      "|    total_timesteps      | 3424256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008484142 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.148      |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 21600       |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1673        |\n",
      "|    time_elapsed         | 3904        |\n",
      "|    total_timesteps      | 3426304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008336991 |\n",
      "|    clip_fraction        | 0.0603      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 21610       |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 148         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1674        |\n",
      "|    time_elapsed         | 3906        |\n",
      "|    total_timesteps      | 3428352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024237797 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 21620       |\n",
      "|    policy_gradient_loss | 0.000334    |\n",
      "|    value_loss           | 36.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3428528, episode_reward=159.00 +/- 87.13\n",
      "Episode length: 341.40 +/- 70.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | 159          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3428528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039751767 |\n",
      "|    clip_fraction        | 0.0346       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 21630        |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 55.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1675    |\n",
      "|    time_elapsed    | 3909    |\n",
      "|    total_timesteps | 3430400 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1676         |\n",
      "|    time_elapsed         | 3911         |\n",
      "|    total_timesteps      | 3432448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067110746 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.172       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.91         |\n",
      "|    n_updates            | 21640        |\n",
      "|    policy_gradient_loss | 0.000726     |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1677        |\n",
      "|    time_elapsed         | 3913        |\n",
      "|    total_timesteps      | 3434496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006305797 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.264      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.14        |\n",
      "|    n_updates            | 21650       |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    value_loss           | 44.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1678        |\n",
      "|    time_elapsed         | 3915        |\n",
      "|    total_timesteps      | 3436544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004186586 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 21660       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3438528, episode_reward=139.01 +/- 131.96\n",
      "Episode length: 273.00 +/- 45.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 273          |\n",
      "|    mean_reward          | 139          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3438528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061771963 |\n",
      "|    clip_fraction        | 0.0569       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 21670        |\n",
      "|    policy_gradient_loss | 0.000571     |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1679    |\n",
      "|    time_elapsed    | 3918    |\n",
      "|    total_timesteps | 3438592 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1680        |\n",
      "|    time_elapsed         | 3920        |\n",
      "|    total_timesteps      | 3440640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002529041 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.6        |\n",
      "|    n_updates            | 21680       |\n",
      "|    policy_gradient_loss | -0.000704   |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1681         |\n",
      "|    time_elapsed         | 3922         |\n",
      "|    total_timesteps      | 3442688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056934794 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.02         |\n",
      "|    n_updates            | 21690        |\n",
      "|    policy_gradient_loss | -0.000129    |\n",
      "|    value_loss           | 30.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1682         |\n",
      "|    time_elapsed         | 3924         |\n",
      "|    total_timesteps      | 3444736      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041118674 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.711        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 21700        |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    value_loss           | 186          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1683         |\n",
      "|    time_elapsed         | 3926         |\n",
      "|    total_timesteps      | 3446784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055957166 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 99           |\n",
      "|    n_updates            | 21710        |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3448528, episode_reward=170.83 +/- 88.18\n",
      "Episode length: 293.40 +/- 28.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 293          |\n",
      "|    mean_reward          | 171          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3448528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048510227 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.1         |\n",
      "|    n_updates            | 21720        |\n",
      "|    policy_gradient_loss | -0.00096     |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1684    |\n",
      "|    time_elapsed    | 3930    |\n",
      "|    total_timesteps | 3448832 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1685         |\n",
      "|    time_elapsed         | 3932         |\n",
      "|    total_timesteps      | 3450880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040216087 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.6         |\n",
      "|    n_updates            | 21730        |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1686        |\n",
      "|    time_elapsed         | 3934        |\n",
      "|    total_timesteps      | 3452928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004112307 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.2        |\n",
      "|    n_updates            | 21740       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 134         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1687        |\n",
      "|    time_elapsed         | 3936        |\n",
      "|    total_timesteps      | 3454976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006716195 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 21750       |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 877        |\n",
      "|    iterations           | 1688       |\n",
      "|    time_elapsed         | 3938       |\n",
      "|    total_timesteps      | 3457024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01082783 |\n",
      "|    clip_fraction        | 0.0533     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.14      |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.5       |\n",
      "|    n_updates            | 21760      |\n",
      "|    policy_gradient_loss | 5e-05      |\n",
      "|    value_loss           | 66.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3458528, episode_reward=201.85 +/- 40.96\n",
      "Episode length: 439.40 +/- 280.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 439         |\n",
      "|    mean_reward          | 202         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3458528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014526011 |\n",
      "|    clip_fraction        | 0.0529      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.4         |\n",
      "|    n_updates            | 21770       |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1689    |\n",
      "|    time_elapsed    | 3942    |\n",
      "|    total_timesteps | 3459072 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1690        |\n",
      "|    time_elapsed         | 3944        |\n",
      "|    total_timesteps      | 3461120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004772025 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 21780       |\n",
      "|    policy_gradient_loss | -0.000235   |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1691         |\n",
      "|    time_elapsed         | 3946         |\n",
      "|    total_timesteps      | 3463168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053310823 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.307       |\n",
      "|    explained_variance   | 0.742        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 21790        |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    value_loss           | 33.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1692         |\n",
      "|    time_elapsed         | 3948         |\n",
      "|    total_timesteps      | 3465216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046711997 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.275       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.12         |\n",
      "|    n_updates            | 21800        |\n",
      "|    policy_gradient_loss | 0.0011       |\n",
      "|    value_loss           | 31.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1693         |\n",
      "|    time_elapsed         | 3951         |\n",
      "|    total_timesteps      | 3467264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077693462 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41         |\n",
      "|    n_updates            | 21810        |\n",
      "|    policy_gradient_loss | 9.51e-05     |\n",
      "|    value_loss           | 31.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3468528, episode_reward=196.12 +/- 60.63\n",
      "Episode length: 464.40 +/- 270.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 464          |\n",
      "|    mean_reward          | 196          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3468528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038617388 |\n",
      "|    clip_fraction        | 0.0549       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 125          |\n",
      "|    n_updates            | 21820        |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1694    |\n",
      "|    time_elapsed    | 3954    |\n",
      "|    total_timesteps | 3469312 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1695         |\n",
      "|    time_elapsed         | 3956         |\n",
      "|    total_timesteps      | 3471360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038057975 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.188       |\n",
      "|    explained_variance   | 0.554        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 21830        |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 160          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1696        |\n",
      "|    time_elapsed         | 3958        |\n",
      "|    total_timesteps      | 3473408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012321753 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 21840       |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    value_loss           | 49.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1697        |\n",
      "|    time_elapsed         | 3960        |\n",
      "|    total_timesteps      | 3475456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009449214 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 21850       |\n",
      "|    policy_gradient_loss | -0.00048    |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1698         |\n",
      "|    time_elapsed         | 3962         |\n",
      "|    total_timesteps      | 3477504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062442664 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.1         |\n",
      "|    n_updates            | 21860        |\n",
      "|    policy_gradient_loss | -0.000835    |\n",
      "|    value_loss           | 72.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3478528, episode_reward=225.56 +/- 16.32\n",
      "Episode length: 305.20 +/- 34.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 305         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3478528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002139713 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 21870       |\n",
      "|    policy_gradient_loss | 5.23e-05    |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1699    |\n",
      "|    time_elapsed    | 3965    |\n",
      "|    total_timesteps | 3479552 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1700        |\n",
      "|    time_elapsed         | 3967        |\n",
      "|    total_timesteps      | 3481600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010916684 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 21880       |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1701         |\n",
      "|    time_elapsed         | 3969         |\n",
      "|    total_timesteps      | 3483648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038021426 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 72.2         |\n",
      "|    n_updates            | 21890        |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    value_loss           | 88.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1702        |\n",
      "|    time_elapsed         | 3971        |\n",
      "|    total_timesteps      | 3485696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007994264 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.91        |\n",
      "|    n_updates            | 21900       |\n",
      "|    policy_gradient_loss | -8.42e-05   |\n",
      "|    value_loss           | 85.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1703        |\n",
      "|    time_elapsed         | 3973        |\n",
      "|    total_timesteps      | 3487744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006527652 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 21910       |\n",
      "|    policy_gradient_loss | 0.00252     |\n",
      "|    value_loss           | 46.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3488528, episode_reward=140.40 +/- 113.48\n",
      "Episode length: 284.60 +/- 51.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | 140         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3488528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004261894 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 21920       |\n",
      "|    policy_gradient_loss | -0.000313   |\n",
      "|    value_loss           | 54.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1704    |\n",
      "|    time_elapsed    | 3976    |\n",
      "|    total_timesteps | 3489792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1705        |\n",
      "|    time_elapsed         | 3978        |\n",
      "|    total_timesteps      | 3491840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002354511 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 21930       |\n",
      "|    policy_gradient_loss | -0.000803   |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1706        |\n",
      "|    time_elapsed         | 3980        |\n",
      "|    total_timesteps      | 3493888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003689414 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.3        |\n",
      "|    n_updates            | 21940       |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1707        |\n",
      "|    time_elapsed         | 3982        |\n",
      "|    total_timesteps      | 3495936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011929033 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 21950       |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1708        |\n",
      "|    time_elapsed         | 3984        |\n",
      "|    total_timesteps      | 3497984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002540131 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.5        |\n",
      "|    n_updates            | 21960       |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 92.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3498528, episode_reward=150.83 +/- 98.19\n",
      "Episode length: 448.80 +/- 276.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 449          |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3498528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075458237 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 21970        |\n",
      "|    policy_gradient_loss | -0.000685    |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1709    |\n",
      "|    time_elapsed    | 3987    |\n",
      "|    total_timesteps | 3500032 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1710         |\n",
      "|    time_elapsed         | 3989         |\n",
      "|    total_timesteps      | 3502080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076027396 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 21980        |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 60.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1711         |\n",
      "|    time_elapsed         | 3991         |\n",
      "|    total_timesteps      | 3504128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020452067 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26           |\n",
      "|    n_updates            | 21990        |\n",
      "|    policy_gradient_loss | -0.000645    |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 1712         |\n",
      "|    time_elapsed         | 3993         |\n",
      "|    total_timesteps      | 3506176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058712214 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 22000        |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1713        |\n",
      "|    time_elapsed         | 3995        |\n",
      "|    total_timesteps      | 3508224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005656598 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 22010       |\n",
      "|    policy_gradient_loss | 0.000646    |\n",
      "|    value_loss           | 71.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3508528, episode_reward=175.69 +/- 102.45\n",
      "Episode length: 279.40 +/- 29.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 279         |\n",
      "|    mean_reward          | 176         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3508528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004799729 |\n",
      "|    clip_fraction        | 0.0413      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.7        |\n",
      "|    n_updates            | 22020       |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1714    |\n",
      "|    time_elapsed    | 3998    |\n",
      "|    total_timesteps | 3510272 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 877         |\n",
      "|    iterations           | 1715        |\n",
      "|    time_elapsed         | 4000        |\n",
      "|    total_timesteps      | 3512320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005864015 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.81        |\n",
      "|    n_updates            | 22030       |\n",
      "|    policy_gradient_loss | -0.000708   |\n",
      "|    value_loss           | 31          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1716        |\n",
      "|    time_elapsed         | 4002        |\n",
      "|    total_timesteps      | 3514368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002944584 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 22040       |\n",
      "|    policy_gradient_loss | 0.000142    |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1717         |\n",
      "|    time_elapsed         | 4004         |\n",
      "|    total_timesteps      | 3516416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056023486 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.4         |\n",
      "|    n_updates            | 22050        |\n",
      "|    policy_gradient_loss | 0.000308     |\n",
      "|    value_loss           | 88.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1718        |\n",
      "|    time_elapsed         | 4006        |\n",
      "|    total_timesteps      | 3518464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007267972 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 22060       |\n",
      "|    policy_gradient_loss | 0.000878    |\n",
      "|    value_loss           | 63.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3518528, episode_reward=197.61 +/- 90.55\n",
      "Episode length: 303.40 +/- 49.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | 198         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3518528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010237842 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.04        |\n",
      "|    n_updates            | 22070       |\n",
      "|    policy_gradient_loss | 0.00051     |\n",
      "|    value_loss           | 43.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 877     |\n",
      "|    iterations      | 1719    |\n",
      "|    time_elapsed    | 4009    |\n",
      "|    total_timesteps | 3520512 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1720        |\n",
      "|    time_elapsed         | 4011        |\n",
      "|    total_timesteps      | 3522560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006901131 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.02        |\n",
      "|    n_updates            | 22080       |\n",
      "|    policy_gradient_loss | -0.000491   |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1721         |\n",
      "|    time_elapsed         | 4013         |\n",
      "|    total_timesteps      | 3524608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077584973 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.174       |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 22090        |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 67.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1722        |\n",
      "|    time_elapsed         | 4015        |\n",
      "|    total_timesteps      | 3526656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011613963 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 22100       |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3528528, episode_reward=239.06 +/- 17.86\n",
      "Episode length: 294.80 +/- 33.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | 239         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3528528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011061735 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.227      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 22110       |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1723    |\n",
      "|    time_elapsed    | 4018    |\n",
      "|    total_timesteps | 3528704 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1724         |\n",
      "|    time_elapsed         | 4020         |\n",
      "|    total_timesteps      | 3530752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097651165 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.15        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.65         |\n",
      "|    n_updates            | 22120        |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 33.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1725         |\n",
      "|    time_elapsed         | 4022         |\n",
      "|    total_timesteps      | 3532800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027011305 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.9         |\n",
      "|    n_updates            | 22130        |\n",
      "|    policy_gradient_loss | -0.000849    |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1726         |\n",
      "|    time_elapsed         | 4024         |\n",
      "|    total_timesteps      | 3534848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038327076 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 22140        |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 77.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1727        |\n",
      "|    time_elapsed         | 4026        |\n",
      "|    total_timesteps      | 3536896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009525727 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 198         |\n",
      "|    n_updates            | 22150       |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3538528, episode_reward=140.14 +/- 110.53\n",
      "Episode length: 329.60 +/- 118.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 140         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3538528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004207152 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.1        |\n",
      "|    n_updates            | 22160       |\n",
      "|    policy_gradient_loss | -0.000482   |\n",
      "|    value_loss           | 147         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1728    |\n",
      "|    time_elapsed    | 4029    |\n",
      "|    total_timesteps | 3538944 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1729         |\n",
      "|    time_elapsed         | 4031         |\n",
      "|    total_timesteps      | 3540992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054124556 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 22170        |\n",
      "|    policy_gradient_loss | -0.000853    |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1730         |\n",
      "|    time_elapsed         | 4033         |\n",
      "|    total_timesteps      | 3543040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058269417 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.762        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.69         |\n",
      "|    n_updates            | 22180        |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1731        |\n",
      "|    time_elapsed         | 4035        |\n",
      "|    total_timesteps      | 3545088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006043595 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 22190       |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 95.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 878        |\n",
      "|    iterations           | 1732       |\n",
      "|    time_elapsed         | 4036       |\n",
      "|    total_timesteps      | 3547136    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00907125 |\n",
      "|    clip_fraction        | 0.055      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.129     |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.6       |\n",
      "|    n_updates            | 22200      |\n",
      "|    policy_gradient_loss | -0.00262   |\n",
      "|    value_loss           | 79.9       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3548528, episode_reward=188.11 +/- 91.74\n",
      "Episode length: 298.60 +/- 62.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3548528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047006346 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 22210        |\n",
      "|    policy_gradient_loss | -6.86e-05    |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1733    |\n",
      "|    time_elapsed    | 4039    |\n",
      "|    total_timesteps | 3549184 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1734         |\n",
      "|    time_elapsed         | 4042         |\n",
      "|    total_timesteps      | 3551232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088261645 |\n",
      "|    clip_fraction        | 0.0879       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.39         |\n",
      "|    n_updates            | 22220        |\n",
      "|    policy_gradient_loss | 0.000338     |\n",
      "|    value_loss           | 18.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1735        |\n",
      "|    time_elapsed         | 4044        |\n",
      "|    total_timesteps      | 3553280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008495273 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.363       |\n",
      "|    n_updates            | 22230       |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1736         |\n",
      "|    time_elapsed         | 4046         |\n",
      "|    total_timesteps      | 3555328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036932714 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41         |\n",
      "|    n_updates            | 22240        |\n",
      "|    policy_gradient_loss | 0.000657     |\n",
      "|    value_loss           | 13.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1737         |\n",
      "|    time_elapsed         | 4048         |\n",
      "|    total_timesteps      | 3557376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051613343 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.4         |\n",
      "|    n_updates            | 22250        |\n",
      "|    policy_gradient_loss | -0.000657    |\n",
      "|    value_loss           | 64.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3558528, episode_reward=136.92 +/- 121.40\n",
      "Episode length: 248.60 +/- 51.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 137          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3558528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052006757 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.312       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.34         |\n",
      "|    n_updates            | 22260        |\n",
      "|    policy_gradient_loss | -0.00052     |\n",
      "|    value_loss           | 64.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1738    |\n",
      "|    time_elapsed    | 4051    |\n",
      "|    total_timesteps | 3559424 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1739         |\n",
      "|    time_elapsed         | 4053         |\n",
      "|    total_timesteps      | 3561472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050081657 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.11         |\n",
      "|    n_updates            | 22270        |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1740        |\n",
      "|    time_elapsed         | 4055        |\n",
      "|    total_timesteps      | 3563520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003771985 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.99        |\n",
      "|    n_updates            | 22280       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1741        |\n",
      "|    time_elapsed         | 4057        |\n",
      "|    total_timesteps      | 3565568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004592267 |\n",
      "|    clip_fraction        | 0.0377      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 22290       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 61          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1742        |\n",
      "|    time_elapsed         | 4059        |\n",
      "|    total_timesteps      | 3567616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007886916 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.08        |\n",
      "|    n_updates            | 22300       |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3568528, episode_reward=124.73 +/- 114.36\n",
      "Episode length: 270.00 +/- 79.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 125         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3568528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004947503 |\n",
      "|    clip_fraction        | 0.0544      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.59        |\n",
      "|    n_updates            | 22310       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 62.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1743    |\n",
      "|    time_elapsed    | 4062    |\n",
      "|    total_timesteps | 3569664 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1744         |\n",
      "|    time_elapsed         | 4064         |\n",
      "|    total_timesteps      | 3571712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032674507 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 22320        |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 69.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1745        |\n",
      "|    time_elapsed         | 4066        |\n",
      "|    total_timesteps      | 3573760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004031431 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.5        |\n",
      "|    n_updates            | 22330       |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1746        |\n",
      "|    time_elapsed         | 4068        |\n",
      "|    total_timesteps      | 3575808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003519605 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 22340       |\n",
      "|    policy_gradient_loss | -0.000196   |\n",
      "|    value_loss           | 81.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1747         |\n",
      "|    time_elapsed         | 4070         |\n",
      "|    total_timesteps      | 3577856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063361092 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.6         |\n",
      "|    n_updates            | 22350        |\n",
      "|    policy_gradient_loss | 0.000805     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3578528, episode_reward=180.95 +/- 43.15\n",
      "Episode length: 470.60 +/- 268.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 471          |\n",
      "|    mean_reward          | 181          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3578528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062489593 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.52         |\n",
      "|    n_updates            | 22360        |\n",
      "|    policy_gradient_loss | -0.000401    |\n",
      "|    value_loss           | 54.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1748    |\n",
      "|    time_elapsed    | 4074    |\n",
      "|    total_timesteps | 3579904 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1749        |\n",
      "|    time_elapsed         | 4076        |\n",
      "|    total_timesteps      | 3581952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007903664 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 22370       |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 28.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1750        |\n",
      "|    time_elapsed         | 4078        |\n",
      "|    total_timesteps      | 3584000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009385608 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 22380       |\n",
      "|    policy_gradient_loss | -0.000664   |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 878        |\n",
      "|    iterations           | 1751       |\n",
      "|    time_elapsed         | 4080       |\n",
      "|    total_timesteps      | 3586048    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00952473 |\n",
      "|    clip_fraction        | 0.0607     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.342     |\n",
      "|    explained_variance   | 0.744      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.6       |\n",
      "|    n_updates            | 22390      |\n",
      "|    policy_gradient_loss | 0.000425   |\n",
      "|    value_loss           | 21.6       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1752         |\n",
      "|    time_elapsed         | 4082         |\n",
      "|    total_timesteps      | 3588096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023852205 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.3         |\n",
      "|    n_updates            | 22400        |\n",
      "|    policy_gradient_loss | 0.000185     |\n",
      "|    value_loss           | 177          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3588528, episode_reward=122.14 +/- 114.36\n",
      "Episode length: 254.80 +/- 53.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 255          |\n",
      "|    mean_reward          | 122          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3588528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033853664 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 22410        |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 52.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1753    |\n",
      "|    time_elapsed    | 4085    |\n",
      "|    total_timesteps | 3590144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1754        |\n",
      "|    time_elapsed         | 4087        |\n",
      "|    total_timesteps      | 3592192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007070381 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.85        |\n",
      "|    n_updates            | 22420       |\n",
      "|    policy_gradient_loss | -0.000471   |\n",
      "|    value_loss           | 31.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1755        |\n",
      "|    time_elapsed         | 4089        |\n",
      "|    total_timesteps      | 3594240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007105306 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.297       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 136         |\n",
      "|    n_updates            | 22430       |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1756        |\n",
      "|    time_elapsed         | 4091        |\n",
      "|    total_timesteps      | 3596288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005153092 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 22440       |\n",
      "|    policy_gradient_loss | 0.000148    |\n",
      "|    value_loss           | 47.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 878        |\n",
      "|    iterations           | 1757       |\n",
      "|    time_elapsed         | 4093       |\n",
      "|    total_timesteps      | 3598336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01692074 |\n",
      "|    clip_fraction        | 0.0781     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.785      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.5       |\n",
      "|    n_updates            | 22450      |\n",
      "|    policy_gradient_loss | -0.00174   |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3598528, episode_reward=187.70 +/- 88.97\n",
      "Episode length: 290.40 +/- 34.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3598528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022982426 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 22460        |\n",
      "|    policy_gradient_loss | 0.0033       |\n",
      "|    value_loss           | 53.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1758    |\n",
      "|    time_elapsed    | 4096    |\n",
      "|    total_timesteps | 3600384 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 1759         |\n",
      "|    time_elapsed         | 4098         |\n",
      "|    total_timesteps      | 3602432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021499437 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 22470        |\n",
      "|    policy_gradient_loss | -0.000701    |\n",
      "|    value_loss           | 68.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 1760        |\n",
      "|    time_elapsed         | 4100        |\n",
      "|    total_timesteps      | 3604480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005775453 |\n",
      "|    clip_fraction        | 0.0754      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 22480       |\n",
      "|    policy_gradient_loss | -0.000133   |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1761         |\n",
      "|    time_elapsed         | 4102         |\n",
      "|    total_timesteps      | 3606528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058512525 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.15         |\n",
      "|    n_updates            | 22490        |\n",
      "|    policy_gradient_loss | 0.00021      |\n",
      "|    value_loss           | 39.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3608528, episode_reward=177.15 +/- 89.70\n",
      "Episode length: 295.00 +/- 52.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 295          |\n",
      "|    mean_reward          | 177          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3608528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035850536 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.319       |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.66         |\n",
      "|    n_updates            | 22500        |\n",
      "|    policy_gradient_loss | -0.000926    |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 878     |\n",
      "|    iterations      | 1762    |\n",
      "|    time_elapsed    | 4105    |\n",
      "|    total_timesteps | 3608576 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1763         |\n",
      "|    time_elapsed         | 4107         |\n",
      "|    total_timesteps      | 3610624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020727748 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.7         |\n",
      "|    n_updates            | 22510        |\n",
      "|    policy_gradient_loss | -0.000951    |\n",
      "|    value_loss           | 60.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1764         |\n",
      "|    time_elapsed         | 4109         |\n",
      "|    total_timesteps      | 3612672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034238428 |\n",
      "|    clip_fraction        | 0.063        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.313       |\n",
      "|    explained_variance   | 0.713        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 22520        |\n",
      "|    policy_gradient_loss | -0.000874    |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1765        |\n",
      "|    time_elapsed         | 4111        |\n",
      "|    total_timesteps      | 3614720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004637481 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 22530       |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 62.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1766         |\n",
      "|    time_elapsed         | 4112         |\n",
      "|    total_timesteps      | 3616768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053179855 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67.9         |\n",
      "|    n_updates            | 22540        |\n",
      "|    policy_gradient_loss | -0.000351    |\n",
      "|    value_loss           | 82.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3618528, episode_reward=155.58 +/- 93.23\n",
      "Episode length: 423.00 +/- 290.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 423          |\n",
      "|    mean_reward          | 156          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3618528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029728985 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.9         |\n",
      "|    n_updates            | 22550        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 195          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1767    |\n",
      "|    time_elapsed    | 4116    |\n",
      "|    total_timesteps | 3618816 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1768        |\n",
      "|    time_elapsed         | 4118        |\n",
      "|    total_timesteps      | 3620864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002116606 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.7        |\n",
      "|    n_updates            | 22560       |\n",
      "|    policy_gradient_loss | -0.000412   |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1769         |\n",
      "|    time_elapsed         | 4120         |\n",
      "|    total_timesteps      | 3622912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064335815 |\n",
      "|    clip_fraction        | 0.0653       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 22570        |\n",
      "|    policy_gradient_loss | -0.000619    |\n",
      "|    value_loss           | 143          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1770        |\n",
      "|    time_elapsed         | 4122        |\n",
      "|    total_timesteps      | 3624960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036505833 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.53        |\n",
      "|    n_updates            | 22580       |\n",
      "|    policy_gradient_loss | 0.000833    |\n",
      "|    value_loss           | 42.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1771         |\n",
      "|    time_elapsed         | 4124         |\n",
      "|    total_timesteps      | 3627008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035451597 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.118       |\n",
      "|    explained_variance   | 0.765        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.5         |\n",
      "|    n_updates            | 22590        |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3628528, episode_reward=149.67 +/- 103.01\n",
      "Episode length: 412.00 +/- 296.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 412         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3628528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006238294 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.9        |\n",
      "|    n_updates            | 22600       |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1772    |\n",
      "|    time_elapsed    | 4128    |\n",
      "|    total_timesteps | 3629056 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1773        |\n",
      "|    time_elapsed         | 4129        |\n",
      "|    total_timesteps      | 3631104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007284712 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 22610       |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1774        |\n",
      "|    time_elapsed         | 4131        |\n",
      "|    total_timesteps      | 3633152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013943803 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 22620       |\n",
      "|    policy_gradient_loss | 0.000747    |\n",
      "|    value_loss           | 35.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1775        |\n",
      "|    time_elapsed         | 4133        |\n",
      "|    total_timesteps      | 3635200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004707638 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.232      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 22630       |\n",
      "|    policy_gradient_loss | -0.000132   |\n",
      "|    value_loss           | 85.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1776         |\n",
      "|    time_elapsed         | 4135         |\n",
      "|    total_timesteps      | 3637248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037524826 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.4         |\n",
      "|    n_updates            | 22640        |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3638528, episode_reward=155.28 +/- 122.14\n",
      "Episode length: 244.60 +/- 49.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 245        |\n",
      "|    mean_reward          | 155        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3638528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00872528 |\n",
      "|    clip_fraction        | 0.0625     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.305     |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.22       |\n",
      "|    n_updates            | 22650      |\n",
      "|    policy_gradient_loss | -0.000521  |\n",
      "|    value_loss           | 48.8       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1777    |\n",
      "|    time_elapsed    | 4138    |\n",
      "|    total_timesteps | 3639296 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1778         |\n",
      "|    time_elapsed         | 4140         |\n",
      "|    total_timesteps      | 3641344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073777004 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.71         |\n",
      "|    n_updates            | 22660        |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    value_loss           | 98.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1779        |\n",
      "|    time_elapsed         | 4142        |\n",
      "|    total_timesteps      | 3643392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012437407 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9           |\n",
      "|    n_updates            | 22670       |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1780        |\n",
      "|    time_elapsed         | 4145        |\n",
      "|    total_timesteps      | 3645440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009995602 |\n",
      "|    clip_fraction        | 0.0372      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 22680       |\n",
      "|    policy_gradient_loss | 0.00234     |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1781         |\n",
      "|    time_elapsed         | 4147         |\n",
      "|    total_timesteps      | 3647488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059378115 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87         |\n",
      "|    n_updates            | 22690        |\n",
      "|    policy_gradient_loss | 0.000929     |\n",
      "|    value_loss           | 28.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3648528, episode_reward=200.50 +/- 94.60\n",
      "Episode length: 283.20 +/- 23.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3648528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069147823 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.299       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 86.6         |\n",
      "|    n_updates            | 22700        |\n",
      "|    policy_gradient_loss | -0.000193    |\n",
      "|    value_loss           | 90.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1782    |\n",
      "|    time_elapsed    | 4150    |\n",
      "|    total_timesteps | 3649536 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1783        |\n",
      "|    time_elapsed         | 4152        |\n",
      "|    total_timesteps      | 3651584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006827656 |\n",
      "|    clip_fraction        | 0.0745      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.6        |\n",
      "|    n_updates            | 22710       |\n",
      "|    policy_gradient_loss | -0.000871   |\n",
      "|    value_loss           | 51.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1784         |\n",
      "|    time_elapsed         | 4154         |\n",
      "|    total_timesteps      | 3653632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041948427 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.205       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.3          |\n",
      "|    n_updates            | 22720        |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 37.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1785         |\n",
      "|    time_elapsed         | 4156         |\n",
      "|    total_timesteps      | 3655680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028577992 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 22730        |\n",
      "|    policy_gradient_loss | -0.000415    |\n",
      "|    value_loss           | 93.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1786         |\n",
      "|    time_elapsed         | 4159         |\n",
      "|    total_timesteps      | 3657728      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028580853 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.4         |\n",
      "|    n_updates            | 22740        |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3658528, episode_reward=174.57 +/- 109.05\n",
      "Episode length: 291.60 +/- 28.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 292          |\n",
      "|    mean_reward          | 175          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3658528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050099418 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 22750        |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1787    |\n",
      "|    time_elapsed    | 4161    |\n",
      "|    total_timesteps | 3659776 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1788         |\n",
      "|    time_elapsed         | 4163         |\n",
      "|    total_timesteps      | 3661824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075717694 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.133       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 22760        |\n",
      "|    policy_gradient_loss | 0.00204      |\n",
      "|    value_loss           | 128          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1789        |\n",
      "|    time_elapsed         | 4165        |\n",
      "|    total_timesteps      | 3663872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003893869 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 22770       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1790        |\n",
      "|    time_elapsed         | 4167        |\n",
      "|    total_timesteps      | 3665920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012001952 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.04        |\n",
      "|    n_updates            | 22780       |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 42.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1791        |\n",
      "|    time_elapsed         | 4169        |\n",
      "|    total_timesteps      | 3667968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002575109 |\n",
      "|    clip_fraction        | 0.0414      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 72.6        |\n",
      "|    n_updates            | 22790       |\n",
      "|    policy_gradient_loss | -0.000622   |\n",
      "|    value_loss           | 209         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3668528, episode_reward=226.07 +/- 54.37\n",
      "Episode length: 427.60 +/- 286.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 428         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3668528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014603249 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.9        |\n",
      "|    n_updates            | 22800       |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    value_loss           | 90.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1792    |\n",
      "|    time_elapsed    | 4173    |\n",
      "|    total_timesteps | 3670016 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1793        |\n",
      "|    time_elapsed         | 4176        |\n",
      "|    total_timesteps      | 3672064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005793137 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 22810       |\n",
      "|    policy_gradient_loss | -0.000153   |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1794        |\n",
      "|    time_elapsed         | 4178        |\n",
      "|    total_timesteps      | 3674112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012370826 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.17        |\n",
      "|    n_updates            | 22820       |\n",
      "|    policy_gradient_loss | -0.00012    |\n",
      "|    value_loss           | 38.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1795         |\n",
      "|    time_elapsed         | 4180         |\n",
      "|    total_timesteps      | 3676160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054480648 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.84         |\n",
      "|    n_updates            | 22830        |\n",
      "|    policy_gradient_loss | -0.00078     |\n",
      "|    value_loss           | 90           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1796        |\n",
      "|    time_elapsed         | 4181        |\n",
      "|    total_timesteps      | 3678208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002730342 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.137      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.8        |\n",
      "|    n_updates            | 22840       |\n",
      "|    policy_gradient_loss | -0.000258   |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3678528, episode_reward=179.75 +/- 106.42\n",
      "Episode length: 296.80 +/- 35.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | 180          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3678528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036616726 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.118       |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.7         |\n",
      "|    n_updates            | 22850        |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 92.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1797    |\n",
      "|    time_elapsed    | 4185    |\n",
      "|    total_timesteps | 3680256 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1798         |\n",
      "|    time_elapsed         | 4187         |\n",
      "|    total_timesteps      | 3682304      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071237935 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.305       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.7         |\n",
      "|    n_updates            | 22860        |\n",
      "|    policy_gradient_loss | 7.16e-05     |\n",
      "|    value_loss           | 95.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1799        |\n",
      "|    time_elapsed         | 4189        |\n",
      "|    total_timesteps      | 3684352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006856925 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 22870       |\n",
      "|    policy_gradient_loss | -9.09e-05   |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1800         |\n",
      "|    time_elapsed         | 4191         |\n",
      "|    total_timesteps      | 3686400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022886717 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.245       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 22880        |\n",
      "|    policy_gradient_loss | 0.00238      |\n",
      "|    value_loss           | 64.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1801         |\n",
      "|    time_elapsed         | 4192         |\n",
      "|    total_timesteps      | 3688448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073656337 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.2         |\n",
      "|    n_updates            | 22890        |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3688528, episode_reward=198.83 +/- 17.99\n",
      "Episode length: 308.40 +/- 34.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 308         |\n",
      "|    mean_reward          | 199         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3688528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008256368 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.126      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.21        |\n",
      "|    n_updates            | 22900       |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    value_loss           | 32.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1802    |\n",
      "|    time_elapsed    | 4195    |\n",
      "|    total_timesteps | 3690496 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1803         |\n",
      "|    time_elapsed         | 4197         |\n",
      "|    total_timesteps      | 3692544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023586168 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.21         |\n",
      "|    n_updates            | 22910        |\n",
      "|    policy_gradient_loss | 0.000316     |\n",
      "|    value_loss           | 22.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1804        |\n",
      "|    time_elapsed         | 4199        |\n",
      "|    total_timesteps      | 3694592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009390697 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.67        |\n",
      "|    n_updates            | 22920       |\n",
      "|    policy_gradient_loss | 0.000137    |\n",
      "|    value_loss           | 60          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1805        |\n",
      "|    time_elapsed         | 4201        |\n",
      "|    total_timesteps      | 3696640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013603544 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.18        |\n",
      "|    n_updates            | 22930       |\n",
      "|    policy_gradient_loss | -0.00022    |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3698528, episode_reward=139.62 +/- 99.14\n",
      "Episode length: 405.00 +/- 281.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 405          |\n",
      "|    mean_reward          | 140          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3698528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049640564 |\n",
      "|    clip_fraction        | 0.0425       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.4         |\n",
      "|    n_updates            | 22940        |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 85.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1806    |\n",
      "|    time_elapsed    | 4204    |\n",
      "|    total_timesteps | 3698688 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1807        |\n",
      "|    time_elapsed         | 4206        |\n",
      "|    total_timesteps      | 3700736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009503961 |\n",
      "|    clip_fraction        | 0.0281      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.6        |\n",
      "|    n_updates            | 22950       |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1808         |\n",
      "|    time_elapsed         | 4209         |\n",
      "|    total_timesteps      | 3702784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040423535 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.257       |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.39         |\n",
      "|    n_updates            | 22960        |\n",
      "|    policy_gradient_loss | 0.00115      |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1809        |\n",
      "|    time_elapsed         | 4211        |\n",
      "|    total_timesteps      | 3704832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005077484 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.295      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.66        |\n",
      "|    n_updates            | 22970       |\n",
      "|    policy_gradient_loss | 0.00161     |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1810         |\n",
      "|    time_elapsed         | 4213         |\n",
      "|    total_timesteps      | 3706880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024616467 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.619        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.7         |\n",
      "|    n_updates            | 22980        |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3708528, episode_reward=245.93 +/- 16.05\n",
      "Episode length: 287.80 +/- 18.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 288          |\n",
      "|    mean_reward          | 246          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3708528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049456065 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.23        |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.21         |\n",
      "|    n_updates            | 22990        |\n",
      "|    policy_gradient_loss | 0.000589     |\n",
      "|    value_loss           | 37.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1811    |\n",
      "|    time_elapsed    | 4215    |\n",
      "|    total_timesteps | 3708928 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1812         |\n",
      "|    time_elapsed         | 4217         |\n",
      "|    total_timesteps      | 3710976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061332183 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.189       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 23000        |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1813        |\n",
      "|    time_elapsed         | 4219        |\n",
      "|    total_timesteps      | 3713024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006159054 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.236      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 23010       |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1814         |\n",
      "|    time_elapsed         | 4221         |\n",
      "|    total_timesteps      | 3715072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037243697 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.133       |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 23020        |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 42.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1815         |\n",
      "|    time_elapsed         | 4223         |\n",
      "|    total_timesteps      | 3717120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036205738 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 23030        |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3718528, episode_reward=99.54 +/- 76.54\n",
      "Episode length: 548.00 +/- 374.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 548         |\n",
      "|    mean_reward          | 99.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3718528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014143936 |\n",
      "|    clip_fraction        | 0.0687      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 23040       |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 63.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1816    |\n",
      "|    time_elapsed    | 4228    |\n",
      "|    total_timesteps | 3719168 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1817        |\n",
      "|    time_elapsed         | 4230        |\n",
      "|    total_timesteps      | 3721216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007624894 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.161      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.18        |\n",
      "|    n_updates            | 23050       |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    value_loss           | 33.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1818         |\n",
      "|    time_elapsed         | 4232         |\n",
      "|    total_timesteps      | 3723264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043362062 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 23060        |\n",
      "|    policy_gradient_loss | -0.000693    |\n",
      "|    value_loss           | 98.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1819         |\n",
      "|    time_elapsed         | 4234         |\n",
      "|    total_timesteps      | 3725312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070633534 |\n",
      "|    clip_fraction        | 0.0633       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.75         |\n",
      "|    n_updates            | 23070        |\n",
      "|    policy_gradient_loss | 0.00115      |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1820        |\n",
      "|    time_elapsed         | 4236        |\n",
      "|    total_timesteps      | 3727360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012336051 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 23080       |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3728528, episode_reward=183.86 +/- 82.29\n",
      "Episode length: 330.20 +/- 115.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 184         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3728528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002739613 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 23090       |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1821    |\n",
      "|    time_elapsed    | 4239    |\n",
      "|    total_timesteps | 3729408 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1822        |\n",
      "|    time_elapsed         | 4241        |\n",
      "|    total_timesteps      | 3731456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005583411 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 23100       |\n",
      "|    policy_gradient_loss | 3.1e-05     |\n",
      "|    value_loss           | 79.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1823        |\n",
      "|    time_elapsed         | 4243        |\n",
      "|    total_timesteps      | 3733504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007588201 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 23110       |\n",
      "|    policy_gradient_loss | 0.00105     |\n",
      "|    value_loss           | 78.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1824         |\n",
      "|    time_elapsed         | 4245         |\n",
      "|    total_timesteps      | 3735552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040113465 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21           |\n",
      "|    n_updates            | 23120        |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 49.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1825         |\n",
      "|    time_elapsed         | 4247         |\n",
      "|    total_timesteps      | 3737600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065593133 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.64         |\n",
      "|    n_updates            | 23130        |\n",
      "|    policy_gradient_loss | -0.000968    |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3738528, episode_reward=126.05 +/- 101.03\n",
      "Episode length: 391.40 +/- 307.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 391         |\n",
      "|    mean_reward          | 126         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3738528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007098044 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.7        |\n",
      "|    n_updates            | 23140       |\n",
      "|    policy_gradient_loss | 0.00119     |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 879     |\n",
      "|    iterations      | 1826    |\n",
      "|    time_elapsed    | 4250    |\n",
      "|    total_timesteps | 3739648 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1827        |\n",
      "|    time_elapsed         | 4252        |\n",
      "|    total_timesteps      | 3741696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003236493 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 23150       |\n",
      "|    policy_gradient_loss | -0.000857   |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 1828         |\n",
      "|    time_elapsed         | 4254         |\n",
      "|    total_timesteps      | 3743744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030538244 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.8         |\n",
      "|    n_updates            | 23160        |\n",
      "|    policy_gradient_loss | 0.000955     |\n",
      "|    value_loss           | 37.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 1829        |\n",
      "|    time_elapsed         | 4256        |\n",
      "|    total_timesteps      | 3745792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013618976 |\n",
      "|    clip_fraction        | 0.0716      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.61        |\n",
      "|    n_updates            | 23170       |\n",
      "|    policy_gradient_loss | 0.00244     |\n",
      "|    value_loss           | 50.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1830        |\n",
      "|    time_elapsed         | 4258        |\n",
      "|    total_timesteps      | 3747840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004888549 |\n",
      "|    clip_fraction        | 0.0342      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.71        |\n",
      "|    n_updates            | 23180       |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 63.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3748528, episode_reward=212.65 +/- 26.59\n",
      "Episode length: 318.20 +/- 33.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3748528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035902034 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 23190        |\n",
      "|    policy_gradient_loss | 0.000252     |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1831    |\n",
      "|    time_elapsed    | 4261    |\n",
      "|    total_timesteps | 3749888 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1832         |\n",
      "|    time_elapsed         | 4262         |\n",
      "|    total_timesteps      | 3751936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036677998 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 69.3         |\n",
      "|    n_updates            | 23200        |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 151          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1833        |\n",
      "|    time_elapsed         | 4265        |\n",
      "|    total_timesteps      | 3753984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002624739 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.6        |\n",
      "|    n_updates            | 23210       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 92.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1834        |\n",
      "|    time_elapsed         | 4267        |\n",
      "|    total_timesteps      | 3756032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014515932 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32          |\n",
      "|    n_updates            | 23220       |\n",
      "|    policy_gradient_loss | 0.00029     |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1835        |\n",
      "|    time_elapsed         | 4269        |\n",
      "|    total_timesteps      | 3758080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003844569 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 23230       |\n",
      "|    policy_gradient_loss | 0.000366    |\n",
      "|    value_loss           | 44.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3758528, episode_reward=137.13 +/- 111.10\n",
      "Episode length: 257.40 +/- 39.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 257        |\n",
      "|    mean_reward          | 137        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3758528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00984302 |\n",
      "|    clip_fraction        | 0.0643     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.186     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.4       |\n",
      "|    n_updates            | 23240      |\n",
      "|    policy_gradient_loss | 0.000546   |\n",
      "|    value_loss           | 20.1       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1836    |\n",
      "|    time_elapsed    | 4272    |\n",
      "|    total_timesteps | 3760128 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1837         |\n",
      "|    time_elapsed         | 4274         |\n",
      "|    total_timesteps      | 3762176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023606047 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.703        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.2         |\n",
      "|    n_updates            | 23250        |\n",
      "|    policy_gradient_loss | -0.000673    |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1838         |\n",
      "|    time_elapsed         | 4276         |\n",
      "|    total_timesteps      | 3764224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028624404 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 23260        |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1839         |\n",
      "|    time_elapsed         | 4277         |\n",
      "|    total_timesteps      | 3766272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069335466 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.468        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.4         |\n",
      "|    n_updates            | 23270        |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 58.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1840        |\n",
      "|    time_elapsed         | 4279        |\n",
      "|    total_timesteps      | 3768320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009717023 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.1        |\n",
      "|    n_updates            | 23280       |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3768528, episode_reward=197.06 +/- 101.16\n",
      "Episode length: 283.20 +/- 72.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | 197         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3768528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003358969 |\n",
      "|    clip_fraction        | 0.069       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.327      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 23290       |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1841    |\n",
      "|    time_elapsed    | 4282    |\n",
      "|    total_timesteps | 3770368 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1842         |\n",
      "|    time_elapsed         | 4284         |\n",
      "|    total_timesteps      | 3772416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013067159 |\n",
      "|    clip_fraction        | 0.0209       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.44         |\n",
      "|    n_updates            | 23300        |\n",
      "|    policy_gradient_loss | -0.000831    |\n",
      "|    value_loss           | 93.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1843        |\n",
      "|    time_elapsed         | 4286        |\n",
      "|    total_timesteps      | 3774464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004522928 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.6        |\n",
      "|    n_updates            | 23310       |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1844        |\n",
      "|    time_elapsed         | 4288        |\n",
      "|    total_timesteps      | 3776512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005783976 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 23320       |\n",
      "|    policy_gradient_loss | 0.000862    |\n",
      "|    value_loss           | 72.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3778528, episode_reward=114.39 +/- 128.85\n",
      "Episode length: 247.80 +/- 38.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 248          |\n",
      "|    mean_reward          | 114          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3778528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027307665 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.2         |\n",
      "|    n_updates            | 23330        |\n",
      "|    policy_gradient_loss | -0.000173    |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1845    |\n",
      "|    time_elapsed    | 4290    |\n",
      "|    total_timesteps | 3778560 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 1846         |\n",
      "|    time_elapsed         | 4292         |\n",
      "|    total_timesteps      | 3780608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019789417 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.696        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 23340        |\n",
      "|    policy_gradient_loss | -0.00091     |\n",
      "|    value_loss           | 82.2         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 880        |\n",
      "|    iterations           | 1847       |\n",
      "|    time_elapsed         | 4294       |\n",
      "|    total_timesteps      | 3782656    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00265828 |\n",
      "|    clip_fraction        | 0.0314     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40.4       |\n",
      "|    n_updates            | 23350      |\n",
      "|    policy_gradient_loss | -0.0024    |\n",
      "|    value_loss           | 122        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1848        |\n",
      "|    time_elapsed         | 4296        |\n",
      "|    total_timesteps      | 3784704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004918472 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.49        |\n",
      "|    n_updates            | 23360       |\n",
      "|    policy_gradient_loss | -0.000577   |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1849        |\n",
      "|    time_elapsed         | 4298        |\n",
      "|    total_timesteps      | 3786752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005326922 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 23370       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3788528, episode_reward=128.48 +/- 86.23\n",
      "Episode length: 282.60 +/- 65.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 128          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3788528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052149594 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.077       |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 23380        |\n",
      "|    policy_gradient_loss | 0.000549     |\n",
      "|    value_loss           | 11.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1850    |\n",
      "|    time_elapsed    | 4301    |\n",
      "|    total_timesteps | 3788800 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 1851        |\n",
      "|    time_elapsed         | 4303        |\n",
      "|    total_timesteps      | 3790848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010558067 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.6        |\n",
      "|    n_updates            | 23390       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1852        |\n",
      "|    time_elapsed         | 4304        |\n",
      "|    total_timesteps      | 3792896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003634274 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 23400       |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1853         |\n",
      "|    time_elapsed         | 4306         |\n",
      "|    total_timesteps      | 3794944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026414604 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.2         |\n",
      "|    n_updates            | 23410        |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    value_loss           | 155          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1854         |\n",
      "|    time_elapsed         | 4308         |\n",
      "|    total_timesteps      | 3796992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051795077 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 23420        |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 91.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3798528, episode_reward=205.57 +/- 20.75\n",
      "Episode length: 391.60 +/- 127.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 392          |\n",
      "|    mean_reward          | 206          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3798528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057613794 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.154       |\n",
      "|    explained_variance   | 0.817        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 23430        |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1855    |\n",
      "|    time_elapsed    | 4311    |\n",
      "|    total_timesteps | 3799040 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1856         |\n",
      "|    time_elapsed         | 4313         |\n",
      "|    total_timesteps      | 3801088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063237585 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.85         |\n",
      "|    n_updates            | 23440        |\n",
      "|    policy_gradient_loss | 0.000319     |\n",
      "|    value_loss           | 43.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1857        |\n",
      "|    time_elapsed         | 4315        |\n",
      "|    total_timesteps      | 3803136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004086025 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 23450       |\n",
      "|    policy_gradient_loss | 5.93e-05    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1858        |\n",
      "|    time_elapsed         | 4317        |\n",
      "|    total_timesteps      | 3805184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006194598 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 23460       |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 97.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1859         |\n",
      "|    time_elapsed         | 4320         |\n",
      "|    total_timesteps      | 3807232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040726643 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.7         |\n",
      "|    n_updates            | 23470        |\n",
      "|    policy_gradient_loss | 0.000634     |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3808528, episode_reward=178.96 +/- 53.64\n",
      "Episode length: 477.80 +/- 269.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 478         |\n",
      "|    mean_reward          | 179         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3808528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023497261 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44        |\n",
      "|    n_updates            | 23480       |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 880     |\n",
      "|    iterations      | 1860    |\n",
      "|    time_elapsed    | 4324    |\n",
      "|    total_timesteps | 3809280 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1861         |\n",
      "|    time_elapsed         | 4325         |\n",
      "|    total_timesteps      | 3811328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065072626 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.5         |\n",
      "|    n_updates            | 23490        |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 96.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1862         |\n",
      "|    time_elapsed         | 4327         |\n",
      "|    total_timesteps      | 3813376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045602205 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 23500        |\n",
      "|    policy_gradient_loss | -0.000164    |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1863         |\n",
      "|    time_elapsed         | 4329         |\n",
      "|    total_timesteps      | 3815424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016807899 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.7         |\n",
      "|    n_updates            | 23510        |\n",
      "|    policy_gradient_loss | -0.000273    |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1864        |\n",
      "|    time_elapsed         | 4331        |\n",
      "|    total_timesteps      | 3817472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006484788 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.5        |\n",
      "|    n_updates            | 23520       |\n",
      "|    policy_gradient_loss | -0.000701   |\n",
      "|    value_loss           | 72.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3818528, episode_reward=222.12 +/- 41.27\n",
      "Episode length: 317.60 +/- 39.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 318          |\n",
      "|    mean_reward          | 222          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3818528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049122106 |\n",
      "|    clip_fraction        | 0.0192       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0863      |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 23530        |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 62.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1865    |\n",
      "|    time_elapsed    | 4334    |\n",
      "|    total_timesteps | 3819520 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1866        |\n",
      "|    time_elapsed         | 4336        |\n",
      "|    total_timesteps      | 3821568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016609274 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.3         |\n",
      "|    n_updates            | 23540       |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    value_loss           | 65.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1867        |\n",
      "|    time_elapsed         | 4337        |\n",
      "|    total_timesteps      | 3823616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008694184 |\n",
      "|    clip_fraction        | 0.0585      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.27        |\n",
      "|    n_updates            | 23550       |\n",
      "|    policy_gradient_loss | 0.000262    |\n",
      "|    value_loss           | 33.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1868        |\n",
      "|    time_elapsed         | 4339        |\n",
      "|    total_timesteps      | 3825664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019191794 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.4        |\n",
      "|    n_updates            | 23560       |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    value_loss           | 66.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1869         |\n",
      "|    time_elapsed         | 4341         |\n",
      "|    total_timesteps      | 3827712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032080899 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 23570        |\n",
      "|    policy_gradient_loss | -0.000718    |\n",
      "|    value_loss           | 193          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3828528, episode_reward=167.67 +/- 113.54\n",
      "Episode length: 338.60 +/- 108.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 339          |\n",
      "|    mean_reward          | 168          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3828528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057050027 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.67         |\n",
      "|    n_updates            | 23580        |\n",
      "|    policy_gradient_loss | 0.00042      |\n",
      "|    value_loss           | 63.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1870    |\n",
      "|    time_elapsed    | 4344    |\n",
      "|    total_timesteps | 3829760 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1871         |\n",
      "|    time_elapsed         | 4346         |\n",
      "|    total_timesteps      | 3831808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045100087 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.796        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 23590        |\n",
      "|    policy_gradient_loss | -0.000311    |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1872        |\n",
      "|    time_elapsed         | 4348        |\n",
      "|    total_timesteps      | 3833856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005871039 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.6        |\n",
      "|    n_updates            | 23600       |\n",
      "|    policy_gradient_loss | 0.00137     |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1873         |\n",
      "|    time_elapsed         | 4350         |\n",
      "|    total_timesteps      | 3835904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018783098 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.128       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.06         |\n",
      "|    n_updates            | 23610        |\n",
      "|    policy_gradient_loss | 0.000331     |\n",
      "|    value_loss           | 26.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1874         |\n",
      "|    time_elapsed         | 4352         |\n",
      "|    total_timesteps      | 3837952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037076864 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.9         |\n",
      "|    n_updates            | 23620        |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3838528, episode_reward=146.73 +/- 89.98\n",
      "Episode length: 439.00 +/- 283.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 439         |\n",
      "|    mean_reward          | 147         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3838528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015564504 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 23630       |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 50.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1875    |\n",
      "|    time_elapsed    | 4356    |\n",
      "|    total_timesteps | 3840000 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1876         |\n",
      "|    time_elapsed         | 4358         |\n",
      "|    total_timesteps      | 3842048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028995932 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 23640        |\n",
      "|    policy_gradient_loss | -0.000453    |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1877         |\n",
      "|    time_elapsed         | 4359         |\n",
      "|    total_timesteps      | 3844096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046132905 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.9          |\n",
      "|    n_updates            | 23650        |\n",
      "|    policy_gradient_loss | 0.00178      |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1878         |\n",
      "|    time_elapsed         | 4362         |\n",
      "|    total_timesteps      | 3846144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039150356 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.7         |\n",
      "|    n_updates            | 23660        |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 84.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1879        |\n",
      "|    time_elapsed         | 4364        |\n",
      "|    total_timesteps      | 3848192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003948254 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 23670       |\n",
      "|    policy_gradient_loss | -0.000579   |\n",
      "|    value_loss           | 87.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3848528, episode_reward=209.06 +/- 45.03\n",
      "Episode length: 425.80 +/- 287.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 426          |\n",
      "|    mean_reward          | 209          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3848528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053008124 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.256       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.2         |\n",
      "|    n_updates            | 23680        |\n",
      "|    policy_gradient_loss | -0.000708    |\n",
      "|    value_loss           | 36.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1880    |\n",
      "|    time_elapsed    | 4367    |\n",
      "|    total_timesteps | 3850240 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1881        |\n",
      "|    time_elapsed         | 4369        |\n",
      "|    total_timesteps      | 3852288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011587286 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 23690       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1882        |\n",
      "|    time_elapsed         | 4372        |\n",
      "|    total_timesteps      | 3854336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006010264 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 23700       |\n",
      "|    policy_gradient_loss | -0.000148   |\n",
      "|    value_loss           | 95.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 881        |\n",
      "|    iterations           | 1883       |\n",
      "|    time_elapsed         | 4374       |\n",
      "|    total_timesteps      | 3856384    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02813227 |\n",
      "|    clip_fraction        | 0.0511     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.2       |\n",
      "|    explained_variance   | 0.734      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.755      |\n",
      "|    n_updates            | 23710      |\n",
      "|    policy_gradient_loss | 0.000855   |\n",
      "|    value_loss           | 17.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1884         |\n",
      "|    time_elapsed         | 4375         |\n",
      "|    total_timesteps      | 3858432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032802615 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.118       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 88.5         |\n",
      "|    n_updates            | 23720        |\n",
      "|    policy_gradient_loss | -0.000672    |\n",
      "|    value_loss           | 82.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3858528, episode_reward=212.95 +/- 34.85\n",
      "Episode length: 456.40 +/- 262.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3858528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067797606 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.3         |\n",
      "|    n_updates            | 23730        |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1885    |\n",
      "|    time_elapsed    | 4379    |\n",
      "|    total_timesteps | 3860480 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1886        |\n",
      "|    time_elapsed         | 4381        |\n",
      "|    total_timesteps      | 3862528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009749439 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 23740       |\n",
      "|    policy_gradient_loss | 0.00116     |\n",
      "|    value_loss           | 31.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1887        |\n",
      "|    time_elapsed         | 4383        |\n",
      "|    total_timesteps      | 3864576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014802014 |\n",
      "|    clip_fraction        | 0.0611      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40          |\n",
      "|    n_updates            | 23750       |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1888         |\n",
      "|    time_elapsed         | 4385         |\n",
      "|    total_timesteps      | 3866624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034334208 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28           |\n",
      "|    n_updates            | 23760        |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 69.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3868528, episode_reward=169.78 +/- 101.26\n",
      "Episode length: 306.60 +/- 72.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | 170         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3868528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008234112 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 95.3        |\n",
      "|    n_updates            | 23770       |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1889    |\n",
      "|    time_elapsed    | 4388    |\n",
      "|    total_timesteps | 3868672 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1890        |\n",
      "|    time_elapsed         | 4391        |\n",
      "|    total_timesteps      | 3870720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004999777 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 23780       |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1891        |\n",
      "|    time_elapsed         | 4393        |\n",
      "|    total_timesteps      | 3872768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038732193 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.64        |\n",
      "|    n_updates            | 23790       |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 35.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1892         |\n",
      "|    time_elapsed         | 4395         |\n",
      "|    total_timesteps      | 3874816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070183272 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.194       |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 122          |\n",
      "|    n_updates            | 23800        |\n",
      "|    policy_gradient_loss | 0.000632     |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1893         |\n",
      "|    time_elapsed         | 4397         |\n",
      "|    total_timesteps      | 3876864      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034276398 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.4         |\n",
      "|    n_updates            | 23810        |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3878528, episode_reward=93.74 +/- 99.71\n",
      "Episode length: 413.20 +/- 294.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 413         |\n",
      "|    mean_reward          | 93.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3878528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011810478 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.796       |\n",
      "|    n_updates            | 23820       |\n",
      "|    policy_gradient_loss | 0.00359     |\n",
      "|    value_loss           | 11.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1894    |\n",
      "|    time_elapsed    | 4401    |\n",
      "|    total_timesteps | 3878912 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1895        |\n",
      "|    time_elapsed         | 4403        |\n",
      "|    total_timesteps      | 3880960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003861347 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 23830       |\n",
      "|    policy_gradient_loss | -0.000624   |\n",
      "|    value_loss           | 55.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1896         |\n",
      "|    time_elapsed         | 4404         |\n",
      "|    total_timesteps      | 3883008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017637237 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.8         |\n",
      "|    n_updates            | 23840        |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1897         |\n",
      "|    time_elapsed         | 4406         |\n",
      "|    total_timesteps      | 3885056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045926743 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.729        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.6         |\n",
      "|    n_updates            | 23850        |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 93.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1898        |\n",
      "|    time_elapsed         | 4408        |\n",
      "|    total_timesteps      | 3887104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005130089 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 23860       |\n",
      "|    policy_gradient_loss | -0.000653   |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3888528, episode_reward=209.08 +/- 41.15\n",
      "Episode length: 400.80 +/- 151.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 401          |\n",
      "|    mean_reward          | 209          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3888528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031641566 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.286       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.1         |\n",
      "|    n_updates            | 23870        |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 60.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1899    |\n",
      "|    time_elapsed    | 4412    |\n",
      "|    total_timesteps | 3889152 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1900        |\n",
      "|    time_elapsed         | 4414        |\n",
      "|    total_timesteps      | 3891200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009204924 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33        |\n",
      "|    n_updates            | 23880       |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1901         |\n",
      "|    time_elapsed         | 4416         |\n",
      "|    total_timesteps      | 3893248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033513175 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 23890        |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 881        |\n",
      "|    iterations           | 1902       |\n",
      "|    time_elapsed         | 4418       |\n",
      "|    total_timesteps      | 3895296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00466049 |\n",
      "|    clip_fraction        | 0.026      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.307     |\n",
      "|    explained_variance   | 0.75       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 23900      |\n",
      "|    policy_gradient_loss | 0.000277   |\n",
      "|    value_loss           | 65.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1903        |\n",
      "|    time_elapsed         | 4419        |\n",
      "|    total_timesteps      | 3897344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004344046 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.8        |\n",
      "|    n_updates            | 23910       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 77.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3898528, episode_reward=167.02 +/- 108.43\n",
      "Episode length: 358.60 +/- 198.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 359          |\n",
      "|    mean_reward          | 167          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3898528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025877291 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.2         |\n",
      "|    n_updates            | 23920        |\n",
      "|    policy_gradient_loss | -0.00016     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1904    |\n",
      "|    time_elapsed    | 4423    |\n",
      "|    total_timesteps | 3899392 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1905         |\n",
      "|    time_elapsed         | 4424         |\n",
      "|    total_timesteps      | 3901440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031851851 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.1         |\n",
      "|    n_updates            | 23930        |\n",
      "|    policy_gradient_loss | -0.000503    |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1906         |\n",
      "|    time_elapsed         | 4427         |\n",
      "|    total_timesteps      | 3903488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062527005 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 23940        |\n",
      "|    policy_gradient_loss | -0.00059     |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1907         |\n",
      "|    time_elapsed         | 4429         |\n",
      "|    total_timesteps      | 3905536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053727757 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.733        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.04         |\n",
      "|    n_updates            | 23950        |\n",
      "|    policy_gradient_loss | 0.00207      |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1908        |\n",
      "|    time_elapsed         | 4431        |\n",
      "|    total_timesteps      | 3907584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007584231 |\n",
      "|    clip_fraction        | 0.044       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 74.6        |\n",
      "|    n_updates            | 23960       |\n",
      "|    policy_gradient_loss | -0.000461   |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3908528, episode_reward=182.16 +/- 114.71\n",
      "Episode length: 269.40 +/- 60.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | 182          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3908528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037625276 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.768        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 23970        |\n",
      "|    policy_gradient_loss | -0.000282    |\n",
      "|    value_loss           | 40.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1909    |\n",
      "|    time_elapsed    | 4433    |\n",
      "|    total_timesteps | 3909632 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1910        |\n",
      "|    time_elapsed         | 4435        |\n",
      "|    total_timesteps      | 3911680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002459637 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.2        |\n",
      "|    n_updates            | 23980       |\n",
      "|    policy_gradient_loss | -0.000472   |\n",
      "|    value_loss           | 99.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 881        |\n",
      "|    iterations           | 1911       |\n",
      "|    time_elapsed         | 4437       |\n",
      "|    total_timesteps      | 3913728    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00859961 |\n",
      "|    clip_fraction        | 0.0362     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.142     |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 94.5       |\n",
      "|    n_updates            | 23990      |\n",
      "|    policy_gradient_loss | -0.00134   |\n",
      "|    value_loss           | 128        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1912         |\n",
      "|    time_elapsed         | 4439         |\n",
      "|    total_timesteps      | 3915776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034342185 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.15        |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.2         |\n",
      "|    n_updates            | 24000        |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1913         |\n",
      "|    time_elapsed         | 4441         |\n",
      "|    total_timesteps      | 3917824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030279798 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 67           |\n",
      "|    n_updates            | 24010        |\n",
      "|    policy_gradient_loss | -0.00056     |\n",
      "|    value_loss           | 212          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3918528, episode_reward=150.39 +/- 122.19\n",
      "Episode length: 445.80 +/- 291.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 446         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3918528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009799396 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43          |\n",
      "|    n_updates            | 24020       |\n",
      "|    policy_gradient_loss | -0.000948   |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1914    |\n",
      "|    time_elapsed    | 4445    |\n",
      "|    total_timesteps | 3919872 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1915        |\n",
      "|    time_elapsed         | 4447        |\n",
      "|    total_timesteps      | 3921920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008544442 |\n",
      "|    clip_fraction        | 0.0634      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 24030       |\n",
      "|    policy_gradient_loss | 0.000834    |\n",
      "|    value_loss           | 61.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1916        |\n",
      "|    time_elapsed         | 4449        |\n",
      "|    total_timesteps      | 3923968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005264258 |\n",
      "|    clip_fraction        | 0.0593      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 24040       |\n",
      "|    policy_gradient_loss | 0.00246     |\n",
      "|    value_loss           | 33.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1917        |\n",
      "|    time_elapsed         | 4451        |\n",
      "|    total_timesteps      | 3926016     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013591245 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.337      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.66        |\n",
      "|    n_updates            | 24050       |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 29.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1918         |\n",
      "|    time_elapsed         | 4454         |\n",
      "|    total_timesteps      | 3928064      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047782315 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24           |\n",
      "|    n_updates            | 24060        |\n",
      "|    policy_gradient_loss | 0.000191     |\n",
      "|    value_loss           | 46.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3928528, episode_reward=174.41 +/- 107.60\n",
      "Episode length: 280.80 +/- 61.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 281         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3928528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008004103 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.52        |\n",
      "|    n_updates            | 24070       |\n",
      "|    policy_gradient_loss | 0.000431    |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1919    |\n",
      "|    time_elapsed    | 4457    |\n",
      "|    total_timesteps | 3930112 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1920        |\n",
      "|    time_elapsed         | 4459        |\n",
      "|    total_timesteps      | 3932160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006763712 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.06        |\n",
      "|    n_updates            | 24080       |\n",
      "|    policy_gradient_loss | 0.000206    |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1921        |\n",
      "|    time_elapsed         | 4461        |\n",
      "|    total_timesteps      | 3934208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003999743 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 24090       |\n",
      "|    policy_gradient_loss | -0.000961   |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1922         |\n",
      "|    time_elapsed         | 4463         |\n",
      "|    total_timesteps      | 3936256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060573476 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 66.4         |\n",
      "|    n_updates            | 24100        |\n",
      "|    policy_gradient_loss | -0.000492    |\n",
      "|    value_loss           | 87.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1923        |\n",
      "|    time_elapsed         | 4465        |\n",
      "|    total_timesteps      | 3938304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008564262 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.337      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 24110       |\n",
      "|    policy_gradient_loss | -0.0003     |\n",
      "|    value_loss           | 33.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3938528, episode_reward=227.20 +/- 31.84\n",
      "Episode length: 319.40 +/- 61.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 227         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3938528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008653572 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.6        |\n",
      "|    n_updates            | 24120       |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 45.4        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1924    |\n",
      "|    time_elapsed    | 4469    |\n",
      "|    total_timesteps | 3940352 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1925        |\n",
      "|    time_elapsed         | 4470        |\n",
      "|    total_timesteps      | 3942400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008077743 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.36        |\n",
      "|    n_updates            | 24130       |\n",
      "|    policy_gradient_loss | -0.000135   |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1926         |\n",
      "|    time_elapsed         | 4472         |\n",
      "|    total_timesteps      | 3944448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049651256 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 24140        |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 77.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1927         |\n",
      "|    time_elapsed         | 4474         |\n",
      "|    total_timesteps      | 3946496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020399585 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 63.2         |\n",
      "|    n_updates            | 24150        |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 179          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3948528, episode_reward=66.33 +/- 104.33\n",
      "Episode length: 386.80 +/- 307.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 387          |\n",
      "|    mean_reward          | 66.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3948528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029855692 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.336       |\n",
      "|    explained_variance   | 0.703        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 24160        |\n",
      "|    policy_gradient_loss | -0.000236    |\n",
      "|    value_loss           | 90.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1928    |\n",
      "|    time_elapsed    | 4478    |\n",
      "|    total_timesteps | 3948544 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 1929         |\n",
      "|    time_elapsed         | 4480         |\n",
      "|    total_timesteps      | 3950592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062899156 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.145       |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45           |\n",
      "|    n_updates            | 24170        |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 95.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1930        |\n",
      "|    time_elapsed         | 4482        |\n",
      "|    total_timesteps      | 3952640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004114691 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 24180       |\n",
      "|    policy_gradient_loss | -0.000166   |\n",
      "|    value_loss           | 96.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 881         |\n",
      "|    iterations           | 1931        |\n",
      "|    time_elapsed         | 4484        |\n",
      "|    total_timesteps      | 3954688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003435451 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 24190       |\n",
      "|    policy_gradient_loss | 0.000628    |\n",
      "|    value_loss           | 25          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1932        |\n",
      "|    time_elapsed         | 4485        |\n",
      "|    total_timesteps      | 3956736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006512017 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.03        |\n",
      "|    n_updates            | 24200       |\n",
      "|    policy_gradient_loss | 0.000993    |\n",
      "|    value_loss           | 56.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3958528, episode_reward=181.29 +/- 103.46\n",
      "Episode length: 300.00 +/- 51.13\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 300          |\n",
      "|    mean_reward          | 181          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3958528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028427816 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 24210        |\n",
      "|    policy_gradient_loss | -0.000412    |\n",
      "|    value_loss           | 40.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 881     |\n",
      "|    iterations      | 1933    |\n",
      "|    time_elapsed    | 4488    |\n",
      "|    total_timesteps | 3958784 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1934         |\n",
      "|    time_elapsed         | 4490         |\n",
      "|    total_timesteps      | 3960832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017383546 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.133       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.33         |\n",
      "|    n_updates            | 24220        |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 88           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 882        |\n",
      "|    iterations           | 1935       |\n",
      "|    time_elapsed         | 4492       |\n",
      "|    total_timesteps      | 3962880    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02528743 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.28      |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.2       |\n",
      "|    n_updates            | 24230      |\n",
      "|    policy_gradient_loss | -0.002     |\n",
      "|    value_loss           | 42.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1936        |\n",
      "|    time_elapsed         | 4494        |\n",
      "|    total_timesteps      | 3964928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007905586 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.35        |\n",
      "|    n_updates            | 24240       |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1937         |\n",
      "|    time_elapsed         | 4496         |\n",
      "|    total_timesteps      | 3966976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061028493 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 24250        |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 40.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3968528, episode_reward=103.66 +/- 131.97\n",
      "Episode length: 335.60 +/- 79.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 336          |\n",
      "|    mean_reward          | 104          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3968528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012310145 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.618        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 213          |\n",
      "|    n_updates            | 24260        |\n",
      "|    policy_gradient_loss | -0.00036     |\n",
      "|    value_loss           | 315          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1938    |\n",
      "|    time_elapsed    | 4499    |\n",
      "|    total_timesteps | 3969024 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1939         |\n",
      "|    time_elapsed         | 4501         |\n",
      "|    total_timesteps      | 3971072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026839818 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 59.9         |\n",
      "|    n_updates            | 24270        |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1940         |\n",
      "|    time_elapsed         | 4503         |\n",
      "|    total_timesteps      | 3973120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057964846 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.74         |\n",
      "|    n_updates            | 24280        |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    value_loss           | 81           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1941         |\n",
      "|    time_elapsed         | 4505         |\n",
      "|    total_timesteps      | 3975168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060115107 |\n",
      "|    clip_fraction        | 0.0583       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.7         |\n",
      "|    n_updates            | 24290        |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1942        |\n",
      "|    time_elapsed         | 4506        |\n",
      "|    total_timesteps      | 3977216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017179739 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.3        |\n",
      "|    n_updates            | 24300       |\n",
      "|    policy_gradient_loss | 0.00137     |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3978528, episode_reward=225.36 +/- 25.61\n",
      "Episode length: 296.40 +/- 45.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 296          |\n",
      "|    mean_reward          | 225          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3978528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043594753 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.1         |\n",
      "|    n_updates            | 24310        |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    value_loss           | 77.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1943    |\n",
      "|    time_elapsed    | 4509    |\n",
      "|    total_timesteps | 3979264 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 882        |\n",
      "|    iterations           | 1944       |\n",
      "|    time_elapsed         | 4511       |\n",
      "|    total_timesteps      | 3981312    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01606597 |\n",
      "|    clip_fraction        | 0.0456     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.5       |\n",
      "|    n_updates            | 24320      |\n",
      "|    policy_gradient_loss | 0.00194    |\n",
      "|    value_loss           | 30.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1945        |\n",
      "|    time_elapsed         | 4514        |\n",
      "|    total_timesteps      | 3983360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003285197 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 24330       |\n",
      "|    policy_gradient_loss | 0.00194     |\n",
      "|    value_loss           | 84.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1946        |\n",
      "|    time_elapsed         | 4516        |\n",
      "|    total_timesteps      | 3985408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010438225 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 24340       |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    value_loss           | 8.08        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1947         |\n",
      "|    time_elapsed         | 4517         |\n",
      "|    total_timesteps      | 3987456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021246634 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.767        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.8         |\n",
      "|    n_updates            | 24350        |\n",
      "|    policy_gradient_loss | -0.00061     |\n",
      "|    value_loss           | 95.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3988528, episode_reward=123.68 +/- 137.54\n",
      "Episode length: 262.00 +/- 38.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 262          |\n",
      "|    mean_reward          | 124          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3988528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018223142 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.3         |\n",
      "|    n_updates            | 24360        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 54.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1948    |\n",
      "|    time_elapsed    | 4520    |\n",
      "|    total_timesteps | 3989504 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1949         |\n",
      "|    time_elapsed         | 4522         |\n",
      "|    total_timesteps      | 3991552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021428308 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 85           |\n",
      "|    n_updates            | 24370        |\n",
      "|    policy_gradient_loss | -0.000114    |\n",
      "|    value_loss           | 78.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1950         |\n",
      "|    time_elapsed         | 4524         |\n",
      "|    total_timesteps      | 3993600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019271648 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.9         |\n",
      "|    n_updates            | 24380        |\n",
      "|    policy_gradient_loss | -0.000877    |\n",
      "|    value_loss           | 159          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1951         |\n",
      "|    time_elapsed         | 4526         |\n",
      "|    total_timesteps      | 3995648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027569567 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.113       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.4         |\n",
      "|    n_updates            | 24390        |\n",
      "|    policy_gradient_loss | -0.000604    |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1952        |\n",
      "|    time_elapsed         | 4528        |\n",
      "|    total_timesteps      | 3997696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013860998 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.65        |\n",
      "|    n_updates            | 24400       |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=3998528, episode_reward=223.66 +/- 55.39\n",
      "Episode length: 428.20 +/- 285.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 428         |\n",
      "|    mean_reward          | 224         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3998528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003786445 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 24410       |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    value_loss           | 41.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1953    |\n",
      "|    time_elapsed    | 4531    |\n",
      "|    total_timesteps | 3999744 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1954         |\n",
      "|    time_elapsed         | 4533         |\n",
      "|    total_timesteps      | 4001792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039656432 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.796        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 24420        |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 80.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 882        |\n",
      "|    iterations           | 1955       |\n",
      "|    time_elapsed         | 4535       |\n",
      "|    total_timesteps      | 4003840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00349837 |\n",
      "|    clip_fraction        | 0.0337     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.02       |\n",
      "|    n_updates            | 24430      |\n",
      "|    policy_gradient_loss | -2.49e-05  |\n",
      "|    value_loss           | 27.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1956         |\n",
      "|    time_elapsed         | 4537         |\n",
      "|    total_timesteps      | 4005888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031938958 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.9         |\n",
      "|    n_updates            | 24440        |\n",
      "|    policy_gradient_loss | -0.000437    |\n",
      "|    value_loss           | 61.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1957         |\n",
      "|    time_elapsed         | 4539         |\n",
      "|    total_timesteps      | 4007936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030707265 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.6         |\n",
      "|    n_updates            | 24450        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 30.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4008528, episode_reward=141.03 +/- 80.56\n",
      "Episode length: 456.40 +/- 265.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | 141          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4008528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069402484 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 24460        |\n",
      "|    policy_gradient_loss | 0.00171      |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1958    |\n",
      "|    time_elapsed    | 4543    |\n",
      "|    total_timesteps | 4009984 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1959        |\n",
      "|    time_elapsed         | 4545        |\n",
      "|    total_timesteps      | 4012032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004813488 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.03        |\n",
      "|    n_updates            | 24470       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1960        |\n",
      "|    time_elapsed         | 4547        |\n",
      "|    total_timesteps      | 4014080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006717478 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.89        |\n",
      "|    n_updates            | 24480       |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1961        |\n",
      "|    time_elapsed         | 4549        |\n",
      "|    total_timesteps      | 4016128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006640179 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.01        |\n",
      "|    n_updates            | 24490       |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1962         |\n",
      "|    time_elapsed         | 4551         |\n",
      "|    total_timesteps      | 4018176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024864166 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.661        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 24500        |\n",
      "|    policy_gradient_loss | 0.000306     |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4018528, episode_reward=154.71 +/- 115.27\n",
      "Episode length: 309.80 +/- 79.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 310          |\n",
      "|    mean_reward          | 155          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4018528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028357697 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.756        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.6         |\n",
      "|    n_updates            | 24510        |\n",
      "|    policy_gradient_loss | -0.000253    |\n",
      "|    value_loss           | 71.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 882     |\n",
      "|    iterations      | 1963    |\n",
      "|    time_elapsed    | 4553    |\n",
      "|    total_timesteps | 4020224 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 882         |\n",
      "|    iterations           | 1964        |\n",
      "|    time_elapsed         | 4556        |\n",
      "|    total_timesteps      | 4022272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004607117 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.21       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 75          |\n",
      "|    n_updates            | 24520       |\n",
      "|    policy_gradient_loss | 7.39e-05    |\n",
      "|    value_loss           | 99.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 1965         |\n",
      "|    time_elapsed         | 4558         |\n",
      "|    total_timesteps      | 4024320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018604065 |\n",
      "|    clip_fraction        | 0.0161       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.733        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 24530        |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 92.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1966        |\n",
      "|    time_elapsed         | 4559        |\n",
      "|    total_timesteps      | 4026368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008029502 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 24540       |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 51          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1967        |\n",
      "|    time_elapsed         | 4561        |\n",
      "|    total_timesteps      | 4028416     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002164547 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 24550       |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4028528, episode_reward=202.99 +/- 100.29\n",
      "Episode length: 267.00 +/- 36.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 267          |\n",
      "|    mean_reward          | 203          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4028528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068736873 |\n",
      "|    clip_fraction        | 0.0529       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 24560        |\n",
      "|    policy_gradient_loss | -0.00048     |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1968    |\n",
      "|    time_elapsed    | 4564    |\n",
      "|    total_timesteps | 4030464 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1969         |\n",
      "|    time_elapsed         | 4566         |\n",
      "|    total_timesteps      | 4032512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040257373 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.8         |\n",
      "|    n_updates            | 24570        |\n",
      "|    policy_gradient_loss | -0.000821    |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1970        |\n",
      "|    time_elapsed         | 4568        |\n",
      "|    total_timesteps      | 4034560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007993152 |\n",
      "|    clip_fraction        | 0.0707      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.8         |\n",
      "|    n_updates            | 24580       |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1971        |\n",
      "|    time_elapsed         | 4570        |\n",
      "|    total_timesteps      | 4036608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003839885 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 24590       |\n",
      "|    policy_gradient_loss | 0.00241     |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4038528, episode_reward=157.29 +/- 106.63\n",
      "Episode length: 438.60 +/- 281.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 439         |\n",
      "|    mean_reward          | 157         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4038528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006816067 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 24600       |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1972    |\n",
      "|    time_elapsed    | 4573    |\n",
      "|    total_timesteps | 4038656 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1973        |\n",
      "|    time_elapsed         | 4575        |\n",
      "|    total_timesteps      | 4040704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004734113 |\n",
      "|    clip_fraction        | 0.0349      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 24610       |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 58          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1974         |\n",
      "|    time_elapsed         | 4577         |\n",
      "|    total_timesteps      | 4042752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016898339 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 24620        |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1975        |\n",
      "|    time_elapsed         | 4579        |\n",
      "|    total_timesteps      | 4044800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002768006 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.11       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 24630       |\n",
      "|    policy_gradient_loss | -0.000832   |\n",
      "|    value_loss           | 32.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1976         |\n",
      "|    time_elapsed         | 4581         |\n",
      "|    total_timesteps      | 4046848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030825036 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.224       |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 104          |\n",
      "|    n_updates            | 24640        |\n",
      "|    policy_gradient_loss | -0.000429    |\n",
      "|    value_loss           | 79.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4048528, episode_reward=212.52 +/- 25.36\n",
      "Episode length: 306.40 +/- 27.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | 213         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4048528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020586092 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.77        |\n",
      "|    n_updates            | 24650       |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1977    |\n",
      "|    time_elapsed    | 4584    |\n",
      "|    total_timesteps | 4048896 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1978        |\n",
      "|    time_elapsed         | 4586        |\n",
      "|    total_timesteps      | 4050944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002088291 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.3        |\n",
      "|    n_updates            | 24660       |\n",
      "|    policy_gradient_loss | 0.000403    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1979        |\n",
      "|    time_elapsed         | 4589        |\n",
      "|    total_timesteps      | 4052992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007207336 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 24670       |\n",
      "|    policy_gradient_loss | 0.000273    |\n",
      "|    value_loss           | 10.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1980        |\n",
      "|    time_elapsed         | 4591        |\n",
      "|    total_timesteps      | 4055040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012199026 |\n",
      "|    clip_fraction        | 0.0764      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.354      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 24680       |\n",
      "|    policy_gradient_loss | 0.00167     |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1981        |\n",
      "|    time_elapsed         | 4593        |\n",
      "|    total_timesteps      | 4057088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006603151 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.47        |\n",
      "|    n_updates            | 24690       |\n",
      "|    policy_gradient_loss | 0.000435    |\n",
      "|    value_loss           | 23.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4058528, episode_reward=106.16 +/- 122.99\n",
      "Episode length: 450.60 +/- 276.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 451          |\n",
      "|    mean_reward          | 106          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4058528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029470646 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.176       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 24700        |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 26.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1982    |\n",
      "|    time_elapsed    | 4596    |\n",
      "|    total_timesteps | 4059136 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1983         |\n",
      "|    time_elapsed         | 4598         |\n",
      "|    total_timesteps      | 4061184      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026174197 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 24710        |\n",
      "|    policy_gradient_loss | -0.000376    |\n",
      "|    value_loss           | 49.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1984        |\n",
      "|    time_elapsed         | 4600        |\n",
      "|    total_timesteps      | 4063232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016070267 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.57        |\n",
      "|    n_updates            | 24720       |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1985         |\n",
      "|    time_elapsed         | 4602         |\n",
      "|    total_timesteps      | 4065280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027540298 |\n",
      "|    clip_fraction        | 0.0262       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.149       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 24730        |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 50.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1986         |\n",
      "|    time_elapsed         | 4604         |\n",
      "|    total_timesteps      | 4067328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017947122 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.58         |\n",
      "|    n_updates            | 24740        |\n",
      "|    policy_gradient_loss | -0.000126    |\n",
      "|    value_loss           | 11.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4068528, episode_reward=170.47 +/- 96.15\n",
      "Episode length: 354.60 +/- 162.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 355         |\n",
      "|    mean_reward          | 170         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4068528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005575278 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.4        |\n",
      "|    n_updates            | 24750       |\n",
      "|    policy_gradient_loss | 0.000482    |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1987    |\n",
      "|    time_elapsed    | 4607    |\n",
      "|    total_timesteps | 4069376 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1988        |\n",
      "|    time_elapsed         | 4610        |\n",
      "|    total_timesteps      | 4071424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008476182 |\n",
      "|    clip_fraction        | 0.0248      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.66        |\n",
      "|    n_updates            | 24760       |\n",
      "|    policy_gradient_loss | -0.000356   |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1989        |\n",
      "|    time_elapsed         | 4612        |\n",
      "|    total_timesteps      | 4073472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014254264 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 24770       |\n",
      "|    policy_gradient_loss | -0.000906   |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 883        |\n",
      "|    iterations           | 1990       |\n",
      "|    time_elapsed         | 4613       |\n",
      "|    total_timesteps      | 4075520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00563862 |\n",
      "|    clip_fraction        | 0.055      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.298     |\n",
      "|    explained_variance   | 0.489      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.1       |\n",
      "|    n_updates            | 24780      |\n",
      "|    policy_gradient_loss | -0.00236   |\n",
      "|    value_loss           | 39.6       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1991         |\n",
      "|    time_elapsed         | 4615         |\n",
      "|    total_timesteps      | 4077568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023915884 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.133       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38           |\n",
      "|    n_updates            | 24790        |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    value_loss           | 81.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4078528, episode_reward=195.24 +/- 106.80\n",
      "Episode length: 287.40 +/- 59.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 287         |\n",
      "|    mean_reward          | 195         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4078528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004080327 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 24800       |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 91.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1992    |\n",
      "|    time_elapsed    | 4618    |\n",
      "|    total_timesteps | 4079616 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1993        |\n",
      "|    time_elapsed         | 4620        |\n",
      "|    total_timesteps      | 4081664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003197934 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 24810       |\n",
      "|    policy_gradient_loss | 0.00018     |\n",
      "|    value_loss           | 73.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1994        |\n",
      "|    time_elapsed         | 4622        |\n",
      "|    total_timesteps      | 4083712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003483228 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.25        |\n",
      "|    n_updates            | 24820       |\n",
      "|    policy_gradient_loss | 0.000828    |\n",
      "|    value_loss           | 30.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1995         |\n",
      "|    time_elapsed         | 4624         |\n",
      "|    total_timesteps      | 4085760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038405922 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.795        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.8         |\n",
      "|    n_updates            | 24830        |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1996        |\n",
      "|    time_elapsed         | 4627        |\n",
      "|    total_timesteps      | 4087808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004715514 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.376      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.359       |\n",
      "|    n_updates            | 24840       |\n",
      "|    policy_gradient_loss | 0.0013      |\n",
      "|    value_loss           | 8.66        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4088528, episode_reward=156.31 +/- 54.14\n",
      "Episode length: 712.60 +/- 352.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 713         |\n",
      "|    mean_reward          | 156         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4088528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013486924 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.64        |\n",
      "|    n_updates            | 24850       |\n",
      "|    policy_gradient_loss | -0.000454   |\n",
      "|    value_loss           | 29.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 1997    |\n",
      "|    time_elapsed    | 4631    |\n",
      "|    total_timesteps | 4089856 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 1998         |\n",
      "|    time_elapsed         | 4633         |\n",
      "|    total_timesteps      | 4091904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037299334 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.84         |\n",
      "|    n_updates            | 24860        |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 30.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 1999        |\n",
      "|    time_elapsed         | 4635        |\n",
      "|    total_timesteps      | 4093952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008514069 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 24870       |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 30          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2000         |\n",
      "|    time_elapsed         | 4637         |\n",
      "|    total_timesteps      | 4096000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024036206 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 24880        |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2001         |\n",
      "|    time_elapsed         | 4639         |\n",
      "|    total_timesteps      | 4098048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073301485 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 102          |\n",
      "|    n_updates            | 24890        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4098528, episode_reward=176.91 +/- 91.46\n",
      "Episode length: 434.20 +/- 284.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 434          |\n",
      "|    mean_reward          | 177          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4098528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025301762 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.6          |\n",
      "|    n_updates            | 24900        |\n",
      "|    policy_gradient_loss | -0.000649    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 2002    |\n",
      "|    time_elapsed    | 4642    |\n",
      "|    total_timesteps | 4100096 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2003         |\n",
      "|    time_elapsed         | 4644         |\n",
      "|    total_timesteps      | 4102144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038359778 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.17         |\n",
      "|    n_updates            | 24910        |\n",
      "|    policy_gradient_loss | -0.000824    |\n",
      "|    value_loss           | 48.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2004         |\n",
      "|    time_elapsed         | 4646         |\n",
      "|    total_timesteps      | 4104192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049799434 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.165       |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 24920        |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    value_loss           | 73.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2005         |\n",
      "|    time_elapsed         | 4648         |\n",
      "|    total_timesteps      | 4106240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045230407 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 24930        |\n",
      "|    policy_gradient_loss | -0.00094     |\n",
      "|    value_loss           | 80           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2006         |\n",
      "|    time_elapsed         | 4650         |\n",
      "|    total_timesteps      | 4108288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034125582 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.68         |\n",
      "|    n_updates            | 24940        |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 67           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4108528, episode_reward=183.90 +/- 115.73\n",
      "Episode length: 280.00 +/- 35.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 184          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4108528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037824088 |\n",
      "|    clip_fraction        | 0.0543       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.82         |\n",
      "|    n_updates            | 24950        |\n",
      "|    policy_gradient_loss | 0.000382     |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 2007    |\n",
      "|    time_elapsed    | 4653    |\n",
      "|    total_timesteps | 4110336 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2008         |\n",
      "|    time_elapsed         | 4655         |\n",
      "|    total_timesteps      | 4112384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047131306 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.235       |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 24960        |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 37.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2009         |\n",
      "|    time_elapsed         | 4657         |\n",
      "|    total_timesteps      | 4114432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045229467 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.16         |\n",
      "|    n_updates            | 24970        |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2010         |\n",
      "|    time_elapsed         | 4659         |\n",
      "|    total_timesteps      | 4116480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044682957 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 24980        |\n",
      "|    policy_gradient_loss | -0.000713    |\n",
      "|    value_loss           | 42.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4118528, episode_reward=156.68 +/- 87.03\n",
      "Episode length: 279.20 +/- 40.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 157          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4118528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027774803 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.5          |\n",
      "|    n_updates            | 24990        |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 96.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 2011    |\n",
      "|    time_elapsed    | 4662    |\n",
      "|    total_timesteps | 4118528 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2012         |\n",
      "|    time_elapsed         | 4664         |\n",
      "|    total_timesteps      | 4120576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025908386 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.3         |\n",
      "|    n_updates            | 25000        |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 90.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 2013        |\n",
      "|    time_elapsed         | 4666        |\n",
      "|    total_timesteps      | 4122624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005497792 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.3        |\n",
      "|    n_updates            | 25010       |\n",
      "|    policy_gradient_loss | 0.000201    |\n",
      "|    value_loss           | 79.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2014         |\n",
      "|    time_elapsed         | 4667         |\n",
      "|    total_timesteps      | 4124672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047517614 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49           |\n",
      "|    n_updates            | 25020        |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    value_loss           | 79.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 2015        |\n",
      "|    time_elapsed         | 4669        |\n",
      "|    total_timesteps      | 4126720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002274637 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.119      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 25030       |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4128528, episode_reward=179.71 +/- 87.56\n",
      "Episode length: 313.60 +/- 39.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 314          |\n",
      "|    mean_reward          | 180          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4128528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074425936 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.5         |\n",
      "|    n_updates            | 25040        |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 54.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 2016    |\n",
      "|    time_elapsed    | 4672    |\n",
      "|    total_timesteps | 4128768 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2017         |\n",
      "|    time_elapsed         | 4674         |\n",
      "|    total_timesteps      | 4130816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031281838 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.6         |\n",
      "|    n_updates            | 25050        |\n",
      "|    policy_gradient_loss | 0.0013       |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 883        |\n",
      "|    iterations           | 2018       |\n",
      "|    time_elapsed         | 4676       |\n",
      "|    total_timesteps      | 4132864    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00454057 |\n",
      "|    clip_fraction        | 0.0651     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.314     |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.3       |\n",
      "|    n_updates            | 25060      |\n",
      "|    policy_gradient_loss | 0.00164    |\n",
      "|    value_loss           | 20         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 2019        |\n",
      "|    time_elapsed         | 4678        |\n",
      "|    total_timesteps      | 4134912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014322782 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.95        |\n",
      "|    n_updates            | 25070       |\n",
      "|    policy_gradient_loss | -0.000911   |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 2020         |\n",
      "|    time_elapsed         | 4680         |\n",
      "|    total_timesteps      | 4136960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041123563 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 25080        |\n",
      "|    policy_gradient_loss | -0.000104    |\n",
      "|    value_loss           | 37.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4138528, episode_reward=131.49 +/- 104.29\n",
      "Episode length: 278.60 +/- 66.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 131          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4138528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064095375 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.124       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 25090        |\n",
      "|    policy_gradient_loss | -0.00012     |\n",
      "|    value_loss           | 67.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 883     |\n",
      "|    iterations      | 2021    |\n",
      "|    time_elapsed    | 4683    |\n",
      "|    total_timesteps | 4139008 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 2022        |\n",
      "|    time_elapsed         | 4685        |\n",
      "|    total_timesteps      | 4141056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004005967 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.6        |\n",
      "|    n_updates            | 25100       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 172         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 2023        |\n",
      "|    time_elapsed         | 4686        |\n",
      "|    total_timesteps      | 4143104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013209142 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 25110       |\n",
      "|    policy_gradient_loss | -0.000511   |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2024         |\n",
      "|    time_elapsed         | 4688         |\n",
      "|    total_timesteps      | 4145152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069391127 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 25120        |\n",
      "|    policy_gradient_loss | 0.000157     |\n",
      "|    value_loss           | 21.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2025         |\n",
      "|    time_elapsed         | 4690         |\n",
      "|    total_timesteps      | 4147200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035709476 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.151       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.3         |\n",
      "|    n_updates            | 25130        |\n",
      "|    policy_gradient_loss | 8.31e-05     |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4148528, episode_reward=166.98 +/- 64.61\n",
      "Episode length: 293.00 +/- 37.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 293          |\n",
      "|    mean_reward          | 167          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4148528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044653704 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 25140        |\n",
      "|    policy_gradient_loss | 0.00131      |\n",
      "|    value_loss           | 19.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2026    |\n",
      "|    time_elapsed    | 4693    |\n",
      "|    total_timesteps | 4149248 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2027         |\n",
      "|    time_elapsed         | 4694         |\n",
      "|    total_timesteps      | 4151296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042139012 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.7          |\n",
      "|    n_updates            | 25150        |\n",
      "|    policy_gradient_loss | -0.000625    |\n",
      "|    value_loss           | 39.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2028        |\n",
      "|    time_elapsed         | 4697        |\n",
      "|    total_timesteps      | 4153344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005273923 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.106      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 25160       |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    value_loss           | 34.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2029        |\n",
      "|    time_elapsed         | 4699        |\n",
      "|    total_timesteps      | 4155392     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010419604 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.99        |\n",
      "|    n_updates            | 25170       |\n",
      "|    policy_gradient_loss | 0.00259     |\n",
      "|    value_loss           | 20.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2030        |\n",
      "|    time_elapsed         | 4700        |\n",
      "|    total_timesteps      | 4157440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015145049 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 79.4        |\n",
      "|    n_updates            | 25180       |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 177         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4158528, episode_reward=154.95 +/- 109.13\n",
      "Episode length: 451.80 +/- 278.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 452          |\n",
      "|    mean_reward          | 155          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4158528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041304994 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.6         |\n",
      "|    n_updates            | 25190        |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 51.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2031    |\n",
      "|    time_elapsed    | 4704    |\n",
      "|    total_timesteps | 4159488 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2032         |\n",
      "|    time_elapsed         | 4706         |\n",
      "|    total_timesteps      | 4161536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062954486 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 25200        |\n",
      "|    policy_gradient_loss | -0.000803    |\n",
      "|    value_loss           | 73.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2033         |\n",
      "|    time_elapsed         | 4708         |\n",
      "|    total_timesteps      | 4163584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037871175 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 25210        |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2034         |\n",
      "|    time_elapsed         | 4710         |\n",
      "|    total_timesteps      | 4165632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027456405 |\n",
      "|    clip_fraction        | 0.0379       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.18         |\n",
      "|    n_updates            | 25220        |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2035         |\n",
      "|    time_elapsed         | 4712         |\n",
      "|    total_timesteps      | 4167680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034707424 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 25230        |\n",
      "|    policy_gradient_loss | 0.000947     |\n",
      "|    value_loss           | 53           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4168528, episode_reward=152.78 +/- 122.47\n",
      "Episode length: 422.80 +/- 290.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 423         |\n",
      "|    mean_reward          | 153         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4168528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004914154 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38          |\n",
      "|    n_updates            | 25240       |\n",
      "|    policy_gradient_loss | -0.000846   |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2036    |\n",
      "|    time_elapsed    | 4716    |\n",
      "|    total_timesteps | 4169728 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2037        |\n",
      "|    time_elapsed         | 4718        |\n",
      "|    total_timesteps      | 4171776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012018877 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.968       |\n",
      "|    n_updates            | 25250       |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    value_loss           | 7.82        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2038         |\n",
      "|    time_elapsed         | 4720         |\n",
      "|    total_timesteps      | 4173824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039252816 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.2         |\n",
      "|    n_updates            | 25260        |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 89.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2039         |\n",
      "|    time_elapsed         | 4722         |\n",
      "|    total_timesteps      | 4175872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048856195 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 25270        |\n",
      "|    policy_gradient_loss | 0.000123     |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2040         |\n",
      "|    time_elapsed         | 4724         |\n",
      "|    total_timesteps      | 4177920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022044028 |\n",
      "|    clip_fraction        | 0.0547       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.72         |\n",
      "|    n_updates            | 25280        |\n",
      "|    policy_gradient_loss | -0.00041     |\n",
      "|    value_loss           | 43.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4178528, episode_reward=193.19 +/- 75.82\n",
      "Episode length: 281.60 +/- 42.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 193         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4178528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010843831 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 25290       |\n",
      "|    policy_gradient_loss | 0.0038      |\n",
      "|    value_loss           | 18.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2041    |\n",
      "|    time_elapsed    | 4727    |\n",
      "|    total_timesteps | 4179968 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2042         |\n",
      "|    time_elapsed         | 4729         |\n",
      "|    total_timesteps      | 4182016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054416386 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43           |\n",
      "|    n_updates            | 25300        |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2043         |\n",
      "|    time_elapsed         | 4731         |\n",
      "|    total_timesteps      | 4184064      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051675327 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.34         |\n",
      "|    n_updates            | 25310        |\n",
      "|    policy_gradient_loss | -0.00736     |\n",
      "|    value_loss           | 70.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2044         |\n",
      "|    time_elapsed         | 4733         |\n",
      "|    total_timesteps      | 4186112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018118889 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.178       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.5          |\n",
      "|    n_updates            | 25320        |\n",
      "|    policy_gradient_loss | 0.000739     |\n",
      "|    value_loss           | 21.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2045        |\n",
      "|    time_elapsed         | 4735        |\n",
      "|    total_timesteps      | 4188160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002434427 |\n",
      "|    clip_fraction        | 0.021       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.33        |\n",
      "|    n_updates            | 25330       |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4188528, episode_reward=192.73 +/- 130.81\n",
      "Episode length: 306.40 +/- 32.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 306          |\n",
      "|    mean_reward          | 193          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4188528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077682287 |\n",
      "|    clip_fraction        | 0.0968       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.426       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.83         |\n",
      "|    n_updates            | 25340        |\n",
      "|    policy_gradient_loss | 0.00255      |\n",
      "|    value_loss           | 5.1          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2046    |\n",
      "|    time_elapsed    | 4738    |\n",
      "|    total_timesteps | 4190208 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2047         |\n",
      "|    time_elapsed         | 4740         |\n",
      "|    total_timesteps      | 4192256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070980843 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.5         |\n",
      "|    n_updates            | 25350        |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 27.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2048         |\n",
      "|    time_elapsed         | 4742         |\n",
      "|    total_timesteps      | 4194304      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036742045 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.226       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 25360        |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 12.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2049         |\n",
      "|    time_elapsed         | 4744         |\n",
      "|    total_timesteps      | 4196352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057535726 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17           |\n",
      "|    n_updates            | 25370        |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    value_loss           | 62           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2050        |\n",
      "|    time_elapsed         | 4746        |\n",
      "|    total_timesteps      | 4198400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010802023 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 25380       |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 60.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4198528, episode_reward=203.42 +/- 40.97\n",
      "Episode length: 300.60 +/- 26.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 301          |\n",
      "|    mean_reward          | 203          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4198528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021636086 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.5         |\n",
      "|    n_updates            | 25390        |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 74.1         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2051    |\n",
      "|    time_elapsed    | 4749    |\n",
      "|    total_timesteps | 4200448 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2052        |\n",
      "|    time_elapsed         | 4751        |\n",
      "|    total_timesteps      | 4202496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004941647 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 25400       |\n",
      "|    policy_gradient_loss | -0.00124    |\n",
      "|    value_loss           | 73.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2053        |\n",
      "|    time_elapsed         | 4753        |\n",
      "|    total_timesteps      | 4204544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002786799 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0859     |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 25410       |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 66.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2054        |\n",
      "|    time_elapsed         | 4754        |\n",
      "|    total_timesteps      | 4206592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002986734 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 25420       |\n",
      "|    policy_gradient_loss | -0.000344   |\n",
      "|    value_loss           | 78.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4208528, episode_reward=213.96 +/- 91.38\n",
      "Episode length: 297.80 +/- 67.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 298         |\n",
      "|    mean_reward          | 214         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4208528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004218081 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 25430       |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2055    |\n",
      "|    time_elapsed    | 4757    |\n",
      "|    total_timesteps | 4208640 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2056         |\n",
      "|    time_elapsed         | 4759         |\n",
      "|    total_timesteps      | 4210688      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068443706 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.597        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.9         |\n",
      "|    n_updates            | 25440        |\n",
      "|    policy_gradient_loss | -0.000721    |\n",
      "|    value_loss           | 152          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2057        |\n",
      "|    time_elapsed         | 4761        |\n",
      "|    total_timesteps      | 4212736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008094294 |\n",
      "|    clip_fraction        | 0.0434      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 25450       |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2058         |\n",
      "|    time_elapsed         | 4764         |\n",
      "|    total_timesteps      | 4214784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025883107 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.66         |\n",
      "|    n_updates            | 25460        |\n",
      "|    policy_gradient_loss | 0.000164     |\n",
      "|    value_loss           | 50.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2059        |\n",
      "|    time_elapsed         | 4766        |\n",
      "|    total_timesteps      | 4216832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003415811 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 25470       |\n",
      "|    policy_gradient_loss | 0.00406     |\n",
      "|    value_loss           | 8.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4218528, episode_reward=131.17 +/- 148.90\n",
      "Episode length: 286.00 +/- 61.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | 131          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4218528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054641725 |\n",
      "|    clip_fraction        | 0.0412       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.318       |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.52         |\n",
      "|    n_updates            | 25480        |\n",
      "|    policy_gradient_loss | 0.00133      |\n",
      "|    value_loss           | 14.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2060    |\n",
      "|    time_elapsed    | 4769    |\n",
      "|    total_timesteps | 4218880 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2061       |\n",
      "|    time_elapsed         | 4771       |\n",
      "|    total_timesteps      | 4220928    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00302915 |\n",
      "|    clip_fraction        | 0.0477     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.182     |\n",
      "|    explained_variance   | 0.593      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 74.3       |\n",
      "|    n_updates            | 25490      |\n",
      "|    policy_gradient_loss | -0.000995  |\n",
      "|    value_loss           | 86         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2062        |\n",
      "|    time_elapsed         | 4773        |\n",
      "|    total_timesteps      | 4222976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007911285 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 25500       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 46.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2063        |\n",
      "|    time_elapsed         | 4775        |\n",
      "|    total_timesteps      | 4225024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004362119 |\n",
      "|    clip_fraction        | 0.0223      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.8        |\n",
      "|    n_updates            | 25510       |\n",
      "|    policy_gradient_loss | -0.000316   |\n",
      "|    value_loss           | 95.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2064         |\n",
      "|    time_elapsed         | 4777         |\n",
      "|    total_timesteps      | 4227072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024513302 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 25520        |\n",
      "|    policy_gradient_loss | -0.000735    |\n",
      "|    value_loss           | 75.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4228528, episode_reward=186.59 +/- 107.84\n",
      "Episode length: 288.40 +/- 41.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 288         |\n",
      "|    mean_reward          | 187         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4228528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010992013 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 25530       |\n",
      "|    policy_gradient_loss | -0.000752   |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2065    |\n",
      "|    time_elapsed    | 4780    |\n",
      "|    total_timesteps | 4229120 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2066         |\n",
      "|    time_elapsed         | 4782         |\n",
      "|    total_timesteps      | 4231168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035597733 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.5         |\n",
      "|    n_updates            | 25540        |\n",
      "|    policy_gradient_loss | -0.00088     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2067        |\n",
      "|    time_elapsed         | 4784        |\n",
      "|    total_timesteps      | 4233216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010824021 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 25550       |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2068         |\n",
      "|    time_elapsed         | 4786         |\n",
      "|    total_timesteps      | 4235264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073829917 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.66         |\n",
      "|    n_updates            | 25560        |\n",
      "|    policy_gradient_loss | 0.000716     |\n",
      "|    value_loss           | 24.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2069         |\n",
      "|    time_elapsed         | 4788         |\n",
      "|    total_timesteps      | 4237312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042856047 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.6         |\n",
      "|    n_updates            | 25570        |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 77           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4238528, episode_reward=226.37 +/- 37.89\n",
      "Episode length: 288.40 +/- 14.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 288          |\n",
      "|    mean_reward          | 226          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4238528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034381403 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 25580        |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 66.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2070    |\n",
      "|    time_elapsed    | 4791    |\n",
      "|    total_timesteps | 4239360 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2071        |\n",
      "|    time_elapsed         | 4793        |\n",
      "|    total_timesteps      | 4241408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013926223 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.78        |\n",
      "|    n_updates            | 25590       |\n",
      "|    policy_gradient_loss | 0.00195     |\n",
      "|    value_loss           | 8.84        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2072         |\n",
      "|    time_elapsed         | 4795         |\n",
      "|    total_timesteps      | 4243456      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019604536 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.07         |\n",
      "|    n_updates            | 25600        |\n",
      "|    policy_gradient_loss | -0.000581    |\n",
      "|    value_loss           | 62.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2073         |\n",
      "|    time_elapsed         | 4797         |\n",
      "|    total_timesteps      | 4245504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015252535 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0895      |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 25610        |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 149          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2074         |\n",
      "|    time_elapsed         | 4799         |\n",
      "|    total_timesteps      | 4247552      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057439087 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.276       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 25620        |\n",
      "|    policy_gradient_loss | 0.000734     |\n",
      "|    value_loss           | 22.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4248528, episode_reward=252.41 +/- 9.74\n",
      "Episode length: 285.20 +/- 14.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4248528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017269723 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.83        |\n",
      "|    n_updates            | 25630       |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2075    |\n",
      "|    time_elapsed    | 4802    |\n",
      "|    total_timesteps | 4249600 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2076         |\n",
      "|    time_elapsed         | 4804         |\n",
      "|    total_timesteps      | 4251648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019415865 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 73.5         |\n",
      "|    n_updates            | 25640        |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 172          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2077         |\n",
      "|    time_elapsed         | 4805         |\n",
      "|    total_timesteps      | 4253696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037714792 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.271       |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.38         |\n",
      "|    n_updates            | 25650        |\n",
      "|    policy_gradient_loss | -0.000361    |\n",
      "|    value_loss           | 27.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2078       |\n",
      "|    time_elapsed         | 4807       |\n",
      "|    total_timesteps      | 4255744    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00478423 |\n",
      "|    clip_fraction        | 0.0311     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0913    |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 62.3       |\n",
      "|    n_updates            | 25660      |\n",
      "|    policy_gradient_loss | 0.00221    |\n",
      "|    value_loss           | 87.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2079        |\n",
      "|    time_elapsed         | 4809        |\n",
      "|    total_timesteps      | 4257792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004448279 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.635       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.5        |\n",
      "|    n_updates            | 25670       |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4258528, episode_reward=185.49 +/- 104.47\n",
      "Episode length: 287.20 +/- 39.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 185          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4258528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039282003 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 25680        |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 165          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2080    |\n",
      "|    time_elapsed    | 4812    |\n",
      "|    total_timesteps | 4259840 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2081        |\n",
      "|    time_elapsed         | 4815        |\n",
      "|    total_timesteps      | 4261888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008173277 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.48        |\n",
      "|    n_updates            | 25690       |\n",
      "|    policy_gradient_loss | 0.00206     |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2082        |\n",
      "|    time_elapsed         | 4817        |\n",
      "|    total_timesteps      | 4263936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250101 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.305      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 25700       |\n",
      "|    policy_gradient_loss | -0.00065    |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2083         |\n",
      "|    time_elapsed         | 4818         |\n",
      "|    total_timesteps      | 4265984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030571853 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.1         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.5          |\n",
      "|    n_updates            | 25710        |\n",
      "|    policy_gradient_loss | -0.000776    |\n",
      "|    value_loss           | 41.1         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 885       |\n",
      "|    iterations           | 2084      |\n",
      "|    time_elapsed         | 4820      |\n",
      "|    total_timesteps      | 4268032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0395764 |\n",
      "|    clip_fraction        | 0.0644    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | 0.772     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.82      |\n",
      "|    n_updates            | 25720     |\n",
      "|    policy_gradient_loss | -0.00346  |\n",
      "|    value_loss           | 23.9      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=4268528, episode_reward=131.35 +/- 96.64\n",
      "Episode length: 473.00 +/- 283.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 473         |\n",
      "|    mean_reward          | 131         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4268528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007391709 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.36        |\n",
      "|    n_updates            | 25730       |\n",
      "|    policy_gradient_loss | -0.00023    |\n",
      "|    value_loss           | 9.78        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2085    |\n",
      "|    time_elapsed    | 4824    |\n",
      "|    total_timesteps | 4270080 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2086         |\n",
      "|    time_elapsed         | 4826         |\n",
      "|    total_timesteps      | 4272128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059162234 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.8         |\n",
      "|    n_updates            | 25740        |\n",
      "|    policy_gradient_loss | -0.00282     |\n",
      "|    value_loss           | 51           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2087        |\n",
      "|    time_elapsed         | 4828        |\n",
      "|    total_timesteps      | 4274176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005294053 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.294      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.77        |\n",
      "|    n_updates            | 25750       |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2088        |\n",
      "|    time_elapsed         | 4830        |\n",
      "|    total_timesteps      | 4276224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005125098 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.97        |\n",
      "|    n_updates            | 25760       |\n",
      "|    policy_gradient_loss | -0.000813   |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2089         |\n",
      "|    time_elapsed         | 4832         |\n",
      "|    total_timesteps      | 4278272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012772637 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0872      |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 25770        |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    value_loss           | 91.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4278528, episode_reward=170.37 +/- 93.04\n",
      "Episode length: 339.20 +/- 116.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 339         |\n",
      "|    mean_reward          | 170         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4278528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009313131 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 25780       |\n",
      "|    policy_gradient_loss | -0.000745   |\n",
      "|    value_loss           | 53.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2090    |\n",
      "|    time_elapsed    | 4835    |\n",
      "|    total_timesteps | 4280320 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2091        |\n",
      "|    time_elapsed         | 4837        |\n",
      "|    total_timesteps      | 4282368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015542001 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 25790       |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2092         |\n",
      "|    time_elapsed         | 4839         |\n",
      "|    total_timesteps      | 4284416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034559611 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 25800        |\n",
      "|    policy_gradient_loss | -0.000534    |\n",
      "|    value_loss           | 76.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2093         |\n",
      "|    time_elapsed         | 4841         |\n",
      "|    total_timesteps      | 4286464      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040253024 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0926      |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.94         |\n",
      "|    n_updates            | 25810        |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 59.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2094         |\n",
      "|    time_elapsed         | 4843         |\n",
      "|    total_timesteps      | 4288512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057919715 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.1         |\n",
      "|    n_updates            | 25820        |\n",
      "|    policy_gradient_loss | 0.0014       |\n",
      "|    value_loss           | 76.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4288528, episode_reward=174.15 +/- 40.38\n",
      "Episode length: 469.20 +/- 271.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4288528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004043294 |\n",
      "|    clip_fraction        | 0.028       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 25830       |\n",
      "|    policy_gradient_loss | 0.000404    |\n",
      "|    value_loss           | 46.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2095    |\n",
      "|    time_elapsed    | 4847    |\n",
      "|    total_timesteps | 4290560 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2096        |\n",
      "|    time_elapsed         | 4849        |\n",
      "|    total_timesteps      | 4292608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005364323 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 25840       |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2097         |\n",
      "|    time_elapsed         | 4851         |\n",
      "|    total_timesteps      | 4294656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038188468 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 25850        |\n",
      "|    policy_gradient_loss | 0.00276      |\n",
      "|    value_loss           | 47.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2098        |\n",
      "|    time_elapsed         | 4853        |\n",
      "|    total_timesteps      | 4296704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003541206 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.8        |\n",
      "|    n_updates            | 25860       |\n",
      "|    policy_gradient_loss | -0.000554   |\n",
      "|    value_loss           | 83          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4298528, episode_reward=194.57 +/- 83.53\n",
      "Episode length: 475.60 +/- 274.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | 195          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4298528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028631245 |\n",
      "|    clip_fraction        | 0.0253       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 25870        |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2099    |\n",
      "|    time_elapsed    | 4857    |\n",
      "|    total_timesteps | 4298752 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2100         |\n",
      "|    time_elapsed         | 4859         |\n",
      "|    total_timesteps      | 4300800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072695226 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.77         |\n",
      "|    n_updates            | 25880        |\n",
      "|    policy_gradient_loss | 0.000801     |\n",
      "|    value_loss           | 37.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2101        |\n",
      "|    time_elapsed         | 4861        |\n",
      "|    total_timesteps      | 4302848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003820348 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 25890       |\n",
      "|    policy_gradient_loss | -0.000699   |\n",
      "|    value_loss           | 89.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2102        |\n",
      "|    time_elapsed         | 4863        |\n",
      "|    total_timesteps      | 4304896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008071301 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.38        |\n",
      "|    n_updates            | 25900       |\n",
      "|    policy_gradient_loss | -0.000233   |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2103         |\n",
      "|    time_elapsed         | 4865         |\n",
      "|    total_timesteps      | 4306944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060183303 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.151       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 94           |\n",
      "|    n_updates            | 25910        |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 90.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4308528, episode_reward=198.91 +/- 46.51\n",
      "Episode length: 433.80 +/- 283.62\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 434       |\n",
      "|    mean_reward          | 199       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 4308528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0059095 |\n",
      "|    clip_fraction        | 0.0308    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.152    |\n",
      "|    explained_variance   | 0.776     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 28.5      |\n",
      "|    n_updates            | 25920     |\n",
      "|    policy_gradient_loss | -0.00143  |\n",
      "|    value_loss           | 109       |\n",
      "---------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2104    |\n",
      "|    time_elapsed    | 4868    |\n",
      "|    total_timesteps | 4308992 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2105        |\n",
      "|    time_elapsed         | 4870        |\n",
      "|    total_timesteps      | 4311040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004157287 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.265      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 25930       |\n",
      "|    policy_gradient_loss | 0.000175    |\n",
      "|    value_loss           | 60.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2106        |\n",
      "|    time_elapsed         | 4872        |\n",
      "|    total_timesteps      | 4313088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003886161 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 25940       |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2107         |\n",
      "|    time_elapsed         | 4874         |\n",
      "|    total_timesteps      | 4315136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032105853 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 25950        |\n",
      "|    policy_gradient_loss | -0.000639    |\n",
      "|    value_loss           | 153          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2108        |\n",
      "|    time_elapsed         | 4876        |\n",
      "|    total_timesteps      | 4317184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005487779 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.29       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.78        |\n",
      "|    n_updates            | 25960       |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    value_loss           | 21.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4318528, episode_reward=258.40 +/- 8.75\n",
      "Episode length: 322.00 +/- 25.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 322          |\n",
      "|    mean_reward          | 258          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4318528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029840511 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 25970        |\n",
      "|    policy_gradient_loss | 0.000153     |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2109    |\n",
      "|    time_elapsed    | 4879    |\n",
      "|    total_timesteps | 4319232 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2110        |\n",
      "|    time_elapsed         | 4881        |\n",
      "|    total_timesteps      | 4321280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007574496 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8           |\n",
      "|    n_updates            | 25980       |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2111        |\n",
      "|    time_elapsed         | 4883        |\n",
      "|    total_timesteps      | 4323328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006363077 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.105      |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 25990       |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2112        |\n",
      "|    time_elapsed         | 4885        |\n",
      "|    total_timesteps      | 4325376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007970912 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 26000       |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    value_loss           | 73.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2113        |\n",
      "|    time_elapsed         | 4887        |\n",
      "|    total_timesteps      | 4327424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016334422 |\n",
      "|    clip_fraction        | 0.0934      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 26010       |\n",
      "|    policy_gradient_loss | -0.0045     |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4328528, episode_reward=161.30 +/- 34.01\n",
      "Episode length: 589.80 +/- 336.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 590         |\n",
      "|    mean_reward          | 161         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4328528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006801947 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 26020       |\n",
      "|    policy_gradient_loss | -0.000859   |\n",
      "|    value_loss           | 47.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2114    |\n",
      "|    time_elapsed    | 4891    |\n",
      "|    total_timesteps | 4329472 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2115        |\n",
      "|    time_elapsed         | 4893        |\n",
      "|    total_timesteps      | 4331520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005266962 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.349      |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 26030       |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2116         |\n",
      "|    time_elapsed         | 4895         |\n",
      "|    total_timesteps      | 4333568      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044440874 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 26040        |\n",
      "|    policy_gradient_loss | 0.00053      |\n",
      "|    value_loss           | 41.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2117        |\n",
      "|    time_elapsed         | 4897        |\n",
      "|    total_timesteps      | 4335616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004696073 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.111      |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 26050       |\n",
      "|    policy_gradient_loss | -0.000566   |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2118         |\n",
      "|    time_elapsed         | 4899         |\n",
      "|    total_timesteps      | 4337664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031923153 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0958      |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.2         |\n",
      "|    n_updates            | 26060        |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    value_loss           | 66.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4338528, episode_reward=162.63 +/- 98.46\n",
      "Episode length: 322.40 +/- 81.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 322         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4338528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007055852 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.19        |\n",
      "|    n_updates            | 26070       |\n",
      "|    policy_gradient_loss | 0.000192    |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2119    |\n",
      "|    time_elapsed    | 4902    |\n",
      "|    total_timesteps | 4339712 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2120       |\n",
      "|    time_elapsed         | 4904       |\n",
      "|    total_timesteps      | 4341760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00594168 |\n",
      "|    clip_fraction        | 0.0322     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.137     |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.09       |\n",
      "|    n_updates            | 26080      |\n",
      "|    policy_gradient_loss | -0.0029    |\n",
      "|    value_loss           | 116        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2121         |\n",
      "|    time_elapsed         | 4906         |\n",
      "|    total_timesteps      | 4343808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029514045 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 174          |\n",
      "|    n_updates            | 26090        |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 221          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2122         |\n",
      "|    time_elapsed         | 4908         |\n",
      "|    total_timesteps      | 4345856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055398326 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 26100        |\n",
      "|    policy_gradient_loss | -0.00241     |\n",
      "|    value_loss           | 72.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2123         |\n",
      "|    time_elapsed         | 4910         |\n",
      "|    total_timesteps      | 4347904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142428875 |\n",
      "|    clip_fraction        | 0.0465       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.82         |\n",
      "|    n_updates            | 26110        |\n",
      "|    policy_gradient_loss | 0.00336      |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4348528, episode_reward=222.48 +/- 18.92\n",
      "Episode length: 335.40 +/- 108.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 335         |\n",
      "|    mean_reward          | 222         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4348528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012079834 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 26120       |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 53.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2124    |\n",
      "|    time_elapsed    | 4913    |\n",
      "|    total_timesteps | 4349952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2125        |\n",
      "|    time_elapsed         | 4915        |\n",
      "|    total_timesteps      | 4352000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003642369 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.226      |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.5        |\n",
      "|    n_updates            | 26130       |\n",
      "|    policy_gradient_loss | -0.000552   |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2126         |\n",
      "|    time_elapsed         | 4917         |\n",
      "|    total_timesteps      | 4354048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024169488 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.1         |\n",
      "|    n_updates            | 26140        |\n",
      "|    policy_gradient_loss | -0.000261    |\n",
      "|    value_loss           | 44.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2127         |\n",
      "|    time_elapsed         | 4919         |\n",
      "|    total_timesteps      | 4356096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107006505 |\n",
      "|    clip_fraction        | 0.0975       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.294       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 26150        |\n",
      "|    policy_gradient_loss | 3.3e-05      |\n",
      "|    value_loss           | 28.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2128         |\n",
      "|    time_elapsed         | 4921         |\n",
      "|    total_timesteps      | 4358144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032334183 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.587        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30           |\n",
      "|    n_updates            | 26160        |\n",
      "|    policy_gradient_loss | -0.000664    |\n",
      "|    value_loss           | 150          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4358528, episode_reward=180.72 +/- 93.82\n",
      "Episode length: 315.80 +/- 64.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 181         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4358528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006106769 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 26170       |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2129    |\n",
      "|    time_elapsed    | 4924    |\n",
      "|    total_timesteps | 4360192 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2130        |\n",
      "|    time_elapsed         | 4926        |\n",
      "|    total_timesteps      | 4362240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009402402 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.8        |\n",
      "|    n_updates            | 26180       |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2131       |\n",
      "|    time_elapsed         | 4929       |\n",
      "|    total_timesteps      | 4364288    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879138 |\n",
      "|    clip_fraction        | 0.0529     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.312     |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.7       |\n",
      "|    n_updates            | 26190      |\n",
      "|    policy_gradient_loss | 0.00147    |\n",
      "|    value_loss           | 54.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2132        |\n",
      "|    time_elapsed         | 4931        |\n",
      "|    total_timesteps      | 4366336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017991561 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 26200       |\n",
      "|    policy_gradient_loss | 0.00599     |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2133         |\n",
      "|    time_elapsed         | 4933         |\n",
      "|    total_timesteps      | 4368384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029705814 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.66         |\n",
      "|    n_updates            | 26210        |\n",
      "|    policy_gradient_loss | -0.000276    |\n",
      "|    value_loss           | 86.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4368528, episode_reward=143.60 +/- 150.88\n",
      "Episode length: 264.80 +/- 62.56\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 265        |\n",
      "|    mean_reward          | 144        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4368528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00900509 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.451     |\n",
      "|    explained_variance   | 0.826      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.08       |\n",
      "|    n_updates            | 26220      |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    value_loss           | 17.3       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2134    |\n",
      "|    time_elapsed    | 4936    |\n",
      "|    total_timesteps | 4370432 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2135        |\n",
      "|    time_elapsed         | 4938        |\n",
      "|    total_timesteps      | 4372480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005963837 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 26230       |\n",
      "|    policy_gradient_loss | -0.0011     |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2136        |\n",
      "|    time_elapsed         | 4940        |\n",
      "|    total_timesteps      | 4374528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008905893 |\n",
      "|    clip_fraction        | 0.0319      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26          |\n",
      "|    n_updates            | 26240       |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 80.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2137         |\n",
      "|    time_elapsed         | 4942         |\n",
      "|    total_timesteps      | 4376576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061196797 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 26250        |\n",
      "|    policy_gradient_loss | 0.000287     |\n",
      "|    value_loss           | 48.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4378528, episode_reward=206.49 +/- 42.11\n",
      "Episode length: 454.80 +/- 279.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 455          |\n",
      "|    mean_reward          | 206          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4378528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039916746 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.39         |\n",
      "|    n_updates            | 26260        |\n",
      "|    policy_gradient_loss | 0.000273     |\n",
      "|    value_loss           | 41.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2138    |\n",
      "|    time_elapsed    | 4946    |\n",
      "|    total_timesteps | 4378624 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2139        |\n",
      "|    time_elapsed         | 4948        |\n",
      "|    total_timesteps      | 4380672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017745536 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.3        |\n",
      "|    n_updates            | 26270       |\n",
      "|    policy_gradient_loss | -0.000488   |\n",
      "|    value_loss           | 34.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2140         |\n",
      "|    time_elapsed         | 4950         |\n",
      "|    total_timesteps      | 4382720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049207304 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.323       |\n",
      "|    explained_variance   | 0.659        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.37         |\n",
      "|    n_updates            | 26280        |\n",
      "|    policy_gradient_loss | -0.000704    |\n",
      "|    value_loss           | 39.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2141         |\n",
      "|    time_elapsed         | 4952         |\n",
      "|    total_timesteps      | 4384768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064165406 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.06         |\n",
      "|    n_updates            | 26290        |\n",
      "|    policy_gradient_loss | -0.000329    |\n",
      "|    value_loss           | 15           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2142         |\n",
      "|    time_elapsed         | 4954         |\n",
      "|    total_timesteps      | 4386816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013774475 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.4         |\n",
      "|    n_updates            | 26300        |\n",
      "|    policy_gradient_loss | 0.000482     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4388528, episode_reward=163.83 +/- 119.23\n",
      "Episode length: 286.40 +/- 27.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 286          |\n",
      "|    mean_reward          | 164          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4388528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019082113 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.3         |\n",
      "|    n_updates            | 26310        |\n",
      "|    policy_gradient_loss | -0.000984    |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2143    |\n",
      "|    time_elapsed    | 4956    |\n",
      "|    total_timesteps | 4388864 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2144         |\n",
      "|    time_elapsed         | 4959         |\n",
      "|    total_timesteps      | 4390912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035565312 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.1         |\n",
      "|    n_updates            | 26320        |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 159          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2145        |\n",
      "|    time_elapsed         | 4961        |\n",
      "|    total_timesteps      | 4392960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008012323 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.79        |\n",
      "|    n_updates            | 26330       |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 38.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2146        |\n",
      "|    time_elapsed         | 4963        |\n",
      "|    total_timesteps      | 4395008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006601689 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33        |\n",
      "|    n_updates            | 26340       |\n",
      "|    policy_gradient_loss | -0.000555   |\n",
      "|    value_loss           | 74.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2147        |\n",
      "|    time_elapsed         | 4965        |\n",
      "|    total_timesteps      | 4397056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013146881 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 26350       |\n",
      "|    policy_gradient_loss | 0.000173    |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4398528, episode_reward=160.35 +/- 69.04\n",
      "Episode length: 321.40 +/- 34.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | 160          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4398528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035599386 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.306       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 26360        |\n",
      "|    policy_gradient_loss | -0.000809    |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2148    |\n",
      "|    time_elapsed    | 4968    |\n",
      "|    total_timesteps | 4399104 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2149        |\n",
      "|    time_elapsed         | 4970        |\n",
      "|    total_timesteps      | 4401152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008151768 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 26370       |\n",
      "|    policy_gradient_loss | 0.000656    |\n",
      "|    value_loss           | 33.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2150         |\n",
      "|    time_elapsed         | 4972         |\n",
      "|    total_timesteps      | 4403200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023873243 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.9         |\n",
      "|    n_updates            | 26380        |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 121          |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 885       |\n",
      "|    iterations           | 2151      |\n",
      "|    time_elapsed         | 4974      |\n",
      "|    total_timesteps      | 4405248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1104338 |\n",
      "|    clip_fraction        | 0.0545    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.917     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 24.1      |\n",
      "|    n_updates            | 26390     |\n",
      "|    policy_gradient_loss | 0.00861   |\n",
      "|    value_loss           | 64.9      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2152         |\n",
      "|    time_elapsed         | 4976         |\n",
      "|    total_timesteps      | 4407296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024157343 |\n",
      "|    clip_fraction        | 0.0276       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.101       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.5         |\n",
      "|    n_updates            | 26400        |\n",
      "|    policy_gradient_loss | -0.00025     |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4408528, episode_reward=202.06 +/- 75.62\n",
      "Episode length: 541.20 +/- 273.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 541         |\n",
      "|    mean_reward          | 202         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4408528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005250847 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.57        |\n",
      "|    n_updates            | 26410       |\n",
      "|    policy_gradient_loss | 0.000989    |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2153    |\n",
      "|    time_elapsed    | 4980    |\n",
      "|    total_timesteps | 4409344 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2154         |\n",
      "|    time_elapsed         | 4981         |\n",
      "|    total_timesteps      | 4411392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054113865 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84         |\n",
      "|    n_updates            | 26420        |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 95.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2155        |\n",
      "|    time_elapsed         | 4984        |\n",
      "|    total_timesteps      | 4413440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004860622 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0962     |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 26430       |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 60.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2156        |\n",
      "|    time_elapsed         | 4986        |\n",
      "|    total_timesteps      | 4415488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011595765 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.35        |\n",
      "|    n_updates            | 26440       |\n",
      "|    policy_gradient_loss | 0.000213    |\n",
      "|    value_loss           | 35.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2157         |\n",
      "|    time_elapsed         | 4988         |\n",
      "|    total_timesteps      | 4417536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053808717 |\n",
      "|    clip_fraction        | 0.0653       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.6         |\n",
      "|    n_updates            | 26450        |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 77.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4418528, episode_reward=125.59 +/- 91.40\n",
      "Episode length: 307.40 +/- 48.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | 126         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4418528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016644843 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.288      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 26460       |\n",
      "|    policy_gradient_loss | -0.000793   |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2158    |\n",
      "|    time_elapsed    | 4991    |\n",
      "|    total_timesteps | 4419584 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2159         |\n",
      "|    time_elapsed         | 4993         |\n",
      "|    total_timesteps      | 4421632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048107915 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.54         |\n",
      "|    n_updates            | 26470        |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 99.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2160         |\n",
      "|    time_elapsed         | 4995         |\n",
      "|    total_timesteps      | 4423680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031505073 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.218       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.33         |\n",
      "|    n_updates            | 26480        |\n",
      "|    policy_gradient_loss | 0.00171      |\n",
      "|    value_loss           | 49.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2161        |\n",
      "|    time_elapsed         | 4998        |\n",
      "|    total_timesteps      | 4425728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003352865 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 26490       |\n",
      "|    policy_gradient_loss | 0.000839    |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2162        |\n",
      "|    time_elapsed         | 5000        |\n",
      "|    total_timesteps      | 4427776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016832257 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.07        |\n",
      "|    n_updates            | 26500       |\n",
      "|    policy_gradient_loss | 0.00341     |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4428528, episode_reward=235.69 +/- 37.05\n",
      "Episode length: 294.00 +/- 26.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 294          |\n",
      "|    mean_reward          | 236          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4428528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130619425 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.252       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.48         |\n",
      "|    n_updates            | 26510        |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 32.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2163    |\n",
      "|    time_elapsed    | 5003    |\n",
      "|    total_timesteps | 4429824 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2164         |\n",
      "|    time_elapsed         | 5005         |\n",
      "|    total_timesteps      | 4431872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013711291 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 26520        |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 93.1         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 885       |\n",
      "|    iterations           | 2165      |\n",
      "|    time_elapsed         | 5008      |\n",
      "|    total_timesteps      | 4433920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0172759 |\n",
      "|    clip_fraction        | 0.0918    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.433    |\n",
      "|    explained_variance   | 0.804     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.61      |\n",
      "|    n_updates            | 26530     |\n",
      "|    policy_gradient_loss | 0.00197   |\n",
      "|    value_loss           | 27.2      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2166       |\n",
      "|    time_elapsed         | 5010       |\n",
      "|    total_timesteps      | 4435968    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01980875 |\n",
      "|    clip_fraction        | 0.0557     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.31      |\n",
      "|    explained_variance   | 0.757      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.87       |\n",
      "|    n_updates            | 26540      |\n",
      "|    policy_gradient_loss | -0.00223   |\n",
      "|    value_loss           | 36.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2167        |\n",
      "|    time_elapsed         | 5011        |\n",
      "|    total_timesteps      | 4438016     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003898303 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 26550       |\n",
      "|    policy_gradient_loss | -0.000746   |\n",
      "|    value_loss           | 76.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4438528, episode_reward=170.84 +/- 83.14\n",
      "Episode length: 287.40 +/- 67.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 171          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4438528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032526285 |\n",
      "|    clip_fraction        | 0.0251       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.7         |\n",
      "|    n_updates            | 26560        |\n",
      "|    policy_gradient_loss | -0.000933    |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2168    |\n",
      "|    time_elapsed    | 5014    |\n",
      "|    total_timesteps | 4440064 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2169        |\n",
      "|    time_elapsed         | 5016        |\n",
      "|    total_timesteps      | 4442112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012178598 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 75.5        |\n",
      "|    n_updates            | 26570       |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    value_loss           | 92.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2170        |\n",
      "|    time_elapsed         | 5018        |\n",
      "|    total_timesteps      | 4444160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011666413 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.77        |\n",
      "|    n_updates            | 26580       |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2171        |\n",
      "|    time_elapsed         | 5021        |\n",
      "|    total_timesteps      | 4446208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007247787 |\n",
      "|    clip_fraction        | 0.0234      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.11       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.5        |\n",
      "|    n_updates            | 26590       |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2172         |\n",
      "|    time_elapsed         | 5023         |\n",
      "|    total_timesteps      | 4448256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048558936 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.23         |\n",
      "|    n_updates            | 26600        |\n",
      "|    policy_gradient_loss | 0.000406     |\n",
      "|    value_loss           | 34.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4448528, episode_reward=146.39 +/- 110.70\n",
      "Episode length: 267.00 +/- 56.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 267          |\n",
      "|    mean_reward          | 146          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4448528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012894551 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.7         |\n",
      "|    n_updates            | 26610        |\n",
      "|    policy_gradient_loss | -0.00044     |\n",
      "|    value_loss           | 59           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2173    |\n",
      "|    time_elapsed    | 5026    |\n",
      "|    total_timesteps | 4450304 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2174        |\n",
      "|    time_elapsed         | 5028        |\n",
      "|    total_timesteps      | 4452352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002137348 |\n",
      "|    clip_fraction        | 0.0194      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 26620       |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 207         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2175         |\n",
      "|    time_elapsed         | 5030         |\n",
      "|    total_timesteps      | 4454400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072042225 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.328       |\n",
      "|    explained_variance   | 0.629        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.19         |\n",
      "|    n_updates            | 26630        |\n",
      "|    policy_gradient_loss | -0.000215    |\n",
      "|    value_loss           | 34           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2176         |\n",
      "|    time_elapsed         | 5032         |\n",
      "|    total_timesteps      | 4456448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034381987 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.266       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.68         |\n",
      "|    n_updates            | 26640        |\n",
      "|    policy_gradient_loss | 0.000781     |\n",
      "|    value_loss           | 47.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2177        |\n",
      "|    time_elapsed         | 5034        |\n",
      "|    total_timesteps      | 4458496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006357698 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.148      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 26650       |\n",
      "|    policy_gradient_loss | 0.000405    |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4458528, episode_reward=186.43 +/- 98.31\n",
      "Episode length: 324.60 +/- 103.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 186          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4458528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052464525 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.285       |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 26660        |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    value_loss           | 50.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2178    |\n",
      "|    time_elapsed    | 5037    |\n",
      "|    total_timesteps | 4460544 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2179        |\n",
      "|    time_elapsed         | 5039        |\n",
      "|    total_timesteps      | 4462592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019996885 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.289      |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.96        |\n",
      "|    n_updates            | 26670       |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 31          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2180        |\n",
      "|    time_elapsed         | 5042        |\n",
      "|    total_timesteps      | 4464640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007666547 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.1        |\n",
      "|    n_updates            | 26680       |\n",
      "|    policy_gradient_loss | -0.000499   |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2181        |\n",
      "|    time_elapsed         | 5044        |\n",
      "|    total_timesteps      | 4466688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068243444 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.38        |\n",
      "|    n_updates            | 26690       |\n",
      "|    policy_gradient_loss | 0.00015     |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4468528, episode_reward=165.78 +/- 77.00\n",
      "Episode length: 436.00 +/- 283.71\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 436          |\n",
      "|    mean_reward          | 166          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4468528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020387485 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 26700        |\n",
      "|    policy_gradient_loss | 0.000244     |\n",
      "|    value_loss           | 98.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2182    |\n",
      "|    time_elapsed    | 5047    |\n",
      "|    total_timesteps | 4468736 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2183         |\n",
      "|    time_elapsed         | 5049         |\n",
      "|    total_timesteps      | 4470784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021764503 |\n",
      "|    clip_fraction        | 0.0224       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.124       |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 26710        |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 191          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2184         |\n",
      "|    time_elapsed         | 5052         |\n",
      "|    total_timesteps      | 4472832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046061724 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 26720        |\n",
      "|    policy_gradient_loss | -0.000949    |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2185        |\n",
      "|    time_elapsed         | 5054        |\n",
      "|    total_timesteps      | 4474880     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004581787 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.83        |\n",
      "|    n_updates            | 26730       |\n",
      "|    policy_gradient_loss | 0.00205     |\n",
      "|    value_loss           | 22.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2186         |\n",
      "|    time_elapsed         | 5056         |\n",
      "|    total_timesteps      | 4476928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049397517 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 26740        |\n",
      "|    policy_gradient_loss | -0.000594    |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4478528, episode_reward=111.57 +/- 107.33\n",
      "Episode length: 269.00 +/- 43.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | 112          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4478528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017486995 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.7         |\n",
      "|    n_updates            | 26750        |\n",
      "|    policy_gradient_loss | -0.00036     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2187    |\n",
      "|    time_elapsed    | 5058    |\n",
      "|    total_timesteps | 4478976 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2188        |\n",
      "|    time_elapsed         | 5060        |\n",
      "|    total_timesteps      | 4481024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003943974 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.8        |\n",
      "|    n_updates            | 26760       |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 144         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2189        |\n",
      "|    time_elapsed         | 5062        |\n",
      "|    total_timesteps      | 4483072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011339072 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 26770       |\n",
      "|    policy_gradient_loss | 0.0023      |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2190        |\n",
      "|    time_elapsed         | 5065        |\n",
      "|    total_timesteps      | 4485120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016809653 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 99.6        |\n",
      "|    n_updates            | 26780       |\n",
      "|    policy_gradient_loss | -0.000317   |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2191         |\n",
      "|    time_elapsed         | 5067         |\n",
      "|    total_timesteps      | 4487168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051800925 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.329       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.5         |\n",
      "|    n_updates            | 26790        |\n",
      "|    policy_gradient_loss | 0.000637     |\n",
      "|    value_loss           | 83.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4488528, episode_reward=114.11 +/- 138.86\n",
      "Episode length: 246.00 +/- 45.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 246         |\n",
      "|    mean_reward          | 114         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4488528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007206452 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.91        |\n",
      "|    n_updates            | 26800       |\n",
      "|    policy_gradient_loss | 0.000677    |\n",
      "|    value_loss           | 32          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2192    |\n",
      "|    time_elapsed    | 5069    |\n",
      "|    total_timesteps | 4489216 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2193         |\n",
      "|    time_elapsed         | 5071         |\n",
      "|    total_timesteps      | 4491264      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032655962 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.309       |\n",
      "|    explained_variance   | 0.643        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.4         |\n",
      "|    n_updates            | 26810        |\n",
      "|    policy_gradient_loss | -0.00058     |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2194        |\n",
      "|    time_elapsed         | 5073        |\n",
      "|    total_timesteps      | 4493312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003930122 |\n",
      "|    clip_fraction        | 0.0278      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 26820       |\n",
      "|    policy_gradient_loss | -0.000916   |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2195         |\n",
      "|    time_elapsed         | 5075         |\n",
      "|    total_timesteps      | 4495360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059457743 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.111       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 26830        |\n",
      "|    policy_gradient_loss | -0.000541    |\n",
      "|    value_loss           | 59.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2196         |\n",
      "|    time_elapsed         | 5077         |\n",
      "|    total_timesteps      | 4497408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028553645 |\n",
      "|    clip_fraction        | 0.0268       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 153          |\n",
      "|    n_updates            | 26840        |\n",
      "|    policy_gradient_loss | -0.000259    |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4498528, episode_reward=172.00 +/- 78.97\n",
      "Episode length: 272.60 +/- 20.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 273         |\n",
      "|    mean_reward          | 172         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4498528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004168376 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.34        |\n",
      "|    n_updates            | 26850       |\n",
      "|    policy_gradient_loss | -0.000428   |\n",
      "|    value_loss           | 53.9        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2197    |\n",
      "|    time_elapsed    | 5080    |\n",
      "|    total_timesteps | 4499456 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2198        |\n",
      "|    time_elapsed         | 5082        |\n",
      "|    total_timesteps      | 4501504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006616843 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 26860       |\n",
      "|    policy_gradient_loss | -0.000597   |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2199        |\n",
      "|    time_elapsed         | 5084        |\n",
      "|    total_timesteps      | 4503552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007701968 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 26870       |\n",
      "|    policy_gradient_loss | 5.73e-05    |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2200         |\n",
      "|    time_elapsed         | 5086         |\n",
      "|    total_timesteps      | 4505600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022896104 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.1         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.9         |\n",
      "|    n_updates            | 26880        |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 98.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2201        |\n",
      "|    time_elapsed         | 5088        |\n",
      "|    total_timesteps      | 4507648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012474407 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.44        |\n",
      "|    n_updates            | 26890       |\n",
      "|    policy_gradient_loss | -0.000462   |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4508528, episode_reward=148.27 +/- 125.83\n",
      "Episode length: 283.20 +/- 57.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | 148         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4508528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009379578 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.06        |\n",
      "|    n_updates            | 26900       |\n",
      "|    policy_gradient_loss | -0.000407   |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2202    |\n",
      "|    time_elapsed    | 5091    |\n",
      "|    total_timesteps | 4509696 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2203         |\n",
      "|    time_elapsed         | 5093         |\n",
      "|    total_timesteps      | 4511744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035146396 |\n",
      "|    clip_fraction        | 0.0361       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 26910        |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 91.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2204         |\n",
      "|    time_elapsed         | 5096         |\n",
      "|    total_timesteps      | 4513792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077683125 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.2         |\n",
      "|    n_updates            | 26920        |\n",
      "|    policy_gradient_loss | 0.00171      |\n",
      "|    value_loss           | 53.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2205         |\n",
      "|    time_elapsed         | 5098         |\n",
      "|    total_timesteps      | 4515840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074878423 |\n",
      "|    clip_fraction        | 0.0743       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.278       |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.2         |\n",
      "|    n_updates            | 26930        |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 45.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2206       |\n",
      "|    time_elapsed         | 5100       |\n",
      "|    total_timesteps      | 4517888    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00156335 |\n",
      "|    clip_fraction        | 0.0308     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.1       |\n",
      "|    n_updates            | 26940      |\n",
      "|    policy_gradient_loss | 0.00118    |\n",
      "|    value_loss           | 64.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4518528, episode_reward=88.26 +/- 122.85\n",
      "Episode length: 263.60 +/- 61.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 264         |\n",
      "|    mean_reward          | 88.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4518528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013037344 |\n",
      "|    clip_fraction        | 0.0652      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.32        |\n",
      "|    n_updates            | 26950       |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 77.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2207    |\n",
      "|    time_elapsed    | 5103    |\n",
      "|    total_timesteps | 4519936 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2208        |\n",
      "|    time_elapsed         | 5105        |\n",
      "|    total_timesteps      | 4521984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008394591 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.218      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 26960       |\n",
      "|    policy_gradient_loss | -0.00241    |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2209        |\n",
      "|    time_elapsed         | 5107        |\n",
      "|    total_timesteps      | 4524032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009806333 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.8         |\n",
      "|    n_updates            | 26970       |\n",
      "|    policy_gradient_loss | -0.000492   |\n",
      "|    value_loss           | 63.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2210         |\n",
      "|    time_elapsed         | 5109         |\n",
      "|    total_timesteps      | 4526080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038845243 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.746        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 26980        |\n",
      "|    policy_gradient_loss | 0.000758     |\n",
      "|    value_loss           | 45.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2211         |\n",
      "|    time_elapsed         | 5111         |\n",
      "|    total_timesteps      | 4528128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029428068 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0841      |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60           |\n",
      "|    n_updates            | 26990        |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4528528, episode_reward=197.41 +/- 66.04\n",
      "Episode length: 384.40 +/- 193.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 384          |\n",
      "|    mean_reward          | 197          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4528528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045687556 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.3         |\n",
      "|    n_updates            | 27000        |\n",
      "|    policy_gradient_loss | 0.000186     |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2212    |\n",
      "|    time_elapsed    | 5114    |\n",
      "|    total_timesteps | 4530176 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2213         |\n",
      "|    time_elapsed         | 5116         |\n",
      "|    total_timesteps      | 4532224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046438454 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 27010        |\n",
      "|    policy_gradient_loss | -0.00074     |\n",
      "|    value_loss           | 68           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2214         |\n",
      "|    time_elapsed         | 5118         |\n",
      "|    total_timesteps      | 4534272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065456047 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.244       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45         |\n",
      "|    n_updates            | 27020        |\n",
      "|    policy_gradient_loss | 0.000116     |\n",
      "|    value_loss           | 82.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2215        |\n",
      "|    time_elapsed         | 5120        |\n",
      "|    total_timesteps      | 4536320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009758878 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 27030       |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 93.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2216         |\n",
      "|    time_elapsed         | 5122         |\n",
      "|    total_timesteps      | 4538368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029149167 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 63.7         |\n",
      "|    n_updates            | 27040        |\n",
      "|    policy_gradient_loss | -0.000334    |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4538528, episode_reward=38.28 +/- 118.46\n",
      "Episode length: 236.40 +/- 33.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 236          |\n",
      "|    mean_reward          | 38.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4538528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068460167 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.301       |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.34         |\n",
      "|    n_updates            | 27050        |\n",
      "|    policy_gradient_loss | 0.000899     |\n",
      "|    value_loss           | 71.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2217    |\n",
      "|    time_elapsed    | 5125    |\n",
      "|    total_timesteps | 4540416 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2218       |\n",
      "|    time_elapsed         | 5127       |\n",
      "|    total_timesteps      | 4542464    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01075165 |\n",
      "|    clip_fraction        | 0.0609     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.302     |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.91       |\n",
      "|    n_updates            | 27060      |\n",
      "|    policy_gradient_loss | 0.000684   |\n",
      "|    value_loss           | 28.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2219         |\n",
      "|    time_elapsed         | 5129         |\n",
      "|    total_timesteps      | 4544512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034460258 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.714        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 105          |\n",
      "|    n_updates            | 27070        |\n",
      "|    policy_gradient_loss | -0.000582    |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2220        |\n",
      "|    time_elapsed         | 5131        |\n",
      "|    total_timesteps      | 4546560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007881916 |\n",
      "|    clip_fraction        | 0.0523      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.41        |\n",
      "|    n_updates            | 27080       |\n",
      "|    policy_gradient_loss | -0.000875   |\n",
      "|    value_loss           | 35.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4548528, episode_reward=151.04 +/- 137.79\n",
      "Episode length: 285.20 +/- 53.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4548528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031399527 |\n",
      "|    clip_fraction        | 0.0312       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 27090        |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 88.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2221    |\n",
      "|    time_elapsed    | 5134    |\n",
      "|    total_timesteps | 4548608 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2222        |\n",
      "|    time_elapsed         | 5136        |\n",
      "|    total_timesteps      | 4550656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010271917 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.148      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 27100       |\n",
      "|    policy_gradient_loss | 0.00199     |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2223        |\n",
      "|    time_elapsed         | 5138        |\n",
      "|    total_timesteps      | 4552704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004410725 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 27110       |\n",
      "|    policy_gradient_loss | 0.00287     |\n",
      "|    value_loss           | 95.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2224         |\n",
      "|    time_elapsed         | 5140         |\n",
      "|    total_timesteps      | 4554752      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070954915 |\n",
      "|    clip_fraction        | 0.0844       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.387       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 27120        |\n",
      "|    policy_gradient_loss | -0.000387    |\n",
      "|    value_loss           | 46.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2225        |\n",
      "|    time_elapsed         | 5142        |\n",
      "|    total_timesteps      | 4556800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003044147 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 27130       |\n",
      "|    policy_gradient_loss | 0.000593    |\n",
      "|    value_loss           | 66.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4558528, episode_reward=177.90 +/- 140.26\n",
      "Episode length: 270.00 +/- 45.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 178         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4558528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008976252 |\n",
      "|    clip_fraction        | 0.0345      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 27140       |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2226    |\n",
      "|    time_elapsed    | 5145    |\n",
      "|    total_timesteps | 4558848 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2227         |\n",
      "|    time_elapsed         | 5147         |\n",
      "|    total_timesteps      | 4560896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034958362 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.111       |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.98         |\n",
      "|    n_updates            | 27150        |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 68.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2228         |\n",
      "|    time_elapsed         | 5149         |\n",
      "|    total_timesteps      | 4562944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020812387 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.151       |\n",
      "|    explained_variance   | 0.789        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 27160        |\n",
      "|    policy_gradient_loss | -0.000617    |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2229         |\n",
      "|    time_elapsed         | 5151         |\n",
      "|    total_timesteps      | 4564992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033397125 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.3         |\n",
      "|    n_updates            | 27170        |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 84.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2230         |\n",
      "|    time_elapsed         | 5153         |\n",
      "|    total_timesteps      | 4567040      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021317694 |\n",
      "|    clip_fraction        | 0.0209       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0.711        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.3         |\n",
      "|    n_updates            | 27180        |\n",
      "|    policy_gradient_loss | 0.000258     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4568528, episode_reward=154.56 +/- 105.47\n",
      "Episode length: 287.40 +/- 47.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 155          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4568528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106348945 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 27190        |\n",
      "|    policy_gradient_loss | -0.000558    |\n",
      "|    value_loss           | 47.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2231    |\n",
      "|    time_elapsed    | 5155    |\n",
      "|    total_timesteps | 4569088 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2232       |\n",
      "|    time_elapsed         | 5158       |\n",
      "|    total_timesteps      | 4571136    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00417808 |\n",
      "|    clip_fraction        | 0.028      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.107     |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 98.6       |\n",
      "|    n_updates            | 27200      |\n",
      "|    policy_gradient_loss | -0.00169   |\n",
      "|    value_loss           | 162        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2233        |\n",
      "|    time_elapsed         | 5160        |\n",
      "|    total_timesteps      | 4573184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009118099 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.107      |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.6        |\n",
      "|    n_updates            | 27210       |\n",
      "|    policy_gradient_loss | 0.000268    |\n",
      "|    value_loss           | 95.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2234         |\n",
      "|    time_elapsed         | 5162         |\n",
      "|    total_timesteps      | 4575232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037609162 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0994      |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.7         |\n",
      "|    n_updates            | 27220        |\n",
      "|    policy_gradient_loss | -0.000603    |\n",
      "|    value_loss           | 42.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2235       |\n",
      "|    time_elapsed         | 5164       |\n",
      "|    total_timesteps      | 4577280    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00236121 |\n",
      "|    clip_fraction        | 0.0222     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.2       |\n",
      "|    n_updates            | 27230      |\n",
      "|    policy_gradient_loss | -0.000831  |\n",
      "|    value_loss           | 67.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4578528, episode_reward=147.59 +/- 78.53\n",
      "Episode length: 288.40 +/- 48.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 288          |\n",
      "|    mean_reward          | 148          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4578528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031034607 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 121          |\n",
      "|    n_updates            | 27240        |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    value_loss           | 94           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2236    |\n",
      "|    time_elapsed    | 5166    |\n",
      "|    total_timesteps | 4579328 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2237        |\n",
      "|    time_elapsed         | 5168        |\n",
      "|    total_timesteps      | 4581376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009812288 |\n",
      "|    clip_fraction        | 0.0671      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 27250       |\n",
      "|    policy_gradient_loss | 0.00105     |\n",
      "|    value_loss           | 47.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2238        |\n",
      "|    time_elapsed         | 5171        |\n",
      "|    total_timesteps      | 4583424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010632955 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 27260       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 91.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2239        |\n",
      "|    time_elapsed         | 5173        |\n",
      "|    total_timesteps      | 4585472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008192343 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.79        |\n",
      "|    n_updates            | 27270       |\n",
      "|    policy_gradient_loss | 0.000127    |\n",
      "|    value_loss           | 31.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2240        |\n",
      "|    time_elapsed         | 5175        |\n",
      "|    total_timesteps      | 4587520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007094012 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.137      |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.49        |\n",
      "|    n_updates            | 27280       |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 42.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4588528, episode_reward=172.10 +/- 52.62\n",
      "Episode length: 428.00 +/- 286.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 428          |\n",
      "|    mean_reward          | 172          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4588528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027992264 |\n",
      "|    clip_fraction        | 0.0328       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.9         |\n",
      "|    n_updates            | 27290        |\n",
      "|    policy_gradient_loss | -0.000894    |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2241    |\n",
      "|    time_elapsed    | 5179    |\n",
      "|    total_timesteps | 4589568 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2242         |\n",
      "|    time_elapsed         | 5181         |\n",
      "|    total_timesteps      | 4591616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024338998 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.651        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 27300        |\n",
      "|    policy_gradient_loss | 0.000261     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2243        |\n",
      "|    time_elapsed         | 5183        |\n",
      "|    total_timesteps      | 4593664     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009892279 |\n",
      "|    clip_fraction        | 0.0913      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 27310       |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    value_loss           | 64.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2244        |\n",
      "|    time_elapsed         | 5185        |\n",
      "|    total_timesteps      | 4595712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010985151 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47          |\n",
      "|    n_updates            | 27320       |\n",
      "|    policy_gradient_loss | -0.000157   |\n",
      "|    value_loss           | 80.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2245         |\n",
      "|    time_elapsed         | 5187         |\n",
      "|    total_timesteps      | 4597760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048119547 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.75         |\n",
      "|    n_updates            | 27330        |\n",
      "|    policy_gradient_loss | 0.00033      |\n",
      "|    value_loss           | 56.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4598528, episode_reward=178.09 +/- 100.11\n",
      "Episode length: 273.80 +/- 30.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | 178          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4598528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056448635 |\n",
      "|    clip_fraction        | 0.0714       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 27340        |\n",
      "|    policy_gradient_loss | -0.00242     |\n",
      "|    value_loss           | 80.9         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2246    |\n",
      "|    time_elapsed    | 5190    |\n",
      "|    total_timesteps | 4599808 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2247        |\n",
      "|    time_elapsed         | 5192        |\n",
      "|    total_timesteps      | 4601856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009087727 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.94        |\n",
      "|    n_updates            | 27350       |\n",
      "|    policy_gradient_loss | -0.000144   |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2248         |\n",
      "|    time_elapsed         | 5194         |\n",
      "|    total_timesteps      | 4603904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026645856 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.106       |\n",
      "|    explained_variance   | 0.786        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.7         |\n",
      "|    n_updates            | 27360        |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2249        |\n",
      "|    time_elapsed         | 5196        |\n",
      "|    total_timesteps      | 4605952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006011271 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0908     |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.7         |\n",
      "|    n_updates            | 27370       |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2250        |\n",
      "|    time_elapsed         | 5198        |\n",
      "|    total_timesteps      | 4608000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008138517 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 27380       |\n",
      "|    policy_gradient_loss | -0.000234   |\n",
      "|    value_loss           | 74.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4608528, episode_reward=165.04 +/- 136.28\n",
      "Episode length: 280.00 +/- 49.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 165          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4608528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053439597 |\n",
      "|    clip_fraction        | 0.0548       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.321       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.8         |\n",
      "|    n_updates            | 27390        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 49.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2251    |\n",
      "|    time_elapsed    | 5202    |\n",
      "|    total_timesteps | 4610048 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2252         |\n",
      "|    time_elapsed         | 5204         |\n",
      "|    total_timesteps      | 4612096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031967051 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.721        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.7         |\n",
      "|    n_updates            | 27400        |\n",
      "|    policy_gradient_loss | -0.000112    |\n",
      "|    value_loss           | 90.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2253         |\n",
      "|    time_elapsed         | 5206         |\n",
      "|    total_timesteps      | 4614144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062683774 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.6         |\n",
      "|    n_updates            | 27410        |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2254         |\n",
      "|    time_elapsed         | 5209         |\n",
      "|    total_timesteps      | 4616192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038809162 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 27420        |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    value_loss           | 139          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2255         |\n",
      "|    time_elapsed         | 5211         |\n",
      "|    total_timesteps      | 4618240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062094703 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 27430        |\n",
      "|    policy_gradient_loss | 0.000402     |\n",
      "|    value_loss           | 47.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4618528, episode_reward=110.37 +/- 118.67\n",
      "Episode length: 314.40 +/- 70.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 314         |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4618528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004230175 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.81        |\n",
      "|    n_updates            | 27440       |\n",
      "|    policy_gradient_loss | -0.00042    |\n",
      "|    value_loss           | 84.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2256    |\n",
      "|    time_elapsed    | 5214    |\n",
      "|    total_timesteps | 4620288 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2257         |\n",
      "|    time_elapsed         | 5216         |\n",
      "|    total_timesteps      | 4622336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056413417 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.102       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.5         |\n",
      "|    n_updates            | 27450        |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2258         |\n",
      "|    time_elapsed         | 5220         |\n",
      "|    total_timesteps      | 4624384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070013627 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.129       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 27460        |\n",
      "|    policy_gradient_loss | 0.00273      |\n",
      "|    value_loss           | 84.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2259        |\n",
      "|    time_elapsed         | 5222        |\n",
      "|    total_timesteps      | 4626432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009005912 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.176      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 27470       |\n",
      "|    policy_gradient_loss | 0.00324     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2260        |\n",
      "|    time_elapsed         | 5224        |\n",
      "|    total_timesteps      | 4628480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011958019 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.94        |\n",
      "|    n_updates            | 27480       |\n",
      "|    policy_gradient_loss | 0.00134     |\n",
      "|    value_loss           | 47.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4628528, episode_reward=208.65 +/- 22.47\n",
      "Episode length: 323.80 +/- 64.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | 209         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4628528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004899333 |\n",
      "|    clip_fraction        | 0.0261      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.13        |\n",
      "|    n_updates            | 27490       |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2261    |\n",
      "|    time_elapsed    | 5227    |\n",
      "|    total_timesteps | 4630528 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2262        |\n",
      "|    time_elapsed         | 5229        |\n",
      "|    total_timesteps      | 4632576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002400037 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 56.2        |\n",
      "|    n_updates            | 27500       |\n",
      "|    policy_gradient_loss | -0.000677   |\n",
      "|    value_loss           | 175         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2263        |\n",
      "|    time_elapsed         | 5231        |\n",
      "|    total_timesteps      | 4634624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009606217 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.15       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 27510       |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 63.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2264         |\n",
      "|    time_elapsed         | 5233         |\n",
      "|    total_timesteps      | 4636672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033011315 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.693        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.8         |\n",
      "|    n_updates            | 27520        |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 195          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4638528, episode_reward=53.08 +/- 134.46\n",
      "Episode length: 268.20 +/- 72.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 268          |\n",
      "|    mean_reward          | 53.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4638528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059339236 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.2         |\n",
      "|    n_updates            | 27530        |\n",
      "|    policy_gradient_loss | -0.00094     |\n",
      "|    value_loss           | 37.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2265    |\n",
      "|    time_elapsed    | 5236    |\n",
      "|    total_timesteps | 4638720 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2266        |\n",
      "|    time_elapsed         | 5238        |\n",
      "|    total_timesteps      | 4640768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009345064 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.31        |\n",
      "|    n_updates            | 27540       |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 45.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2267         |\n",
      "|    time_elapsed         | 5240         |\n",
      "|    total_timesteps      | 4642816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023201457 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.114       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.2         |\n",
      "|    n_updates            | 27550        |\n",
      "|    policy_gradient_loss | 0.000344     |\n",
      "|    value_loss           | 68.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2268        |\n",
      "|    time_elapsed         | 5242        |\n",
      "|    total_timesteps      | 4644864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005298785 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62          |\n",
      "|    n_updates            | 27560       |\n",
      "|    policy_gradient_loss | -0.000343   |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2269        |\n",
      "|    time_elapsed         | 5244        |\n",
      "|    total_timesteps      | 4646912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004211006 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.1        |\n",
      "|    n_updates            | 27570       |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4648528, episode_reward=66.93 +/- 165.46\n",
      "Episode length: 352.00 +/- 104.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | 66.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4648528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005852312 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.156      |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 27580       |\n",
      "|    policy_gradient_loss | 0.000236    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2270    |\n",
      "|    time_elapsed    | 5247    |\n",
      "|    total_timesteps | 4648960 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2271        |\n",
      "|    time_elapsed         | 5249        |\n",
      "|    total_timesteps      | 4651008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019340217 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.56        |\n",
      "|    n_updates            | 27590       |\n",
      "|    policy_gradient_loss | -0.000382   |\n",
      "|    value_loss           | 77.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2272        |\n",
      "|    time_elapsed         | 5251        |\n",
      "|    total_timesteps      | 4653056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009950284 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.331      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 27600       |\n",
      "|    policy_gradient_loss | -0.000716   |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2273        |\n",
      "|    time_elapsed         | 5253        |\n",
      "|    total_timesteps      | 4655104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010917051 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.8        |\n",
      "|    n_updates            | 27610       |\n",
      "|    policy_gradient_loss | -0.000828   |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2274        |\n",
      "|    time_elapsed         | 5255        |\n",
      "|    total_timesteps      | 4657152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007652824 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6           |\n",
      "|    n_updates            | 27620       |\n",
      "|    policy_gradient_loss | -0.000506   |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4658528, episode_reward=115.07 +/- 137.23\n",
      "Episode length: 243.40 +/- 38.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 243         |\n",
      "|    mean_reward          | 115         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4658528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015972607 |\n",
      "|    clip_fraction        | 0.0621      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.57        |\n",
      "|    n_updates            | 27630       |\n",
      "|    policy_gradient_loss | 0.00429     |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2275    |\n",
      "|    time_elapsed    | 5257    |\n",
      "|    total_timesteps | 4659200 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2276        |\n",
      "|    time_elapsed         | 5260        |\n",
      "|    total_timesteps      | 4661248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002422023 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 27640       |\n",
      "|    policy_gradient_loss | -0.000607   |\n",
      "|    value_loss           | 65.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2277        |\n",
      "|    time_elapsed         | 5262        |\n",
      "|    total_timesteps      | 4663296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009515548 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 27650       |\n",
      "|    policy_gradient_loss | 0.000265    |\n",
      "|    value_loss           | 41.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2278         |\n",
      "|    time_elapsed         | 5264         |\n",
      "|    total_timesteps      | 4665344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038513057 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | 0.551        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 27660        |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 183          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2279         |\n",
      "|    time_elapsed         | 5266         |\n",
      "|    total_timesteps      | 4667392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037869874 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.55         |\n",
      "|    n_updates            | 27670        |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4668528, episode_reward=156.79 +/- 83.86\n",
      "Episode length: 320.00 +/- 87.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 320          |\n",
      "|    mean_reward          | 157          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4668528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075350506 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 27680        |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    value_loss           | 61.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2280    |\n",
      "|    time_elapsed    | 5269    |\n",
      "|    total_timesteps | 4669440 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2281       |\n",
      "|    time_elapsed         | 5271       |\n",
      "|    total_timesteps      | 4671488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01101199 |\n",
      "|    clip_fraction        | 0.0728     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.183     |\n",
      "|    explained_variance   | 0.839      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 30.3       |\n",
      "|    n_updates            | 27690      |\n",
      "|    policy_gradient_loss | -0.0026    |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2282         |\n",
      "|    time_elapsed         | 5273         |\n",
      "|    total_timesteps      | 4673536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043355664 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.304       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 27700        |\n",
      "|    policy_gradient_loss | -0.000284    |\n",
      "|    value_loss           | 59.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2283        |\n",
      "|    time_elapsed         | 5275        |\n",
      "|    total_timesteps      | 4675584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008469416 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.3         |\n",
      "|    n_updates            | 27710       |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    value_loss           | 68.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2284        |\n",
      "|    time_elapsed         | 5277        |\n",
      "|    total_timesteps      | 4677632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004465365 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.747       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47          |\n",
      "|    n_updates            | 27720       |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4678528, episode_reward=219.52 +/- 45.37\n",
      "Episode length: 320.80 +/- 41.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | 220          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4678528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052036513 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.267       |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.5         |\n",
      "|    n_updates            | 27730        |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 71.3         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2285    |\n",
      "|    time_elapsed    | 5280    |\n",
      "|    total_timesteps | 4679680 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2286        |\n",
      "|    time_elapsed         | 5282        |\n",
      "|    total_timesteps      | 4681728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004170128 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 27740       |\n",
      "|    policy_gradient_loss | 0.000582    |\n",
      "|    value_loss           | 85.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2287        |\n",
      "|    time_elapsed         | 5284        |\n",
      "|    total_timesteps      | 4683776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004273168 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 27750       |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 61.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2288        |\n",
      "|    time_elapsed         | 5286        |\n",
      "|    total_timesteps      | 4685824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017214045 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.25        |\n",
      "|    n_updates            | 27760       |\n",
      "|    policy_gradient_loss | 0.00128     |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2289         |\n",
      "|    time_elapsed         | 5289         |\n",
      "|    total_timesteps      | 4687872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018236813 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.277       |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62         |\n",
      "|    n_updates            | 27770        |\n",
      "|    policy_gradient_loss | -1.25e-05    |\n",
      "|    value_loss           | 92.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4688528, episode_reward=173.21 +/- 49.55\n",
      "Episode length: 417.80 +/- 173.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 418         |\n",
      "|    mean_reward          | 173         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4688528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008452898 |\n",
      "|    clip_fraction        | 0.0663      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 27780       |\n",
      "|    policy_gradient_loss | 0.000123    |\n",
      "|    value_loss           | 62.1        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2290    |\n",
      "|    time_elapsed    | 5292    |\n",
      "|    total_timesteps | 4689920 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2291         |\n",
      "|    time_elapsed         | 5295         |\n",
      "|    total_timesteps      | 4691968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059678718 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.4         |\n",
      "|    n_updates            | 27790        |\n",
      "|    policy_gradient_loss | -0.000404    |\n",
      "|    value_loss           | 52.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2292         |\n",
      "|    time_elapsed         | 5297         |\n",
      "|    total_timesteps      | 4694016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057831067 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.282       |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 27800        |\n",
      "|    policy_gradient_loss | 0.00122      |\n",
      "|    value_loss           | 50           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2293        |\n",
      "|    time_elapsed         | 5299        |\n",
      "|    total_timesteps      | 4696064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004163783 |\n",
      "|    clip_fraction        | 0.0239      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 27810       |\n",
      "|    policy_gradient_loss | -0.000692   |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2294       |\n",
      "|    time_elapsed         | 5301       |\n",
      "|    total_timesteps      | 4698112    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01600919 |\n",
      "|    clip_fraction        | 0.0981     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.273     |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14         |\n",
      "|    n_updates            | 27820      |\n",
      "|    policy_gradient_loss | -0.00146   |\n",
      "|    value_loss           | 24.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4698528, episode_reward=101.72 +/- 145.16\n",
      "Episode length: 253.80 +/- 29.48\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 254          |\n",
      "|    mean_reward          | 102          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4698528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074416073 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.15        |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 27830        |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 54           |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2295    |\n",
      "|    time_elapsed    | 5304    |\n",
      "|    total_timesteps | 4700160 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2296         |\n",
      "|    time_elapsed         | 5306         |\n",
      "|    total_timesteps      | 4702208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029419928 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.98         |\n",
      "|    n_updates            | 27840        |\n",
      "|    policy_gradient_loss | 0.000137     |\n",
      "|    value_loss           | 73.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2297         |\n",
      "|    time_elapsed         | 5308         |\n",
      "|    total_timesteps      | 4704256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027301954 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.303       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 27850        |\n",
      "|    policy_gradient_loss | -0.000983    |\n",
      "|    value_loss           | 74.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2298       |\n",
      "|    time_elapsed         | 5310       |\n",
      "|    total_timesteps      | 4706304    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00700395 |\n",
      "|    clip_fraction        | 0.0479     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.542      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 54         |\n",
      "|    n_updates            | 27860      |\n",
      "|    policy_gradient_loss | -0.00252   |\n",
      "|    value_loss           | 130        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2299        |\n",
      "|    time_elapsed         | 5312        |\n",
      "|    total_timesteps      | 4708352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008156503 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 137         |\n",
      "|    n_updates            | 27870       |\n",
      "|    policy_gradient_loss | -0.000349   |\n",
      "|    value_loss           | 96.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4708528, episode_reward=161.77 +/- 113.37\n",
      "Episode length: 455.60 +/- 277.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 456         |\n",
      "|    mean_reward          | 162         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4708528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024197128 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 27880       |\n",
      "|    policy_gradient_loss | -0.00421    |\n",
      "|    value_loss           | 54.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2300    |\n",
      "|    time_elapsed    | 5316    |\n",
      "|    total_timesteps | 4710400 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 886        |\n",
      "|    iterations           | 2301       |\n",
      "|    time_elapsed         | 5318       |\n",
      "|    total_timesteps      | 4712448    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00598909 |\n",
      "|    clip_fraction        | 0.0378     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.4        |\n",
      "|    n_updates            | 27890      |\n",
      "|    policy_gradient_loss | -5.41e-05  |\n",
      "|    value_loss           | 31.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2302        |\n",
      "|    time_elapsed         | 5320        |\n",
      "|    total_timesteps      | 4714496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007448996 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.51        |\n",
      "|    n_updates            | 27900       |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 74.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2303         |\n",
      "|    time_elapsed         | 5322         |\n",
      "|    total_timesteps      | 4716544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044663195 |\n",
      "|    clip_fraction        | 0.0487       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.247       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.5          |\n",
      "|    n_updates            | 27910        |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 72           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4718528, episode_reward=131.26 +/- 139.54\n",
      "Episode length: 270.40 +/- 43.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 131         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4718528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004769246 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.91        |\n",
      "|    n_updates            | 27920       |\n",
      "|    policy_gradient_loss | -4.65e-05   |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2304    |\n",
      "|    time_elapsed    | 5325    |\n",
      "|    total_timesteps | 4718592 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2305        |\n",
      "|    time_elapsed         | 5327        |\n",
      "|    total_timesteps      | 4720640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004857642 |\n",
      "|    clip_fraction        | 0.0582      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.99        |\n",
      "|    n_updates            | 27930       |\n",
      "|    policy_gradient_loss | 0.0039      |\n",
      "|    value_loss           | 22.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2306        |\n",
      "|    time_elapsed         | 5329        |\n",
      "|    total_timesteps      | 4722688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008414447 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 27940       |\n",
      "|    policy_gradient_loss | -0.00082    |\n",
      "|    value_loss           | 18.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2307        |\n",
      "|    time_elapsed         | 5331        |\n",
      "|    total_timesteps      | 4724736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004745633 |\n",
      "|    clip_fraction        | 0.0288      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 27950       |\n",
      "|    policy_gradient_loss | -0.000359   |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2308         |\n",
      "|    time_elapsed         | 5333         |\n",
      "|    total_timesteps      | 4726784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051687146 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.76         |\n",
      "|    n_updates            | 27960        |\n",
      "|    policy_gradient_loss | -0.000782    |\n",
      "|    value_loss           | 66.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4728528, episode_reward=141.65 +/- 144.10\n",
      "Episode length: 274.00 +/- 39.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 274         |\n",
      "|    mean_reward          | 142         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4728528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005131303 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 27970       |\n",
      "|    policy_gradient_loss | -0.000564   |\n",
      "|    value_loss           | 31.3        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 886     |\n",
      "|    iterations      | 2309    |\n",
      "|    time_elapsed    | 5336    |\n",
      "|    total_timesteps | 4728832 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2310         |\n",
      "|    time_elapsed         | 5338         |\n",
      "|    total_timesteps      | 4730880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026519175 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.7         |\n",
      "|    n_updates            | 27980        |\n",
      "|    policy_gradient_loss | -5.88e-05    |\n",
      "|    value_loss           | 56.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2311        |\n",
      "|    time_elapsed         | 5340        |\n",
      "|    total_timesteps      | 4732928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013237046 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 27990       |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 2312        |\n",
      "|    time_elapsed         | 5342        |\n",
      "|    total_timesteps      | 4734976     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016249498 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.5        |\n",
      "|    n_updates            | 28000       |\n",
      "|    policy_gradient_loss | -0.000148   |\n",
      "|    value_loss           | 46.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2313         |\n",
      "|    time_elapsed         | 5344         |\n",
      "|    total_timesteps      | 4737024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022450686 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.127       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 28010        |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4738528, episode_reward=22.24 +/- 99.01\n",
      "Episode length: 455.60 +/- 282.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | 22.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4738528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036848434 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.4         |\n",
      "|    n_updates            | 28020        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2314    |\n",
      "|    time_elapsed    | 5349    |\n",
      "|    total_timesteps | 4739072 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2315         |\n",
      "|    time_elapsed         | 5351         |\n",
      "|    total_timesteps      | 4741120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036920435 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.265       |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.5         |\n",
      "|    n_updates            | 28030        |\n",
      "|    policy_gradient_loss | 0.00141      |\n",
      "|    value_loss           | 44           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2316         |\n",
      "|    time_elapsed         | 5353         |\n",
      "|    total_timesteps      | 4743168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075826123 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.73         |\n",
      "|    n_updates            | 28040        |\n",
      "|    policy_gradient_loss | 0.000472     |\n",
      "|    value_loss           | 40.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 2317         |\n",
      "|    time_elapsed         | 5355         |\n",
      "|    total_timesteps      | 4745216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062315613 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 28050        |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    value_loss           | 83.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2318        |\n",
      "|    time_elapsed         | 5358        |\n",
      "|    total_timesteps      | 4747264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003092772 |\n",
      "|    clip_fraction        | 0.025       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.11       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 28060       |\n",
      "|    policy_gradient_loss | -0.00044    |\n",
      "|    value_loss           | 89.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4748528, episode_reward=162.92 +/- 65.02\n",
      "Episode length: 519.20 +/- 295.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 519          |\n",
      "|    mean_reward          | 163          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4748528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072704903 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.166       |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.68         |\n",
      "|    n_updates            | 28070        |\n",
      "|    policy_gradient_loss | 6.94e-05     |\n",
      "|    value_loss           | 71.5         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2319    |\n",
      "|    time_elapsed    | 5362    |\n",
      "|    total_timesteps | 4749312 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2320        |\n",
      "|    time_elapsed         | 5364        |\n",
      "|    total_timesteps      | 4751360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004506053 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.38        |\n",
      "|    n_updates            | 28080       |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2321       |\n",
      "|    time_elapsed         | 5367       |\n",
      "|    total_timesteps      | 4753408    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00427479 |\n",
      "|    clip_fraction        | 0.0391     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.197     |\n",
      "|    explained_variance   | 0.744      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.23       |\n",
      "|    n_updates            | 28090      |\n",
      "|    policy_gradient_loss | 7.99e-05   |\n",
      "|    value_loss           | 63.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2322        |\n",
      "|    time_elapsed         | 5369        |\n",
      "|    total_timesteps      | 4755456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008230876 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.7        |\n",
      "|    n_updates            | 28100       |\n",
      "|    policy_gradient_loss | -0.000693   |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2323       |\n",
      "|    time_elapsed         | 5371       |\n",
      "|    total_timesteps      | 4757504    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01674526 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.292     |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.5       |\n",
      "|    n_updates            | 28110      |\n",
      "|    policy_gradient_loss | -0.0017    |\n",
      "|    value_loss           | 75.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4758528, episode_reward=106.38 +/- 111.16\n",
      "Episode length: 404.40 +/- 298.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 404        |\n",
      "|    mean_reward          | 106        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4758528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01722201 |\n",
      "|    clip_fraction        | 0.0882     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.322     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.62       |\n",
      "|    n_updates            | 28120      |\n",
      "|    policy_gradient_loss | -0.00111   |\n",
      "|    value_loss           | 38.3       |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2324    |\n",
      "|    time_elapsed    | 5375    |\n",
      "|    total_timesteps | 4759552 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2325         |\n",
      "|    time_elapsed         | 5377         |\n",
      "|    total_timesteps      | 4761600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019778921 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 66.1         |\n",
      "|    n_updates            | 28130        |\n",
      "|    policy_gradient_loss | 0.00163      |\n",
      "|    value_loss           | 150          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2326         |\n",
      "|    time_elapsed         | 5379         |\n",
      "|    total_timesteps      | 4763648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059257103 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.1         |\n",
      "|    n_updates            | 28140        |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2327       |\n",
      "|    time_elapsed         | 5381       |\n",
      "|    total_timesteps      | 4765696    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03129319 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.269     |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.7       |\n",
      "|    n_updates            | 28150      |\n",
      "|    policy_gradient_loss | 0.000274   |\n",
      "|    value_loss           | 22.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2328         |\n",
      "|    time_elapsed         | 5384         |\n",
      "|    total_timesteps      | 4767744      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040599206 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22           |\n",
      "|    n_updates            | 28160        |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 86.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4768528, episode_reward=39.88 +/- 118.47\n",
      "Episode length: 341.80 +/- 120.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | 39.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4768528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024584052 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.134       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.9         |\n",
      "|    n_updates            | 28170        |\n",
      "|    policy_gradient_loss | 0.000738     |\n",
      "|    value_loss           | 48.8         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2329    |\n",
      "|    time_elapsed    | 5387    |\n",
      "|    total_timesteps | 4769792 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2330        |\n",
      "|    time_elapsed         | 5389        |\n",
      "|    total_timesteps      | 4771840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005114956 |\n",
      "|    clip_fraction        | 0.0451      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.131      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.11        |\n",
      "|    n_updates            | 28180       |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 45.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2331         |\n",
      "|    time_elapsed         | 5391         |\n",
      "|    total_timesteps      | 4773888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054809544 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.5         |\n",
      "|    n_updates            | 28190        |\n",
      "|    policy_gradient_loss | -0.000331    |\n",
      "|    value_loss           | 86           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2332         |\n",
      "|    time_elapsed         | 5393         |\n",
      "|    total_timesteps      | 4775936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059044296 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.146       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47.8         |\n",
      "|    n_updates            | 28200        |\n",
      "|    policy_gradient_loss | -0.000806    |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2333        |\n",
      "|    time_elapsed         | 5396        |\n",
      "|    total_timesteps      | 4777984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003085489 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 28210       |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 92          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4778528, episode_reward=101.33 +/- 126.36\n",
      "Episode length: 280.80 +/- 136.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 281         |\n",
      "|    mean_reward          | 101         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4778528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004576922 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.113      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 28220       |\n",
      "|    policy_gradient_loss | -0.000991   |\n",
      "|    value_loss           | 159         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2334    |\n",
      "|    time_elapsed    | 5399    |\n",
      "|    total_timesteps | 4780032 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2335        |\n",
      "|    time_elapsed         | 5401        |\n",
      "|    total_timesteps      | 4782080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003311363 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.135      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 28230       |\n",
      "|    policy_gradient_loss | -0.000186   |\n",
      "|    value_loss           | 90.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2336        |\n",
      "|    time_elapsed         | 5403        |\n",
      "|    total_timesteps      | 4784128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008560731 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 28240       |\n",
      "|    policy_gradient_loss | 0.000153    |\n",
      "|    value_loss           | 33.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2337        |\n",
      "|    time_elapsed         | 5405        |\n",
      "|    total_timesteps      | 4786176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021711621 |\n",
      "|    clip_fraction        | 0.0604      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.127      |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.2        |\n",
      "|    n_updates            | 28250       |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 266         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2338        |\n",
      "|    time_elapsed         | 5408        |\n",
      "|    total_timesteps      | 4788224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008803272 |\n",
      "|    clip_fraction        | 0.0598      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.3        |\n",
      "|    n_updates            | 28260       |\n",
      "|    policy_gradient_loss | -0.00035    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4788528, episode_reward=162.14 +/- 73.47\n",
      "Episode length: 419.20 +/- 292.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 419          |\n",
      "|    mean_reward          | 162          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4788528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074748406 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 28270        |\n",
      "|    policy_gradient_loss | 0.000797     |\n",
      "|    value_loss           | 35.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2339    |\n",
      "|    time_elapsed    | 5412    |\n",
      "|    total_timesteps | 4790272 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2340         |\n",
      "|    time_elapsed         | 5414         |\n",
      "|    total_timesteps      | 4792320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051919185 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.2         |\n",
      "|    n_updates            | 28280        |\n",
      "|    policy_gradient_loss | 0.000402     |\n",
      "|    value_loss           | 61.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2341        |\n",
      "|    time_elapsed         | 5416        |\n",
      "|    total_timesteps      | 4794368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006872408 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 28290       |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2342       |\n",
      "|    time_elapsed         | 5418       |\n",
      "|    total_timesteps      | 4796416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00300041 |\n",
      "|    clip_fraction        | 0.0362     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 76.7       |\n",
      "|    n_updates            | 28300      |\n",
      "|    policy_gradient_loss | -0.000907  |\n",
      "|    value_loss           | 140        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2343         |\n",
      "|    time_elapsed         | 5420         |\n",
      "|    total_timesteps      | 4798464      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045218375 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.9         |\n",
      "|    n_updates            | 28310        |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 75.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4798528, episode_reward=152.42 +/- 126.43\n",
      "Episode length: 441.60 +/- 283.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 442         |\n",
      "|    mean_reward          | 152         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4798528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006840098 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 28320       |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2344    |\n",
      "|    time_elapsed    | 5424    |\n",
      "|    total_timesteps | 4800512 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2345        |\n",
      "|    time_elapsed         | 5426        |\n",
      "|    total_timesteps      | 4802560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008869652 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 28330       |\n",
      "|    policy_gradient_loss | 0.000645    |\n",
      "|    value_loss           | 56.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2346         |\n",
      "|    time_elapsed         | 5428         |\n",
      "|    total_timesteps      | 4804608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055855224 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.116       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 89.6         |\n",
      "|    n_updates            | 28340        |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 80.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2347         |\n",
      "|    time_elapsed         | 5430         |\n",
      "|    total_timesteps      | 4806656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041667675 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.115       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.4         |\n",
      "|    n_updates            | 28350        |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4808528, episode_reward=181.05 +/- 107.48\n",
      "Episode length: 294.80 +/- 55.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | 181         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4808528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005407011 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 28360       |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2348    |\n",
      "|    time_elapsed    | 5433    |\n",
      "|    total_timesteps | 4808704 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2349        |\n",
      "|    time_elapsed         | 5435        |\n",
      "|    total_timesteps      | 4810752     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005545622 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.138      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 28370       |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2350         |\n",
      "|    time_elapsed         | 5437         |\n",
      "|    total_timesteps      | 4812800      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072242543 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.125       |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.5         |\n",
      "|    n_updates            | 28380        |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    value_loss           | 60.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2351        |\n",
      "|    time_elapsed         | 5439        |\n",
      "|    total_timesteps      | 4814848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013787108 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.7        |\n",
      "|    n_updates            | 28390       |\n",
      "|    policy_gradient_loss | -0.000575   |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 885       |\n",
      "|    iterations           | 2352      |\n",
      "|    time_elapsed         | 5441      |\n",
      "|    total_timesteps      | 4816896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0060537 |\n",
      "|    clip_fraction        | 0.0394    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.22     |\n",
      "|    explained_variance   | 0.86      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 33.3      |\n",
      "|    n_updates            | 28400     |\n",
      "|    policy_gradient_loss | -0.00215  |\n",
      "|    value_loss           | 54.4      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4818528, episode_reward=90.16 +/- 130.04\n",
      "Episode length: 367.00 +/- 189.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 367         |\n",
      "|    mean_reward          | 90.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4818528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011918393 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.137      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.73        |\n",
      "|    n_updates            | 28410       |\n",
      "|    policy_gradient_loss | 0.000535    |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2353    |\n",
      "|    time_elapsed    | 5444    |\n",
      "|    total_timesteps | 4818944 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2354         |\n",
      "|    time_elapsed         | 5447         |\n",
      "|    total_timesteps      | 4820992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055538374 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.183       |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.4         |\n",
      "|    n_updates            | 28420        |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 78.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2355        |\n",
      "|    time_elapsed         | 5449        |\n",
      "|    total_timesteps      | 4823040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008620297 |\n",
      "|    clip_fraction        | 0.0867      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 28430       |\n",
      "|    policy_gradient_loss | -0.000753   |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2356        |\n",
      "|    time_elapsed         | 5452        |\n",
      "|    total_timesteps      | 4825088     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005015529 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 28440       |\n",
      "|    policy_gradient_loss | -0.000141   |\n",
      "|    value_loss           | 59.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2357        |\n",
      "|    time_elapsed         | 5454        |\n",
      "|    total_timesteps      | 4827136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010452181 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.966       |\n",
      "|    n_updates            | 28450       |\n",
      "|    policy_gradient_loss | 0.000681    |\n",
      "|    value_loss           | 7.42        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4828528, episode_reward=138.30 +/- 52.60\n",
      "Episode length: 702.20 +/- 245.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 702          |\n",
      "|    mean_reward          | 138          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4828528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029125982 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31           |\n",
      "|    n_updates            | 28460        |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 84.2         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2358    |\n",
      "|    time_elapsed    | 5460    |\n",
      "|    total_timesteps | 4829184 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2359         |\n",
      "|    time_elapsed         | 5462         |\n",
      "|    total_timesteps      | 4831232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038009766 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.9         |\n",
      "|    n_updates            | 28470        |\n",
      "|    policy_gradient_loss | -0.000382    |\n",
      "|    value_loss           | 41.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2360        |\n",
      "|    time_elapsed         | 5464        |\n",
      "|    total_timesteps      | 4833280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004885339 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.157      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 28480       |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 64.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2361        |\n",
      "|    time_elapsed         | 5466        |\n",
      "|    total_timesteps      | 4835328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006407836 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.24        |\n",
      "|    n_updates            | 28490       |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 49.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2362        |\n",
      "|    time_elapsed         | 5468        |\n",
      "|    total_timesteps      | 4837376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004801536 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 28500       |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4838528, episode_reward=164.67 +/- 122.72\n",
      "Episode length: 361.80 +/- 110.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 362          |\n",
      "|    mean_reward          | 165          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4838528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031913538 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.249       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.8         |\n",
      "|    n_updates            | 28510        |\n",
      "|    policy_gradient_loss | 0.000248     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2363    |\n",
      "|    time_elapsed    | 5471    |\n",
      "|    total_timesteps | 4839424 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2364        |\n",
      "|    time_elapsed         | 5473        |\n",
      "|    total_timesteps      | 4841472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003794941 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 28520       |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 79          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2365         |\n",
      "|    time_elapsed         | 5476         |\n",
      "|    total_timesteps      | 4843520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029649413 |\n",
      "|    clip_fraction        | 0.0475       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.135       |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.2         |\n",
      "|    n_updates            | 28530        |\n",
      "|    policy_gradient_loss | 0.000441     |\n",
      "|    value_loss           | 82.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2366        |\n",
      "|    time_elapsed         | 5478        |\n",
      "|    total_timesteps      | 4845568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006952606 |\n",
      "|    clip_fraction        | 0.06        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 28540       |\n",
      "|    policy_gradient_loss | -0.00091    |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2367       |\n",
      "|    time_elapsed         | 5480       |\n",
      "|    total_timesteps      | 4847616    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01589474 |\n",
      "|    clip_fraction        | 0.0588     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.359     |\n",
      "|    explained_variance   | 0.37       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23         |\n",
      "|    n_updates            | 28550      |\n",
      "|    policy_gradient_loss | -3.91e-05  |\n",
      "|    value_loss           | 90.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4848528, episode_reward=146.59 +/- 105.56\n",
      "Episode length: 249.00 +/- 29.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 249          |\n",
      "|    mean_reward          | 147          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4848528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046467534 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.263       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.9         |\n",
      "|    n_updates            | 28560        |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2368    |\n",
      "|    time_elapsed    | 5483    |\n",
      "|    total_timesteps | 4849664 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2369         |\n",
      "|    time_elapsed         | 5485         |\n",
      "|    total_timesteps      | 4851712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028149332 |\n",
      "|    clip_fraction        | 0.0387       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.76         |\n",
      "|    n_updates            | 28570        |\n",
      "|    policy_gradient_loss | -6.83e-05    |\n",
      "|    value_loss           | 41.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2370        |\n",
      "|    time_elapsed         | 5487        |\n",
      "|    total_timesteps      | 4853760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012166509 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 28580       |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2371        |\n",
      "|    time_elapsed         | 5489        |\n",
      "|    total_timesteps      | 4855808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004633926 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 178         |\n",
      "|    n_updates            | 28590       |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2372        |\n",
      "|    time_elapsed         | 5491        |\n",
      "|    total_timesteps      | 4857856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008319483 |\n",
      "|    clip_fraction        | 0.0556      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 28600       |\n",
      "|    policy_gradient_loss | -0.000103   |\n",
      "|    value_loss           | 46.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4858528, episode_reward=107.12 +/- 100.32\n",
      "Episode length: 254.80 +/- 56.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 255         |\n",
      "|    mean_reward          | 107         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4858528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013027454 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 28610       |\n",
      "|    policy_gradient_loss | 0.000569    |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2373    |\n",
      "|    time_elapsed    | 5494    |\n",
      "|    total_timesteps | 4859904 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2374        |\n",
      "|    time_elapsed         | 5496        |\n",
      "|    total_timesteps      | 4861952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012168576 |\n",
      "|    clip_fraction        | 0.0409      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64.3        |\n",
      "|    n_updates            | 28620       |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2375        |\n",
      "|    time_elapsed         | 5498        |\n",
      "|    total_timesteps      | 4864000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017968245 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 28630       |\n",
      "|    policy_gradient_loss | 0.000263    |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2376        |\n",
      "|    time_elapsed         | 5500        |\n",
      "|    total_timesteps      | 4866048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005866384 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.2        |\n",
      "|    n_updates            | 28640       |\n",
      "|    policy_gradient_loss | -0.000829   |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2377         |\n",
      "|    time_elapsed         | 5501         |\n",
      "|    total_timesteps      | 4868096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058045033 |\n",
      "|    clip_fraction        | 0.047        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.268       |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.8         |\n",
      "|    n_updates            | 28650        |\n",
      "|    policy_gradient_loss | -0.000992    |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4868528, episode_reward=207.95 +/- 18.83\n",
      "Episode length: 304.00 +/- 27.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 304        |\n",
      "|    mean_reward          | 208        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4868528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00470604 |\n",
      "|    clip_fraction        | 0.0357     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 41.5       |\n",
      "|    n_updates            | 28660      |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    value_loss           | 160        |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2378    |\n",
      "|    time_elapsed    | 5504    |\n",
      "|    total_timesteps | 4870144 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2379        |\n",
      "|    time_elapsed         | 5506        |\n",
      "|    total_timesteps      | 4872192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005077785 |\n",
      "|    clip_fraction        | 0.0205      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.104      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.6        |\n",
      "|    n_updates            | 28670       |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2380        |\n",
      "|    time_elapsed         | 5508        |\n",
      "|    total_timesteps      | 4874240     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006994809 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 125         |\n",
      "|    n_updates            | 28680       |\n",
      "|    policy_gradient_loss | -0.000338   |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2381        |\n",
      "|    time_elapsed         | 5510        |\n",
      "|    total_timesteps      | 4876288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006034074 |\n",
      "|    clip_fraction        | 0.0408      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 28690       |\n",
      "|    policy_gradient_loss | 0.000278    |\n",
      "|    value_loss           | 48.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2382         |\n",
      "|    time_elapsed         | 5512         |\n",
      "|    total_timesteps      | 4878336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056004315 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.6          |\n",
      "|    n_updates            | 28700        |\n",
      "|    policy_gradient_loss | 0.00237      |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4878528, episode_reward=224.10 +/- 38.84\n",
      "Episode length: 323.60 +/- 24.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | 224          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4878528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077546844 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.156       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16           |\n",
      "|    n_updates            | 28710        |\n",
      "|    policy_gradient_loss | 0.00143      |\n",
      "|    value_loss           | 36.6         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2383    |\n",
      "|    time_elapsed    | 5516    |\n",
      "|    total_timesteps | 4880384 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2384        |\n",
      "|    time_elapsed         | 5518        |\n",
      "|    total_timesteps      | 4882432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007857649 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 28720       |\n",
      "|    policy_gradient_loss | -0.000866   |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2385         |\n",
      "|    time_elapsed         | 5520         |\n",
      "|    total_timesteps      | 4884480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045026354 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 28730        |\n",
      "|    policy_gradient_loss | -0.000575    |\n",
      "|    value_loss           | 75.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2386       |\n",
      "|    time_elapsed         | 5522       |\n",
      "|    total_timesteps      | 4886528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00981221 |\n",
      "|    clip_fraction        | 0.0558     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.145     |\n",
      "|    explained_variance   | 0.872      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.1        |\n",
      "|    n_updates            | 28740      |\n",
      "|    policy_gradient_loss | -0.00168   |\n",
      "|    value_loss           | 72.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4888528, episode_reward=154.37 +/- 102.26\n",
      "Episode length: 282.60 +/- 20.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 154          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4888528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045991465 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.753        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.8         |\n",
      "|    n_updates            | 28750        |\n",
      "|    policy_gradient_loss | -0.000133    |\n",
      "|    value_loss           | 82.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2387    |\n",
      "|    time_elapsed    | 5525    |\n",
      "|    total_timesteps | 4888576 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2388        |\n",
      "|    time_elapsed         | 5527        |\n",
      "|    total_timesteps      | 4890624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010738365 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 28760       |\n",
      "|    policy_gradient_loss | -0.000947   |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2389        |\n",
      "|    time_elapsed         | 5529        |\n",
      "|    total_timesteps      | 4892672     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004817074 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0927     |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 28770       |\n",
      "|    policy_gradient_loss | -0.00042    |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2390        |\n",
      "|    time_elapsed         | 5532        |\n",
      "|    total_timesteps      | 4894720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009926371 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 28780       |\n",
      "|    policy_gradient_loss | -7.62e-05   |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2391         |\n",
      "|    time_elapsed         | 5534         |\n",
      "|    total_timesteps      | 4896768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055816956 |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.169       |\n",
      "|    explained_variance   | 0.645        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.4         |\n",
      "|    n_updates            | 28790        |\n",
      "|    policy_gradient_loss | 0.000616     |\n",
      "|    value_loss           | 128          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4898528, episode_reward=162.82 +/- 92.28\n",
      "Episode length: 331.80 +/- 101.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | 163          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4898528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037836663 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 95.7         |\n",
      "|    n_updates            | 28800        |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 228          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2392    |\n",
      "|    time_elapsed    | 5537    |\n",
      "|    total_timesteps | 4898816 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2393        |\n",
      "|    time_elapsed         | 5539        |\n",
      "|    total_timesteps      | 4900864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006959796 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.72        |\n",
      "|    n_updates            | 28810       |\n",
      "|    policy_gradient_loss | -0.000498   |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2394        |\n",
      "|    time_elapsed         | 5542        |\n",
      "|    total_timesteps      | 4902912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005275609 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37          |\n",
      "|    n_updates            | 28820       |\n",
      "|    policy_gradient_loss | -0.000113   |\n",
      "|    value_loss           | 64.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2395       |\n",
      "|    time_elapsed         | 5544       |\n",
      "|    total_timesteps      | 4904960    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02892248 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.315     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.72       |\n",
      "|    n_updates            | 28830      |\n",
      "|    policy_gradient_loss | 0.00188    |\n",
      "|    value_loss           | 23.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2396        |\n",
      "|    time_elapsed         | 5546        |\n",
      "|    total_timesteps      | 4907008     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004517856 |\n",
      "|    clip_fraction        | 0.054       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.8         |\n",
      "|    n_updates            | 28840       |\n",
      "|    policy_gradient_loss | 0.00111     |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4908528, episode_reward=95.82 +/- 124.84\n",
      "Episode length: 252.20 +/- 56.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 252          |\n",
      "|    mean_reward          | 95.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4908528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039404007 |\n",
      "|    clip_fraction        | 0.0994       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.76         |\n",
      "|    n_updates            | 28850        |\n",
      "|    policy_gradient_loss | -0.000978    |\n",
      "|    value_loss           | 46.7         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2397    |\n",
      "|    time_elapsed    | 5548    |\n",
      "|    total_timesteps | 4909056 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2398        |\n",
      "|    time_elapsed         | 5551        |\n",
      "|    total_timesteps      | 4911104     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010389628 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.124      |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 69.8        |\n",
      "|    n_updates            | 28860       |\n",
      "|    policy_gradient_loss | 0.000261    |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2399         |\n",
      "|    time_elapsed         | 5553         |\n",
      "|    total_timesteps      | 4913152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060257493 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 28870        |\n",
      "|    policy_gradient_loss | 0.000642     |\n",
      "|    value_loss           | 26.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2400         |\n",
      "|    time_elapsed         | 5555         |\n",
      "|    total_timesteps      | 4915200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032589892 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.9         |\n",
      "|    n_updates            | 28880        |\n",
      "|    policy_gradient_loss | 0.000758     |\n",
      "|    value_loss           | 31.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2401        |\n",
      "|    time_elapsed         | 5557        |\n",
      "|    total_timesteps      | 4917248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003019785 |\n",
      "|    clip_fraction        | 0.0213      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.115      |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68          |\n",
      "|    n_updates            | 28890       |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4918528, episode_reward=68.50 +/- 167.58\n",
      "Episode length: 251.40 +/- 28.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | 68.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4918528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118469335 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.288       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 28900        |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    value_loss           | 32.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2402    |\n",
      "|    time_elapsed    | 5560    |\n",
      "|    total_timesteps | 4919296 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2403        |\n",
      "|    time_elapsed         | 5562        |\n",
      "|    total_timesteps      | 4921344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004567666 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 63.9        |\n",
      "|    n_updates            | 28910       |\n",
      "|    policy_gradient_loss | -8.48e-05   |\n",
      "|    value_loss           | 98.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2404         |\n",
      "|    time_elapsed         | 5564         |\n",
      "|    total_timesteps      | 4923392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135791255 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.965        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 28920        |\n",
      "|    policy_gradient_loss | 0.0015       |\n",
      "|    value_loss           | 4.35         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2405        |\n",
      "|    time_elapsed         | 5567        |\n",
      "|    total_timesteps      | 4925440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009130619 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.313      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.2        |\n",
      "|    n_updates            | 28930       |\n",
      "|    policy_gradient_loss | -0.000962   |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2406       |\n",
      "|    time_elapsed         | 5569       |\n",
      "|    total_timesteps      | 4927488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00696259 |\n",
      "|    clip_fraction        | 0.0283     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.29      |\n",
      "|    explained_variance   | 0.821      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.48       |\n",
      "|    n_updates            | 28940      |\n",
      "|    policy_gradient_loss | -0.00144   |\n",
      "|    value_loss           | 55.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4928528, episode_reward=145.30 +/- 129.42\n",
      "Episode length: 278.00 +/- 39.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 278          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4928528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047680065 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 53.8         |\n",
      "|    n_updates            | 28950        |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    value_loss           | 57.4         |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2407    |\n",
      "|    time_elapsed    | 5572    |\n",
      "|    total_timesteps | 4929536 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2408         |\n",
      "|    time_elapsed         | 5574         |\n",
      "|    total_timesteps      | 4931584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064768884 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.58         |\n",
      "|    n_updates            | 28960        |\n",
      "|    policy_gradient_loss | 0.000819     |\n",
      "|    value_loss           | 8.37         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2409         |\n",
      "|    time_elapsed         | 5577         |\n",
      "|    total_timesteps      | 4933632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016499313 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 28970        |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2410         |\n",
      "|    time_elapsed         | 5579         |\n",
      "|    total_timesteps      | 4935680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063396078 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.236       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.2         |\n",
      "|    n_updates            | 28980        |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 40.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2411        |\n",
      "|    time_elapsed         | 5581        |\n",
      "|    total_timesteps      | 4937728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005224947 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 28990       |\n",
      "|    policy_gradient_loss | -0.000381   |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4938528, episode_reward=145.40 +/- 118.08\n",
      "Episode length: 250.20 +/- 23.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 250          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4938528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050361212 |\n",
      "|    clip_fraction        | 0.0305       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.118       |\n",
      "|    explained_variance   | 0.757        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 29000        |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2412    |\n",
      "|    time_elapsed    | 5583    |\n",
      "|    total_timesteps | 4939776 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2413        |\n",
      "|    time_elapsed         | 5585        |\n",
      "|    total_timesteps      | 4941824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003777174 |\n",
      "|    clip_fraction        | 0.0296      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 29010       |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2414         |\n",
      "|    time_elapsed         | 5587         |\n",
      "|    total_timesteps      | 4943872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043008085 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0961      |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.7         |\n",
      "|    n_updates            | 29020        |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2415        |\n",
      "|    time_elapsed         | 5589        |\n",
      "|    total_timesteps      | 4945920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005476318 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.106      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 29030       |\n",
      "|    policy_gradient_loss | -0.000568   |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2416         |\n",
      "|    time_elapsed         | 5591         |\n",
      "|    total_timesteps      | 4947968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047880984 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 29040        |\n",
      "|    policy_gradient_loss | 0.000772     |\n",
      "|    value_loss           | 36.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4948528, episode_reward=74.54 +/- 99.74\n",
      "Episode length: 273.60 +/- 57.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | 74.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4948528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022591706 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.107       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 29050        |\n",
      "|    policy_gradient_loss | -0.0009      |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2417    |\n",
      "|    time_elapsed    | 5593    |\n",
      "|    total_timesteps | 4950016 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2418        |\n",
      "|    time_elapsed         | 5596        |\n",
      "|    total_timesteps      | 4952064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003497477 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 29060       |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2419         |\n",
      "|    time_elapsed         | 5598         |\n",
      "|    total_timesteps      | 4954112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065798676 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.317       |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 29070        |\n",
      "|    policy_gradient_loss | 0.00213      |\n",
      "|    value_loss           | 48.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2420         |\n",
      "|    time_elapsed         | 5600         |\n",
      "|    total_timesteps      | 4956160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030758486 |\n",
      "|    clip_fraction        | 0.0339       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.111       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 99.5         |\n",
      "|    n_updates            | 29080        |\n",
      "|    policy_gradient_loss | -0.000431    |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2421        |\n",
      "|    time_elapsed         | 5602        |\n",
      "|    total_timesteps      | 4958208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010162087 |\n",
      "|    clip_fraction        | 0.0427      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.295      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.96        |\n",
      "|    n_updates            | 29090       |\n",
      "|    policy_gradient_loss | 0.000288    |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4958528, episode_reward=131.04 +/- 120.43\n",
      "Episode length: 299.00 +/- 118.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 299         |\n",
      "|    mean_reward          | 131         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4958528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005361163 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.15        |\n",
      "|    n_updates            | 29100       |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    value_loss           | 65.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2422    |\n",
      "|    time_elapsed    | 5605    |\n",
      "|    total_timesteps | 4960256 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2423        |\n",
      "|    time_elapsed         | 5608        |\n",
      "|    total_timesteps      | 4962304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005461594 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.328      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 29110       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2424        |\n",
      "|    time_elapsed         | 5610        |\n",
      "|    total_timesteps      | 4964352     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006030481 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 29120       |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 34.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2425         |\n",
      "|    time_elapsed         | 5612         |\n",
      "|    total_timesteps      | 4966400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072705937 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8            |\n",
      "|    n_updates            | 29130        |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    value_loss           | 58.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2426         |\n",
      "|    time_elapsed         | 5614         |\n",
      "|    total_timesteps      | 4968448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042628255 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.119       |\n",
      "|    explained_variance   | 0.73         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.2         |\n",
      "|    n_updates            | 29140        |\n",
      "|    policy_gradient_loss | 0.00183      |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4968528, episode_reward=133.17 +/- 118.97\n",
      "Episode length: 251.40 +/- 52.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | 133         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4968528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005133773 |\n",
      "|    clip_fraction        | 0.0482      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.7        |\n",
      "|    n_updates            | 29150       |\n",
      "|    policy_gradient_loss | -0.000565   |\n",
      "|    value_loss           | 83.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2427    |\n",
      "|    time_elapsed    | 5616    |\n",
      "|    total_timesteps | 4970496 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2428         |\n",
      "|    time_elapsed         | 5618         |\n",
      "|    total_timesteps      | 4972544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019937116 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0988      |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.6         |\n",
      "|    n_updates            | 29160        |\n",
      "|    policy_gradient_loss | -0.000649    |\n",
      "|    value_loss           | 47           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2429         |\n",
      "|    time_elapsed         | 5620         |\n",
      "|    total_timesteps      | 4974592      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043800613 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.9         |\n",
      "|    n_updates            | 29170        |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2430         |\n",
      "|    time_elapsed         | 5623         |\n",
      "|    total_timesteps      | 4976640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076042973 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.201       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.56         |\n",
      "|    n_updates            | 29180        |\n",
      "|    policy_gradient_loss | 0.00224      |\n",
      "|    value_loss           | 23.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4978528, episode_reward=163.05 +/- 94.99\n",
      "Episode length: 313.40 +/- 93.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4978528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015783068 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.299      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.95        |\n",
      "|    n_updates            | 29190       |\n",
      "|    policy_gradient_loss | 0.000374    |\n",
      "|    value_loss           | 14.2        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2431    |\n",
      "|    time_elapsed    | 5626    |\n",
      "|    total_timesteps | 4978688 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2432        |\n",
      "|    time_elapsed         | 5628        |\n",
      "|    total_timesteps      | 4980736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004210237 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 64          |\n",
      "|    n_updates            | 29200       |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 92.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 884        |\n",
      "|    iterations           | 2433       |\n",
      "|    time_elapsed         | 5631       |\n",
      "|    total_timesteps      | 4982784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00454346 |\n",
      "|    clip_fraction        | 0.0344     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.285     |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.34       |\n",
      "|    n_updates            | 29210      |\n",
      "|    policy_gradient_loss | -0.00137   |\n",
      "|    value_loss           | 38.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2434        |\n",
      "|    time_elapsed         | 5633        |\n",
      "|    total_timesteps      | 4984832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004073246 |\n",
      "|    clip_fraction        | 0.0488      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.07        |\n",
      "|    n_updates            | 29220       |\n",
      "|    policy_gradient_loss | 0.00196     |\n",
      "|    value_loss           | 6.93        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2435         |\n",
      "|    time_elapsed         | 5635         |\n",
      "|    total_timesteps      | 4986880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023350879 |\n",
      "|    clip_fraction        | 0.0331       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.144       |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 29230        |\n",
      "|    policy_gradient_loss | -0.000674    |\n",
      "|    value_loss           | 40.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4988528, episode_reward=204.30 +/- 36.72\n",
      "Episode length: 326.80 +/- 33.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 327         |\n",
      "|    mean_reward          | 204         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4988528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004726421 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 29240       |\n",
      "|    policy_gradient_loss | 0.000707    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 884     |\n",
      "|    iterations      | 2436    |\n",
      "|    time_elapsed    | 5638    |\n",
      "|    total_timesteps | 4988928 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 2437         |\n",
      "|    time_elapsed         | 5640         |\n",
      "|    total_timesteps      | 4990976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020233253 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.445        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39         |\n",
      "|    n_updates            | 29250        |\n",
      "|    policy_gradient_loss | -0.000186    |\n",
      "|    value_loss           | 75.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 2438        |\n",
      "|    time_elapsed         | 5642        |\n",
      "|    total_timesteps      | 4993024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006044679 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.148      |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 29260       |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 62.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 2439         |\n",
      "|    time_elapsed         | 5643         |\n",
      "|    total_timesteps      | 4995072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018538458 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.509        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.6         |\n",
      "|    n_updates            | 29270        |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 219          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 2440        |\n",
      "|    time_elapsed         | 5645        |\n",
      "|    total_timesteps      | 4997120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002616243 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.93        |\n",
      "|    n_updates            | 29280       |\n",
      "|    policy_gradient_loss | 1.84e-06    |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4998528, episode_reward=195.66 +/- 22.61\n",
      "Episode length: 320.80 +/- 36.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 321        |\n",
      "|    mean_reward          | 196        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4998528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00391386 |\n",
      "|    clip_fraction        | 0.0345     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.131     |\n",
      "|    explained_variance   | 0.742      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 50         |\n",
      "|    n_updates            | 29290      |\n",
      "|    policy_gradient_loss | -0.0011    |\n",
      "|    value_loss           | 159        |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 885     |\n",
      "|    iterations      | 2441    |\n",
      "|    time_elapsed    | 5648    |\n",
      "|    total_timesteps | 4999168 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 885        |\n",
      "|    iterations           | 2442       |\n",
      "|    time_elapsed         | 5650       |\n",
      "|    total_timesteps      | 5001216    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01757169 |\n",
      "|    clip_fraction        | 0.082      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.133     |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 53.6       |\n",
      "|    n_updates            | 29300      |\n",
      "|    policy_gradient_loss | -0.00183   |\n",
      "|    value_loss           | 144        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 33min 32s, sys: 38.6 s, total: 1h 34min 10s\n",
      "Wall time: 1h 34min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fc65468eaf0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "PPO.learn(total_timesteps = 5000000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187.6421043513385, 124.2451386349196)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "evaluate_policy(PPO, env, n_eval_episodes = 5, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')\n",
    "\n",
    "training_log_path\n",
    "\n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image/PPO_5Msteps_LunarLander-v2.png'/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_5Msteps_LunarLander-v2')\n",
    "\n",
    "PPO.save(ppo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark:\n",
    "- Average reward rise to above 160 consistently after 2.25M timesteps\n",
    "- Average length of episode after 2.25M timesteps is about 350\n",
    "- Time spend on 5 million learning timesteps in 1h 34min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Video Record and Play the Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_session(env):\n",
    "    \n",
    "    scores = evaluate_policy(PPO, env, n_eval_episodes = 5, render = True)\n",
    "    \n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 09:55:13.685 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc629e058d0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-22 09:55:13.686 python[2795:235555] Warning: Expected min height of view: (<NSButton: 0x7fc6295caad0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-22 09:55:13.689 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc6292ed1f0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-22 09:55:13.691 python[2795:235555] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fc629f06830>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208.33409903674266, 107.30229097335949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190.30625617561114, 109.64759819021504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159.02238713608764, 112.69960824498226)\n",
      "(146.65572410488576, 80.78026301795776)\n",
      "(172.25651209872575, 95.81809787111614)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222.90814897862643, 26.248394390101012)\n",
      "(118.28230840089154, 123.77364678856313)\n",
      "(157.21256232320448, 101.72396750400965)\n",
      "(73.17782842394664, 98.04375351193067)\n",
      "(132.37805894210157, 100.19011262063101)\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"LunarLander-v2_videos\", force=True) as env_monitor:\n",
    "    \n",
    "    sessions = [generate_final_session(env_monitor) for _ in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"LunarLander-v2_videos/openaigym.video.2.2795.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play the recorded video\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('LunarLander-v2_videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[0]))  # Play the video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End of Page***"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "gym_interface.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
