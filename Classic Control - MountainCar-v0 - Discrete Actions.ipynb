{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869c4a71",
   "metadata": {},
   "source": [
    "# 0. What is MountainCar?\n",
    "\n",
    "A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. \n",
    "\n",
    "### Action Space\n",
    "- There are 3 discrete deterministic actions:\n",
    "\n",
    "\n",
    " Num | Observation                                                 | Value | Unit         |\n",
    "-----|-------------------------------------------------------------|-------|--------------|\n",
    " 0   | Accelerate to the left                                      | Inf   | position (m) |\n",
    " 1   | Don't accelerate                                            | Inf   | position (m) |\n",
    " 2   | Accelerate to the right                                     | Inf   | position (m) |\n",
    "\n",
    " \n",
    "### Observation Space\n",
    "The observation space is a 2-dim vector:\n",
    "- the 1st element represents the \"car position\" \n",
    "- the 2nd element represents the \"car velocity\"\n",
    "\n",
    "\n",
    " Num | Observation                                                 | Min                | Max  | Unit         |\n",
    "-----|-------------------------------------------------------------|--------------------|------|--------------|\n",
    " 0   | position of the car along the x-axis                        | -Inf               | Inf  | position (m) |\n",
    " 1   | velocity of the car                                         | -Inf               | Inf  | position (m) |\n",
    " \n",
    "### Rewards\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep it isn't at the goal and is not penalised (reward = 0) for when it reaches the goal.\n",
    "\n",
    "### Transition Dynamics:\n",
    "Given an action, the mountain car follows the following transition dynamics:\n",
    "- *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "- *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>* where force = 0.001 and gravity = 0.0025.\n",
    "\n",
    "- The collisions at either end are inelastic with the velocity set to 0 upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "\n",
    "\n",
    "### Episode Termination\n",
    "The episode terminates if either of the following happens:\n",
    "1. The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "2. The length of the episode is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5e6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 19:26:36.510 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8671c23150>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 19:26:36.510 python[3199:181180] Warning: Expected min height of view: (<NSButton: 0x7f8671d0fed0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 19:26:36.512 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8671d13270>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 19:26:36.514 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8671d1afb0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc268cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc8237",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAi Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cec68f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.06167909,  0.04642748], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space.shape[0])\n",
    "\n",
    "env.observation_space.sample()               # Observation is just two numbers: car position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12760e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())            # 3 Actions are:\n",
    "                                             # [0]:Accelerate to left,\n",
    "                                             # [1]:Don't Accelerate, \n",
    "                                             # [2]:Accelerate to Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1887155e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2df2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.44491715,  0.00084615]), -1.0, False, {})\n",
      "n_state: [-4.45763294e-01  4.24623156e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "n_state, reward, done, info = env.step(action)\n",
    "\n",
    "print(env.step(action))\n",
    "\n",
    "print('n_state:',n_state)\n",
    "print('reward:',reward)\n",
    "print('done:',done)\n",
    "print('info:',info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585c162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-200.0\n",
      "Episode:2 Score:-200.0\n",
      "Episode:3 Score:-200.0\n",
      "Episode:4 Score:-200.0\n",
      "Episode:5 Score:-200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to episodes + 1\n",
    "    \n",
    "    state = env.reset()                                    # initial set of observation\n",
    "    \n",
    "    done = False                                           # maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment(outside colab only)\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        n_state, reward, done, info = env.step(action)      # pass random actions to environment to get back \n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward (Positive value increment, negative value decrement)\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e29bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e20fe",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596d407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b25bb49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States in the environemtn: 2\n",
      "Number of Actions in the environemtn: 3\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]      \n",
    "\n",
    "print('Number of States in the environemtn:', states)\n",
    "\n",
    "actions = env.action_space.n                 # 3 Actions: Left, Don't accelerate, Right\n",
    "\n",
    "print('Number of Actions in the environemtn:', actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0554e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape = (1, states)))\n",
    "    \n",
    "    model.add(Dense(24, activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(24, activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(actions, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "962fb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a08a202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 747\n",
      "Trainable params: 747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5c60",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL\n",
    "\n",
    "***Two Types of Policy in RL:***\n",
    "1. Value Base RL\n",
    "2. Policy Base RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9932db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent              # used DQNAgent here, should try other agents: SARSAAgent      \n",
    "from rl.policy import BoltzmannQPolicy      # used Policy base RL with BoltzmannQPolicy \n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15f69206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):            # build an Agent to train our model\n",
    "    \n",
    "    policy = BoltzmannQPolicy()\n",
    "    \n",
    "    memory = SequentialMemory(limit = 50000,\n",
    "                              window_length = 1\n",
    "                             )\n",
    "    \n",
    "    dqn = DQNAgent(model = model, \n",
    "                   memory = memory,\n",
    "                   policy = policy,\n",
    "                   nb_actions = actions, \n",
    "                   nb_steps_warmup = 10,\n",
    "                   target_model_update = 1e-2\n",
    "                  )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05ca489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "log_path = os.path.join(\"Training\", \"Logs\", \"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c158c18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs/MountainCar-v0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1128f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 200, verbose = 1)  # stop traing when achieve 200 reward\n",
    "\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best = stop_callback,      # every time new best model, then run stop_callback. When stop_callback achieve 200 reward, then stop the training\n",
    "                             eval_freq = 10000,                         # set how often to run eval_callback \n",
    "                             best_model_save_path = save_path,\n",
    "                             verbose = 1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae20117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 31:51 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 45s 4ms/step - reward: -1.0000\n",
      "50 episodes - episode_reward: -200.000 [-200.000, -200.000] - loss: 7.025 - mae: 35.900 - mean_q: -53.318\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "  212/10000 [..............................] - ETA: 44s - reward: -1.0000done, took 45.993 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f869d7e6b50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)                               # Used build_agent to setup a dqn model\n",
    "\n",
    "dqn.compile(Adam(learning_rate = 1e-4), \n",
    "            metrics = ['mae']\n",
    "           )      \n",
    "\n",
    "dqn.fit(env, \n",
    "        nb_steps = 50000, \n",
    "        visualize = False, \n",
    "        verbose =1\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e415206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -200.000, steps: 200\n",
      "Episode 6: reward: -200.000, steps: 200\n",
      "Episode 7: reward: -200.000, steps: 200\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "Episode 10: reward: -200.000, steps: 200\n",
      "Episode 11: reward: -200.000, steps: 200\n",
      "Episode 12: reward: -200.000, steps: 200\n",
      "Episode 13: reward: -200.000, steps: 200\n",
      "Episode 14: reward: -200.000, steps: 200\n",
      "Episode 15: reward: -200.000, steps: 200\n",
      "Episode 16: reward: -200.000, steps: 200\n",
      "Episode 17: reward: -200.000, steps: 200\n",
      "Episode 18: reward: -200.000, steps: 200\n",
      "Episode 19: reward: -200.000, steps: 200\n",
      "Episode 20: reward: -200.000, steps: 200\n",
      "Episode 21: reward: -200.000, steps: 200\n",
      "Episode 22: reward: -200.000, steps: 200\n",
      "Episode 23: reward: -200.000, steps: 200\n",
      "Episode 24: reward: -200.000, steps: 200\n",
      "Episode 25: reward: -200.000, steps: 200\n",
      "Episode 26: reward: -200.000, steps: 200\n",
      "Episode 27: reward: -200.000, steps: 200\n",
      "Episode 28: reward: -200.000, steps: 200\n",
      "Episode 29: reward: -200.000, steps: 200\n",
      "Episode 30: reward: -200.000, steps: 200\n",
      "Episode 31: reward: -200.000, steps: 200\n",
      "Episode 32: reward: -200.000, steps: 200\n",
      "Episode 33: reward: -200.000, steps: 200\n",
      "Episode 34: reward: -200.000, steps: 200\n",
      "Episode 35: reward: -200.000, steps: 200\n",
      "Episode 36: reward: -200.000, steps: 200\n",
      "Episode 37: reward: -200.000, steps: 200\n",
      "Episode 38: reward: -200.000, steps: 200\n",
      "Episode 39: reward: -200.000, steps: 200\n",
      "Episode 40: reward: -200.000, steps: 200\n",
      "Episode 41: reward: -200.000, steps: 200\n",
      "Episode 42: reward: -200.000, steps: 200\n",
      "Episode 43: reward: -200.000, steps: 200\n",
      "Episode 44: reward: -200.000, steps: 200\n",
      "Episode 45: reward: -200.000, steps: 200\n",
      "Episode 46: reward: -200.000, steps: 200\n",
      "Episode 47: reward: -200.000, steps: 200\n",
      "Episode 48: reward: -200.000, steps: 200\n",
      "Episode 49: reward: -200.000, steps: 200\n",
      "Episode 50: reward: -200.000, steps: 200\n",
      "Episode 51: reward: -200.000, steps: 200\n",
      "Episode 52: reward: -200.000, steps: 200\n",
      "Episode 53: reward: -200.000, steps: 200\n",
      "Episode 54: reward: -200.000, steps: 200\n",
      "Episode 55: reward: -200.000, steps: 200\n",
      "Episode 56: reward: -200.000, steps: 200\n",
      "Episode 57: reward: -200.000, steps: 200\n",
      "Episode 58: reward: -200.000, steps: 200\n",
      "Episode 59: reward: -200.000, steps: 200\n",
      "Episode 60: reward: -200.000, steps: 200\n",
      "Episode 61: reward: -200.000, steps: 200\n",
      "Episode 62: reward: -200.000, steps: 200\n",
      "Episode 63: reward: -200.000, steps: 200\n",
      "Episode 64: reward: -200.000, steps: 200\n",
      "Episode 65: reward: -200.000, steps: 200\n",
      "Episode 66: reward: -200.000, steps: 200\n",
      "Episode 67: reward: -200.000, steps: 200\n",
      "Episode 68: reward: -200.000, steps: 200\n",
      "Episode 69: reward: -200.000, steps: 200\n",
      "Episode 70: reward: -200.000, steps: 200\n",
      "Episode 71: reward: -200.000, steps: 200\n",
      "Episode 72: reward: -200.000, steps: 200\n",
      "Episode 73: reward: -200.000, steps: 200\n",
      "Episode 74: reward: -200.000, steps: 200\n",
      "Episode 75: reward: -200.000, steps: 200\n",
      "Episode 76: reward: -200.000, steps: 200\n",
      "Episode 77: reward: -200.000, steps: 200\n",
      "Episode 78: reward: -200.000, steps: 200\n",
      "Episode 79: reward: -200.000, steps: 200\n",
      "Episode 80: reward: -200.000, steps: 200\n",
      "Episode 81: reward: -200.000, steps: 200\n",
      "Episode 82: reward: -200.000, steps: 200\n",
      "Episode 83: reward: -200.000, steps: 200\n",
      "Episode 84: reward: -200.000, steps: 200\n",
      "Episode 85: reward: -200.000, steps: 200\n",
      "Episode 86: reward: -200.000, steps: 200\n",
      "Episode 87: reward: -200.000, steps: 200\n",
      "Episode 88: reward: -200.000, steps: 200\n",
      "Episode 89: reward: -200.000, steps: 200\n",
      "Episode 90: reward: -200.000, steps: 200\n",
      "Episode 91: reward: -200.000, steps: 200\n",
      "Episode 92: reward: -200.000, steps: 200\n",
      "Episode 93: reward: -200.000, steps: 200\n",
      "Episode 94: reward: -200.000, steps: 200\n",
      "Episode 95: reward: -200.000, steps: 200\n",
      "Episode 96: reward: -200.000, steps: 200\n",
      "Episode 97: reward: -200.000, steps: 200\n",
      "Episode 98: reward: -200.000, steps: 200\n",
      "Episode 99: reward: -200.000, steps: 200\n",
      "Episode 100: reward: -200.000, steps: 200\n",
      "-200.0\n"
     ]
    }
   ],
   "source": [
    "# Use dqn.test model to test our trained model\n",
    "scores = dqn.test(env,                             # pass our environment into the dqn.test model\n",
    "                  nb_episodes = 100,\n",
    "                  visualize = False                # set True if we want to visualize it\n",
    "                 )\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad4a7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 20:56:45.032 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f86673797a0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:56:45.033 python[3199:181180] Warning: Expected min height of view: (<NSButton: 0x7f86660824d0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:56:45.036 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8666085060>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:56:45.038 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f86673825f0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -200.000, steps: 200\n",
      "Episode 2: reward: -200.000, steps: 200\n",
      "Episode 3: reward: -200.000, steps: 200\n",
      "Episode 4: reward: -200.000, steps: 200\n",
      "Episode 5: reward: -200.000, steps: 200\n",
      "Episode 6: reward: -200.000, steps: 200\n",
      "Episode 7: reward: -200.000, steps: 200\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "Episode 9: reward: -200.000, steps: 200\n",
      "Episode 10: reward: -200.000, steps: 200\n",
      "Episode 11: reward: -200.000, steps: 200\n",
      "Episode 12: reward: -200.000, steps: 200\n",
      "Episode 13: reward: -200.000, steps: 200\n",
      "Episode 14: reward: -200.000, steps: 200\n",
      "Episode 15: reward: -200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env,\n",
    "             nb_episodes = 15,\n",
    "             visualize = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e1dde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fff13",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3f70f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Saved_weight/MountainCar_v0_dqn_weights_50K_steps_220301.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "131e4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a5d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 15:46:22.389878: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the model\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "states = env.observation_space.shape[0]      # 4 States under CartPole\n",
    "\n",
    "actions = env.action_space.n                 # 2 Actions: Left, Right\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "dqn = build_agent(model, actions)            # Used build_agent to setup a dqn model\n",
    "\n",
    "dqn.compile(Adam(learning_rate = 1e-3), \n",
    "            metrics = ['mae']\n",
    "           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee033f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('Saved_weight/MountainCar_v0_dqn_weights_50K_steps_220301.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "785e7c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 20:57:03.880 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8665ef7fb0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:57:03.880 python[3199:181180] Warning: Expected min height of view: (<NSButton: 0x7f86658bbb80>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:57:03.883 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f86658a31b0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-01 20:57:03.885 python[3199:181180] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f8661cc0720>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -114.000, steps: 114\n",
      "Episode 2: reward: -85.000, steps: 85\n",
      "Episode 3: reward: -115.000, steps: 115\n",
      "Episode 4: reward: -113.000, steps: 113\n",
      "Episode 5: reward: -117.000, steps: 117\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env,\n",
    "             nb_episodes = 5,\n",
    "             visualize = True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ed7f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8970f1f",
   "metadata": {},
   "source": [
    "# 5. View Log in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36392839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs/MountainCar-v0'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e328fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83132b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
