{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "I'm using the environment in the OpenAI gym - Classic Control to practice how to train an agent by reinforcement learning. In this notebook, a car is on a one-dimensional track, positioned between two \"mountains\". ***The goal is to drive up the mountain reaches the flag on the right***\n",
    "\n",
    "I try differnet methods to complete this goal:\n",
    "\n",
    "***1. Create a simple policy (without any learning applied)*** (See Section 4)\n",
    "- Result: ***The car reaches the flag on the right by only using the simple_policy with 111.8 timesteps spend on average*** \n",
    "\n",
    "\n",
    "***2. Create a Deep Learning Model with Keras*** (See Section 5)\n",
    "##### First Agent's Performance\n",
    "- Result: The first Agent after training with ***learning rate 1e-4 for 200k training steps***, ***the car reaches the flag on the right with 131.4 timesteps spend on average***, but such performance is not better than the simple policy without any learning.\n",
    "- loss: 0.309 at the end of the last episode means that we are still able to improve the performance. \n",
    "\n",
    "##### Second Agent's Performance\n",
    "- Result: The second Agent after training with ***learning rate 1e-3 for 500k training teps***, ***the car reaches the flag on the right with 89.4 timesteps spend on average***, which is ***20% timesteps saving than the simple policy*** without any learning and ***32% timesteps saving than the first agent***.\n",
    "- loss: 0.128 at the end of the last episode\n",
    "\n",
    "\n",
    "\n",
    "### ***Table of Content:***\n",
    "\n",
    "1. What is MountainCar\n",
    "2. Import Dependencies\n",
    "3. Understanding The Environment\n",
    "4. Create a Simple Policy (without any Learning to Comlete the Goal)\n",
    "5. Create a Deep Learning Model with Keras\n",
    "   - 5.1 Build Agent with Keras-RL\n",
    "   - 5.2 Train the First Agents\n",
    "   - 5.3 Train the Second Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is MountainCar?\n",
    "\n",
    "<img src='http://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/MountainCarContinuous-v0/poster.jpg' width='250px'/>\n",
    "\n",
    "A car is on a one-dimensional track, positioned between two \"mountains\". ***The goal is to drive up the mountain reaches the flag on the right***; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. \n",
    "\n",
    "### Action Space\n",
    "- There are 3 discrete deterministic actions:\n",
    "\n",
    "\n",
    " Num | Observation                                                 | Value | Unit         |\n",
    "-----|-------------------------------------------------------------|-------|--------------|\n",
    " 0   | Accelerate to the left                                      | Inf   | position (m) |\n",
    " 1   | Don't accelerate                                            | Inf   | position (m) |\n",
    " 2   | Accelerate to the right                                     | Inf   | position (m) |\n",
    "\n",
    " \n",
    "### Observation Space\n",
    "The observation space is a 2-dim vector:\n",
    "- the 1st element represents the \"car position\" \n",
    "- the 2nd element represents the \"car velocity\"\n",
    "\n",
    "\n",
    " Num | Observation                                                 | Min                | Max  | Unit         |\n",
    "-----|-------------------------------------------------------------|--------------------|------|--------------|\n",
    " 0   | position of the car along the x-axis                        | -Inf               | Inf  | position (m) |\n",
    " 1   | velocity of the car                                         | -Inf               | Inf  | position (m) |\n",
    " \n",
    "### Rewards\n",
    "***The goal is to reach the flag placed on top of the right hill as quickly as possible***, as such the agent is penalised with a ***reward of -1 for each timestep it isn't at the goal*** and is ***not penalised (reward = 0) for when it reaches the goal***.\n",
    "\n",
    "### Transition Dynamics:\n",
    "Given an action, the mountain car follows the following transition dynamics:\n",
    "- *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "- *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>* where force = 0.001 and gravity = 0.0025.\n",
    "\n",
    "- The collisions at either end are inelastic with the velocity set to 0 upon collision with the wall. \n",
    "- The position is clipped to the range `[-1.2, 0.6]` and velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "\n",
    "\n",
    "### Episode Termination\n",
    "The episode terminates if either of the following happens:\n",
    "1. The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "2. The length of the episode is 200.\n",
    "\n",
    "### Gym interface\n",
    "The three main methods of an environment are:\n",
    "* `reset()`: reset environment to the initial state, _return first observation_\n",
    "* `render()`: show current environment state\n",
    "* `step(a)`: commit action `a` and return `(new_observation, reward, is_done, info)`\n",
    " * `new_observation`: an observation right after committing the action `a`\n",
    " * `reward`: a number representing your reward for committing action `a`\n",
    " * `is_done`: True if the MDP has just finished, False if still in progress\n",
    " * `info`: some auxiliary stuff about what just happened. For now, ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zdDFE3eKB_EG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:03:07.388 python[1030:16639] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9ea4f27720>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-17 16:03:07.389 python[1030:16639] Warning: Expected min height of view: (<NSButton: 0x7f9ea3d68080>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-17 16:03:07.391 python[1030:16639] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9ea3d6ac90>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-17 16:03:07.393 python[1030:16639] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f9ea3d96dc0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")                       # Using MountainCar verion 0 in this notebook\n",
    "env.reset()                                            # reset the environment to initial default\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))                    # display the environment\n",
    "print(\"Observation space:\", env.observation_space)     # position: [-1.2, 0.6], velocity: [-0.07, 0.07] - [lower bound, upper bound]\n",
    "print(\"Action space:\", env.action_space)               # 3 actions could be taken in this environment: Accelerate to left, Don't accelerate, Acelerate to right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "29AZ1eL3B_EH",
    "outputId": "2db66c4e-6d20-411d-8614-70f9d427be06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<MountainCarEnv<MountainCar-v0>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close                                    # close the opened environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Understanding The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjrCiZlLB_EJ",
    "outputId": "231b81d7-f0df-4e16-8e2e-a4ad781e9b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: [-0.51635909  0.        ]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(\"initial observation code:\", obs)\n",
    "\n",
    "# Note: in MountainCar, observation is just two numbers: car position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PN06EBaGB_EJ",
    "outputId": "500a7cfd-e6ee-4cfe-e55a-534949e6f74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 2 (right)\n",
      "new observation code: [-0.51541338  0.00094571]\n",
      "reward: -1.0\n",
      "is game over?: False\n"
     ]
    }
   ],
   "source": [
    "# If an action 2: acelerate to right is given into the environment\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"taking action 2 (right)\")\n",
    "\n",
    "# Then the environment will reply 3 things new_observation, reward and is_done to the agent\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "\n",
    "# Note: the car has moved to the right slightly (from -0.4775 to -0.4769 around 0.001 moved to right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Baseline Use Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-200.0 Timesteps Spend:200\n",
      "Episode:2 Score:-200.0 Timesteps Spend:200\n",
      "Episode:3 Score:-200.0 Timesteps Spend:200\n",
      "Episode:4 Score:-200.0 Timesteps Spend:200\n",
      "Episode:5 Score:-200.0 Timesteps Spend:200\n",
      "CPU times: user 586 ms, sys: 106 ms, total: 692 ms\n",
      "Wall time: 747 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    obs = env.reset()                                      # initial the set of observation\n",
    "    \n",
    "    done = False                                           # initial the game is over = False, until reach maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    timesteps_counter = 0                                  # count the number of timestep spend\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment\n",
    "        \n",
    "        action = env.action_space.sample()                 # random choose an action \n",
    "        \n",
    "        n_obs, reward, done, info = env.step(action)       # pass random actions into environment to get back\n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward (Positive value increment, negative value decrement)\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward                                    # accumulate each episodes' reward received into score\n",
    "        \n",
    "        timesteps_counter += 1                             # +1 in each timestep spend\n",
    "        \n",
    "    print('Episode:{} Score:{} Timesteps Spend:{}'.format(episode, score,timesteps_counter))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, by the end of each episode (length per episode is 200). ***The car would not be able to drive up the mountain reaches the flag on the right by only choosing random actions*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfOpeNUdB_EK"
   },
   "source": [
    "# 4. Create a Simple Policy (without any Learning to Comlete the Goal)\n",
    "\n",
    "So, now let's create a simply policy (without any learning) to drive up the mountain reaches the flag on the right.\n",
    "- Use the observation (a tuple of position and velocity) to make a swing go farther and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qhx_zeynB_EK"
   },
   "outputs": [],
   "source": [
    "# Define 3 actions as Dictionary\n",
    "actions = {'Accelerate to left': 0, 'stop': 1, 'Acelerate to right': 2}      \n",
    "\n",
    "# Define a policy\n",
    "def simple_policy(obs):\n",
    "    \n",
    "    position, velocity = obs                                # observe current postion and velocity\n",
    "\n",
    "    if velocity > 0:                                        # if current velocity is positive\n",
    "        return actions['Acelerate to right']                # the policy return an actions 'Accelerate to right' \n",
    "\n",
    "    if velocity < 0:                                        # if current velocity is negative\n",
    "        return actions['Accelerate to left']                # the policy choose an actions 'Accelerate to left'\n",
    "    \n",
    "    else:                                                   # if current velocit is 0\n",
    "        return actions['stop']                              # the policy choose an actions 'stop' the car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-115.0 Timesteps Spend:115\n",
      "Episode:2 Score:-118.0 Timesteps Spend:118\n",
      "Episode:3 Score:-114.0 Timesteps Spend:114\n",
      "Episode:4 Score:-119.0 Timesteps Spend:119\n",
      "Episode:5 Score:-93.0 Timesteps Spend:93\n",
      "CPU times: user 309 ms, sys: 57.2 ms, total: 366 ms\n",
      "Wall time: 404 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    obs = env.reset()                                      # initial the set of observation\n",
    "    \n",
    "    done = False                                           # initial the game is over = False, until reach maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    timesteps_counter = 0                                  # count the number of timestep spend\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment\n",
    "        \n",
    "        action = simple_policy(obs)                        # random choose an action \n",
    "        \n",
    "        obs, reward, done, info = env.step(action)       # pass random actions into environment to get back\n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward (Positive value increment, negative value decrement)\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward                                    # accumulate each episodes' reward received into score\n",
    "        \n",
    "        timesteps_counter += 1                             # +1 in each timestep spend\n",
    "        \n",
    "    print('Episode:{} Score:{} Timesteps Spend:{}'.format(episode, score,timesteps_counter))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, ***The car reaches the flag on the right by only using the simple_policy with 111.8 timesteps spend on average*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Sequential Model\n",
    "def build_model(states, actions):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape = (1, states), name = 'States'))      # use Flatten layer as the input layer, input_shape = 1 x number of state \n",
    "    \n",
    "    model.add(Dense(512, activation = 'relu', name = 'Dense_1'))         # use Dense layer with 512 units of tensor with relu activation function\n",
    "    \n",
    "    model.add(Dense(256, activation = 'relu', name = 'Dense_2'))         # use another Dense layer with 256 units of tensor with relu activation function\n",
    "    \n",
    "    model.add(Dense(actions, activation = 'linear', name = 'Actions'))  # use third Dense as the out layer, output shape = number of actions with linear activation function\n",
    "    \n",
    "    return model                                                        # build_model return a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SqUOJSU5YxzR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "States (Flatten)             (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Actions (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 133,635\n",
      "Trainable params: 133,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input_shape of the build_model function\n",
    "states = env.observation_space.shape[0]                         # use .shape[0] to return number of observation space\n",
    "\n",
    "# Set the output_shape of the build_model function\n",
    "actions = env.action_space.n                                    # use .n to return number of action\n",
    "\n",
    "# Run the build_model function to build the model\n",
    "model = build_model(states, actions)                            \n",
    "\n",
    "# See the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Build Agent with Keras-RL\n",
    "\n",
    "***Two Types of Policy in Reinforcement Learning (RL):***\n",
    "1. Value Base RL\n",
    "2. Policy Base RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from rl.agents import DQNAgent                                                      # used DQNAgent here, should try other agents: SARSAAgent      \n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy      # used Policy base RL  \n",
    "from rl.memory import SequentialMemory\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an Agent to learn from the model\n",
    "def build_agent(model, actions):            \n",
    "    \n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),    # use EpsGreedyQPolicy()\n",
    "                                  attr='eps',\n",
    "                                  value_max = 1,\n",
    "                                  value_min = .1,\n",
    "                                  value_test = 0.2,\n",
    "                                  nb_steps = 10000\n",
    "    )                    \n",
    "\n",
    "    memory = SequentialMemory(limit = 1000,              # buffer limit: number of episode\n",
    "                              window_length = 1          # store pass 1 window for 1000 episode \n",
    "                             )\n",
    "    \n",
    "    dqn = DQNAgent(model = model, \n",
    "                   memory = memory,\n",
    "                   policy = policy,\n",
    "                   enable_dueling_network = True,        # Dueling Networks split value and advantage, help the model learn when to take action and when not to bother\n",
    "                   dueling_type = 'avg',\n",
    "                   nb_actions = actions,                 # 3 actions to learn\n",
    "                   nb_steps_warmup = 1000,           \n",
    "                   target_model_update = 1e-2\n",
    "                  )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Train the First Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to save the log\n",
    "log_path = os.path.join(\"Training\", \"Logs\", \"MountainCar-v0_Keras-RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n",
      "    200/200000: episode: 1, duration: 0.143s, episode steps: 200, steps per second: 1403, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    400/200000: episode: 2, duration: 0.095s, episode steps: 200, steps per second: 2104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    600/200000: episode: 3, duration: 0.090s, episode steps: 200, steps per second: 2224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    800/200000: episode: 4, duration: 0.094s, episode steps: 200, steps per second: 2138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1000/200000: episode: 5, duration: 0.092s, episode steps: 200, steps per second: 2177, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1200/200000: episode: 6, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.139418, mae: 0.715063, mean_q: -0.827640, mean_eps: 0.901000\n",
      "   1400/200000: episode: 7, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.013064, mae: 1.738857, mean_q: -2.543445, mean_eps: 0.883045\n",
      "   1600/200000: episode: 8, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.029993, mae: 2.822795, mean_q: -4.167069, mean_eps: 0.865045\n",
      "   1800/200000: episode: 9, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.078236, mae: 3.938225, mean_q: -5.819538, mean_eps: 0.847045\n",
      "   2000/200000: episode: 10, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.106786, mae: 5.036026, mean_q: -7.446151, mean_eps: 0.829045\n",
      "   2200/200000: episode: 11, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.132280, mae: 6.166982, mean_q: -9.147197, mean_eps: 0.811045\n",
      "   2400/200000: episode: 12, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.233928, mae: 7.217169, mean_q: -10.682763, mean_eps: 0.793045\n",
      "   2600/200000: episode: 13, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.313552, mae: 8.203892, mean_q: -12.142624, mean_eps: 0.775045\n",
      "   2800/200000: episode: 14, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.277198, mae: 9.151329, mean_q: -13.568598, mean_eps: 0.757045\n",
      "   3000/200000: episode: 15, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.638287, mae: 10.036880, mean_q: -14.801455, mean_eps: 0.739045\n",
      "   3200/200000: episode: 16, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.654396, mae: 10.772193, mean_q: -15.941460, mean_eps: 0.721045\n",
      "   3400/200000: episode: 17, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.803940, mae: 11.617677, mean_q: -17.187716, mean_eps: 0.703045\n",
      "   3600/200000: episode: 18, duration: 0.920s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.772127, mae: 12.467886, mean_q: -18.526096, mean_eps: 0.685045\n",
      "   3800/200000: episode: 19, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.044021, mae: 13.337725, mean_q: -19.748644, mean_eps: 0.667045\n",
      "   4000/200000: episode: 20, duration: 0.872s, episode steps: 200, steps per second: 229, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.932878, mae: 14.206298, mean_q: -21.091028, mean_eps: 0.649045\n",
      "   4200/200000: episode: 21, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.382519, mae: 15.036667, mean_q: -22.256283, mean_eps: 0.631045\n",
      "   4400/200000: episode: 22, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.256934, mae: 15.877089, mean_q: -23.590919, mean_eps: 0.613045\n",
      "   4600/200000: episode: 23, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.962479, mae: 16.713224, mean_q: -24.753110, mean_eps: 0.595045\n",
      "   4800/200000: episode: 24, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 1.994673, mae: 17.490824, mean_q: -25.935812, mean_eps: 0.577045\n",
      "   5000/200000: episode: 25, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 1.762596, mae: 18.248830, mean_q: -27.101251, mean_eps: 0.559045\n",
      "   5200/200000: episode: 26, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 2.047319, mae: 19.047789, mean_q: -28.289630, mean_eps: 0.541045\n",
      "   5400/200000: episode: 27, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 1.832154, mae: 19.726977, mean_q: -29.321179, mean_eps: 0.523045\n",
      "   5600/200000: episode: 28, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 1.823119, mae: 20.397401, mean_q: -30.391991, mean_eps: 0.505045\n",
      "   5800/200000: episode: 29, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 2.255802, mae: 21.030567, mean_q: -31.254980, mean_eps: 0.487045\n",
      "   6000/200000: episode: 30, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.272932, mae: 21.563117, mean_q: -32.036790, mean_eps: 0.469045\n",
      "   6200/200000: episode: 31, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 2.000],  loss: 1.849747, mae: 21.981049, mean_q: -32.680308, mean_eps: 0.451045\n",
      "   6400/200000: episode: 32, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 3.420921, mae: 22.287165, mean_q: -32.992675, mean_eps: 0.433045\n",
      "   6600/200000: episode: 33, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.028793, mae: 22.677969, mean_q: -33.690923, mean_eps: 0.415045\n",
      "   6800/200000: episode: 34, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 3.316179, mae: 23.204148, mean_q: -34.453535, mean_eps: 0.397045\n",
      "   7000/200000: episode: 35, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 2.446744, mae: 23.533870, mean_q: -34.970095, mean_eps: 0.379045\n",
      "   7200/200000: episode: 36, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.226430, mae: 23.931781, mean_q: -35.466067, mean_eps: 0.361045\n",
      "   7400/200000: episode: 37, duration: 0.916s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 2.989842, mae: 24.216737, mean_q: -35.980533, mean_eps: 0.343045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7600/200000: episode: 38, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000],  loss: 4.395757, mae: 24.784417, mean_q: -36.618443, mean_eps: 0.325045\n",
      "   7800/200000: episode: 39, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.833337, mae: 25.115788, mean_q: -37.157150, mean_eps: 0.307045\n",
      "   8000/200000: episode: 40, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 2.706774, mae: 25.543455, mean_q: -37.939907, mean_eps: 0.289045\n",
      "   8200/200000: episode: 41, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 3.626161, mae: 25.868976, mean_q: -38.421581, mean_eps: 0.271045\n",
      "   8400/200000: episode: 42, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 2.000],  loss: 5.164437, mae: 26.267892, mean_q: -38.853458, mean_eps: 0.253045\n",
      "   8600/200000: episode: 43, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.650 [0.000, 2.000],  loss: 4.812629, mae: 26.368870, mean_q: -38.991260, mean_eps: 0.235045\n",
      "   8800/200000: episode: 44, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.740 [0.000, 2.000],  loss: 4.000354, mae: 26.604505, mean_q: -39.319424, mean_eps: 0.217045\n",
      "   9000/200000: episode: 45, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.810 [0.000, 2.000],  loss: 3.620258, mae: 26.900368, mean_q: -39.833533, mean_eps: 0.199045\n",
      "   9200/200000: episode: 46, duration: 0.903s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.785 [0.000, 2.000],  loss: 4.394797, mae: 27.255119, mean_q: -40.292360, mean_eps: 0.181045\n",
      "   9400/200000: episode: 47, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.790 [0.000, 2.000],  loss: 5.226875, mae: 27.593588, mean_q: -40.718895, mean_eps: 0.163045\n",
      "   9600/200000: episode: 48, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.735 [0.000, 2.000],  loss: 4.026442, mae: 27.802881, mean_q: -41.199161, mean_eps: 0.145045\n",
      "   9800/200000: episode: 49, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.800 [0.000, 2.000],  loss: 3.807039, mae: 28.271097, mean_q: -41.952422, mean_eps: 0.127045\n",
      "  10000/200000: episode: 50, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 4.487787, mae: 28.613608, mean_q: -42.452464, mean_eps: 0.109045\n",
      "  10200/200000: episode: 51, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.640 [0.000, 2.000],  loss: 4.422994, mae: 29.020500, mean_q: -42.996900, mean_eps: 0.100000\n",
      "  10400/200000: episode: 52, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000],  loss: 3.966178, mae: 29.412124, mean_q: -43.669046, mean_eps: 0.100000\n",
      "  10600/200000: episode: 53, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.128487, mae: 29.685000, mean_q: -43.954265, mean_eps: 0.100000\n",
      "  10800/200000: episode: 54, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000],  loss: 5.280727, mae: 29.928016, mean_q: -44.362327, mean_eps: 0.100000\n",
      "  11000/200000: episode: 55, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 4.874858, mae: 30.270492, mean_q: -44.855245, mean_eps: 0.100000\n",
      "  11200/200000: episode: 56, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 4.989749, mae: 30.572368, mean_q: -45.311625, mean_eps: 0.100000\n",
      "  11400/200000: episode: 57, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 6.284212, mae: 30.827893, mean_q: -45.629267, mean_eps: 0.100000\n",
      "  11600/200000: episode: 58, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000],  loss: 4.242813, mae: 30.864585, mean_q: -45.852437, mean_eps: 0.100000\n",
      "  11800/200000: episode: 59, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 6.141527, mae: 31.073194, mean_q: -46.067652, mean_eps: 0.100000\n",
      "  12000/200000: episode: 60, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 2.000],  loss: 4.881671, mae: 31.283790, mean_q: -46.376831, mean_eps: 0.100000\n",
      "  12200/200000: episode: 61, duration: 0.903s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 2.000],  loss: 4.844545, mae: 31.501749, mean_q: -46.698093, mean_eps: 0.100000\n",
      "  12400/200000: episode: 62, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.640 [0.000, 2.000],  loss: 5.864439, mae: 31.722837, mean_q: -47.010502, mean_eps: 0.100000\n",
      "  12600/200000: episode: 63, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.865 [0.000, 2.000],  loss: 5.817283, mae: 31.984467, mean_q: -47.284390, mean_eps: 0.100000\n",
      "  12800/200000: episode: 64, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 2.000],  loss: 5.085016, mae: 32.128339, mean_q: -47.487248, mean_eps: 0.100000\n",
      "  13000/200000: episode: 65, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.905 [0.000, 2.000],  loss: 7.351859, mae: 32.376457, mean_q: -47.650753, mean_eps: 0.100000\n",
      "  13200/200000: episode: 66, duration: 0.937s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 2.000],  loss: 4.977275, mae: 32.515185, mean_q: -48.059213, mean_eps: 0.100000\n",
      "  13400/200000: episode: 67, duration: 0.968s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.920 [0.000, 2.000],  loss: 5.774022, mae: 32.823033, mean_q: -48.530505, mean_eps: 0.100000\n",
      "  13600/200000: episode: 68, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.905 [0.000, 2.000],  loss: 5.235294, mae: 33.142599, mean_q: -49.088420, mean_eps: 0.100000\n",
      "  13800/200000: episode: 69, duration: 0.946s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.845 [0.000, 2.000],  loss: 5.338175, mae: 33.474503, mean_q: -49.581901, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14000/200000: episode: 70, duration: 0.921s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000],  loss: 4.822670, mae: 33.763752, mean_q: -50.125878, mean_eps: 0.100000\n",
      "  14200/200000: episode: 71, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.905 [0.000, 2.000],  loss: 6.200498, mae: 34.154759, mean_q: -50.631778, mean_eps: 0.100000\n",
      "  14400/200000: episode: 72, duration: 0.919s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.905 [0.000, 2.000],  loss: 8.437739, mae: 34.454089, mean_q: -50.862922, mean_eps: 0.100000\n",
      "  14600/200000: episode: 73, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.830 [0.000, 2.000],  loss: 7.355672, mae: 34.625626, mean_q: -51.207116, mean_eps: 0.100000\n",
      "  14800/200000: episode: 74, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.875 [0.000, 2.000],  loss: 5.184760, mae: 34.835451, mean_q: -51.672433, mean_eps: 0.100000\n",
      "  15000/200000: episode: 75, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.905 [0.000, 2.000],  loss: 6.510192, mae: 35.135041, mean_q: -52.087801, mean_eps: 0.100000\n",
      "  15200/200000: episode: 76, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.860 [0.000, 2.000],  loss: 6.949724, mae: 35.409562, mean_q: -52.398955, mean_eps: 0.100000\n",
      "  15400/200000: episode: 77, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.895 [0.000, 2.000],  loss: 7.337212, mae: 35.635092, mean_q: -52.693656, mean_eps: 0.100000\n",
      "  15600/200000: episode: 78, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.840 [0.000, 2.000],  loss: 5.479805, mae: 35.854035, mean_q: -53.155827, mean_eps: 0.100000\n",
      "  15800/200000: episode: 79, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 7.125009, mae: 36.001432, mean_q: -53.365320, mean_eps: 0.100000\n",
      "  16000/200000: episode: 80, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 7.496548, mae: 35.980002, mean_q: -53.265486, mean_eps: 0.100000\n",
      "  16200/200000: episode: 81, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.603117, mae: 36.102112, mean_q: -53.473883, mean_eps: 0.100000\n",
      "  16400/200000: episode: 82, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.510 [0.000, 2.000],  loss: 7.104687, mae: 36.297314, mean_q: -53.807048, mean_eps: 0.100000\n",
      "  16600/200000: episode: 83, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 7.670052, mae: 36.377380, mean_q: -53.827836, mean_eps: 0.100000\n",
      "  16800/200000: episode: 84, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.319787, mae: 36.319127, mean_q: -53.873446, mean_eps: 0.100000\n",
      "  17000/200000: episode: 85, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.740 [0.000, 2.000],  loss: 6.604060, mae: 36.424204, mean_q: -54.093773, mean_eps: 0.100000\n",
      "  17200/200000: episode: 86, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.710 [0.000, 2.000],  loss: 4.968712, mae: 36.649561, mean_q: -54.477651, mean_eps: 0.100000\n",
      "  17400/200000: episode: 87, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.275 [0.000, 2.000],  loss: 9.480312, mae: 36.782717, mean_q: -54.488858, mean_eps: 0.100000\n",
      "  17600/200000: episode: 88, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 7.752494, mae: 36.884395, mean_q: -54.670704, mean_eps: 0.100000\n",
      "  17800/200000: episode: 89, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.330 [0.000, 2.000],  loss: 7.427377, mae: 37.105217, mean_q: -54.986079, mean_eps: 0.100000\n",
      "  18000/200000: episode: 90, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.440 [0.000, 2.000],  loss: 6.287158, mae: 37.287043, mean_q: -55.390972, mean_eps: 0.100000\n",
      "  18200/200000: episode: 91, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.390 [0.000, 2.000],  loss: 7.951508, mae: 37.510842, mean_q: -55.628426, mean_eps: 0.100000\n",
      "  18400/200000: episode: 92, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.320 [0.000, 2.000],  loss: 5.898271, mae: 37.711569, mean_q: -56.082720, mean_eps: 0.100000\n",
      "  18600/200000: episode: 93, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.260 [0.000, 2.000],  loss: 6.367105, mae: 37.983744, mean_q: -56.454878, mean_eps: 0.100000\n",
      "  18800/200000: episode: 94, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.210 [0.000, 2.000],  loss: 7.143261, mae: 38.190036, mean_q: -56.727573, mean_eps: 0.100000\n",
      "  19000/200000: episode: 95, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 8.055383, mae: 38.409771, mean_q: -56.981066, mean_eps: 0.100000\n",
      "  19200/200000: episode: 96, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.125 [0.000, 2.000],  loss: 10.528802, mae: 38.584721, mean_q: -57.124560, mean_eps: 0.100000\n",
      "  19400/200000: episode: 97, duration: 0.903s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.405 [0.000, 2.000],  loss: 9.419890, mae: 38.656486, mean_q: -57.305004, mean_eps: 0.100000\n",
      "  19600/200000: episode: 98, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.210 [0.000, 2.000],  loss: 6.351039, mae: 38.697730, mean_q: -57.562280, mean_eps: 0.100000\n",
      "  19800/200000: episode: 99, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 7.415998, mae: 38.570516, mean_q: -57.344088, mean_eps: 0.100000\n",
      "  20000/200000: episode: 100, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 7.044898, mae: 38.356104, mean_q: -56.961977, mean_eps: 0.100000\n",
      "  20200/200000: episode: 101, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 6.967419, mae: 38.322206, mean_q: -56.903861, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20400/200000: episode: 102, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 8.144292, mae: 38.217694, mean_q: -56.742116, mean_eps: 0.100000\n",
      "  20600/200000: episode: 103, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 9.128014, mae: 38.158824, mean_q: -56.655801, mean_eps: 0.100000\n",
      "  20800/200000: episode: 104, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.754308, mae: 37.949712, mean_q: -56.305938, mean_eps: 0.100000\n",
      "  21000/200000: episode: 105, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.642659, mae: 37.961472, mean_q: -56.480829, mean_eps: 0.100000\n",
      "  21200/200000: episode: 106, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 8.612931, mae: 38.033708, mean_q: -56.527092, mean_eps: 0.100000\n",
      "  21400/200000: episode: 107, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 2.000],  loss: 7.252226, mae: 38.113270, mean_q: -56.714271, mean_eps: 0.100000\n",
      "  21600/200000: episode: 108, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 2.000],  loss: 7.902445, mae: 38.179155, mean_q: -56.776310, mean_eps: 0.100000\n",
      "  21800/200000: episode: 109, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.765 [0.000, 2.000],  loss: 11.175382, mae: 38.034636, mean_q: -56.347440, mean_eps: 0.100000\n",
      "  22000/200000: episode: 110, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 4.953225, mae: 38.061221, mean_q: -56.752628, mean_eps: 0.100000\n",
      "  22200/200000: episode: 111, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000],  loss: 5.741577, mae: 38.302783, mean_q: -57.006515, mean_eps: 0.100000\n",
      "  22400/200000: episode: 112, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.705 [0.000, 2.000],  loss: 8.102271, mae: 38.460378, mean_q: -57.095634, mean_eps: 0.100000\n",
      "  22600/200000: episode: 113, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 8.833730, mae: 38.487367, mean_q: -57.097975, mean_eps: 0.100000\n",
      "  22800/200000: episode: 114, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 8.728242, mae: 38.420112, mean_q: -56.862413, mean_eps: 0.100000\n",
      "  23000/200000: episode: 115, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 6.671663, mae: 38.348416, mean_q: -56.846332, mean_eps: 0.100000\n",
      "  23200/200000: episode: 116, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 7.314099, mae: 38.394300, mean_q: -56.986698, mean_eps: 0.100000\n",
      "  23400/200000: episode: 117, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 6.076375, mae: 38.490859, mean_q: -57.205727, mean_eps: 0.100000\n",
      "  23600/200000: episode: 118, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.840 [0.000, 2.000],  loss: 10.689287, mae: 38.506713, mean_q: -56.970424, mean_eps: 0.100000\n",
      "  23800/200000: episode: 119, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.720 [0.000, 2.000],  loss: 7.866591, mae: 38.364394, mean_q: -56.774819, mean_eps: 0.100000\n",
      "  24000/200000: episode: 120, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.660 [0.000, 2.000],  loss: 7.299454, mae: 38.346345, mean_q: -56.795049, mean_eps: 0.100000\n",
      "  24200/200000: episode: 121, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.695 [0.000, 2.000],  loss: 6.973993, mae: 38.425054, mean_q: -56.983007, mean_eps: 0.100000\n",
      "  24400/200000: episode: 122, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.680 [0.000, 2.000],  loss: 9.058234, mae: 38.542658, mean_q: -57.131631, mean_eps: 0.100000\n",
      "  24600/200000: episode: 123, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 2.000],  loss: 8.930286, mae: 38.587408, mean_q: -57.137055, mean_eps: 0.100000\n",
      "  24800/200000: episode: 124, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 2.000],  loss: 12.767058, mae: 38.566473, mean_q: -57.003961, mean_eps: 0.100000\n",
      "  25000/200000: episode: 125, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.670 [0.000, 2.000],  loss: 10.982173, mae: 38.579889, mean_q: -57.036926, mean_eps: 0.100000\n",
      "  25200/200000: episode: 126, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.710 [0.000, 2.000],  loss: 6.034190, mae: 38.582368, mean_q: -57.362746, mean_eps: 0.100000\n",
      "  25400/200000: episode: 127, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.640 [0.000, 2.000],  loss: 8.194440, mae: 38.635686, mean_q: -57.432436, mean_eps: 0.100000\n",
      "  25600/200000: episode: 128, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 2.000],  loss: 9.606809, mae: 38.681571, mean_q: -57.440238, mean_eps: 0.100000\n",
      "  25800/200000: episode: 129, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 9.836112, mae: 38.616884, mean_q: -57.301738, mean_eps: 0.100000\n",
      "  26000/200000: episode: 130, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 8.917502, mae: 38.488542, mean_q: -57.099645, mean_eps: 0.100000\n",
      "  26200/200000: episode: 131, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 7.120888, mae: 38.445149, mean_q: -57.170902, mean_eps: 0.100000\n",
      "  26400/200000: episode: 132, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 5.792415, mae: 38.482598, mean_q: -57.209844, mean_eps: 0.100000\n",
      "  26600/200000: episode: 133, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.109481, mae: 38.498481, mean_q: -57.178505, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  26800/200000: episode: 134, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 5.257500, mae: 38.604941, mean_q: -57.528828, mean_eps: 0.100000\n",
      "  27000/200000: episode: 135, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.730 [0.000, 2.000],  loss: 7.610494, mae: 38.761819, mean_q: -57.581711, mean_eps: 0.100000\n",
      "  27200/200000: episode: 136, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.575 [0.000, 2.000],  loss: 8.324044, mae: 38.851743, mean_q: -57.713230, mean_eps: 0.100000\n",
      "  27400/200000: episode: 137, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 9.824375, mae: 38.989096, mean_q: -57.846967, mean_eps: 0.100000\n",
      "  27600/200000: episode: 138, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.775 [0.000, 2.000],  loss: 7.672142, mae: 39.194758, mean_q: -58.236510, mean_eps: 0.100000\n",
      "  27800/200000: episode: 139, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.700 [0.000, 2.000],  loss: 8.419737, mae: 39.492785, mean_q: -58.612306, mean_eps: 0.100000\n",
      "  28000/200000: episode: 140, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.700 [0.000, 2.000],  loss: 7.777291, mae: 39.556324, mean_q: -58.803897, mean_eps: 0.100000\n",
      "  28200/200000: episode: 141, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.855 [0.000, 2.000],  loss: 7.829668, mae: 39.694060, mean_q: -58.945359, mean_eps: 0.100000\n",
      "  28400/200000: episode: 142, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.935 [0.000, 2.000],  loss: 11.656809, mae: 39.910480, mean_q: -59.005464, mean_eps: 0.100000\n",
      "  28600/200000: episode: 143, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 2.000],  loss: 9.356294, mae: 39.961875, mean_q: -59.210315, mean_eps: 0.100000\n",
      "  28800/200000: episode: 144, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.860 [0.000, 2.000],  loss: 9.340758, mae: 40.090130, mean_q: -59.443592, mean_eps: 0.100000\n",
      "  29000/200000: episode: 145, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000],  loss: 6.181849, mae: 40.277967, mean_q: -59.889295, mean_eps: 0.100000\n",
      "  29200/200000: episode: 146, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.865 [0.000, 2.000],  loss: 7.579313, mae: 40.521472, mean_q: -60.244726, mean_eps: 0.100000\n",
      "  29400/200000: episode: 147, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.890 [0.000, 2.000],  loss: 12.145455, mae: 40.698693, mean_q: -60.205858, mean_eps: 0.100000\n",
      "  29600/200000: episode: 148, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.945 [0.000, 2.000],  loss: 8.013760, mae: 40.753889, mean_q: -60.428879, mean_eps: 0.100000\n",
      "  29800/200000: episode: 149, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.875 [0.000, 2.000],  loss: 6.581666, mae: 40.842003, mean_q: -60.740922, mean_eps: 0.100000\n",
      "  30000/200000: episode: 150, duration: 0.874s, episode steps: 200, steps per second: 229, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.925 [0.000, 2.000],  loss: 8.894367, mae: 40.982328, mean_q: -60.829298, mean_eps: 0.100000\n",
      "  30200/200000: episode: 151, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.830 [0.000, 2.000],  loss: 9.184932, mae: 41.081585, mean_q: -61.012746, mean_eps: 0.100000\n",
      "  30400/200000: episode: 152, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000],  loss: 10.290717, mae: 41.165289, mean_q: -61.089557, mean_eps: 0.100000\n",
      "  30600/200000: episode: 153, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.965 [1.000, 2.000],  loss: 9.495559, mae: 41.256191, mean_q: -61.223667, mean_eps: 0.100000\n",
      "  30800/200000: episode: 154, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.915 [0.000, 2.000],  loss: 10.979570, mae: 41.337751, mean_q: -61.276754, mean_eps: 0.100000\n",
      "  31000/200000: episode: 155, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.910 [0.000, 2.000],  loss: 9.498311, mae: 41.352686, mean_q: -61.301054, mean_eps: 0.100000\n",
      "  31200/200000: episode: 156, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.895 [0.000, 2.000],  loss: 7.544098, mae: 41.418628, mean_q: -61.573765, mean_eps: 0.100000\n",
      "  31400/200000: episode: 157, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.855 [0.000, 2.000],  loss: 9.123420, mae: 41.574243, mean_q: -61.798384, mean_eps: 0.100000\n",
      "  31600/200000: episode: 158, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.865 [0.000, 2.000],  loss: 7.912808, mae: 41.660498, mean_q: -61.934891, mean_eps: 0.100000\n",
      "  31800/200000: episode: 159, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.835 [0.000, 2.000],  loss: 14.017710, mae: 41.791383, mean_q: -61.857100, mean_eps: 0.100000\n",
      "  32000/200000: episode: 160, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.915 [0.000, 2.000],  loss: 9.555466, mae: 41.697879, mean_q: -61.857087, mean_eps: 0.100000\n",
      "  32200/200000: episode: 161, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.850 [0.000, 2.000],  loss: 10.426711, mae: 41.697964, mean_q: -61.829758, mean_eps: 0.100000\n",
      "  32400/200000: episode: 162, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000],  loss: 6.702039, mae: 41.754367, mean_q: -62.182744, mean_eps: 0.100000\n",
      "  32600/200000: episode: 163, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.870 [0.000, 2.000],  loss: 11.988426, mae: 41.895902, mean_q: -62.015835, mean_eps: 0.100000\n",
      "  32800/200000: episode: 164, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.945 [0.000, 2.000],  loss: 11.429365, mae: 41.789276, mean_q: -61.878771, mean_eps: 0.100000\n",
      "  33000/200000: episode: 165, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.845 [0.000, 2.000],  loss: 8.250122, mae: 41.725514, mean_q: -62.045664, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  33200/200000: episode: 166, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.670 [0.000, 2.000],  loss: 10.192335, mae: 41.819849, mean_q: -62.059781, mean_eps: 0.100000\n",
      "  33400/200000: episode: 167, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.630 [0.000, 2.000],  loss: 7.712550, mae: 41.850534, mean_q: -62.302995, mean_eps: 0.100000\n",
      "  33600/200000: episode: 168, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.855 [0.000, 2.000],  loss: 11.035535, mae: 41.927961, mean_q: -62.297741, mean_eps: 0.100000\n",
      "  33800/200000: episode: 169, duration: 0.905s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.850 [0.000, 2.000],  loss: 8.822632, mae: 41.851108, mean_q: -62.232118, mean_eps: 0.100000\n",
      "  34000/200000: episode: 170, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.735 [0.000, 2.000],  loss: 9.286955, mae: 41.948448, mean_q: -62.455283, mean_eps: 0.100000\n",
      "  34200/200000: episode: 171, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.860 [0.000, 2.000],  loss: 12.686850, mae: 41.960297, mean_q: -62.233496, mean_eps: 0.100000\n",
      "  34400/200000: episode: 172, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.565 [0.000, 2.000],  loss: 9.385940, mae: 41.913059, mean_q: -62.365835, mean_eps: 0.100000\n",
      "  34600/200000: episode: 173, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.710 [0.000, 2.000],  loss: 10.023637, mae: 41.993084, mean_q: -62.435754, mean_eps: 0.100000\n",
      "  34800/200000: episode: 174, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.740 [0.000, 2.000],  loss: 8.754760, mae: 41.995777, mean_q: -62.528179, mean_eps: 0.100000\n",
      "  35000/200000: episode: 175, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 2.000],  loss: 10.029461, mae: 42.099702, mean_q: -62.623351, mean_eps: 0.100000\n",
      "  35200/200000: episode: 176, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.620 [0.000, 2.000],  loss: 8.601427, mae: 42.159613, mean_q: -62.748220, mean_eps: 0.100000\n",
      "  35400/200000: episode: 177, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.585 [0.000, 2.000],  loss: 10.492187, mae: 41.902215, mean_q: -62.170058, mean_eps: 0.100000\n",
      "  35600/200000: episode: 178, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.590 [0.000, 2.000],  loss: 9.219077, mae: 41.536191, mean_q: -61.600249, mean_eps: 0.100000\n",
      "  35800/200000: episode: 179, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.465 [0.000, 2.000],  loss: 8.364789, mae: 41.362357, mean_q: -61.335012, mean_eps: 0.100000\n",
      "  36000/200000: episode: 180, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.180 [0.000, 2.000],  loss: 6.350494, mae: 41.305433, mean_q: -61.317889, mean_eps: 0.100000\n",
      "  36200/200000: episode: 181, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 10.584673, mae: 41.333233, mean_q: -60.983508, mean_eps: 0.100000\n",
      "  36400/200000: episode: 182, duration: 0.921s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 7.712747, mae: 41.275209, mean_q: -61.103138, mean_eps: 0.100000\n",
      "  36600/200000: episode: 183, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 8.450475, mae: 41.290909, mean_q: -61.213325, mean_eps: 0.100000\n",
      "  36800/200000: episode: 184, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 6.490348, mae: 41.438434, mean_q: -61.569507, mean_eps: 0.100000\n",
      "  37000/200000: episode: 185, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 9.758400, mae: 41.680879, mean_q: -61.721147, mean_eps: 0.100000\n",
      "  37200/200000: episode: 186, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.060 [0.000, 2.000],  loss: 9.314626, mae: 41.744748, mean_q: -61.804489, mean_eps: 0.100000\n",
      "  37400/200000: episode: 187, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 9.645375, mae: 41.769971, mean_q: -61.832313, mean_eps: 0.100000\n",
      "  37600/200000: episode: 188, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 12.027339, mae: 41.798639, mean_q: -61.784527, mean_eps: 0.100000\n",
      "  37800/200000: episode: 189, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 9.458525, mae: 41.775437, mean_q: -61.885812, mean_eps: 0.100000\n",
      "  38000/200000: episode: 190, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.170 [0.000, 2.000],  loss: 7.911097, mae: 41.792710, mean_q: -62.024256, mean_eps: 0.100000\n",
      "  38200/200000: episode: 191, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 12.135834, mae: 41.895414, mean_q: -61.948343, mean_eps: 0.100000\n",
      "  38400/200000: episode: 192, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 13.438068, mae: 41.795831, mean_q: -61.668690, mean_eps: 0.100000\n",
      "  38600/200000: episode: 193, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 8.960891, mae: 41.724731, mean_q: -61.801475, mean_eps: 0.100000\n",
      "  38800/200000: episode: 194, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 8.621314, mae: 41.765690, mean_q: -61.965796, mean_eps: 0.100000\n",
      "  39000/200000: episode: 195, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 2.000],  loss: 8.850963, mae: 41.826277, mean_q: -62.081777, mean_eps: 0.100000\n",
      "  39200/200000: episode: 196, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 8.541150, mae: 41.878300, mean_q: -62.167487, mean_eps: 0.100000\n",
      "  39400/200000: episode: 197, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 7.091126, mae: 42.000660, mean_q: -62.526140, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  39600/200000: episode: 198, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.150 [0.000, 2.000],  loss: 9.214097, mae: 42.125906, mean_q: -62.609618, mean_eps: 0.100000\n",
      "  39800/200000: episode: 199, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 9.222167, mae: 42.216849, mean_q: -62.659197, mean_eps: 0.100000\n",
      "  40000/200000: episode: 200, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 11.363277, mae: 42.245787, mean_q: -62.589641, mean_eps: 0.100000\n",
      "  40200/200000: episode: 201, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 11.998490, mae: 42.275581, mean_q: -62.587242, mean_eps: 0.100000\n",
      "  40400/200000: episode: 202, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 12.189232, mae: 42.201090, mean_q: -62.443605, mean_eps: 0.100000\n",
      "  40600/200000: episode: 203, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 2.000],  loss: 8.056095, mae: 42.121033, mean_q: -62.510061, mean_eps: 0.100000\n",
      "  40800/200000: episode: 204, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 8.321138, mae: 42.174752, mean_q: -62.670523, mean_eps: 0.100000\n",
      "  41000/200000: episode: 205, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 9.830816, mae: 42.249970, mean_q: -62.732450, mean_eps: 0.100000\n",
      "  41200/200000: episode: 206, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 14.992046, mae: 42.228322, mean_q: -62.403125, mean_eps: 0.100000\n",
      "  41400/200000: episode: 207, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 10.840357, mae: 42.115912, mean_q: -62.438563, mean_eps: 0.100000\n",
      "  41600/200000: episode: 208, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 7.949418, mae: 42.119249, mean_q: -62.622792, mean_eps: 0.100000\n",
      "  41800/200000: episode: 209, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 12.223688, mae: 42.165799, mean_q: -62.491155, mean_eps: 0.100000\n",
      "  42000/200000: episode: 210, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 9.796324, mae: 42.064456, mean_q: -62.422590, mean_eps: 0.100000\n",
      "  42200/200000: episode: 211, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 8.966870, mae: 42.075253, mean_q: -62.536070, mean_eps: 0.100000\n",
      "  42400/200000: episode: 212, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 11.683462, mae: 42.060969, mean_q: -62.379365, mean_eps: 0.100000\n",
      "  42600/200000: episode: 213, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 9.070960, mae: 42.013241, mean_q: -62.420922, mean_eps: 0.100000\n",
      "  42800/200000: episode: 214, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 11.151003, mae: 42.016236, mean_q: -62.384892, mean_eps: 0.100000\n",
      "  43000/200000: episode: 215, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 11.432388, mae: 41.933552, mean_q: -62.213087, mean_eps: 0.100000\n",
      "  43200/200000: episode: 216, duration: 0.905s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 8.588758, mae: 41.930132, mean_q: -62.235763, mean_eps: 0.100000\n",
      "  43400/200000: episode: 217, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 9.176394, mae: 41.902863, mean_q: -62.125722, mean_eps: 0.100000\n",
      "  43600/200000: episode: 218, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 8.130970, mae: 41.899830, mean_q: -62.285774, mean_eps: 0.100000\n",
      "  43800/200000: episode: 219, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.645 [0.000, 2.000],  loss: 7.241126, mae: 41.942552, mean_q: -62.349052, mean_eps: 0.100000\n",
      "  44000/200000: episode: 220, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.172104, mae: 41.792388, mean_q: -61.937550, mean_eps: 0.100000\n",
      "  44200/200000: episode: 221, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 9.113068, mae: 42.145442, mean_q: -62.581589, mean_eps: 0.100000\n",
      "  44400/200000: episode: 222, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.470 [0.000, 2.000],  loss: 8.894184, mae: 42.391504, mean_q: -62.959954, mean_eps: 0.100000\n",
      "  44600/200000: episode: 223, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.460 [0.000, 2.000],  loss: 8.373780, mae: 42.265318, mean_q: -62.756603, mean_eps: 0.100000\n",
      "  44800/200000: episode: 224, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.190 [0.000, 2.000],  loss: 7.203591, mae: 42.158126, mean_q: -62.625591, mean_eps: 0.100000\n",
      "  45000/200000: episode: 225, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.125 [0.000, 2.000],  loss: 8.865429, mae: 41.812944, mean_q: -62.077218, mean_eps: 0.100000\n",
      "  45200/200000: episode: 226, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.615 [0.000, 2.000],  loss: 8.202220, mae: 41.454896, mean_q: -61.563321, mean_eps: 0.100000\n",
      "  45400/200000: episode: 227, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 12.393703, mae: 41.305464, mean_q: -61.242402, mean_eps: 0.100000\n",
      "  45600/200000: episode: 228, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 6.498393, mae: 41.552580, mean_q: -61.802760, mean_eps: 0.100000\n",
      "  45800/200000: episode: 229, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.210 [0.000, 2.000],  loss: 9.995865, mae: 41.652315, mean_q: -61.764669, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  46000/200000: episode: 230, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.420 [0.000, 2.000],  loss: 8.895026, mae: 41.654576, mean_q: -61.826871, mean_eps: 0.100000\n",
      "  46200/200000: episode: 231, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 6.942946, mae: 41.590135, mean_q: -61.727621, mean_eps: 0.100000\n",
      "  46400/200000: episode: 232, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.440 [0.000, 2.000],  loss: 9.822652, mae: 41.262062, mean_q: -61.215752, mean_eps: 0.100000\n",
      "  46600/200000: episode: 233, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.245 [0.000, 2.000],  loss: 8.864807, mae: 41.097708, mean_q: -60.925831, mean_eps: 0.100000\n",
      "  46800/200000: episode: 234, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 9.235372, mae: 41.168868, mean_q: -60.878081, mean_eps: 0.100000\n",
      "  47000/200000: episode: 235, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 8.003808, mae: 41.002911, mean_q: -60.638213, mean_eps: 0.100000\n",
      "  47200/200000: episode: 236, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 11.438028, mae: 40.922471, mean_q: -60.256511, mean_eps: 0.100000\n",
      "  47400/200000: episode: 237, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 10.582994, mae: 40.740928, mean_q: -60.035138, mean_eps: 0.100000\n",
      "  47600/200000: episode: 238, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 7.765315, mae: 40.383304, mean_q: -59.748624, mean_eps: 0.100000\n",
      "  47800/200000: episode: 239, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 8.157085, mae: 40.334210, mean_q: -59.746495, mean_eps: 0.100000\n",
      "  48000/200000: episode: 240, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 6.295412, mae: 40.389482, mean_q: -59.878851, mean_eps: 0.100000\n",
      "  48200/200000: episode: 241, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.025 [0.000, 2.000],  loss: 8.035065, mae: 40.364879, mean_q: -59.834370, mean_eps: 0.100000\n",
      "  48400/200000: episode: 242, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 5.809123, mae: 40.455759, mean_q: -60.158562, mean_eps: 0.100000\n",
      "  48600/200000: episode: 243, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 7.467447, mae: 40.583026, mean_q: -60.229878, mean_eps: 0.100000\n",
      "  48800/200000: episode: 244, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.165 [0.000, 2.000],  loss: 8.073713, mae: 40.646866, mean_q: -60.253601, mean_eps: 0.100000\n",
      "  49000/200000: episode: 245, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 7.227056, mae: 40.702654, mean_q: -60.331265, mean_eps: 0.100000\n",
      "  49200/200000: episode: 246, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000],  loss: 6.752420, mae: 40.829229, mean_q: -60.596466, mean_eps: 0.100000\n",
      "  49400/200000: episode: 247, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.615 [0.000, 2.000],  loss: 7.086727, mae: 41.248711, mean_q: -61.060042, mean_eps: 0.100000\n",
      "  49600/200000: episode: 248, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.425 [0.000, 2.000],  loss: 9.297603, mae: 41.605730, mean_q: -61.683033, mean_eps: 0.100000\n",
      "  49800/200000: episode: 249, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.325 [0.000, 2.000],  loss: 8.219758, mae: 41.851226, mean_q: -61.996338, mean_eps: 0.100000\n",
      "  50000/200000: episode: 250, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.580 [0.000, 2.000],  loss: 9.302894, mae: 42.049770, mean_q: -62.301419, mean_eps: 0.100000\n",
      "  50200/200000: episode: 251, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.710 [0.000, 2.000],  loss: 11.223762, mae: 42.200034, mean_q: -62.435453, mean_eps: 0.100000\n",
      "  50400/200000: episode: 252, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.580 [0.000, 2.000],  loss: 11.981887, mae: 42.176961, mean_q: -62.423824, mean_eps: 0.100000\n",
      "  50600/200000: episode: 253, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000],  loss: 10.467120, mae: 42.193042, mean_q: -62.404097, mean_eps: 0.100000\n",
      "  50800/200000: episode: 254, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 12.206082, mae: 42.619580, mean_q: -62.796021, mean_eps: 0.100000\n",
      "  51000/200000: episode: 255, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000],  loss: 13.468443, mae: 42.601943, mean_q: -62.951187, mean_eps: 0.100000\n",
      "  51200/200000: episode: 256, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.265 [0.000, 2.000],  loss: 12.417123, mae: 42.292797, mean_q: -62.447956, mean_eps: 0.100000\n",
      "  51400/200000: episode: 257, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000],  loss: 11.129891, mae: 41.943101, mean_q: -62.019577, mean_eps: 0.100000\n",
      "  51600/200000: episode: 258, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 7.625318, mae: 41.676660, mean_q: -61.780424, mean_eps: 0.100000\n",
      "  51800/200000: episode: 259, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 7.722075, mae: 41.234535, mean_q: -61.012806, mean_eps: 0.100000\n",
      "  52000/200000: episode: 260, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 8.392562, mae: 40.851030, mean_q: -60.412535, mean_eps: 0.100000\n",
      "  52200/200000: episode: 261, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 7.433163, mae: 40.687121, mean_q: -60.288584, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  52400/200000: episode: 262, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 11.059145, mae: 40.601700, mean_q: -59.942536, mean_eps: 0.100000\n",
      "  52600/200000: episode: 263, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 8.735842, mae: 40.542620, mean_q: -59.974246, mean_eps: 0.100000\n",
      "  52800/200000: episode: 264, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 7.755511, mae: 40.603691, mean_q: -60.111836, mean_eps: 0.100000\n",
      "  53000/200000: episode: 265, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.050 [0.000, 2.000],  loss: 7.774850, mae: 40.669204, mean_q: -60.274737, mean_eps: 0.100000\n",
      "  53200/200000: episode: 266, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 9.680498, mae: 40.785297, mean_q: -60.402940, mean_eps: 0.100000\n",
      "  53400/200000: episode: 267, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 8.176023, mae: 40.819611, mean_q: -60.446663, mean_eps: 0.100000\n",
      "  53600/200000: episode: 268, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 9.147366, mae: 40.828260, mean_q: -60.618025, mean_eps: 0.100000\n",
      "  53796/200000: episode: 269, duration: 0.866s, episode steps: 196, steps per second: 226, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.408 [0.000, 2.000],  loss: 10.084094, mae: 41.540590, mean_q: -61.656246, mean_eps: 0.100000\n",
      "  53996/200000: episode: 270, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.651524, mae: 42.075539, mean_q: -62.497496, mean_eps: 0.100000\n",
      "  54196/200000: episode: 271, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 12.593793, mae: 42.485444, mean_q: -63.004756, mean_eps: 0.100000\n",
      "  54396/200000: episode: 272, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 13.178156, mae: 42.720193, mean_q: -63.346584, mean_eps: 0.100000\n",
      "  54596/200000: episode: 273, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.560 [0.000, 2.000],  loss: 9.011822, mae: 42.922821, mean_q: -63.797311, mean_eps: 0.100000\n",
      "  54796/200000: episode: 274, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 14.547504, mae: 42.630945, mean_q: -62.803900, mean_eps: 0.100000\n",
      "  54996/200000: episode: 275, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.867153, mae: 42.447794, mean_q: -62.726705, mean_eps: 0.100000\n",
      "  55196/200000: episode: 276, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.738317, mae: 42.464600, mean_q: -62.720288, mean_eps: 0.100000\n",
      "  55396/200000: episode: 277, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.215782, mae: 42.453118, mean_q: -62.766034, mean_eps: 0.100000\n",
      "  55596/200000: episode: 278, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 10.748162, mae: 42.441833, mean_q: -62.721598, mean_eps: 0.100000\n",
      "  55796/200000: episode: 279, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 1.000],  loss: 9.708969, mae: 42.412702, mean_q: -62.862937, mean_eps: 0.100000\n",
      "  55996/200000: episode: 280, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.525790, mae: 42.382374, mean_q: -62.973293, mean_eps: 0.100000\n",
      "  56196/200000: episode: 281, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.365 [0.000, 2.000],  loss: 9.165882, mae: 42.325360, mean_q: -62.879527, mean_eps: 0.100000\n",
      "  56396/200000: episode: 282, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 8.419468, mae: 42.173912, mean_q: -62.628494, mean_eps: 0.100000\n",
      "  56596/200000: episode: 283, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 11.747381, mae: 42.030643, mean_q: -62.158812, mean_eps: 0.100000\n",
      "  56796/200000: episode: 284, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.335 [0.000, 2.000],  loss: 8.452792, mae: 41.871334, mean_q: -62.028078, mean_eps: 0.100000\n",
      "  56996/200000: episode: 285, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.165 [0.000, 2.000],  loss: 8.537805, mae: 41.863671, mean_q: -61.902645, mean_eps: 0.100000\n",
      "  57196/200000: episode: 286, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 7.674842, mae: 41.808062, mean_q: -61.914363, mean_eps: 0.100000\n",
      "  57396/200000: episode: 287, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.180 [0.000, 2.000],  loss: 5.896009, mae: 41.873943, mean_q: -62.160132, mean_eps: 0.100000\n",
      "  57596/200000: episode: 288, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 10.720202, mae: 41.958839, mean_q: -61.996832, mean_eps: 0.100000\n",
      "  57796/200000: episode: 289, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.055 [0.000, 2.000],  loss: 9.085457, mae: 41.897727, mean_q: -61.899638, mean_eps: 0.100000\n",
      "  57996/200000: episode: 290, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.055 [0.000, 2.000],  loss: 10.301392, mae: 41.886659, mean_q: -61.933520, mean_eps: 0.100000\n",
      "  58196/200000: episode: 291, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.305 [0.000, 2.000],  loss: 10.639087, mae: 41.877653, mean_q: -61.980677, mean_eps: 0.100000\n",
      "  58396/200000: episode: 292, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.310 [0.000, 2.000],  loss: 8.018790, mae: 41.859186, mean_q: -61.999093, mean_eps: 0.100000\n",
      "  58596/200000: episode: 293, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.600 [0.000, 2.000],  loss: 7.115607, mae: 41.851672, mean_q: -62.151891, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  58796/200000: episode: 294, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.461512, mae: 41.922410, mean_q: -62.035854, mean_eps: 0.100000\n",
      "  58996/200000: episode: 295, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.739643, mae: 42.079012, mean_q: -62.303196, mean_eps: 0.100000\n",
      "  59196/200000: episode: 296, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.707249, mae: 42.176980, mean_q: -62.352344, mean_eps: 0.100000\n",
      "  59396/200000: episode: 297, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.973951, mae: 42.295610, mean_q: -62.487008, mean_eps: 0.100000\n",
      "  59596/200000: episode: 298, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.670173, mae: 42.382586, mean_q: -62.568438, mean_eps: 0.100000\n",
      "  59796/200000: episode: 299, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.699634, mae: 42.295575, mean_q: -62.780002, mean_eps: 0.100000\n",
      "  59996/200000: episode: 300, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 9.481048, mae: 42.334722, mean_q: -62.866636, mean_eps: 0.100000\n",
      "  60196/200000: episode: 301, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 8.613319, mae: 42.290280, mean_q: -62.754006, mean_eps: 0.100000\n",
      "  60396/200000: episode: 302, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.935 [0.000, 2.000],  loss: 11.379300, mae: 42.010313, mean_q: -62.158769, mean_eps: 0.100000\n",
      "  60596/200000: episode: 303, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 8.068252, mae: 42.038014, mean_q: -62.503374, mean_eps: 0.100000\n",
      "  60796/200000: episode: 304, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 2.000],  loss: 7.745423, mae: 42.120924, mean_q: -62.727266, mean_eps: 0.100000\n",
      "  60996/200000: episode: 305, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.515 [0.000, 2.000],  loss: 10.375940, mae: 42.221316, mean_q: -62.708063, mean_eps: 0.100000\n",
      "  61196/200000: episode: 306, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 10.251903, mae: 42.290282, mean_q: -62.799864, mean_eps: 0.100000\n",
      "  61396/200000: episode: 307, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.043120, mae: 42.199515, mean_q: -62.723225, mean_eps: 0.100000\n",
      "  61596/200000: episode: 308, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.657751, mae: 42.197386, mean_q: -62.668919, mean_eps: 0.100000\n",
      "  61796/200000: episode: 309, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.220 [0.000, 2.000],  loss: 8.514319, mae: 41.723275, mean_q: -61.809170, mean_eps: 0.100000\n",
      "  61996/200000: episode: 310, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.325 [0.000, 2.000],  loss: 12.345808, mae: 41.427981, mean_q: -61.245464, mean_eps: 0.100000\n",
      "  62196/200000: episode: 311, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.340 [0.000, 2.000],  loss: 10.061302, mae: 41.248873, mean_q: -61.115372, mean_eps: 0.100000\n",
      "  62396/200000: episode: 312, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.255 [0.000, 2.000],  loss: 9.028906, mae: 41.179564, mean_q: -60.993967, mean_eps: 0.100000\n",
      "  62596/200000: episode: 313, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 10.582654, mae: 41.167150, mean_q: -60.718823, mean_eps: 0.100000\n",
      "  62796/200000: episode: 314, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 10.839488, mae: 41.054306, mean_q: -60.565721, mean_eps: 0.100000\n",
      "  62996/200000: episode: 315, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 7.170660, mae: 40.996658, mean_q: -60.693214, mean_eps: 0.100000\n",
      "  63196/200000: episode: 316, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 9.343692, mae: 41.112675, mean_q: -60.805255, mean_eps: 0.100000\n",
      "  63396/200000: episode: 317, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 7.317826, mae: 41.162811, mean_q: -60.954075, mean_eps: 0.100000\n",
      "  63596/200000: episode: 318, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 8.709294, mae: 41.275132, mean_q: -61.165881, mean_eps: 0.100000\n",
      "  63796/200000: episode: 319, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 10.227798, mae: 41.379568, mean_q: -61.190788, mean_eps: 0.100000\n",
      "  63996/200000: episode: 320, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 9.880317, mae: 41.365949, mean_q: -61.179827, mean_eps: 0.100000\n",
      "  64196/200000: episode: 321, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 9.245661, mae: 41.331283, mean_q: -61.170224, mean_eps: 0.100000\n",
      "  64396/200000: episode: 322, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000],  loss: 9.593630, mae: 41.392325, mean_q: -61.240875, mean_eps: 0.100000\n",
      "  64596/200000: episode: 323, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 8.255993, mae: 41.434604, mean_q: -61.391702, mean_eps: 0.100000\n",
      "  64796/200000: episode: 324, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.050 [0.000, 2.000],  loss: 9.596999, mae: 41.447524, mean_q: -61.297003, mean_eps: 0.100000\n",
      "  64996/200000: episode: 325, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 6.739468, mae: 41.463835, mean_q: -61.463721, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  65196/200000: episode: 326, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.235 [0.000, 2.000],  loss: 10.105334, mae: 41.692247, mean_q: -61.719754, mean_eps: 0.100000\n",
      "  65396/200000: episode: 327, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 6.574776, mae: 41.623616, mean_q: -61.701471, mean_eps: 0.100000\n",
      "  65596/200000: episode: 328, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 10.694049, mae: 41.767188, mean_q: -61.782518, mean_eps: 0.100000\n",
      "  65796/200000: episode: 329, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 2.000],  loss: 9.172102, mae: 41.830441, mean_q: -61.898996, mean_eps: 0.100000\n",
      "  65996/200000: episode: 330, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.210 [0.000, 2.000],  loss: 9.938678, mae: 41.884869, mean_q: -61.933446, mean_eps: 0.100000\n",
      "  66196/200000: episode: 331, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 2.000],  loss: 8.404291, mae: 41.830546, mean_q: -61.846853, mean_eps: 0.100000\n",
      "  66396/200000: episode: 332, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 8.529552, mae: 41.858942, mean_q: -61.914500, mean_eps: 0.100000\n",
      "  66596/200000: episode: 333, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 9.892765, mae: 41.881345, mean_q: -61.882238, mean_eps: 0.100000\n",
      "  66796/200000: episode: 334, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.150 [0.000, 2.000],  loss: 8.005661, mae: 41.839415, mean_q: -61.907381, mean_eps: 0.100000\n",
      "  66996/200000: episode: 335, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.275 [0.000, 2.000],  loss: 9.577211, mae: 41.834733, mean_q: -61.794965, mean_eps: 0.100000\n",
      "  67196/200000: episode: 336, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 10.540743, mae: 41.867827, mean_q: -61.809359, mean_eps: 0.100000\n",
      "  67396/200000: episode: 337, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 8.938431, mae: 41.927271, mean_q: -61.953361, mean_eps: 0.100000\n",
      "  67596/200000: episode: 338, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 8.707329, mae: 41.989933, mean_q: -62.107175, mean_eps: 0.100000\n",
      "  67796/200000: episode: 339, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 8.875999, mae: 41.873189, mean_q: -62.086595, mean_eps: 0.100000\n",
      "  67996/200000: episode: 340, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.637642, mae: 41.863926, mean_q: -61.917045, mean_eps: 0.100000\n",
      "  68196/200000: episode: 341, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.505481, mae: 41.907230, mean_q: -61.952215, mean_eps: 0.100000\n",
      "  68396/200000: episode: 342, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.549399, mae: 42.080396, mean_q: -62.174271, mean_eps: 0.100000\n",
      "  68596/200000: episode: 343, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.364099, mae: 42.286973, mean_q: -62.558254, mean_eps: 0.100000\n",
      "  68796/200000: episode: 344, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.410918, mae: 42.566079, mean_q: -62.886036, mean_eps: 0.100000\n",
      "  68996/200000: episode: 345, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.982611, mae: 42.425854, mean_q: -62.900414, mean_eps: 0.100000\n",
      "  69196/200000: episode: 346, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.343781, mae: 42.416282, mean_q: -63.046129, mean_eps: 0.100000\n",
      "  69396/200000: episode: 347, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.976784, mae: 42.512461, mean_q: -63.118050, mean_eps: 0.100000\n",
      "  69596/200000: episode: 348, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 9.324613, mae: 42.428583, mean_q: -63.183042, mean_eps: 0.100000\n",
      "  69796/200000: episode: 349, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.695 [0.000, 2.000],  loss: 10.371376, mae: 42.400006, mean_q: -62.935091, mean_eps: 0.100000\n",
      "  69996/200000: episode: 350, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 2.000],  loss: 10.471511, mae: 42.472001, mean_q: -63.036936, mean_eps: 0.100000\n",
      "  70196/200000: episode: 351, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000],  loss: 11.244371, mae: 42.596797, mean_q: -63.250899, mean_eps: 0.100000\n",
      "  70396/200000: episode: 352, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.650 [0.000, 2.000],  loss: 8.357265, mae: 42.680363, mean_q: -63.513638, mean_eps: 0.100000\n",
      "  70596/200000: episode: 353, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 8.648406, mae: 42.845762, mean_q: -63.792410, mean_eps: 0.100000\n",
      "  70796/200000: episode: 354, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 7.302747, mae: 42.784863, mean_q: -63.709695, mean_eps: 0.100000\n",
      "  70996/200000: episode: 355, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 10.253970, mae: 42.523249, mean_q: -63.022505, mean_eps: 0.100000\n",
      "  71196/200000: episode: 356, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 7.189256, mae: 42.352614, mean_q: -63.085833, mean_eps: 0.100000\n",
      "  71396/200000: episode: 357, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 6.349634, mae: 42.436487, mean_q: -63.267639, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  71580/200000: episode: 358, duration: 0.818s, episode steps: 184, steps per second: 225, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.859 [0.000, 2.000],  loss: 6.752912, mae: 42.318891, mean_q: -63.030298, mean_eps: 0.100000\n",
      "  71780/200000: episode: 359, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.530 [0.000, 2.000],  loss: 9.525326, mae: 42.282467, mean_q: -62.850794, mean_eps: 0.100000\n",
      "  71980/200000: episode: 360, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.285 [0.000, 2.000],  loss: 11.039479, mae: 41.927092, mean_q: -62.149721, mean_eps: 0.100000\n",
      "  72180/200000: episode: 361, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 10.511971, mae: 41.564827, mean_q: -61.448994, mean_eps: 0.100000\n",
      "  72380/200000: episode: 362, duration: 0.891s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 10.010930, mae: 41.153720, mean_q: -60.706482, mean_eps: 0.100000\n",
      "  72580/200000: episode: 363, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.395 [0.000, 2.000],  loss: 9.681194, mae: 41.051884, mean_q: -60.525604, mean_eps: 0.100000\n",
      "  72780/200000: episode: 364, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.125 [0.000, 2.000],  loss: 8.235729, mae: 40.887438, mean_q: -60.371403, mean_eps: 0.100000\n",
      "  72980/200000: episode: 365, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000],  loss: 8.219040, mae: 40.882603, mean_q: -60.426581, mean_eps: 0.100000\n",
      "  73180/200000: episode: 366, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.035 [0.000, 2.000],  loss: 7.951258, mae: 40.912671, mean_q: -60.470118, mean_eps: 0.100000\n",
      "  73380/200000: episode: 367, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.360 [0.000, 2.000],  loss: 8.787061, mae: 41.012788, mean_q: -60.503195, mean_eps: 0.100000\n",
      "  73580/200000: episode: 368, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 7.127109, mae: 41.074287, mean_q: -60.681892, mean_eps: 0.100000\n",
      "  73780/200000: episode: 369, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.350 [0.000, 2.000],  loss: 7.964387, mae: 41.074742, mean_q: -60.648464, mean_eps: 0.100000\n",
      "  73980/200000: episode: 370, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 5.941322, mae: 41.215718, mean_q: -60.902169, mean_eps: 0.100000\n",
      "  74180/200000: episode: 371, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 7.545538, mae: 41.307494, mean_q: -60.962715, mean_eps: 0.100000\n",
      "  74380/200000: episode: 372, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 10.076514, mae: 41.404743, mean_q: -61.009196, mean_eps: 0.100000\n",
      "  74580/200000: episode: 373, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 9.313621, mae: 41.122491, mean_q: -60.832959, mean_eps: 0.100000\n",
      "  74780/200000: episode: 374, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 8.204128, mae: 41.223923, mean_q: -60.999740, mean_eps: 0.100000\n",
      "  74980/200000: episode: 375, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.263265, mae: 41.235620, mean_q: -61.094774, mean_eps: 0.100000\n",
      "  75180/200000: episode: 376, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.565 [0.000, 2.000],  loss: 9.046142, mae: 41.447776, mean_q: -61.158416, mean_eps: 0.100000\n",
      "  75380/200000: episode: 377, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000],  loss: 9.583028, mae: 41.586073, mean_q: -61.289130, mean_eps: 0.100000\n",
      "  75580/200000: episode: 378, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 2.000],  loss: 7.139619, mae: 41.665217, mean_q: -61.552877, mean_eps: 0.100000\n",
      "  75780/200000: episode: 379, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000],  loss: 7.761859, mae: 41.741466, mean_q: -61.699948, mean_eps: 0.100000\n",
      "  75980/200000: episode: 380, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.650 [0.000, 2.000],  loss: 7.719543, mae: 41.941160, mean_q: -62.156133, mean_eps: 0.100000\n",
      "  76180/200000: episode: 381, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.860 [0.000, 2.000],  loss: 8.956229, mae: 42.033483, mean_q: -62.382867, mean_eps: 0.100000\n",
      "  76380/200000: episode: 382, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 10.848086, mae: 42.005171, mean_q: -62.398603, mean_eps: 0.100000\n",
      "  76580/200000: episode: 383, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.800 [0.000, 2.000],  loss: 10.470092, mae: 41.878541, mean_q: -62.099232, mean_eps: 0.100000\n",
      "  76780/200000: episode: 384, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.820 [0.000, 2.000],  loss: 7.387224, mae: 41.858062, mean_q: -62.267227, mean_eps: 0.100000\n",
      "  76959/200000: episode: 385, duration: 0.838s, episode steps: 179, steps per second: 214, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 6.229619, mae: 41.778517, mean_q: -62.177736, mean_eps: 0.100000\n",
      "  77159/200000: episode: 386, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.711143, mae: 41.788731, mean_q: -61.878642, mean_eps: 0.100000\n",
      "  77359/200000: episode: 387, duration: 0.941s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000],  loss: 7.570355, mae: 41.792538, mean_q: -61.965967, mean_eps: 0.100000\n",
      "  77559/200000: episode: 388, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.235 [0.000, 2.000],  loss: 9.094815, mae: 42.001994, mean_q: -62.087852, mean_eps: 0.100000\n",
      "  77759/200000: episode: 389, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.640 [0.000, 2.000],  loss: 5.560230, mae: 42.284436, mean_q: -62.701495, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  77959/200000: episode: 390, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 10.511821, mae: 42.618340, mean_q: -62.831906, mean_eps: 0.100000\n",
      "  78159/200000: episode: 391, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 8.181859, mae: 43.263470, mean_q: -63.963188, mean_eps: 0.100000\n",
      "  78359/200000: episode: 392, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.155 [0.000, 2.000],  loss: 9.784418, mae: 43.630177, mean_q: -64.497556, mean_eps: 0.100000\n",
      "  78559/200000: episode: 393, duration: 0.915s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 9.700232, mae: 43.726475, mean_q: -64.601918, mean_eps: 0.100000\n",
      "  78759/200000: episode: 394, duration: 0.914s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 11.881721, mae: 43.699610, mean_q: -64.477411, mean_eps: 0.100000\n",
      "  78959/200000: episode: 395, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 12.118641, mae: 43.755211, mean_q: -64.515004, mean_eps: 0.100000\n",
      "  79159/200000: episode: 396, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 2.000],  loss: 11.637416, mae: 43.643604, mean_q: -64.425673, mean_eps: 0.100000\n",
      "  79359/200000: episode: 397, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.415 [0.000, 2.000],  loss: 9.328048, mae: 43.565531, mean_q: -64.485622, mean_eps: 0.100000\n",
      "  79559/200000: episode: 398, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 11.407613, mae: 43.320159, mean_q: -63.982619, mean_eps: 0.100000\n",
      "  79759/200000: episode: 399, duration: 0.911s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.355 [0.000, 2.000],  loss: 11.158206, mae: 43.271900, mean_q: -63.919965, mean_eps: 0.100000\n",
      "  79959/200000: episode: 400, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 12.004010, mae: 43.089361, mean_q: -63.585143, mean_eps: 0.100000\n",
      "  80159/200000: episode: 401, duration: 0.973s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 11.588968, mae: 42.894721, mean_q: -63.286457, mean_eps: 0.100000\n",
      "  80359/200000: episode: 402, duration: 1.044s, episode steps: 200, steps per second: 192, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.245 [0.000, 2.000],  loss: 9.586485, mae: 42.921157, mean_q: -63.419560, mean_eps: 0.100000\n",
      "  80559/200000: episode: 403, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000],  loss: 11.000687, mae: 43.015914, mean_q: -63.524475, mean_eps: 0.100000\n",
      "  80759/200000: episode: 404, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 8.225942, mae: 43.059760, mean_q: -63.746193, mean_eps: 0.100000\n",
      "  80959/200000: episode: 405, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 11.926470, mae: 43.159496, mean_q: -63.689840, mean_eps: 0.100000\n",
      "  81159/200000: episode: 406, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.380 [0.000, 2.000],  loss: 8.893718, mae: 43.108661, mean_q: -63.730558, mean_eps: 0.100000\n",
      "  81359/200000: episode: 407, duration: 0.928s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.225 [0.000, 2.000],  loss: 8.591073, mae: 43.074550, mean_q: -63.702607, mean_eps: 0.100000\n",
      "  81559/200000: episode: 408, duration: 0.915s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 9.552102, mae: 43.110695, mean_q: -63.738006, mean_eps: 0.100000\n",
      "  81759/200000: episode: 409, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 10.512761, mae: 43.168374, mean_q: -63.701287, mean_eps: 0.100000\n",
      "  81959/200000: episode: 410, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 11.592961, mae: 43.085938, mean_q: -63.474746, mean_eps: 0.100000\n",
      "  82159/200000: episode: 411, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 10.109527, mae: 42.988255, mean_q: -63.459901, mean_eps: 0.100000\n",
      "  82359/200000: episode: 412, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 11.611835, mae: 43.034991, mean_q: -63.510861, mean_eps: 0.100000\n",
      "  82559/200000: episode: 413, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.265 [0.000, 2.000],  loss: 11.761038, mae: 42.984131, mean_q: -63.376641, mean_eps: 0.100000\n",
      "  82759/200000: episode: 414, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 9.345053, mae: 42.868111, mean_q: -63.306026, mean_eps: 0.100000\n",
      "  82959/200000: episode: 415, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.410 [0.000, 2.000],  loss: 8.760501, mae: 42.885584, mean_q: -63.422576, mean_eps: 0.100000\n",
      "  83159/200000: episode: 416, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 2.000],  loss: 9.142074, mae: 42.858525, mean_q: -63.315857, mean_eps: 0.100000\n",
      "  83359/200000: episode: 417, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 8.959820, mae: 42.627010, mean_q: -63.056515, mean_eps: 0.100000\n",
      "  83559/200000: episode: 418, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.461022, mae: 42.551417, mean_q: -62.889429, mean_eps: 0.100000\n",
      "  83759/200000: episode: 419, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 11.548729, mae: 42.450870, mean_q: -62.757474, mean_eps: 0.100000\n",
      "  83959/200000: episode: 420, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.589738, mae: 42.409930, mean_q: -62.747526, mean_eps: 0.100000\n",
      "  84159/200000: episode: 421, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 13.251753, mae: 42.536185, mean_q: -62.594515, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84359/200000: episode: 422, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.203373, mae: 42.452013, mean_q: -62.661927, mean_eps: 0.100000\n",
      "  84559/200000: episode: 423, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.919012, mae: 42.454206, mean_q: -62.667428, mean_eps: 0.100000\n",
      "  84759/200000: episode: 424, duration: 0.870s, episode steps: 200, steps per second: 230, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 11.130231, mae: 42.385920, mean_q: -62.542567, mean_eps: 0.100000\n",
      "  84959/200000: episode: 425, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.959036, mae: 42.273877, mean_q: -62.510981, mean_eps: 0.100000\n",
      "  85159/200000: episode: 426, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 9.223479, mae: 42.200859, mean_q: -62.624602, mean_eps: 0.100000\n",
      "  85359/200000: episode: 427, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 7.975111, mae: 42.270287, mean_q: -62.680413, mean_eps: 0.100000\n",
      "  85559/200000: episode: 428, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.254990, mae: 42.355508, mean_q: -62.716394, mean_eps: 0.100000\n",
      "  85759/200000: episode: 429, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.850 [0.000, 2.000],  loss: 9.026578, mae: 41.924669, mean_q: -62.089717, mean_eps: 0.100000\n",
      "  85959/200000: episode: 430, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.705234, mae: 41.960160, mean_q: -62.086496, mean_eps: 0.100000\n",
      "  86159/200000: episode: 431, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 8.873732, mae: 41.877442, mean_q: -62.195690, mean_eps: 0.100000\n",
      "  86359/200000: episode: 432, duration: 0.877s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 8.722671, mae: 42.010758, mean_q: -62.401213, mean_eps: 0.100000\n",
      "  86559/200000: episode: 433, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 11.181782, mae: 41.970188, mean_q: -62.350142, mean_eps: 0.100000\n",
      "  86759/200000: episode: 434, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 11.727548, mae: 41.658889, mean_q: -61.801019, mean_eps: 0.100000\n",
      "  86959/200000: episode: 435, duration: 0.907s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.220 [0.000, 2.000],  loss: 8.462970, mae: 41.530962, mean_q: -61.814439, mean_eps: 0.100000\n",
      "  87159/200000: episode: 436, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 9.622925, mae: 41.336526, mean_q: -61.405782, mean_eps: 0.100000\n",
      "  87359/200000: episode: 437, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.060 [0.000, 2.000],  loss: 10.123631, mae: 41.095083, mean_q: -60.869144, mean_eps: 0.100000\n",
      "  87559/200000: episode: 438, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.540 [0.000, 2.000],  loss: 8.585021, mae: 41.005174, mean_q: -60.664635, mean_eps: 0.100000\n",
      "  87759/200000: episode: 439, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.155 [0.000, 2.000],  loss: 8.422087, mae: 40.869132, mean_q: -60.373336, mean_eps: 0.100000\n",
      "  87959/200000: episode: 440, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 10.849151, mae: 40.809363, mean_q: -60.441319, mean_eps: 0.100000\n",
      "  88159/200000: episode: 441, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000],  loss: 8.694741, mae: 40.899446, mean_q: -60.552089, mean_eps: 0.100000\n",
      "  88359/200000: episode: 442, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.826072, mae: 40.936873, mean_q: -60.672355, mean_eps: 0.100000\n",
      "  88559/200000: episode: 443, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 11.908146, mae: 40.917747, mean_q: -60.549331, mean_eps: 0.100000\n",
      "  88759/200000: episode: 444, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 7.195602, mae: 41.101549, mean_q: -61.005322, mean_eps: 0.100000\n",
      "  88959/200000: episode: 445, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.185 [0.000, 2.000],  loss: 10.430002, mae: 41.044404, mean_q: -60.648372, mean_eps: 0.100000\n",
      "  89159/200000: episode: 446, duration: 0.899s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.435 [0.000, 2.000],  loss: 8.580792, mae: 41.089839, mean_q: -60.784955, mean_eps: 0.100000\n",
      "  89359/200000: episode: 447, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 9.265317, mae: 41.074300, mean_q: -60.691128, mean_eps: 0.100000\n",
      "  89559/200000: episode: 448, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.515 [0.000, 2.000],  loss: 6.390941, mae: 40.936407, mean_q: -60.713987, mean_eps: 0.100000\n",
      "  89759/200000: episode: 449, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.840 [0.000, 2.000],  loss: 9.340088, mae: 40.736539, mean_q: -60.357594, mean_eps: 0.100000\n",
      "  89959/200000: episode: 450, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.850 [0.000, 2.000],  loss: 10.075825, mae: 40.630022, mean_q: -60.098989, mean_eps: 0.100000\n",
      "  90159/200000: episode: 451, duration: 0.891s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 8.321507, mae: 40.615825, mean_q: -60.273123, mean_eps: 0.100000\n",
      "  90359/200000: episode: 452, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 10.782901, mae: 40.629829, mean_q: -60.228537, mean_eps: 0.100000\n",
      "  90559/200000: episode: 453, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 8.254682, mae: 40.286901, mean_q: -59.585225, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  90759/200000: episode: 454, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.065 [0.000, 2.000],  loss: 7.627477, mae: 39.983074, mean_q: -59.039149, mean_eps: 0.100000\n",
      "  90959/200000: episode: 455, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000],  loss: 6.891227, mae: 39.838094, mean_q: -58.892201, mean_eps: 0.100000\n",
      "  91159/200000: episode: 456, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000],  loss: 6.567818, mae: 39.922550, mean_q: -58.896057, mean_eps: 0.100000\n",
      "  91359/200000: episode: 457, duration: 0.907s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 6.411896, mae: 40.027650, mean_q: -59.145134, mean_eps: 0.100000\n",
      "  91559/200000: episode: 458, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 8.056252, mae: 40.172758, mean_q: -59.351836, mean_eps: 0.100000\n",
      "  91759/200000: episode: 459, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.440 [0.000, 2.000],  loss: 7.634737, mae: 40.261582, mean_q: -59.546238, mean_eps: 0.100000\n",
      "  91959/200000: episode: 460, duration: 0.935s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 6.653736, mae: 40.469552, mean_q: -59.865669, mean_eps: 0.100000\n",
      "  92159/200000: episode: 461, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 5.800172, mae: 40.705705, mean_q: -60.206085, mean_eps: 0.100000\n",
      "  92359/200000: episode: 462, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 8.650369, mae: 40.864389, mean_q: -60.322516, mean_eps: 0.100000\n",
      "  92559/200000: episode: 463, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.275 [0.000, 2.000],  loss: 10.236668, mae: 40.850019, mean_q: -60.176643, mean_eps: 0.100000\n",
      "  92759/200000: episode: 464, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.050 [0.000, 2.000],  loss: 8.724672, mae: 40.639468, mean_q: -59.921262, mean_eps: 0.100000\n",
      "  92959/200000: episode: 465, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 8.464768, mae: 40.587563, mean_q: -59.915219, mean_eps: 0.100000\n",
      "  93159/200000: episode: 466, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 8.845184, mae: 40.550637, mean_q: -59.873836, mean_eps: 0.100000\n",
      "  93359/200000: episode: 467, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.420 [0.000, 2.000],  loss: 8.083994, mae: 40.582553, mean_q: -59.927139, mean_eps: 0.100000\n",
      "  93559/200000: episode: 468, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.490 [0.000, 2.000],  loss: 6.158288, mae: 40.726253, mean_q: -60.295549, mean_eps: 0.100000\n",
      "  93759/200000: episode: 469, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.209857, mae: 40.884619, mean_q: -60.532735, mean_eps: 0.100000\n",
      "  93959/200000: episode: 470, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.350 [0.000, 2.000],  loss: 10.716903, mae: 41.110503, mean_q: -60.740627, mean_eps: 0.100000\n",
      "  94159/200000: episode: 471, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 8.699468, mae: 41.238336, mean_q: -60.950238, mean_eps: 0.100000\n",
      "  94359/200000: episode: 472, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.305 [0.000, 2.000],  loss: 6.120497, mae: 41.315083, mean_q: -61.266172, mean_eps: 0.100000\n",
      "  94559/200000: episode: 473, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.285 [0.000, 2.000],  loss: 8.662220, mae: 41.427393, mean_q: -61.309958, mean_eps: 0.100000\n",
      "  94759/200000: episode: 474, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 10.090121, mae: 41.385870, mean_q: -60.969545, mean_eps: 0.100000\n",
      "  94959/200000: episode: 475, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 9.470801, mae: 41.298845, mean_q: -61.006010, mean_eps: 0.100000\n",
      "  95159/200000: episode: 476, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000],  loss: 10.713485, mae: 40.683231, mean_q: -60.070294, mean_eps: 0.100000\n",
      "  95359/200000: episode: 477, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 9.059574, mae: 40.185341, mean_q: -59.496992, mean_eps: 0.100000\n",
      "  95559/200000: episode: 478, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 5.659867, mae: 39.962113, mean_q: -59.340060, mean_eps: 0.100000\n",
      "  95759/200000: episode: 479, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.610 [0.000, 2.000],  loss: 6.172750, mae: 40.181122, mean_q: -59.686689, mean_eps: 0.100000\n",
      "  95959/200000: episode: 480, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 8.198340, mae: 40.122716, mean_q: -59.519691, mean_eps: 0.100000\n",
      "  96159/200000: episode: 481, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 7.862946, mae: 40.211384, mean_q: -59.624052, mean_eps: 0.100000\n",
      "  96359/200000: episode: 482, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 5.716976, mae: 40.388494, mean_q: -59.928726, mean_eps: 0.100000\n",
      "  96559/200000: episode: 483, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.976315, mae: 40.638990, mean_q: -60.183303, mean_eps: 0.100000\n",
      "  96759/200000: episode: 484, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.543351, mae: 40.745512, mean_q: -60.341684, mean_eps: 0.100000\n",
      "  96959/200000: episode: 485, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.150017, mae: 40.850033, mean_q: -60.550247, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  97159/200000: episode: 486, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 7.094710, mae: 40.997440, mean_q: -60.919463, mean_eps: 0.100000\n",
      "  97359/200000: episode: 487, duration: 0.988s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.128283, mae: 41.214073, mean_q: -61.158644, mean_eps: 0.100000\n",
      "  97559/200000: episode: 488, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 12.823347, mae: 41.393315, mean_q: -61.072933, mean_eps: 0.100000\n",
      "  97759/200000: episode: 489, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 11.588149, mae: 41.330998, mean_q: -61.010372, mean_eps: 0.100000\n",
      "  97959/200000: episode: 490, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 2.000],  loss: 6.858822, mae: 41.024086, mean_q: -60.894310, mean_eps: 0.100000\n",
      "  98159/200000: episode: 491, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.690 [0.000, 2.000],  loss: 5.926983, mae: 40.635016, mean_q: -60.377652, mean_eps: 0.100000\n",
      "  98359/200000: episode: 492, duration: 0.875s, episode steps: 200, steps per second: 229, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 2.000],  loss: 10.176976, mae: 40.568238, mean_q: -60.323788, mean_eps: 0.100000\n",
      "  98559/200000: episode: 493, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.357685, mae: 40.446118, mean_q: -60.130454, mean_eps: 0.100000\n",
      "  98759/200000: episode: 494, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 7.171724, mae: 40.061037, mean_q: -59.496414, mean_eps: 0.100000\n",
      "  98959/200000: episode: 495, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.700 [0.000, 2.000],  loss: 6.509001, mae: 39.915247, mean_q: -59.443523, mean_eps: 0.100000\n",
      "  99159/200000: episode: 496, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.470 [0.000, 2.000],  loss: 7.484235, mae: 39.893420, mean_q: -59.318118, mean_eps: 0.100000\n",
      "  99359/200000: episode: 497, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.355 [0.000, 2.000],  loss: 6.824317, mae: 39.833865, mean_q: -59.078405, mean_eps: 0.100000\n",
      "  99559/200000: episode: 498, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.615 [0.000, 2.000],  loss: 8.606471, mae: 39.802791, mean_q: -58.996366, mean_eps: 0.100000\n",
      "  99759/200000: episode: 499, duration: 0.915s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 4.575520, mae: 39.592094, mean_q: -58.811230, mean_eps: 0.100000\n",
      "  99959/200000: episode: 500, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.290 [0.000, 2.000],  loss: 4.583689, mae: 39.835244, mean_q: -59.246764, mean_eps: 0.100000\n",
      " 100159/200000: episode: 501, duration: 0.918s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.505 [0.000, 2.000],  loss: 6.257019, mae: 40.188453, mean_q: -59.666351, mean_eps: 0.100000\n",
      " 100359/200000: episode: 502, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.480 [0.000, 2.000],  loss: 5.783438, mae: 40.421996, mean_q: -60.033788, mean_eps: 0.100000\n",
      " 100559/200000: episode: 503, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 4.397978, mae: 40.565047, mean_q: -60.265860, mean_eps: 0.100000\n",
      " 100759/200000: episode: 504, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.200 [0.000, 2.000],  loss: 8.458018, mae: 41.032447, mean_q: -60.684860, mean_eps: 0.100000\n",
      " 100959/200000: episode: 505, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 8.989233, mae: 41.086139, mean_q: -60.738766, mean_eps: 0.100000\n",
      " 101159/200000: episode: 506, duration: 0.907s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 7.855575, mae: 41.144492, mean_q: -60.871984, mean_eps: 0.100000\n",
      " 101359/200000: episode: 507, duration: 0.895s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000],  loss: 9.143810, mae: 41.476529, mean_q: -61.228593, mean_eps: 0.100000\n",
      " 101559/200000: episode: 508, duration: 0.905s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 9.490487, mae: 41.711599, mean_q: -61.546337, mean_eps: 0.100000\n",
      " 101759/200000: episode: 509, duration: 0.918s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 10.748222, mae: 41.678708, mean_q: -61.460742, mean_eps: 0.100000\n",
      " 101959/200000: episode: 510, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 6.300038, mae: 41.649203, mean_q: -61.621120, mean_eps: 0.100000\n",
      " 102159/200000: episode: 511, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 10.077250, mae: 41.691744, mean_q: -61.515966, mean_eps: 0.100000\n",
      " 102359/200000: episode: 512, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.340 [0.000, 2.000],  loss: 11.126693, mae: 41.643205, mean_q: -61.443503, mean_eps: 0.100000\n",
      " 102559/200000: episode: 513, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.400 [0.000, 2.000],  loss: 8.705001, mae: 41.327111, mean_q: -61.111043, mean_eps: 0.100000\n",
      " 102759/200000: episode: 514, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.340 [0.000, 2.000],  loss: 8.778002, mae: 41.312964, mean_q: -61.084209, mean_eps: 0.100000\n",
      " 102959/200000: episode: 515, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.730693, mae: 41.095478, mean_q: -60.908833, mean_eps: 0.100000\n",
      " 103159/200000: episode: 516, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.785 [0.000, 2.000],  loss: 7.516513, mae: 40.724039, mean_q: -60.292325, mean_eps: 0.100000\n",
      " 103359/200000: episode: 517, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.655 [0.000, 2.000],  loss: 5.873577, mae: 40.797799, mean_q: -60.518504, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 103559/200000: episode: 518, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 2.000],  loss: 9.905758, mae: 40.913288, mean_q: -60.582971, mean_eps: 0.100000\n",
      " 103759/200000: episode: 519, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 7.743294, mae: 40.942918, mean_q: -60.628405, mean_eps: 0.100000\n",
      " 103959/200000: episode: 520, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 8.963298, mae: 41.111813, mean_q: -60.896979, mean_eps: 0.100000\n",
      " 104159/200000: episode: 521, duration: 0.883s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 9.993549, mae: 41.415582, mean_q: -61.479354, mean_eps: 0.100000\n",
      " 104359/200000: episode: 522, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.051206, mae: 41.374140, mean_q: -61.418714, mean_eps: 0.100000\n",
      " 104559/200000: episode: 523, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 7.448467, mae: 41.576377, mean_q: -61.734910, mean_eps: 0.100000\n",
      " 104759/200000: episode: 524, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 9.171407, mae: 41.608989, mean_q: -61.625932, mean_eps: 0.100000\n",
      " 104959/200000: episode: 525, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.053025, mae: 41.853745, mean_q: -61.982777, mean_eps: 0.100000\n",
      " 105159/200000: episode: 526, duration: 0.919s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.488559, mae: 42.199223, mean_q: -62.273662, mean_eps: 0.100000\n",
      " 105359/200000: episode: 527, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 11.198551, mae: 42.418317, mean_q: -62.719654, mean_eps: 0.100000\n",
      " 105559/200000: episode: 528, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 1.000],  loss: 6.259228, mae: 42.399547, mean_q: -63.019611, mean_eps: 0.100000\n",
      " 105759/200000: episode: 529, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.610 [0.000, 2.000],  loss: 11.026986, mae: 42.526139, mean_q: -63.071437, mean_eps: 0.100000\n",
      " 105959/200000: episode: 530, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.016347, mae: 42.480028, mean_q: -62.781964, mean_eps: 0.100000\n",
      " 106159/200000: episode: 531, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.590 [0.000, 2.000],  loss: 9.302829, mae: 42.358397, mean_q: -62.711096, mean_eps: 0.100000\n",
      " 106359/200000: episode: 532, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.205455, mae: 42.277886, mean_q: -62.597180, mean_eps: 0.100000\n",
      " 106559/200000: episode: 533, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.172757, mae: 42.426694, mean_q: -63.021640, mean_eps: 0.100000\n",
      " 106759/200000: episode: 534, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.372961, mae: 42.839484, mean_q: -63.428121, mean_eps: 0.100000\n",
      " 106959/200000: episode: 535, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.123952, mae: 43.153550, mean_q: -63.994905, mean_eps: 0.100000\n",
      " 107159/200000: episode: 536, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 11.614127, mae: 43.248975, mean_q: -63.848611, mean_eps: 0.100000\n",
      " 107359/200000: episode: 537, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.109935, mae: 43.171807, mean_q: -63.812550, mean_eps: 0.100000\n",
      " 107559/200000: episode: 538, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.573606, mae: 43.159250, mean_q: -64.020599, mean_eps: 0.100000\n",
      " 107759/200000: episode: 539, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.590 [0.000, 2.000],  loss: 9.308542, mae: 43.171623, mean_q: -63.952211, mean_eps: 0.100000\n",
      " 107959/200000: episode: 540, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.340164, mae: 43.042394, mean_q: -63.581735, mean_eps: 0.100000\n",
      " 108159/200000: episode: 541, duration: 0.899s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.485 [0.000, 2.000],  loss: 10.818495, mae: 42.719389, mean_q: -63.003133, mean_eps: 0.100000\n",
      " 108359/200000: episode: 542, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.855 [0.000, 2.000],  loss: 11.554583, mae: 41.778816, mean_q: -61.625691, mean_eps: 0.100000\n",
      " 108559/200000: episode: 543, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.645 [0.000, 2.000],  loss: 5.788414, mae: 40.956544, mean_q: -60.754817, mean_eps: 0.100000\n",
      " 108759/200000: episode: 544, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.740 [0.000, 2.000],  loss: 9.563854, mae: 40.655504, mean_q: -60.197308, mean_eps: 0.100000\n",
      " 108959/200000: episode: 545, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.915 [0.000, 2.000],  loss: 11.048596, mae: 40.208490, mean_q: -59.293247, mean_eps: 0.100000\n",
      " 109159/200000: episode: 546, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.885 [0.000, 2.000],  loss: 8.272066, mae: 39.678978, mean_q: -58.843269, mean_eps: 0.100000\n",
      " 109359/200000: episode: 547, duration: 0.883s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.910 [0.000, 2.000],  loss: 8.288407, mae: 39.692483, mean_q: -58.899423, mean_eps: 0.100000\n",
      " 109559/200000: episode: 548, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.900 [0.000, 2.000],  loss: 10.784040, mae: 39.760499, mean_q: -58.815443, mean_eps: 0.100000\n",
      " 109759/200000: episode: 549, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 7.080976, mae: 39.658854, mean_q: -58.912653, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 109959/200000: episode: 550, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 9.408795, mae: 39.501700, mean_q: -58.605664, mean_eps: 0.100000\n",
      " 110159/200000: episode: 551, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 6.289390, mae: 39.804811, mean_q: -58.998947, mean_eps: 0.100000\n",
      " 110359/200000: episode: 552, duration: 0.915s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 5.925542, mae: 39.705098, mean_q: -58.735806, mean_eps: 0.100000\n",
      " 110559/200000: episode: 553, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 4.083114, mae: 39.426576, mean_q: -58.156906, mean_eps: 0.100000\n",
      " 110759/200000: episode: 554, duration: 0.911s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 4.159005, mae: 39.460464, mean_q: -58.109622, mean_eps: 0.100000\n",
      " 110959/200000: episode: 555, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 4.879695, mae: 40.290247, mean_q: -59.279481, mean_eps: 0.100000\n",
      " 111159/200000: episode: 556, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 3.518476, mae: 39.881860, mean_q: -58.650019, mean_eps: 0.100000\n",
      " 111359/200000: episode: 557, duration: 0.905s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 2.416669, mae: 39.767144, mean_q: -58.504348, mean_eps: 0.100000\n",
      " 111559/200000: episode: 558, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.381442, mae: 40.750913, mean_q: -60.067728, mean_eps: 0.100000\n",
      " 111759/200000: episode: 559, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 4.885952, mae: 40.609374, mean_q: -59.883171, mean_eps: 0.100000\n",
      " 111959/200000: episode: 560, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 3.883050, mae: 40.502471, mean_q: -59.820138, mean_eps: 0.100000\n",
      " 112159/200000: episode: 561, duration: 0.895s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 6.300490, mae: 40.279928, mean_q: -59.427585, mean_eps: 0.100000\n",
      " 112359/200000: episode: 562, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 8.272191, mae: 40.523649, mean_q: -59.685593, mean_eps: 0.100000\n",
      " 112556/200000: episode: 563, duration: 0.934s, episode steps: 197, steps per second: 211, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.459042, mae: 39.921636, mean_q: -58.802610, mean_eps: 0.100000\n",
      " 112756/200000: episode: 564, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.835 [0.000, 2.000],  loss: 2.796980, mae: 39.173064, mean_q: -57.516929, mean_eps: 0.100000\n",
      " 112956/200000: episode: 565, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 5.772709, mae: 38.961336, mean_q: -57.114112, mean_eps: 0.100000\n",
      " 113156/200000: episode: 566, duration: 0.914s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 6.887352, mae: 39.032568, mean_q: -57.216920, mean_eps: 0.100000\n",
      " 113348/200000: episode: 567, duration: 0.863s, episode steps: 192, steps per second: 222, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.953 [0.000, 2.000],  loss: 4.029793, mae: 38.665886, mean_q: -56.674399, mean_eps: 0.100000\n",
      " 113548/200000: episode: 568, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 4.358571, mae: 38.415230, mean_q: -56.263108, mean_eps: 0.100000\n",
      " 113748/200000: episode: 569, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 4.643472, mae: 38.550069, mean_q: -56.519564, mean_eps: 0.100000\n",
      " 113948/200000: episode: 570, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.135467, mae: 38.331718, mean_q: -56.149345, mean_eps: 0.100000\n",
      " 114148/200000: episode: 571, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 5.682038, mae: 38.532558, mean_q: -56.503135, mean_eps: 0.100000\n",
      " 114348/200000: episode: 572, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000],  loss: 6.962095, mae: 38.407850, mean_q: -56.300949, mean_eps: 0.100000\n",
      " 114548/200000: episode: 573, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 2.000],  loss: 6.842159, mae: 39.839668, mean_q: -58.607459, mean_eps: 0.100000\n",
      " 114748/200000: episode: 574, duration: 0.879s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.054796, mae: 40.843605, mean_q: -60.202500, mean_eps: 0.100000\n",
      " 114948/200000: episode: 575, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 7.872870, mae: 40.911889, mean_q: -60.228250, mean_eps: 0.100000\n",
      " 115110/200000: episode: 576, duration: 0.734s, episode steps: 162, steps per second: 221, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 5.777508, mae: 40.401528, mean_q: -59.452128, mean_eps: 0.100000\n",
      " 115310/200000: episode: 577, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.880 [0.000, 2.000],  loss: 5.686287, mae: 39.744955, mean_q: -58.399237, mean_eps: 0.100000\n",
      " 115510/200000: episode: 578, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.740 [0.000, 2.000],  loss: 6.657317, mae: 39.689224, mean_q: -58.201498, mean_eps: 0.100000\n",
      " 115710/200000: episode: 579, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.710 [0.000, 2.000],  loss: 3.948500, mae: 39.261212, mean_q: -57.557608, mean_eps: 0.100000\n",
      " 115910/200000: episode: 580, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 2.000],  loss: 5.360425, mae: 39.308961, mean_q: -57.498495, mean_eps: 0.100000\n",
      " 116110/200000: episode: 581, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.920 [0.000, 2.000],  loss: 5.278823, mae: 39.608173, mean_q: -57.904839, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 116310/200000: episode: 582, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.925 [0.000, 2.000],  loss: 5.362319, mae: 40.771442, mean_q: -59.704011, mean_eps: 0.100000\n",
      " 116510/200000: episode: 583, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.745 [0.000, 2.000],  loss: 6.586896, mae: 40.839217, mean_q: -59.814115, mean_eps: 0.100000\n",
      " 116710/200000: episode: 584, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 3.956842, mae: 40.121792, mean_q: -58.918865, mean_eps: 0.100000\n",
      " 116852/200000: episode: 585, duration: 0.642s, episode steps: 142, steps per second: 221, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.338 [0.000, 2.000],  loss: 5.568450, mae: 39.645771, mean_q: -58.266476, mean_eps: 0.100000\n",
      " 117052/200000: episode: 586, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 3.053946, mae: 39.267357, mean_q: -57.744081, mean_eps: 0.100000\n",
      " 117149/200000: episode: 587, duration: 0.464s, episode steps:  97, steps per second: 209, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 5.824780, mae: 38.046073, mean_q: -55.493642, mean_eps: 0.100000\n",
      " 117326/200000: episode: 588, duration: 0.848s, episode steps: 177, steps per second: 209, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 3.237626, mae: 37.022945, mean_q: -53.931643, mean_eps: 0.100000\n",
      " 117526/200000: episode: 589, duration: 0.918s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 1.497233, mae: 36.187904, mean_q: -52.568479, mean_eps: 0.100000\n",
      " 117716/200000: episode: 590, duration: 0.864s, episode steps: 190, steps per second: 220, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 1.035798, mae: 36.079114, mean_q: -52.435288, mean_eps: 0.100000\n",
      " 117916/200000: episode: 591, duration: 0.887s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.883215, mae: 35.757174, mean_q: -51.867812, mean_eps: 0.100000\n",
      " 118091/200000: episode: 592, duration: 0.780s, episode steps: 175, steps per second: 224, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.497 [0.000, 2.000],  loss: 0.728490, mae: 36.012840, mean_q: -52.245230, mean_eps: 0.100000\n",
      " 118291/200000: episode: 593, duration: 0.907s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 0.998971, mae: 36.464737, mean_q: -52.952924, mean_eps: 0.100000\n",
      " 118491/200000: episode: 594, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.615 [0.000, 2.000],  loss: 1.552624, mae: 36.872584, mean_q: -53.712239, mean_eps: 0.100000\n",
      " 118666/200000: episode: 595, duration: 0.783s, episode steps: 175, steps per second: 223, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 1.462685, mae: 36.432609, mean_q: -52.978498, mean_eps: 0.100000\n",
      " 118866/200000: episode: 596, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.153105, mae: 36.759336, mean_q: -53.641157, mean_eps: 0.100000\n",
      " 119066/200000: episode: 597, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.277186, mae: 37.436964, mean_q: -54.799637, mean_eps: 0.100000\n",
      " 119266/200000: episode: 598, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000],  loss: 6.029533, mae: 38.535030, mean_q: -56.479690, mean_eps: 0.100000\n",
      " 119466/200000: episode: 599, duration: 0.962s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 3.650026, mae: 38.598315, mean_q: -56.685814, mean_eps: 0.100000\n",
      " 119616/200000: episode: 600, duration: 0.693s, episode steps: 150, steps per second: 217, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.513 [0.000, 2.000],  loss: 6.182443, mae: 38.810425, mean_q: -56.972817, mean_eps: 0.100000\n",
      " 119816/200000: episode: 601, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000],  loss: 6.817017, mae: 38.796102, mean_q: -56.876265, mean_eps: 0.100000\n",
      " 120016/200000: episode: 602, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 3.792481, mae: 39.158666, mean_q: -57.366206, mean_eps: 0.100000\n",
      " 120216/200000: episode: 603, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 2.000],  loss: 3.786483, mae: 38.270757, mean_q: -55.838530, mean_eps: 0.100000\n",
      " 120367/200000: episode: 604, duration: 0.748s, episode steps: 151, steps per second: 202, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.417 [0.000, 2.000],  loss: 3.511686, mae: 37.587241, mean_q: -54.799321, mean_eps: 0.100000\n",
      " 120546/200000: episode: 605, duration: 0.802s, episode steps: 179, steps per second: 223, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 2.056904, mae: 36.294906, mean_q: -52.607369, mean_eps: 0.100000\n",
      " 120746/200000: episode: 606, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.340 [0.000, 2.000],  loss: 2.525690, mae: 35.807960, mean_q: -51.772974, mean_eps: 0.100000\n",
      " 120906/200000: episode: 607, duration: 0.748s, episode steps: 160, steps per second: 214, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 3.798279, mae: 34.965681, mean_q: -50.681577, mean_eps: 0.100000\n",
      " 121106/200000: episode: 608, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 3.174900, mae: 33.709802, mean_q: -48.683554, mean_eps: 0.100000\n",
      " 121306/200000: episode: 609, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.125 [0.000, 2.000],  loss: 5.973700, mae: 33.691140, mean_q: -48.444666, mean_eps: 0.100000\n",
      " 121506/200000: episode: 610, duration: 0.915s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 6.696252, mae: 34.060601, mean_q: -49.195903, mean_eps: 0.100000\n",
      " 121706/200000: episode: 611, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 6.854922, mae: 34.840916, mean_q: -50.903366, mean_eps: 0.100000\n",
      " 121906/200000: episode: 612, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 7.708621, mae: 34.893632, mean_q: -50.862801, mean_eps: 0.100000\n",
      " 122106/200000: episode: 613, duration: 0.908s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.230 [0.000, 2.000],  loss: 7.668469, mae: 35.395272, mean_q: -51.919417, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 122306/200000: episode: 614, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000],  loss: 4.408687, mae: 35.314753, mean_q: -52.053402, mean_eps: 0.100000\n",
      " 122506/200000: episode: 615, duration: 1.007s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000],  loss: 6.127623, mae: 35.503944, mean_q: -52.299469, mean_eps: 0.100000\n",
      " 122653/200000: episode: 616, duration: 0.677s, episode steps: 147, steps per second: 217, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.088 [0.000, 2.000],  loss: 6.012182, mae: 35.663958, mean_q: -52.504458, mean_eps: 0.100000\n",
      " 122853/200000: episode: 617, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 4.499157, mae: 35.268930, mean_q: -51.907971, mean_eps: 0.100000\n",
      " 123053/200000: episode: 618, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 6.098901, mae: 35.443721, mean_q: -52.071460, mean_eps: 0.100000\n",
      " 123253/200000: episode: 619, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.150 [0.000, 2.000],  loss: 5.370589, mae: 35.662678, mean_q: -52.528436, mean_eps: 0.100000\n",
      " 123453/200000: episode: 620, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 5.063620, mae: 35.827010, mean_q: -52.845646, mean_eps: 0.100000\n",
      " 123653/200000: episode: 621, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 4.307708, mae: 36.014667, mean_q: -53.192819, mean_eps: 0.100000\n",
      " 123853/200000: episode: 622, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.195 [0.000, 2.000],  loss: 7.711719, mae: 36.834082, mean_q: -54.481941, mean_eps: 0.100000\n",
      " 123945/200000: episode: 623, duration: 0.411s, episode steps:  92, steps per second: 224, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.326 [0.000, 2.000],  loss: 7.441398, mae: 36.912859, mean_q: -54.518043, mean_eps: 0.100000\n",
      " 124145/200000: episode: 624, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.745851, mae: 36.648488, mean_q: -54.047586, mean_eps: 0.100000\n",
      " 124296/200000: episode: 625, duration: 0.667s, episode steps: 151, steps per second: 226, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 6.763511, mae: 36.374325, mean_q: -53.447942, mean_eps: 0.100000\n",
      " 124476/200000: episode: 626, duration: 0.796s, episode steps: 180, steps per second: 226, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.083 [0.000, 2.000],  loss: 6.650020, mae: 35.774716, mean_q: -52.275840, mean_eps: 0.100000\n",
      " 124676/200000: episode: 627, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 3.544916, mae: 35.539658, mean_q: -51.755636, mean_eps: 0.100000\n",
      " 124875/200000: episode: 628, duration: 0.953s, episode steps: 199, steps per second: 209, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.274160, mae: 35.600751, mean_q: -51.720664, mean_eps: 0.100000\n",
      " 125075/200000: episode: 629, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.580 [0.000, 2.000],  loss: 1.818394, mae: 35.448363, mean_q: -51.363743, mean_eps: 0.100000\n",
      " 125275/200000: episode: 630, duration: 0.928s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.660 [0.000, 2.000],  loss: 1.959746, mae: 35.587362, mean_q: -51.617601, mean_eps: 0.100000\n",
      " 125475/200000: episode: 631, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.950956, mae: 36.545530, mean_q: -53.263223, mean_eps: 0.100000\n",
      " 125621/200000: episode: 632, duration: 0.677s, episode steps: 146, steps per second: 216, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000],  loss: 2.854460, mae: 36.516325, mean_q: -53.183251, mean_eps: 0.100000\n",
      " 125772/200000: episode: 633, duration: 0.696s, episode steps: 151, steps per second: 217, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 2.721121, mae: 35.917605, mean_q: -52.196232, mean_eps: 0.100000\n",
      " 125936/200000: episode: 634, duration: 0.737s, episode steps: 164, steps per second: 223, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.851087, mae: 35.465074, mean_q: -51.290094, mean_eps: 0.100000\n",
      " 126092/200000: episode: 635, duration: 0.686s, episode steps: 156, steps per second: 228, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.853 [0.000, 2.000],  loss: 1.858297, mae: 35.195442, mean_q: -50.770704, mean_eps: 0.100000\n",
      " 126292/200000: episode: 636, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 1.816507, mae: 34.698024, mean_q: -49.729870, mean_eps: 0.100000\n",
      " 126492/200000: episode: 637, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.450 [0.000, 2.000],  loss: 4.217265, mae: 34.870304, mean_q: -49.904476, mean_eps: 0.100000\n",
      " 126680/200000: episode: 638, duration: 0.828s, episode steps: 188, steps per second: 227, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000],  loss: 3.200189, mae: 35.379740, mean_q: -51.070713, mean_eps: 0.100000\n",
      " 126880/200000: episode: 639, duration: 0.880s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.125 [0.000, 2.000],  loss: 2.724561, mae: 35.668643, mean_q: -51.944831, mean_eps: 0.100000\n",
      " 127080/200000: episode: 640, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 5.845316, mae: 36.988142, mean_q: -54.138593, mean_eps: 0.100000\n",
      " 127280/200000: episode: 641, duration: 0.879s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.365 [0.000, 2.000],  loss: 6.259783, mae: 37.800188, mean_q: -55.698095, mean_eps: 0.100000\n",
      " 127480/200000: episode: 642, duration: 0.896s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.170 [0.000, 2.000],  loss: 9.158458, mae: 38.130805, mean_q: -55.978010, mean_eps: 0.100000\n",
      " 127623/200000: episode: 643, duration: 0.630s, episode steps: 143, steps per second: 227, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 7.808679, mae: 38.165082, mean_q: -56.184885, mean_eps: 0.100000\n",
      " 127823/200000: episode: 644, duration: 0.878s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.650 [0.000, 2.000],  loss: 5.096431, mae: 38.245013, mean_q: -56.347349, mean_eps: 0.100000\n",
      " 128023/200000: episode: 645, duration: 0.876s, episode steps: 200, steps per second: 228, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 7.853463, mae: 38.272190, mean_q: -56.126742, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 128223/200000: episode: 646, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 7.196118, mae: 38.354685, mean_q: -56.109528, mean_eps: 0.100000\n",
      " 128374/200000: episode: 647, duration: 0.660s, episode steps: 151, steps per second: 229, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.361780, mae: 37.971331, mean_q: -55.645940, mean_eps: 0.100000\n",
      " 128525/200000: episode: 648, duration: 0.665s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 6.460493, mae: 36.985836, mean_q: -53.721274, mean_eps: 0.100000\n",
      " 128678/200000: episode: 649, duration: 0.676s, episode steps: 153, steps per second: 226, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000],  loss: 3.919398, mae: 36.622766, mean_q: -53.055138, mean_eps: 0.100000\n",
      " 128825/200000: episode: 650, duration: 0.653s, episode steps: 147, steps per second: 225, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.027 [0.000, 2.000],  loss: 3.905787, mae: 35.864717, mean_q: -51.791228, mean_eps: 0.100000\n",
      " 129025/200000: episode: 651, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 4.439955, mae: 35.128842, mean_q: -50.377258, mean_eps: 0.100000\n",
      " 129119/200000: episode: 652, duration: 0.416s, episode steps:  94, steps per second: 226, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 4.021810, mae: 35.066317, mean_q: -50.193229, mean_eps: 0.100000\n",
      " 129266/200000: episode: 653, duration: 0.690s, episode steps: 147, steps per second: 213, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 4.619362, mae: 34.412437, mean_q: -49.087376, mean_eps: 0.100000\n",
      " 129415/200000: episode: 654, duration: 0.690s, episode steps: 149, steps per second: 216, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 1.629042, mae: 34.033237, mean_q: -48.485740, mean_eps: 0.100000\n",
      " 129567/200000: episode: 655, duration: 0.679s, episode steps: 152, steps per second: 224, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 1.607596, mae: 33.627137, mean_q: -47.831774, mean_eps: 0.100000\n",
      " 129718/200000: episode: 656, duration: 0.691s, episode steps: 151, steps per second: 219, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 2.873148, mae: 33.652926, mean_q: -47.822774, mean_eps: 0.100000\n",
      " 129865/200000: episode: 657, duration: 0.699s, episode steps: 147, steps per second: 210, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.088 [0.000, 2.000],  loss: 2.234694, mae: 33.736985, mean_q: -47.970069, mean_eps: 0.100000\n",
      " 130012/200000: episode: 658, duration: 0.712s, episode steps: 147, steps per second: 206, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 2.342509, mae: 32.811564, mean_q: -46.574640, mean_eps: 0.100000\n",
      " 130212/200000: episode: 659, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.529377, mae: 32.192291, mean_q: -45.500146, mean_eps: 0.100000\n",
      " 130412/200000: episode: 660, duration: 1.043s, episode steps: 200, steps per second: 192, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.500 [0.000, 2.000],  loss: 1.678040, mae: 33.765549, mean_q: -48.262023, mean_eps: 0.100000\n",
      " 130561/200000: episode: 661, duration: 0.756s, episode steps: 149, steps per second: 197, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 1.784223, mae: 34.473685, mean_q: -49.559116, mean_eps: 0.100000\n",
      " 130761/200000: episode: 662, duration: 0.950s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.411382, mae: 35.375411, mean_q: -51.105358, mean_eps: 0.100000\n",
      " 130904/200000: episode: 663, duration: 0.648s, episode steps: 143, steps per second: 221, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 2.454096, mae: 35.810835, mean_q: -51.850349, mean_eps: 0.100000\n",
      " 131046/200000: episode: 664, duration: 0.659s, episode steps: 142, steps per second: 216, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 2.784368, mae: 35.611354, mean_q: -51.458594, mean_eps: 0.100000\n",
      " 131246/200000: episode: 665, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.643787, mae: 35.195306, mean_q: -50.852117, mean_eps: 0.100000\n",
      " 131446/200000: episode: 666, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 1.591852, mae: 34.637627, mean_q: -49.837421, mean_eps: 0.100000\n",
      " 131592/200000: episode: 667, duration: 0.665s, episode steps: 146, steps per second: 219, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 1.274527, mae: 34.729111, mean_q: -49.950363, mean_eps: 0.100000\n",
      " 131741/200000: episode: 668, duration: 0.714s, episode steps: 149, steps per second: 209, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.975777, mae: 33.829513, mean_q: -48.504809, mean_eps: 0.100000\n",
      " 131889/200000: episode: 669, duration: 0.711s, episode steps: 148, steps per second: 208, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.972818, mae: 33.149851, mean_q: -47.326500, mean_eps: 0.100000\n",
      " 132035/200000: episode: 670, duration: 0.694s, episode steps: 146, steps per second: 210, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 1.073604, mae: 32.990384, mean_q: -47.101424, mean_eps: 0.100000\n",
      " 132180/200000: episode: 671, duration: 0.704s, episode steps: 145, steps per second: 206, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 1.117214, mae: 33.126277, mean_q: -47.373940, mean_eps: 0.100000\n",
      " 132380/200000: episode: 672, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.565 [0.000, 2.000],  loss: 1.073026, mae: 32.414094, mean_q: -46.266436, mean_eps: 0.100000\n",
      " 132561/200000: episode: 673, duration: 0.861s, episode steps: 181, steps per second: 210, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 1.628102, mae: 32.630814, mean_q: -46.642798, mean_eps: 0.100000\n",
      " 132761/200000: episode: 674, duration: 0.918s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.045297, mae: 33.583675, mean_q: -48.320590, mean_eps: 0.100000\n",
      " 132961/200000: episode: 675, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 1.623724, mae: 34.343620, mean_q: -49.580307, mean_eps: 0.100000\n",
      " 133099/200000: episode: 676, duration: 0.617s, episode steps: 138, steps per second: 224, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 1.145558, mae: 35.058814, mean_q: -50.731760, mean_eps: 0.100000\n",
      " 133241/200000: episode: 677, duration: 0.678s, episode steps: 142, steps per second: 209, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.103345, mae: 35.291368, mean_q: -51.174468, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 133441/200000: episode: 678, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.200 [0.000, 2.000],  loss: 1.170050, mae: 35.230511, mean_q: -51.031463, mean_eps: 0.100000\n",
      " 133641/200000: episode: 679, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.165 [0.000, 2.000],  loss: 2.466961, mae: 35.736732, mean_q: -51.767297, mean_eps: 0.100000\n",
      " 133740/200000: episode: 680, duration: 0.451s, episode steps:  99, steps per second: 220, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 1.825199, mae: 36.056425, mean_q: -52.376497, mean_eps: 0.100000\n",
      " 133940/200000: episode: 681, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.455 [0.000, 2.000],  loss: 3.847317, mae: 36.265825, mean_q: -52.709769, mean_eps: 0.100000\n",
      " 134087/200000: episode: 682, duration: 0.692s, episode steps: 147, steps per second: 212, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 2.942758, mae: 36.639816, mean_q: -53.463381, mean_eps: 0.100000\n",
      " 134287/200000: episode: 683, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.620 [0.000, 2.000],  loss: 4.674054, mae: 37.365482, mean_q: -54.719926, mean_eps: 0.100000\n",
      " 134452/200000: episode: 684, duration: 0.749s, episode steps: 165, steps per second: 220, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 3.523748, mae: 36.910090, mean_q: -53.943663, mean_eps: 0.100000\n",
      " 134607/200000: episode: 685, duration: 0.699s, episode steps: 155, steps per second: 222, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.884 [0.000, 2.000],  loss: 4.547125, mae: 35.322698, mean_q: -50.935414, mean_eps: 0.100000\n",
      " 134699/200000: episode: 686, duration: 0.423s, episode steps:  92, steps per second: 217, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 4.667607, mae: 34.520158, mean_q: -49.447715, mean_eps: 0.100000\n",
      " 134849/200000: episode: 687, duration: 0.705s, episode steps: 150, steps per second: 213, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.027 [0.000, 2.000],  loss: 1.879364, mae: 34.041018, mean_q: -48.766910, mean_eps: 0.100000\n",
      " 134997/200000: episode: 688, duration: 0.696s, episode steps: 148, steps per second: 213, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 2.389987, mae: 33.185731, mean_q: -47.055137, mean_eps: 0.100000\n",
      " 135146/200000: episode: 689, duration: 0.693s, episode steps: 149, steps per second: 215, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 3.267019, mae: 32.290491, mean_q: -45.276469, mean_eps: 0.100000\n",
      " 135346/200000: episode: 690, duration: 0.933s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 2.907166, mae: 31.909437, mean_q: -44.743750, mean_eps: 0.100000\n",
      " 135546/200000: episode: 691, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 5.432228, mae: 33.804467, mean_q: -47.912153, mean_eps: 0.100000\n",
      " 135694/200000: episode: 692, duration: 0.664s, episode steps: 148, steps per second: 223, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000],  loss: 6.065632, mae: 35.084374, mean_q: -50.227510, mean_eps: 0.100000\n",
      " 135788/200000: episode: 693, duration: 0.416s, episode steps:  94, steps per second: 226, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 4.861187, mae: 34.687700, mean_q: -49.383107, mean_eps: 0.100000\n",
      " 135988/200000: episode: 694, duration: 0.890s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.190 [0.000, 2.000],  loss: 6.481617, mae: 35.163778, mean_q: -50.525526, mean_eps: 0.100000\n",
      " 136188/200000: episode: 695, duration: 0.911s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.131373, mae: 37.188689, mean_q: -54.346760, mean_eps: 0.100000\n",
      " 136328/200000: episode: 696, duration: 0.639s, episode steps: 140, steps per second: 219, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 3.341706, mae: 36.784189, mean_q: -53.705814, mean_eps: 0.100000\n",
      " 136471/200000: episode: 697, duration: 0.634s, episode steps: 143, steps per second: 226, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 3.686614, mae: 35.254564, mean_q: -51.011992, mean_eps: 0.100000\n",
      " 136607/200000: episode: 698, duration: 0.603s, episode steps: 136, steps per second: 225, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.630954, mae: 33.685233, mean_q: -48.321561, mean_eps: 0.100000\n",
      " 136753/200000: episode: 699, duration: 0.650s, episode steps: 146, steps per second: 225, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 2.233568, mae: 33.519263, mean_q: -47.950203, mean_eps: 0.100000\n",
      " 136895/200000: episode: 700, duration: 0.674s, episode steps: 142, steps per second: 211, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 2.250853, mae: 33.786206, mean_q: -48.330326, mean_eps: 0.100000\n",
      " 137039/200000: episode: 701, duration: 0.652s, episode steps: 144, steps per second: 221, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 2.577240, mae: 32.787931, mean_q: -46.672313, mean_eps: 0.100000\n",
      " 137183/200000: episode: 702, duration: 0.645s, episode steps: 144, steps per second: 223, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 1.196321, mae: 31.684886, mean_q: -44.811239, mean_eps: 0.100000\n",
      " 137326/200000: episode: 703, duration: 0.662s, episode steps: 143, steps per second: 216, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 1.803035, mae: 31.574124, mean_q: -44.521716, mean_eps: 0.100000\n",
      " 137469/200000: episode: 704, duration: 0.670s, episode steps: 143, steps per second: 214, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 1.556228, mae: 31.790023, mean_q: -44.908249, mean_eps: 0.100000\n",
      " 137619/200000: episode: 705, duration: 0.683s, episode steps: 150, steps per second: 220, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 1.013837, mae: 32.180429, mean_q: -45.638894, mean_eps: 0.100000\n",
      " 137763/200000: episode: 706, duration: 0.654s, episode steps: 144, steps per second: 220, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.083 [0.000, 2.000],  loss: 1.212796, mae: 32.019287, mean_q: -45.512075, mean_eps: 0.100000\n",
      " 137901/200000: episode: 707, duration: 0.631s, episode steps: 138, steps per second: 219, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 1.050704, mae: 32.196562, mean_q: -45.855867, mean_eps: 0.100000\n",
      " 138044/200000: episode: 708, duration: 0.654s, episode steps: 143, steps per second: 219, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 1.421636, mae: 32.199421, mean_q: -45.835627, mean_eps: 0.100000\n",
      " 138244/200000: episode: 709, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.921792, mae: 32.528858, mean_q: -46.571627, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 138444/200000: episode: 710, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.077175, mae: 33.179885, mean_q: -47.570579, mean_eps: 0.100000\n",
      " 138585/200000: episode: 711, duration: 0.644s, episode steps: 141, steps per second: 219, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.638805, mae: 32.995258, mean_q: -47.416513, mean_eps: 0.100000\n",
      " 138724/200000: episode: 712, duration: 0.657s, episode steps: 139, steps per second: 212, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.855720, mae: 32.774424, mean_q: -46.960244, mean_eps: 0.100000\n",
      " 138870/200000: episode: 713, duration: 0.684s, episode steps: 146, steps per second: 213, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.898718, mae: 32.729186, mean_q: -46.772052, mean_eps: 0.100000\n",
      " 139017/200000: episode: 714, duration: 0.674s, episode steps: 147, steps per second: 218, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.664448, mae: 33.263330, mean_q: -47.766962, mean_eps: 0.100000\n",
      " 139157/200000: episode: 715, duration: 0.655s, episode steps: 140, steps per second: 214, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.460170, mae: 33.178858, mean_q: -47.745291, mean_eps: 0.100000\n",
      " 139357/200000: episode: 716, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.918518, mae: 32.579742, mean_q: -46.674148, mean_eps: 0.100000\n",
      " 139557/200000: episode: 717, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.839758, mae: 32.921372, mean_q: -47.252529, mean_eps: 0.100000\n",
      " 139701/200000: episode: 718, duration: 0.661s, episode steps: 144, steps per second: 218, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000],  loss: 0.525305, mae: 32.709964, mean_q: -46.981326, mean_eps: 0.100000\n",
      " 139901/200000: episode: 719, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.584162, mae: 32.927417, mean_q: -47.362139, mean_eps: 0.100000\n",
      " 140064/200000: episode: 720, duration: 0.750s, episode steps: 163, steps per second: 217, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 1.097251, mae: 33.252965, mean_q: -47.847357, mean_eps: 0.100000\n",
      " 140264/200000: episode: 721, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.028405, mae: 33.628880, mean_q: -48.609740, mean_eps: 0.100000\n",
      " 140414/200000: episode: 722, duration: 0.693s, episode steps: 150, steps per second: 216, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 1.203112, mae: 34.009355, mean_q: -49.207831, mean_eps: 0.100000\n",
      " 140567/200000: episode: 723, duration: 0.755s, episode steps: 153, steps per second: 203, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 2.956019, mae: 32.693456, mean_q: -46.767732, mean_eps: 0.100000\n",
      " 140716/200000: episode: 724, duration: 0.678s, episode steps: 149, steps per second: 220, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 3.039271, mae: 32.157358, mean_q: -45.736228, mean_eps: 0.100000\n",
      " 140864/200000: episode: 725, duration: 0.699s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 2.123897, mae: 31.387057, mean_q: -44.507738, mean_eps: 0.100000\n",
      " 141014/200000: episode: 726, duration: 0.778s, episode steps: 150, steps per second: 193, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 1.826663, mae: 30.156194, mean_q: -42.536397, mean_eps: 0.100000\n",
      " 141162/200000: episode: 727, duration: 0.693s, episode steps: 148, steps per second: 214, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 1.157482, mae: 29.454003, mean_q: -41.729212, mean_eps: 0.100000\n",
      " 141362/200000: episode: 728, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 1.387277, mae: 29.234550, mean_q: -41.303637, mean_eps: 0.100000\n",
      " 141509/200000: episode: 729, duration: 0.669s, episode steps: 147, steps per second: 220, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 1.723417, mae: 30.858617, mean_q: -44.067894, mean_eps: 0.100000\n",
      " 141666/200000: episode: 730, duration: 0.709s, episode steps: 157, steps per second: 222, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 1.102454, mae: 30.768216, mean_q: -44.161288, mean_eps: 0.100000\n",
      " 141814/200000: episode: 731, duration: 0.665s, episode steps: 148, steps per second: 223, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 1.195671, mae: 30.509842, mean_q: -43.751853, mean_eps: 0.100000\n",
      " 141964/200000: episode: 732, duration: 0.674s, episode steps: 150, steps per second: 223, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 1.212840, mae: 30.930448, mean_q: -44.303503, mean_eps: 0.100000\n",
      " 142164/200000: episode: 733, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.400 [0.000, 2.000],  loss: 1.030205, mae: 31.278679, mean_q: -45.010883, mean_eps: 0.100000\n",
      " 142299/200000: episode: 734, duration: 0.624s, episode steps: 135, steps per second: 216, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 5.137565, mae: 33.105444, mean_q: -47.746284, mean_eps: 0.100000\n",
      " 142499/200000: episode: 735, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 3.364643, mae: 33.062463, mean_q: -47.941507, mean_eps: 0.100000\n",
      " 142653/200000: episode: 736, duration: 0.691s, episode steps: 154, steps per second: 223, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000],  loss: 3.564920, mae: 34.461990, mean_q: -50.206633, mean_eps: 0.100000\n",
      " 142803/200000: episode: 737, duration: 0.684s, episode steps: 150, steps per second: 219, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 3.177285, mae: 34.858029, mean_q: -50.991313, mean_eps: 0.100000\n",
      " 142956/200000: episode: 738, duration: 0.700s, episode steps: 153, steps per second: 219, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 2.600318, mae: 34.539981, mean_q: -50.615485, mean_eps: 0.100000\n",
      " 143100/200000: episode: 739, duration: 0.649s, episode steps: 144, steps per second: 222, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 2.694332, mae: 35.055431, mean_q: -51.310690, mean_eps: 0.100000\n",
      " 143247/200000: episode: 740, duration: 0.672s, episode steps: 147, steps per second: 219, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.958494, mae: 32.505911, mean_q: -47.346544, mean_eps: 0.100000\n",
      " 143397/200000: episode: 741, duration: 0.680s, episode steps: 150, steps per second: 221, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.617734, mae: 31.407296, mean_q: -45.691366, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143597/200000: episode: 742, duration: 0.892s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.706943, mae: 30.116084, mean_q: -43.649725, mean_eps: 0.100000\n",
      " 143748/200000: episode: 743, duration: 0.674s, episode steps: 151, steps per second: 224, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.505032, mae: 30.839354, mean_q: -44.808226, mean_eps: 0.100000\n",
      " 143893/200000: episode: 744, duration: 0.654s, episode steps: 145, steps per second: 222, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.468651, mae: 30.652876, mean_q: -44.522004, mean_eps: 0.100000\n",
      " 144040/200000: episode: 745, duration: 0.696s, episode steps: 147, steps per second: 211, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.527602, mae: 29.684153, mean_q: -43.012831, mean_eps: 0.100000\n",
      " 144240/200000: episode: 746, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.400392, mae: 29.973995, mean_q: -43.573984, mean_eps: 0.100000\n",
      " 144385/200000: episode: 747, duration: 0.696s, episode steps: 145, steps per second: 208, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.403621, mae: 30.684002, mean_q: -44.662658, mean_eps: 0.100000\n",
      " 144537/200000: episode: 748, duration: 0.692s, episode steps: 152, steps per second: 220, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.433809, mae: 30.496700, mean_q: -44.384366, mean_eps: 0.100000\n",
      " 144737/200000: episode: 749, duration: 0.901s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.410820, mae: 30.041276, mean_q: -43.762619, mean_eps: 0.100000\n",
      " 144937/200000: episode: 750, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 0.432996, mae: 31.871846, mean_q: -46.585777, mean_eps: 0.100000\n",
      " 145137/200000: episode: 751, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.150 [0.000, 2.000],  loss: 0.988690, mae: 35.468202, mean_q: -52.161452, mean_eps: 0.100000\n",
      " 145337/200000: episode: 752, duration: 0.910s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.205 [0.000, 2.000],  loss: 7.003969, mae: 37.499492, mean_q: -55.031273, mean_eps: 0.100000\n",
      " 145537/200000: episode: 753, duration: 0.919s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 6.588735, mae: 40.363919, mean_q: -59.352527, mean_eps: 0.100000\n",
      " 145737/200000: episode: 754, duration: 0.956s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 11.830224, mae: 42.397158, mean_q: -62.116926, mean_eps: 0.100000\n",
      " 145937/200000: episode: 755, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 10.578616, mae: 43.543659, mean_q: -63.935930, mean_eps: 0.100000\n",
      " 146137/200000: episode: 756, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 12.696275, mae: 43.197378, mean_q: -63.284960, mean_eps: 0.100000\n",
      " 146337/200000: episode: 757, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.090 [0.000, 2.000],  loss: 10.588081, mae: 43.212794, mean_q: -63.582604, mean_eps: 0.100000\n",
      " 146537/200000: episode: 758, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000],  loss: 10.935279, mae: 43.482761, mean_q: -64.014797, mean_eps: 0.100000\n",
      " 146683/200000: episode: 759, duration: 0.665s, episode steps: 146, steps per second: 220, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 7.026968, mae: 43.054918, mean_q: -63.617140, mean_eps: 0.100000\n",
      " 146883/200000: episode: 760, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 8.054985, mae: 41.536269, mean_q: -61.173905, mean_eps: 0.100000\n",
      " 147083/200000: episode: 761, duration: 0.913s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 10.096872, mae: 41.544484, mean_q: -61.102453, mean_eps: 0.100000\n",
      " 147283/200000: episode: 762, duration: 0.911s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 9.762371, mae: 41.376261, mean_q: -60.940582, mean_eps: 0.100000\n",
      " 147432/200000: episode: 763, duration: 0.678s, episode steps: 149, steps per second: 220, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.636475, mae: 40.655955, mean_q: -59.867246, mean_eps: 0.100000\n",
      " 147632/200000: episode: 764, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 5.750361, mae: 39.044381, mean_q: -57.522308, mean_eps: 0.100000\n",
      " 147779/200000: episode: 765, duration: 0.651s, episode steps: 147, steps per second: 226, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 3.157105, mae: 38.900858, mean_q: -57.487066, mean_eps: 0.100000\n",
      " 147979/200000: episode: 766, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.492851, mae: 37.283050, mean_q: -54.926260, mean_eps: 0.100000\n",
      " 148179/200000: episode: 767, duration: 0.933s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000],  loss: 4.089726, mae: 35.482452, mean_q: -52.052849, mean_eps: 0.100000\n",
      " 148379/200000: episode: 768, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 1.926901, mae: 34.983209, mean_q: -51.350128, mean_eps: 0.100000\n",
      " 148579/200000: episode: 769, duration: 0.906s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.255 [0.000, 2.000],  loss: 3.224975, mae: 37.414784, mean_q: -55.042176, mean_eps: 0.100000\n",
      " 148779/200000: episode: 770, duration: 0.911s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.689010, mae: 39.466408, mean_q: -57.985022, mean_eps: 0.100000\n",
      " 148926/200000: episode: 771, duration: 0.662s, episode steps: 147, steps per second: 222, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 4.870089, mae: 39.028104, mean_q: -57.402275, mean_eps: 0.100000\n",
      " 149126/200000: episode: 772, duration: 0.902s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 8.126311, mae: 39.367776, mean_q: -57.750645, mean_eps: 0.100000\n",
      " 149326/200000: episode: 773, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.130 [0.000, 2.000],  loss: 7.486854, mae: 40.370191, mean_q: -59.114988, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 149526/200000: episode: 774, duration: 0.904s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.135 [0.000, 2.000],  loss: 7.670652, mae: 40.502154, mean_q: -59.333383, mean_eps: 0.100000\n",
      " 149621/200000: episode: 775, duration: 0.436s, episode steps:  95, steps per second: 218, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 6.661769, mae: 39.628289, mean_q: -58.107858, mean_eps: 0.100000\n",
      " 149821/200000: episode: 776, duration: 0.909s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 9.160859, mae: 39.664510, mean_q: -58.040343, mean_eps: 0.100000\n",
      " 150021/200000: episode: 777, duration: 0.893s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.080 [0.000, 2.000],  loss: 9.084483, mae: 42.364667, mean_q: -62.286515, mean_eps: 0.100000\n",
      " 150221/200000: episode: 778, duration: 0.900s, episode steps: 200, steps per second: 222, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 7.225569, mae: 42.835542, mean_q: -63.191121, mean_eps: 0.100000\n",
      " 150421/200000: episode: 779, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.120 [0.000, 2.000],  loss: 12.210363, mae: 42.381768, mean_q: -62.318467, mean_eps: 0.100000\n",
      " 150621/200000: episode: 780, duration: 0.922s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.095 [0.000, 2.000],  loss: 10.323425, mae: 42.568103, mean_q: -62.619676, mean_eps: 0.100000\n",
      " 150821/200000: episode: 781, duration: 0.885s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.115 [0.000, 2.000],  loss: 12.543067, mae: 43.507369, mean_q: -64.119473, mean_eps: 0.100000\n",
      " 151021/200000: episode: 782, duration: 0.884s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000],  loss: 8.398911, mae: 43.079160, mean_q: -63.681451, mean_eps: 0.100000\n",
      " 151168/200000: episode: 783, duration: 0.651s, episode steps: 147, steps per second: 226, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 4.868003, mae: 41.134470, mean_q: -60.955721, mean_eps: 0.100000\n",
      " 151315/200000: episode: 784, duration: 0.657s, episode steps: 147, steps per second: 224, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 4.465508, mae: 38.499204, mean_q: -56.780989, mean_eps: 0.100000\n",
      " 151462/200000: episode: 785, duration: 0.653s, episode steps: 147, steps per second: 225, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 2.747613, mae: 36.325412, mean_q: -53.483342, mean_eps: 0.100000\n",
      " 151609/200000: episode: 786, duration: 0.663s, episode steps: 147, steps per second: 222, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 3.835871, mae: 33.691861, mean_q: -49.322778, mean_eps: 0.100000\n",
      " 151757/200000: episode: 787, duration: 0.682s, episode steps: 148, steps per second: 217, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 2.140374, mae: 30.933297, mean_q: -45.131623, mean_eps: 0.100000\n",
      " 151906/200000: episode: 788, duration: 0.669s, episode steps: 149, steps per second: 223, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.287976, mae: 28.479860, mean_q: -41.373507, mean_eps: 0.100000\n",
      " 152053/200000: episode: 789, duration: 0.657s, episode steps: 147, steps per second: 224, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000],  loss: 0.264656, mae: 27.517946, mean_q: -39.847779, mean_eps: 0.100000\n",
      " 152244/200000: episode: 790, duration: 0.888s, episode steps: 191, steps per second: 215, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.300569, mae: 28.418206, mean_q: -41.276617, mean_eps: 0.100000\n",
      " 152390/200000: episode: 791, duration: 0.662s, episode steps: 146, steps per second: 220, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.626468, mae: 29.864214, mean_q: -43.523925, mean_eps: 0.100000\n",
      " 152538/200000: episode: 792, duration: 0.667s, episode steps: 148, steps per second: 222, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.495999, mae: 30.722912, mean_q: -44.852968, mean_eps: 0.100000\n",
      " 152683/200000: episode: 793, duration: 0.638s, episode steps: 145, steps per second: 227, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.456663, mae: 30.794530, mean_q: -44.979981, mean_eps: 0.100000\n",
      " 152826/200000: episode: 794, duration: 0.633s, episode steps: 143, steps per second: 226, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.430131, mae: 31.564934, mean_q: -46.152871, mean_eps: 0.100000\n",
      " 153026/200000: episode: 795, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.247968, mae: 32.648053, mean_q: -47.851597, mean_eps: 0.100000\n",
      " 153226/200000: episode: 796, duration: 0.903s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.309662, mae: 32.773010, mean_q: -48.001339, mean_eps: 0.100000\n",
      " 153367/200000: episode: 797, duration: 0.630s, episode steps: 141, steps per second: 224, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.470509, mae: 32.286642, mean_q: -47.271081, mean_eps: 0.100000\n",
      " 153545/200000: episode: 798, duration: 0.787s, episode steps: 178, steps per second: 226, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.487593, mae: 33.089390, mean_q: -48.466943, mean_eps: 0.100000\n",
      " 153745/200000: episode: 799, duration: 0.894s, episode steps: 200, steps per second: 224, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 0.409189, mae: 34.771083, mean_q: -51.043381, mean_eps: 0.100000\n",
      " 153886/200000: episode: 800, duration: 0.619s, episode steps: 141, steps per second: 228, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 2.119589, mae: 37.379249, mean_q: -55.065956, mean_eps: 0.100000\n",
      " 154086/200000: episode: 801, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.591833, mae: 36.534735, mean_q: -53.580774, mean_eps: 0.100000\n",
      " 154286/200000: episode: 802, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 5.025986, mae: 39.199876, mean_q: -57.607422, mean_eps: 0.100000\n",
      " 154473/200000: episode: 803, duration: 0.882s, episode steps: 187, steps per second: 212, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 4.963414, mae: 40.586140, mean_q: -59.870975, mean_eps: 0.100000\n",
      " 154623/200000: episode: 804, duration: 0.676s, episode steps: 150, steps per second: 222, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.567 [0.000, 2.000],  loss: 5.250887, mae: 40.759892, mean_q: -60.154040, mean_eps: 0.100000\n",
      " 154823/200000: episode: 805, duration: 0.912s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 5.690470, mae: 39.176973, mean_q: -57.645844, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 155023/200000: episode: 806, duration: 0.907s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 3.237918, mae: 39.665393, mean_q: -58.497468, mean_eps: 0.100000\n",
      " 155173/200000: episode: 807, duration: 0.659s, episode steps: 150, steps per second: 227, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 4.819945, mae: 37.911261, mean_q: -55.626181, mean_eps: 0.100000\n",
      " 155315/200000: episode: 808, duration: 0.628s, episode steps: 142, steps per second: 226, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 2.000],  loss: 1.565289, mae: 35.994283, mean_q: -52.666011, mean_eps: 0.100000\n",
      " 155467/200000: episode: 809, duration: 0.694s, episode steps: 152, steps per second: 219, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.405459, mae: 34.580780, mean_q: -50.531612, mean_eps: 0.100000\n",
      " 155667/200000: episode: 810, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 2.000],  loss: 0.450682, mae: 35.163221, mean_q: -51.465061, mean_eps: 0.100000\n",
      " 155867/200000: episode: 811, duration: 0.886s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000],  loss: 2.885655, mae: 36.903021, mean_q: -54.098077, mean_eps: 0.100000\n",
      " 156026/200000: episode: 812, duration: 0.702s, episode steps: 159, steps per second: 227, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 1.577034, mae: 36.228284, mean_q: -53.170419, mean_eps: 0.100000\n",
      " 156226/200000: episode: 813, duration: 0.907s, episode steps: 200, steps per second: 221, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 3.924343, mae: 35.656107, mean_q: -52.143003, mean_eps: 0.100000\n",
      " 156426/200000: episode: 814, duration: 0.889s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.310 [0.000, 2.000],  loss: 1.611519, mae: 36.761909, mean_q: -53.927315, mean_eps: 0.100000\n",
      " 156576/200000: episode: 815, duration: 0.683s, episode steps: 150, steps per second: 219, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.453 [0.000, 2.000],  loss: 5.364228, mae: 38.633727, mean_q: -56.815309, mean_eps: 0.100000\n",
      " 156720/200000: episode: 816, duration: 0.637s, episode steps: 144, steps per second: 226, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.403 [0.000, 2.000],  loss: 3.037154, mae: 36.707060, mean_q: -53.857978, mean_eps: 0.100000\n",
      " 156868/200000: episode: 817, duration: 0.668s, episode steps: 148, steps per second: 222, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 1.135479, mae: 34.714717, mean_q: -50.798988, mean_eps: 0.100000\n",
      " 157016/200000: episode: 818, duration: 0.657s, episode steps: 148, steps per second: 225, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 1.740101, mae: 34.504721, mean_q: -50.456776, mean_eps: 0.100000\n",
      " 157159/200000: episode: 819, duration: 0.642s, episode steps: 143, steps per second: 223, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000],  loss: 1.931320, mae: 34.713206, mean_q: -50.776500, mean_eps: 0.100000\n",
      " 157313/200000: episode: 820, duration: 0.688s, episode steps: 154, steps per second: 224, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 0.953605, mae: 34.930274, mean_q: -51.171227, mean_eps: 0.100000\n",
      " 157457/200000: episode: 821, duration: 0.652s, episode steps: 144, steps per second: 221, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.989189, mae: 32.840425, mean_q: -47.935229, mean_eps: 0.100000\n",
      " 157607/200000: episode: 822, duration: 0.672s, episode steps: 150, steps per second: 223, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.202366, mae: 30.839915, mean_q: -44.893876, mean_eps: 0.100000\n",
      " 157764/200000: episode: 823, duration: 0.714s, episode steps: 157, steps per second: 220, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.316662, mae: 29.928157, mean_q: -43.470124, mean_eps: 0.100000\n",
      " 157926/200000: episode: 824, duration: 0.724s, episode steps: 162, steps per second: 224, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.497780, mae: 28.600445, mean_q: -41.347296, mean_eps: 0.100000\n",
      " 158078/200000: episode: 825, duration: 0.701s, episode steps: 152, steps per second: 217, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.746615, mae: 27.726352, mean_q: -39.828684, mean_eps: 0.100000\n",
      " 158278/200000: episode: 826, duration: 0.907s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 0.618278, mae: 27.815924, mean_q: -40.078484, mean_eps: 0.100000\n",
      " 158430/200000: episode: 827, duration: 0.711s, episode steps: 152, steps per second: 214, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000],  loss: 0.451021, mae: 28.316119, mean_q: -40.986865, mean_eps: 0.100000\n",
      " 158585/200000: episode: 828, duration: 0.697s, episode steps: 155, steps per second: 222, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.458 [0.000, 2.000],  loss: 0.440234, mae: 28.829768, mean_q: -41.833733, mean_eps: 0.100000\n",
      " 158735/200000: episode: 829, duration: 0.668s, episode steps: 150, steps per second: 224, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.276238, mae: 29.065380, mean_q: -42.258428, mean_eps: 0.100000\n",
      " 158890/200000: episode: 830, duration: 0.682s, episode steps: 155, steps per second: 227, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.429843, mae: 30.658455, mean_q: -44.707118, mean_eps: 0.100000\n",
      " 159046/200000: episode: 831, duration: 0.685s, episode steps: 156, steps per second: 228, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.157252, mae: 31.379284, mean_q: -45.983318, mean_eps: 0.100000\n",
      " 159206/200000: episode: 832, duration: 0.719s, episode steps: 160, steps per second: 222, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.173921, mae: 31.340235, mean_q: -45.948810, mean_eps: 0.100000\n",
      " 159406/200000: episode: 833, duration: 0.907s, episode steps: 200, steps per second: 220, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.450 [0.000, 2.000],  loss: 0.216200, mae: 30.584982, mean_q: -44.712894, mean_eps: 0.100000\n",
      " 159561/200000: episode: 834, duration: 0.702s, episode steps: 155, steps per second: 221, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 1.442596, mae: 32.393358, mean_q: -47.292092, mean_eps: 0.100000\n",
      " 159721/200000: episode: 835, duration: 0.738s, episode steps: 160, steps per second: 217, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 1.888059, mae: 32.935876, mean_q: -48.126150, mean_eps: 0.100000\n",
      " 159877/200000: episode: 836, duration: 0.732s, episode steps: 156, steps per second: 213, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 1.059025, mae: 31.916212, mean_q: -46.567875, mean_eps: 0.100000\n",
      " 160021/200000: episode: 837, duration: 0.658s, episode steps: 144, steps per second: 219, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 1.176886, mae: 31.333277, mean_q: -45.527215, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 160186/200000: episode: 838, duration: 0.757s, episode steps: 165, steps per second: 218, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 1.956881, mae: 32.233822, mean_q: -46.857226, mean_eps: 0.100000\n",
      " 160341/200000: episode: 839, duration: 0.719s, episode steps: 155, steps per second: 216, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.913719, mae: 32.135657, mean_q: -46.857935, mean_eps: 0.100000\n",
      " 160494/200000: episode: 840, duration: 0.695s, episode steps: 153, steps per second: 220, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.653905, mae: 29.823572, mean_q: -43.450000, mean_eps: 0.100000\n",
      " 160681/200000: episode: 841, duration: 0.846s, episode steps: 187, steps per second: 221, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.390289, mae: 29.449970, mean_q: -42.844075, mean_eps: 0.100000\n",
      " 160839/200000: episode: 842, duration: 0.705s, episode steps: 158, steps per second: 224, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.285452, mae: 30.014218, mean_q: -43.664832, mean_eps: 0.100000\n",
      " 160967/200000: episode: 843, duration: 0.571s, episode steps: 128, steps per second: 224, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 0.378681, mae: 29.395772, mean_q: -42.720466, mean_eps: 0.100000\n",
      " 161128/200000: episode: 844, duration: 0.718s, episode steps: 161, steps per second: 224, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.481643, mae: 28.665061, mean_q: -41.620214, mean_eps: 0.100000\n",
      " 161289/200000: episode: 845, duration: 0.722s, episode steps: 161, steps per second: 223, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.491240, mae: 28.048211, mean_q: -40.584018, mean_eps: 0.100000\n",
      " 161385/200000: episode: 846, duration: 0.465s, episode steps:  96, steps per second: 206, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.604880, mae: 27.977673, mean_q: -40.460707, mean_eps: 0.100000\n",
      " 161495/200000: episode: 847, duration: 0.497s, episode steps: 110, steps per second: 221, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.421067, mae: 27.874211, mean_q: -40.316925, mean_eps: 0.100000\n",
      " 161695/200000: episode: 848, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 0.429311, mae: 28.474375, mean_q: -41.250371, mean_eps: 0.100000\n",
      " 161851/200000: episode: 849, duration: 0.694s, episode steps: 156, steps per second: 225, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 2.488277, mae: 30.671302, mean_q: -44.562965, mean_eps: 0.100000\n",
      " 162051/200000: episode: 850, duration: 0.881s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.075 [0.000, 2.000],  loss: 2.169160, mae: 31.944969, mean_q: -46.601925, mean_eps: 0.100000\n",
      " 162251/200000: episode: 851, duration: 0.882s, episode steps: 200, steps per second: 227, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.100 [0.000, 2.000],  loss: 3.783914, mae: 35.993411, mean_q: -52.757544, mean_eps: 0.100000\n",
      " 162451/200000: episode: 852, duration: 0.897s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.490 [0.000, 2.000],  loss: 5.144830, mae: 39.423692, mean_q: -57.970522, mean_eps: 0.100000\n",
      " 162609/200000: episode: 853, duration: 0.709s, episode steps: 158, steps per second: 223, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.231944, mae: 40.042753, mean_q: -58.969962, mean_eps: 0.100000\n",
      " 162773/200000: episode: 854, duration: 0.736s, episode steps: 164, steps per second: 223, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 4.378018, mae: 36.929186, mean_q: -54.295065, mean_eps: 0.100000\n",
      " 162922/200000: episode: 855, duration: 0.673s, episode steps: 149, steps per second: 221, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 3.476021, mae: 35.815321, mean_q: -52.575567, mean_eps: 0.100000\n",
      " 163033/200000: episode: 856, duration: 0.505s, episode steps: 111, steps per second: 220, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 5.296893, mae: 33.705492, mean_q: -49.141259, mean_eps: 0.100000\n",
      " 163182/200000: episode: 857, duration: 0.678s, episode steps: 149, steps per second: 220, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 3.235880, mae: 32.423925, mean_q: -47.288218, mean_eps: 0.100000\n",
      " 163334/200000: episode: 858, duration: 0.699s, episode steps: 152, steps per second: 217, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.079 [0.000, 2.000],  loss: 1.434815, mae: 29.327509, mean_q: -42.566106, mean_eps: 0.100000\n",
      " 163485/200000: episode: 859, duration: 0.680s, episode steps: 151, steps per second: 222, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.529680, mae: 27.175768, mean_q: -39.257615, mean_eps: 0.100000\n",
      " 163639/200000: episode: 860, duration: 0.684s, episode steps: 154, steps per second: 225, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.961 [0.000, 2.000],  loss: 0.313805, mae: 27.075077, mean_q: -39.184405, mean_eps: 0.100000\n",
      " 163793/200000: episode: 861, duration: 0.678s, episode steps: 154, steps per second: 227, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.244332, mae: 27.150505, mean_q: -39.372301, mean_eps: 0.100000\n",
      " 163993/200000: episode: 862, duration: 0.887s, episode steps: 200, steps per second: 226, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 0.240326, mae: 28.280753, mean_q: -41.139598, mean_eps: 0.100000\n",
      " 164089/200000: episode: 863, duration: 0.428s, episode steps:  96, steps per second: 224, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.177654, mae: 28.980470, mean_q: -42.240123, mean_eps: 0.100000\n",
      " 164242/200000: episode: 864, duration: 0.678s, episode steps: 153, steps per second: 226, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.385565, mae: 29.771955, mean_q: -43.419890, mean_eps: 0.100000\n",
      " 164395/200000: episode: 865, duration: 0.682s, episode steps: 153, steps per second: 224, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.270978, mae: 30.202070, mean_q: -44.081280, mean_eps: 0.100000\n",
      " 164539/200000: episode: 866, duration: 0.642s, episode steps: 144, steps per second: 224, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.271629, mae: 29.957417, mean_q: -43.728242, mean_eps: 0.100000\n",
      " 164684/200000: episode: 867, duration: 0.653s, episode steps: 145, steps per second: 222, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.274788, mae: 29.938815, mean_q: -43.713470, mean_eps: 0.100000\n",
      " 164833/200000: episode: 868, duration: 0.670s, episode steps: 149, steps per second: 223, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.305214, mae: 30.406418, mean_q: -44.373553, mean_eps: 0.100000\n",
      " 164977/200000: episode: 869, duration: 0.634s, episode steps: 144, steps per second: 227, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.345560, mae: 29.513105, mean_q: -42.967789, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 165126/200000: episode: 870, duration: 0.659s, episode steps: 149, steps per second: 226, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.200656, mae: 28.902005, mean_q: -42.028061, mean_eps: 0.100000\n",
      " 165278/200000: episode: 871, duration: 0.672s, episode steps: 152, steps per second: 226, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.059 [0.000, 2.000],  loss: 0.168259, mae: 28.438094, mean_q: -41.323753, mean_eps: 0.100000\n",
      " 165426/200000: episode: 872, duration: 0.655s, episode steps: 148, steps per second: 226, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.188400, mae: 28.313243, mean_q: -41.151251, mean_eps: 0.100000\n",
      " 165576/200000: episode: 873, duration: 0.666s, episode steps: 150, steps per second: 225, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.158282, mae: 28.355850, mean_q: -41.267174, mean_eps: 0.100000\n",
      " 165724/200000: episode: 874, duration: 0.657s, episode steps: 148, steps per second: 225, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.130549, mae: 28.653604, mean_q: -41.786789, mean_eps: 0.100000\n",
      " 165876/200000: episode: 875, duration: 0.676s, episode steps: 152, steps per second: 225, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 0.126139, mae: 28.009274, mean_q: -40.796567, mean_eps: 0.100000\n",
      " 166045/200000: episode: 876, duration: 0.747s, episode steps: 169, steps per second: 226, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.116969, mae: 27.672305, mean_q: -40.280718, mean_eps: 0.100000\n",
      " 166161/200000: episode: 877, duration: 0.522s, episode steps: 116, steps per second: 222, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.106794, mae: 27.938924, mean_q: -40.712247, mean_eps: 0.100000\n",
      " 166305/200000: episode: 878, duration: 0.650s, episode steps: 144, steps per second: 221, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.951 [0.000, 2.000],  loss: 0.093517, mae: 28.658208, mean_q: -41.854550, mean_eps: 0.100000\n",
      " 166451/200000: episode: 879, duration: 0.656s, episode steps: 146, steps per second: 223, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.095435, mae: 29.107782, mean_q: -42.541563, mean_eps: 0.100000\n",
      " 166606/200000: episode: 880, duration: 0.757s, episode steps: 155, steps per second: 205, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.086679, mae: 28.932980, mean_q: -42.304544, mean_eps: 0.100000\n",
      " 166751/200000: episode: 881, duration: 0.710s, episode steps: 145, steps per second: 204, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.117802, mae: 28.617483, mean_q: -41.852017, mean_eps: 0.100000\n",
      " 166904/200000: episode: 882, duration: 0.736s, episode steps: 153, steps per second: 208, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.101835, mae: 29.033907, mean_q: -42.538482, mean_eps: 0.100000\n",
      " 167058/200000: episode: 883, duration: 0.740s, episode steps: 154, steps per second: 208, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.123 [0.000, 2.000],  loss: 0.088177, mae: 28.488342, mean_q: -41.675108, mean_eps: 0.100000\n",
      " 167204/200000: episode: 884, duration: 0.707s, episode steps: 146, steps per second: 207, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.123 [0.000, 2.000],  loss: 0.090711, mae: 28.305953, mean_q: -41.436440, mean_eps: 0.100000\n",
      " 167298/200000: episode: 885, duration: 0.448s, episode steps:  94, steps per second: 210, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.086350, mae: 27.664266, mean_q: -40.443862, mean_eps: 0.100000\n",
      " 167415/200000: episode: 886, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000],  loss: 0.164468, mae: 27.585908, mean_q: -40.288442, mean_eps: 0.100000\n",
      " 167566/200000: episode: 887, duration: 0.732s, episode steps: 151, steps per second: 206, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.130582, mae: 28.168839, mean_q: -41.136085, mean_eps: 0.100000\n",
      " 167684/200000: episode: 888, duration: 0.568s, episode steps: 118, steps per second: 208, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.170121, mae: 28.740747, mean_q: -41.971919, mean_eps: 0.100000\n",
      " 167862/200000: episode: 889, duration: 0.807s, episode steps: 178, steps per second: 221, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.091023, mae: 28.504031, mean_q: -41.618292, mean_eps: 0.100000\n",
      " 168017/200000: episode: 890, duration: 0.720s, episode steps: 155, steps per second: 215, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.230787, mae: 28.694243, mean_q: -41.891912, mean_eps: 0.100000\n",
      " 168173/200000: episode: 891, duration: 0.709s, episode steps: 156, steps per second: 220, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.149706, mae: 29.289802, mean_q: -42.824809, mean_eps: 0.100000\n",
      " 168265/200000: episode: 892, duration: 0.416s, episode steps:  92, steps per second: 221, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.177986, mae: 29.122127, mean_q: -42.543778, mean_eps: 0.100000\n",
      " 168418/200000: episode: 893, duration: 0.680s, episode steps: 153, steps per second: 225, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.197039, mae: 29.035501, mean_q: -42.425113, mean_eps: 0.100000\n",
      " 168607/200000: episode: 894, duration: 0.841s, episode steps: 189, steps per second: 225, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.213973, mae: 28.419526, mean_q: -41.520058, mean_eps: 0.100000\n",
      " 168760/200000: episode: 895, duration: 0.680s, episode steps: 153, steps per second: 225, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.170202, mae: 28.469795, mean_q: -41.597119, mean_eps: 0.100000\n",
      " 168916/200000: episode: 896, duration: 0.687s, episode steps: 156, steps per second: 227, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.168911, mae: 28.765413, mean_q: -42.077814, mean_eps: 0.100000\n",
      " 169088/200000: episode: 897, duration: 0.763s, episode steps: 172, steps per second: 225, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 0.157074, mae: 28.122271, mean_q: -41.095721, mean_eps: 0.100000\n",
      " 169207/200000: episode: 898, duration: 0.528s, episode steps: 119, steps per second: 226, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.191542, mae: 27.403164, mean_q: -39.997827, mean_eps: 0.100000\n",
      " 169322/200000: episode: 899, duration: 0.522s, episode steps: 115, steps per second: 220, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.168549, mae: 27.561425, mean_q: -40.259315, mean_eps: 0.100000\n",
      " 169436/200000: episode: 900, duration: 0.509s, episode steps: 114, steps per second: 224, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.092455, mae: 27.431332, mean_q: -40.075477, mean_eps: 0.100000\n",
      " 169549/200000: episode: 901, duration: 0.505s, episode steps: 113, steps per second: 224, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.064386, mae: 27.465083, mean_q: -40.151991, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 169718/200000: episode: 902, duration: 0.759s, episode steps: 169, steps per second: 223, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.059 [0.000, 2.000],  loss: 0.074506, mae: 27.432038, mean_q: -40.125416, mean_eps: 0.100000\n",
      " 169832/200000: episode: 903, duration: 0.537s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.077367, mae: 26.812632, mean_q: -39.195107, mean_eps: 0.100000\n",
      " 169948/200000: episode: 904, duration: 0.528s, episode steps: 116, steps per second: 220, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.094157, mae: 26.838453, mean_q: -39.284344, mean_eps: 0.100000\n",
      " 170062/200000: episode: 905, duration: 0.552s, episode steps: 114, steps per second: 206, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.095175, mae: 27.131157, mean_q: -39.749601, mean_eps: 0.100000\n",
      " 170209/200000: episode: 906, duration: 0.667s, episode steps: 147, steps per second: 220, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.092344, mae: 27.577796, mean_q: -40.391981, mean_eps: 0.100000\n",
      " 170373/200000: episode: 907, duration: 0.730s, episode steps: 164, steps per second: 225, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.092694, mae: 27.386485, mean_q: -40.072090, mean_eps: 0.100000\n",
      " 170489/200000: episode: 908, duration: 0.531s, episode steps: 116, steps per second: 218, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.120643, mae: 26.953747, mean_q: -39.379298, mean_eps: 0.100000\n",
      " 170642/200000: episode: 909, duration: 0.694s, episode steps: 153, steps per second: 221, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.059 [0.000, 2.000],  loss: 0.103242, mae: 27.880730, mean_q: -40.815699, mean_eps: 0.100000\n",
      " 170778/200000: episode: 910, duration: 0.612s, episode steps: 136, steps per second: 222, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.086300, mae: 28.090002, mean_q: -41.161834, mean_eps: 0.100000\n",
      " 170928/200000: episode: 911, duration: 0.684s, episode steps: 150, steps per second: 219, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.090978, mae: 28.197728, mean_q: -41.270541, mean_eps: 0.100000\n",
      " 171072/200000: episode: 912, duration: 0.655s, episode steps: 144, steps per second: 220, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.099078, mae: 28.034877, mean_q: -40.956670, mean_eps: 0.100000\n",
      " 171223/200000: episode: 913, duration: 0.670s, episode steps: 151, steps per second: 225, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.072776, mae: 27.525129, mean_q: -40.185912, mean_eps: 0.100000\n",
      " 171313/200000: episode: 914, duration: 0.402s, episode steps:  90, steps per second: 224, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.058981, mae: 28.038319, mean_q: -40.962713, mean_eps: 0.100000\n",
      " 171477/200000: episode: 915, duration: 0.726s, episode steps: 164, steps per second: 226, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.353881, mae: 28.322717, mean_q: -41.351290, mean_eps: 0.100000\n",
      " 171625/200000: episode: 916, duration: 0.658s, episode steps: 148, steps per second: 225, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.242185, mae: 27.676606, mean_q: -40.314541, mean_eps: 0.100000\n",
      " 171759/200000: episode: 917, duration: 0.607s, episode steps: 134, steps per second: 221, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.218336, mae: 28.374614, mean_q: -41.389764, mean_eps: 0.100000\n",
      " 171899/200000: episode: 918, duration: 0.626s, episode steps: 140, steps per second: 223, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.221478, mae: 28.413784, mean_q: -41.485771, mean_eps: 0.100000\n",
      " 172059/200000: episode: 919, duration: 0.722s, episode steps: 160, steps per second: 222, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.268240, mae: 28.061743, mean_q: -40.919255, mean_eps: 0.100000\n",
      " 172205/200000: episode: 920, duration: 0.644s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.154497, mae: 28.333530, mean_q: -41.359730, mean_eps: 0.100000\n",
      " 172346/200000: episode: 921, duration: 0.687s, episode steps: 141, steps per second: 205, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.273642, mae: 28.397759, mean_q: -41.447082, mean_eps: 0.100000\n",
      " 172447/200000: episode: 922, duration: 0.459s, episode steps: 101, steps per second: 220, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.091884, mae: 28.316515, mean_q: -41.345182, mean_eps: 0.100000\n",
      " 172585/200000: episode: 923, duration: 0.656s, episode steps: 138, steps per second: 210, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.504521, mae: 28.840544, mean_q: -42.121359, mean_eps: 0.100000\n",
      " 172723/200000: episode: 924, duration: 0.673s, episode steps: 138, steps per second: 205, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.051 [0.000, 2.000],  loss: 0.367886, mae: 29.236119, mean_q: -42.718264, mean_eps: 0.100000\n",
      " 172866/200000: episode: 925, duration: 0.670s, episode steps: 143, steps per second: 214, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.104124, mae: 29.500665, mean_q: -43.158989, mean_eps: 0.100000\n",
      " 172953/200000: episode: 926, duration: 0.394s, episode steps:  87, steps per second: 221, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.164822, mae: 29.704658, mean_q: -43.453300, mean_eps: 0.100000\n",
      " 173094/200000: episode: 927, duration: 0.649s, episode steps: 141, steps per second: 217, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.474888, mae: 29.806571, mean_q: -43.530652, mean_eps: 0.100000\n",
      " 173236/200000: episode: 928, duration: 0.630s, episode steps: 142, steps per second: 225, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.406334, mae: 30.265512, mean_q: -44.185129, mean_eps: 0.100000\n",
      " 173384/200000: episode: 929, duration: 0.653s, episode steps: 148, steps per second: 227, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.189112, mae: 29.747190, mean_q: -43.386427, mean_eps: 0.100000\n",
      " 173533/200000: episode: 930, duration: 0.654s, episode steps: 149, steps per second: 228, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.176590, mae: 29.666209, mean_q: -43.303319, mean_eps: 0.100000\n",
      " 173624/200000: episode: 931, duration: 0.406s, episode steps:  91, steps per second: 224, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.153468, mae: 29.000742, mean_q: -42.247617, mean_eps: 0.100000\n",
      " 173769/200000: episode: 932, duration: 0.648s, episode steps: 145, steps per second: 224, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.174870, mae: 29.479347, mean_q: -43.007993, mean_eps: 0.100000\n",
      " 173920/200000: episode: 933, duration: 0.665s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.219592, mae: 29.374030, mean_q: -42.804492, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 174073/200000: episode: 934, duration: 0.677s, episode steps: 153, steps per second: 226, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.179261, mae: 29.452013, mean_q: -42.919718, mean_eps: 0.100000\n",
      " 174221/200000: episode: 935, duration: 0.650s, episode steps: 148, steps per second: 228, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.147171, mae: 28.983271, mean_q: -42.201904, mean_eps: 0.100000\n",
      " 174366/200000: episode: 936, duration: 0.645s, episode steps: 145, steps per second: 225, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.062 [0.000, 2.000],  loss: 0.105151, mae: 29.231546, mean_q: -42.600711, mean_eps: 0.100000\n",
      " 174566/200000: episode: 937, duration: 0.898s, episode steps: 200, steps per second: 223, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.220 [0.000, 2.000],  loss: 0.128476, mae: 30.504051, mean_q: -44.581979, mean_eps: 0.100000\n",
      " 174722/200000: episode: 938, duration: 0.689s, episode steps: 156, steps per second: 227, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 3.624102, mae: 32.663523, mean_q: -47.716709, mean_eps: 0.100000\n",
      " 174867/200000: episode: 939, duration: 0.642s, episode steps: 145, steps per second: 226, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 2.460691, mae: 33.438651, mean_q: -49.103178, mean_eps: 0.100000\n",
      " 175019/200000: episode: 940, duration: 0.674s, episode steps: 152, steps per second: 226, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.039 [0.000, 2.000],  loss: 1.158365, mae: 33.066257, mean_q: -48.655194, mean_eps: 0.100000\n",
      " 175158/200000: episode: 941, duration: 0.614s, episode steps: 139, steps per second: 226, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.591521, mae: 33.107379, mean_q: -48.761932, mean_eps: 0.100000\n",
      " 175270/200000: episode: 942, duration: 0.500s, episode steps: 112, steps per second: 224, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 1.985508, mae: 33.001828, mean_q: -48.480339, mean_eps: 0.100000\n",
      " 175416/200000: episode: 943, duration: 0.643s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 3.596308, mae: 33.351975, mean_q: -48.945765, mean_eps: 0.100000\n",
      " 175592/200000: episode: 944, duration: 0.774s, episode steps: 176, steps per second: 228, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.088319, mae: 30.927077, mean_q: -45.484583, mean_eps: 0.100000\n",
      " 175736/200000: episode: 945, duration: 0.633s, episode steps: 144, steps per second: 227, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.093830, mae: 29.074765, mean_q: -42.658882, mean_eps: 0.100000\n",
      " 175892/200000: episode: 946, duration: 0.702s, episode steps: 156, steps per second: 222, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.066977, mae: 28.661485, mean_q: -42.049317, mean_eps: 0.100000\n",
      " 176026/200000: episode: 947, duration: 0.588s, episode steps: 134, steps per second: 228, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.124079, mae: 28.678597, mean_q: -42.056200, mean_eps: 0.100000\n",
      " 176141/200000: episode: 948, duration: 0.511s, episode steps: 115, steps per second: 225, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.083595, mae: 28.570117, mean_q: -41.855502, mean_eps: 0.100000\n",
      " 176286/200000: episode: 949, duration: 0.637s, episode steps: 145, steps per second: 228, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.093854, mae: 28.447986, mean_q: -41.699037, mean_eps: 0.100000\n",
      " 176442/200000: episode: 950, duration: 0.684s, episode steps: 156, steps per second: 228, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.064289, mae: 28.398620, mean_q: -41.598093, mean_eps: 0.100000\n",
      " 176589/200000: episode: 951, duration: 0.650s, episode steps: 147, steps per second: 226, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.059425, mae: 28.517567, mean_q: -41.756240, mean_eps: 0.100000\n",
      " 176742/200000: episode: 952, duration: 0.674s, episode steps: 153, steps per second: 227, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.057854, mae: 28.475033, mean_q: -41.680610, mean_eps: 0.100000\n",
      " 176888/200000: episode: 953, duration: 0.644s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.058630, mae: 28.311030, mean_q: -41.437328, mean_eps: 0.100000\n",
      " 177055/200000: episode: 954, duration: 0.732s, episode steps: 167, steps per second: 228, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.054471, mae: 28.508869, mean_q: -41.778356, mean_eps: 0.100000\n",
      " 177213/200000: episode: 955, duration: 0.694s, episode steps: 158, steps per second: 228, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.054468, mae: 27.750677, mean_q: -40.647622, mean_eps: 0.100000\n",
      " 177366/200000: episode: 956, duration: 0.672s, episode steps: 153, steps per second: 228, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000],  loss: 0.057964, mae: 27.923063, mean_q: -40.857894, mean_eps: 0.100000\n",
      " 177514/200000: episode: 957, duration: 0.653s, episode steps: 148, steps per second: 227, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.043481, mae: 28.578862, mean_q: -41.821678, mean_eps: 0.100000\n",
      " 177666/200000: episode: 958, duration: 0.672s, episode steps: 152, steps per second: 226, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.967 [0.000, 2.000],  loss: 0.045959, mae: 29.134942, mean_q: -42.656343, mean_eps: 0.100000\n",
      " 177776/200000: episode: 959, duration: 0.483s, episode steps: 110, steps per second: 228, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.035777, mae: 28.922003, mean_q: -42.298744, mean_eps: 0.100000\n",
      " 177921/200000: episode: 960, duration: 0.643s, episode steps: 145, steps per second: 226, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.169074, mae: 29.582879, mean_q: -43.309362, mean_eps: 0.100000\n",
      " 178068/200000: episode: 961, duration: 0.649s, episode steps: 147, steps per second: 226, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.294868, mae: 30.275871, mean_q: -44.308731, mean_eps: 0.100000\n",
      " 178161/200000: episode: 962, duration: 0.415s, episode steps:  93, steps per second: 224, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.251601, mae: 30.276413, mean_q: -44.272126, mean_eps: 0.100000\n",
      " 178310/200000: episode: 963, duration: 0.657s, episode steps: 149, steps per second: 227, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.240811, mae: 31.409183, mean_q: -45.996203, mean_eps: 0.100000\n",
      " 178456/200000: episode: 964, duration: 0.646s, episode steps: 146, steps per second: 226, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.310070, mae: 30.182794, mean_q: -44.141585, mean_eps: 0.100000\n",
      " 178622/200000: episode: 965, duration: 0.730s, episode steps: 166, steps per second: 228, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.409979, mae: 30.252043, mean_q: -44.249947, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 178772/200000: episode: 966, duration: 0.663s, episode steps: 150, steps per second: 226, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.382208, mae: 29.883478, mean_q: -43.690054, mean_eps: 0.100000\n",
      " 178923/200000: episode: 967, duration: 0.663s, episode steps: 151, steps per second: 228, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000],  loss: 0.217808, mae: 29.370324, mean_q: -42.896616, mean_eps: 0.100000\n",
      " 179077/200000: episode: 968, duration: 0.673s, episode steps: 154, steps per second: 229, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.961 [0.000, 2.000],  loss: 0.122221, mae: 29.331393, mean_q: -42.859323, mean_eps: 0.100000\n",
      " 179221/200000: episode: 969, duration: 0.631s, episode steps: 144, steps per second: 228, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.122151, mae: 29.741036, mean_q: -43.527297, mean_eps: 0.100000\n",
      " 179368/200000: episode: 970, duration: 0.650s, episode steps: 147, steps per second: 226, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000],  loss: 0.085829, mae: 29.229155, mean_q: -42.695221, mean_eps: 0.100000\n",
      " 179535/200000: episode: 971, duration: 0.736s, episode steps: 167, steps per second: 227, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000],  loss: 0.061265, mae: 29.292576, mean_q: -42.802755, mean_eps: 0.100000\n",
      " 179709/200000: episode: 972, duration: 0.766s, episode steps: 174, steps per second: 227, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.061539, mae: 29.268840, mean_q: -42.753086, mean_eps: 0.100000\n",
      " 179855/200000: episode: 973, duration: 0.643s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.058730, mae: 29.202743, mean_q: -42.666713, mean_eps: 0.100000\n",
      " 180010/200000: episode: 974, duration: 0.686s, episode steps: 155, steps per second: 226, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.929 [0.000, 2.000],  loss: 0.047756, mae: 29.582615, mean_q: -43.214547, mean_eps: 0.100000\n",
      " 180168/200000: episode: 975, duration: 0.709s, episode steps: 158, steps per second: 223, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.049145, mae: 29.535998, mean_q: -43.138388, mean_eps: 0.100000\n",
      " 180321/200000: episode: 976, duration: 0.687s, episode steps: 153, steps per second: 223, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.044377, mae: 29.195439, mean_q: -42.664265, mean_eps: 0.100000\n",
      " 180474/200000: episode: 977, duration: 0.680s, episode steps: 153, steps per second: 225, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.047676, mae: 29.370541, mean_q: -42.928611, mean_eps: 0.100000\n",
      " 180626/200000: episode: 978, duration: 0.670s, episode steps: 152, steps per second: 227, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.037604, mae: 29.730752, mean_q: -43.507265, mean_eps: 0.100000\n",
      " 180777/200000: episode: 979, duration: 0.666s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.057209, mae: 30.173499, mean_q: -44.124780, mean_eps: 0.100000\n",
      " 180924/200000: episode: 980, duration: 0.654s, episode steps: 147, steps per second: 225, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.034740, mae: 30.726313, mean_q: -44.993839, mean_eps: 0.100000\n",
      " 181068/200000: episode: 981, duration: 0.636s, episode steps: 144, steps per second: 227, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000],  loss: 0.042468, mae: 30.198159, mean_q: -44.187359, mean_eps: 0.100000\n",
      " 181210/200000: episode: 982, duration: 0.624s, episode steps: 142, steps per second: 227, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.063 [0.000, 2.000],  loss: 0.045466, mae: 30.853316, mean_q: -45.177041, mean_eps: 0.100000\n",
      " 181357/200000: episode: 983, duration: 0.664s, episode steps: 147, steps per second: 221, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.056016, mae: 30.477559, mean_q: -44.583375, mean_eps: 0.100000\n",
      " 181501/200000: episode: 984, duration: 0.637s, episode steps: 144, steps per second: 226, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.041684, mae: 30.526866, mean_q: -44.672401, mean_eps: 0.100000\n",
      " 181643/200000: episode: 985, duration: 0.629s, episode steps: 142, steps per second: 226, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.031268, mae: 30.573991, mean_q: -44.782195, mean_eps: 0.100000\n",
      " 181794/200000: episode: 986, duration: 0.669s, episode steps: 151, steps per second: 226, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.032240, mae: 30.644171, mean_q: -44.859523, mean_eps: 0.100000\n",
      " 181946/200000: episode: 987, duration: 0.666s, episode steps: 152, steps per second: 228, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.042419, mae: 30.872058, mean_q: -45.264476, mean_eps: 0.100000\n",
      " 182097/200000: episode: 988, duration: 0.664s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.054111, mae: 29.880151, mean_q: -43.751617, mean_eps: 0.100000\n",
      " 182240/200000: episode: 989, duration: 0.632s, episode steps: 143, steps per second: 226, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.047651, mae: 30.067665, mean_q: -44.051346, mean_eps: 0.100000\n",
      " 182391/200000: episode: 990, duration: 0.666s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.045874, mae: 30.090947, mean_q: -44.104832, mean_eps: 0.100000\n",
      " 182536/200000: episode: 991, duration: 0.642s, episode steps: 145, steps per second: 226, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.038686, mae: 30.422559, mean_q: -44.654756, mean_eps: 0.100000\n",
      " 182692/200000: episode: 992, duration: 0.680s, episode steps: 156, steps per second: 229, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.033793, mae: 29.980149, mean_q: -43.984777, mean_eps: 0.100000\n",
      " 182843/200000: episode: 993, duration: 0.686s, episode steps: 151, steps per second: 220, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.047219, mae: 29.912929, mean_q: -43.860228, mean_eps: 0.100000\n",
      " 183019/200000: episode: 994, duration: 0.773s, episode steps: 176, steps per second: 228, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000],  loss: 0.039516, mae: 30.299421, mean_q: -44.456475, mean_eps: 0.100000\n",
      " 183188/200000: episode: 995, duration: 0.753s, episode steps: 169, steps per second: 224, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.039436, mae: 31.003335, mean_q: -45.561687, mean_eps: 0.100000\n",
      " 183332/200000: episode: 996, duration: 0.637s, episode steps: 144, steps per second: 226, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.040241, mae: 31.385600, mean_q: -46.170617, mean_eps: 0.100000\n",
      " 183475/200000: episode: 997, duration: 0.637s, episode steps: 143, steps per second: 225, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.039934, mae: 31.032636, mean_q: -45.660232, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 183621/200000: episode: 998, duration: 0.643s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.959 [0.000, 2.000],  loss: 0.043886, mae: 31.554721, mean_q: -46.473610, mean_eps: 0.100000\n",
      " 183771/200000: episode: 999, duration: 0.668s, episode steps: 150, steps per second: 224, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.040085, mae: 31.522200, mean_q: -46.399299, mean_eps: 0.100000\n",
      " 183921/200000: episode: 1000, duration: 0.664s, episode steps: 150, steps per second: 226, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.042208, mae: 31.192744, mean_q: -45.880646, mean_eps: 0.100000\n",
      " 184061/200000: episode: 1001, duration: 0.639s, episode steps: 140, steps per second: 219, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.033269, mae: 30.916723, mean_q: -45.440167, mean_eps: 0.100000\n",
      " 184206/200000: episode: 1002, duration: 0.641s, episode steps: 145, steps per second: 226, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.042503, mae: 30.917877, mean_q: -45.395008, mean_eps: 0.100000\n",
      " 184358/200000: episode: 1003, duration: 0.670s, episode steps: 152, steps per second: 227, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.059 [0.000, 2.000],  loss: 0.041446, mae: 30.564660, mean_q: -44.843224, mean_eps: 0.100000\n",
      " 184512/200000: episode: 1004, duration: 0.679s, episode steps: 154, steps per second: 227, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.044283, mae: 30.203112, mean_q: -44.215324, mean_eps: 0.100000\n",
      " 184600/200000: episode: 1005, duration: 0.389s, episode steps:  88, steps per second: 226, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.042437, mae: 29.806711, mean_q: -43.669610, mean_eps: 0.100000\n",
      " 184747/200000: episode: 1006, duration: 0.644s, episode steps: 147, steps per second: 228, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.118364, mae: 30.419847, mean_q: -44.532353, mean_eps: 0.100000\n",
      " 184922/200000: episode: 1007, duration: 0.774s, episode steps: 175, steps per second: 226, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.204580, mae: 30.746567, mean_q: -44.908693, mean_eps: 0.100000\n",
      " 185111/200000: episode: 1008, duration: 0.834s, episode steps: 189, steps per second: 227, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000],  loss: 0.108045, mae: 30.722167, mean_q: -44.890119, mean_eps: 0.100000\n",
      " 185273/200000: episode: 1009, duration: 0.719s, episode steps: 162, steps per second: 225, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.209554, mae: 30.630317, mean_q: -44.685944, mean_eps: 0.100000\n",
      " 185430/200000: episode: 1010, duration: 0.693s, episode steps: 157, steps per second: 227, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.166933, mae: 30.455385, mean_q: -44.430935, mean_eps: 0.100000\n",
      " 185575/200000: episode: 1011, duration: 0.660s, episode steps: 145, steps per second: 220, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.146040, mae: 30.573004, mean_q: -44.572398, mean_eps: 0.100000\n",
      " 185726/200000: episode: 1012, duration: 0.665s, episode steps: 151, steps per second: 227, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.055276, mae: 30.598658, mean_q: -44.688415, mean_eps: 0.100000\n",
      " 185816/200000: episode: 1013, duration: 0.401s, episode steps:  90, steps per second: 224, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.057451, mae: 30.682015, mean_q: -44.873729, mean_eps: 0.100000\n",
      " 185962/200000: episode: 1014, duration: 0.649s, episode steps: 146, steps per second: 225, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.299204, mae: 31.181730, mean_q: -45.572824, mean_eps: 0.100000\n",
      " 186112/200000: episode: 1015, duration: 0.667s, episode steps: 150, steps per second: 225, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.180609, mae: 30.824299, mean_q: -45.020778, mean_eps: 0.100000\n",
      " 186205/200000: episode: 1016, duration: 0.416s, episode steps:  93, steps per second: 223, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.211073, mae: 30.589108, mean_q: -44.668735, mean_eps: 0.100000\n",
      " 186354/200000: episode: 1017, duration: 0.657s, episode steps: 149, steps per second: 227, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.283373, mae: 31.356758, mean_q: -45.913690, mean_eps: 0.100000\n",
      " 186528/200000: episode: 1018, duration: 0.761s, episode steps: 174, steps per second: 229, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.250928, mae: 31.069101, mean_q: -45.462123, mean_eps: 0.100000\n",
      " 186705/200000: episode: 1019, duration: 0.780s, episode steps: 177, steps per second: 227, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000],  loss: 0.200222, mae: 30.315119, mean_q: -44.271454, mean_eps: 0.100000\n",
      " 186855/200000: episode: 1020, duration: 0.654s, episode steps: 150, steps per second: 229, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.229402, mae: 29.876883, mean_q: -43.577073, mean_eps: 0.100000\n",
      " 187012/200000: episode: 1021, duration: 0.713s, episode steps: 157, steps per second: 220, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.113498, mae: 29.789015, mean_q: -43.409727, mean_eps: 0.100000\n",
      " 187168/200000: episode: 1022, duration: 0.690s, episode steps: 156, steps per second: 226, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.115224, mae: 29.916790, mean_q: -43.594783, mean_eps: 0.100000\n",
      " 187322/200000: episode: 1023, duration: 0.677s, episode steps: 154, steps per second: 228, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.062725, mae: 30.101222, mean_q: -43.970740, mean_eps: 0.100000\n",
      " 187470/200000: episode: 1024, duration: 0.651s, episode steps: 148, steps per second: 227, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.052262, mae: 30.238368, mean_q: -44.200718, mean_eps: 0.100000\n",
      " 187628/200000: episode: 1025, duration: 0.696s, episode steps: 158, steps per second: 227, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.060411, mae: 31.074847, mean_q: -45.459768, mean_eps: 0.100000\n",
      " 187775/200000: episode: 1026, duration: 0.649s, episode steps: 147, steps per second: 227, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.973 [0.000, 2.000],  loss: 0.063683, mae: 30.936081, mean_q: -45.289236, mean_eps: 0.100000\n",
      " 187928/200000: episode: 1027, duration: 0.676s, episode steps: 153, steps per second: 226, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.061197, mae: 31.041918, mean_q: -45.449142, mean_eps: 0.100000\n",
      " 188084/200000: episode: 1028, duration: 0.711s, episode steps: 156, steps per second: 220, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.052196, mae: 31.358677, mean_q: -45.990205, mean_eps: 0.100000\n",
      " 188233/200000: episode: 1029, duration: 0.656s, episode steps: 149, steps per second: 227, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.973 [0.000, 2.000],  loss: 0.056690, mae: 30.769492, mean_q: -45.083122, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 188403/200000: episode: 1030, duration: 0.745s, episode steps: 170, steps per second: 228, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.063225, mae: 30.526965, mean_q: -44.712420, mean_eps: 0.100000\n",
      " 188553/200000: episode: 1031, duration: 0.661s, episode steps: 150, steps per second: 227, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.049706, mae: 30.368525, mean_q: -44.499745, mean_eps: 0.100000\n",
      " 188696/200000: episode: 1032, duration: 0.631s, episode steps: 143, steps per second: 227, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.051802, mae: 30.379308, mean_q: -44.526408, mean_eps: 0.100000\n",
      " 188844/200000: episode: 1033, duration: 0.661s, episode steps: 148, steps per second: 224, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.042505, mae: 30.886587, mean_q: -45.295788, mean_eps: 0.100000\n",
      " 188985/200000: episode: 1034, duration: 0.619s, episode steps: 141, steps per second: 228, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.039951, mae: 31.049204, mean_q: -45.560907, mean_eps: 0.100000\n",
      " 189139/200000: episode: 1035, duration: 0.741s, episode steps: 154, steps per second: 208, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.045802, mae: 31.282445, mean_q: -45.914970, mean_eps: 0.100000\n",
      " 189295/200000: episode: 1036, duration: 0.714s, episode steps: 156, steps per second: 219, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.036351, mae: 30.842837, mean_q: -45.237209, mean_eps: 0.100000\n",
      " 189441/200000: episode: 1037, duration: 0.661s, episode steps: 146, steps per second: 221, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.039095, mae: 30.539932, mean_q: -44.721106, mean_eps: 0.100000\n",
      " 189582/200000: episode: 1038, duration: 0.636s, episode steps: 141, steps per second: 222, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.035100, mae: 30.624891, mean_q: -44.834109, mean_eps: 0.100000\n",
      " 189730/200000: episode: 1039, duration: 0.674s, episode steps: 148, steps per second: 220, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.042193, mae: 30.249537, mean_q: -44.246651, mean_eps: 0.100000\n",
      " 189834/200000: episode: 1040, duration: 0.475s, episode steps: 104, steps per second: 219, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.040164, mae: 29.604133, mean_q: -43.226293, mean_eps: 0.100000\n",
      " 189988/200000: episode: 1041, duration: 0.707s, episode steps: 154, steps per second: 218, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.285687, mae: 30.334491, mean_q: -44.301059, mean_eps: 0.100000\n",
      " 190140/200000: episode: 1042, duration: 0.697s, episode steps: 152, steps per second: 218, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.336881, mae: 30.574678, mean_q: -44.641227, mean_eps: 0.100000\n",
      " 190292/200000: episode: 1043, duration: 0.691s, episode steps: 152, steps per second: 220, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000],  loss: 0.336152, mae: 31.140492, mean_q: -45.507203, mean_eps: 0.100000\n",
      " 190443/200000: episode: 1044, duration: 0.689s, episode steps: 151, steps per second: 219, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.318174, mae: 30.860861, mean_q: -45.093877, mean_eps: 0.100000\n",
      " 190589/200000: episode: 1045, duration: 0.665s, episode steps: 146, steps per second: 220, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.280354, mae: 30.712332, mean_q: -44.823264, mean_eps: 0.100000\n",
      " 190680/200000: episode: 1046, duration: 0.417s, episode steps:  91, steps per second: 218, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.233300, mae: 30.723255, mean_q: -44.799010, mean_eps: 0.100000\n",
      " 190831/200000: episode: 1047, duration: 0.686s, episode steps: 151, steps per second: 220, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.894 [0.000, 2.000],  loss: 0.215544, mae: 31.433220, mean_q: -45.939015, mean_eps: 0.100000\n",
      " 190978/200000: episode: 1048, duration: 0.672s, episode steps: 147, steps per second: 219, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.109768, mae: 31.449739, mean_q: -45.997084, mean_eps: 0.100000\n",
      " 191123/200000: episode: 1049, duration: 0.653s, episode steps: 145, steps per second: 222, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.092395, mae: 31.386626, mean_q: -45.941754, mean_eps: 0.100000\n",
      " 191232/200000: episode: 1050, duration: 0.493s, episode steps: 109, steps per second: 221, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.157100, mae: 31.507826, mean_q: -46.079123, mean_eps: 0.100000\n",
      " 191391/200000: episode: 1051, duration: 0.722s, episode steps: 159, steps per second: 220, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.210300, mae: 31.553983, mean_q: -46.140252, mean_eps: 0.100000\n",
      " 191536/200000: episode: 1052, duration: 0.637s, episode steps: 145, steps per second: 228, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.159368, mae: 31.744587, mean_q: -46.497274, mean_eps: 0.100000\n",
      " 191683/200000: episode: 1053, duration: 0.658s, episode steps: 147, steps per second: 224, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.205309, mae: 32.026185, mean_q: -46.909069, mean_eps: 0.100000\n",
      " 191834/200000: episode: 1054, duration: 0.684s, episode steps: 151, steps per second: 221, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.198600, mae: 31.257950, mean_q: -45.748913, mean_eps: 0.100000\n",
      " 191980/200000: episode: 1055, duration: 0.647s, episode steps: 146, steps per second: 226, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.125743, mae: 31.595489, mean_q: -46.267484, mean_eps: 0.100000\n",
      " 192075/200000: episode: 1056, duration: 0.425s, episode steps:  95, steps per second: 224, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.085906, mae: 31.063116, mean_q: -45.431231, mean_eps: 0.100000\n",
      " 192220/200000: episode: 1057, duration: 0.659s, episode steps: 145, steps per second: 220, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.021 [0.000, 2.000],  loss: 0.148678, mae: 31.576423, mean_q: -46.158204, mean_eps: 0.100000\n",
      " 192381/200000: episode: 1058, duration: 0.725s, episode steps: 161, steps per second: 222, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.104498, mae: 31.086518, mean_q: -45.431631, mean_eps: 0.100000\n",
      " 192530/200000: episode: 1059, duration: 0.655s, episode steps: 149, steps per second: 227, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.074609, mae: 31.134350, mean_q: -45.579765, mean_eps: 0.100000\n",
      " 192680/200000: episode: 1060, duration: 0.679s, episode steps: 150, steps per second: 221, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.106350, mae: 31.351107, mean_q: -45.951350, mean_eps: 0.100000\n",
      " 192835/200000: episode: 1061, duration: 0.691s, episode steps: 155, steps per second: 224, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.100737, mae: 31.214883, mean_q: -45.737673, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 193001/200000: episode: 1062, duration: 0.736s, episode steps: 166, steps per second: 225, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.095565, mae: 30.938497, mean_q: -45.292007, mean_eps: 0.100000\n",
      " 193149/200000: episode: 1063, duration: 0.666s, episode steps: 148, steps per second: 222, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.076178, mae: 30.714318, mean_q: -44.942635, mean_eps: 0.100000\n",
      " 193294/200000: episode: 1064, duration: 0.648s, episode steps: 145, steps per second: 224, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.021 [0.000, 2.000],  loss: 0.048058, mae: 30.535791, mean_q: -44.757166, mean_eps: 0.100000\n",
      " 193436/200000: episode: 1065, duration: 0.629s, episode steps: 142, steps per second: 226, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.042960, mae: 30.275323, mean_q: -44.345600, mean_eps: 0.100000\n",
      " 193584/200000: episode: 1066, duration: 0.673s, episode steps: 148, steps per second: 220, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.032236, mae: 30.385222, mean_q: -44.532295, mean_eps: 0.100000\n",
      " 193771/200000: episode: 1067, duration: 0.831s, episode steps: 187, steps per second: 225, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.033687, mae: 30.991506, mean_q: -45.488773, mean_eps: 0.100000\n",
      " 193937/200000: episode: 1068, duration: 0.742s, episode steps: 166, steps per second: 224, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.028967, mae: 30.260456, mean_q: -44.434828, mean_eps: 0.100000\n",
      " 194079/200000: episode: 1069, duration: 0.643s, episode steps: 142, steps per second: 221, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.029981, mae: 29.992942, mean_q: -44.061842, mean_eps: 0.100000\n",
      " 194253/200000: episode: 1070, duration: 0.776s, episode steps: 174, steps per second: 224, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.023 [0.000, 2.000],  loss: 0.024104, mae: 30.257535, mean_q: -44.466369, mean_eps: 0.100000\n",
      " 194401/200000: episode: 1071, duration: 0.666s, episode steps: 148, steps per second: 222, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 0.024005, mae: 30.572395, mean_q: -44.926964, mean_eps: 0.100000\n",
      " 194554/200000: episode: 1072, duration: 0.688s, episode steps: 153, steps per second: 222, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.023340, mae: 30.583937, mean_q: -44.940292, mean_eps: 0.100000\n",
      " 194700/200000: episode: 1073, duration: 0.644s, episode steps: 146, steps per second: 227, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.027 [0.000, 2.000],  loss: 0.022281, mae: 30.002828, mean_q: -44.050343, mean_eps: 0.100000\n",
      " 194854/200000: episode: 1074, duration: 0.689s, episode steps: 154, steps per second: 223, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000],  loss: 0.027178, mae: 29.716187, mean_q: -43.621478, mean_eps: 0.100000\n",
      " 194999/200000: episode: 1075, duration: 0.659s, episode steps: 145, steps per second: 220, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.025203, mae: 29.760899, mean_q: -43.658180, mean_eps: 0.100000\n",
      " 195147/200000: episode: 1076, duration: 0.659s, episode steps: 148, steps per second: 225, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.022063, mae: 29.854355, mean_q: -43.816621, mean_eps: 0.100000\n",
      " 195292/200000: episode: 1077, duration: 0.647s, episode steps: 145, steps per second: 224, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.028185, mae: 29.708477, mean_q: -43.620957, mean_eps: 0.100000\n",
      " 195446/200000: episode: 1078, duration: 0.704s, episode steps: 154, steps per second: 219, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 0.034455, mae: 29.977349, mean_q: -43.953370, mean_eps: 0.100000\n",
      " 195597/200000: episode: 1079, duration: 0.668s, episode steps: 151, steps per second: 226, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.032092, mae: 30.573927, mean_q: -44.858148, mean_eps: 0.100000\n",
      " 195750/200000: episode: 1080, duration: 0.702s, episode steps: 153, steps per second: 218, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.032965, mae: 30.700572, mean_q: -45.043428, mean_eps: 0.100000\n",
      " 195902/200000: episode: 1081, duration: 0.685s, episode steps: 152, steps per second: 222, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000],  loss: 0.028870, mae: 31.033585, mean_q: -45.571265, mean_eps: 0.100000\n",
      " 196055/200000: episode: 1082, duration: 0.670s, episode steps: 153, steps per second: 228, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000],  loss: 0.029423, mae: 31.021504, mean_q: -45.589092, mean_eps: 0.100000\n",
      " 196200/200000: episode: 1083, duration: 0.651s, episode steps: 145, steps per second: 223, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000],  loss: 0.022241, mae: 31.484727, mean_q: -46.334348, mean_eps: 0.100000\n",
      " 196348/200000: episode: 1084, duration: 0.671s, episode steps: 148, steps per second: 221, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.919 [0.000, 2.000],  loss: 0.023258, mae: 31.567705, mean_q: -46.476424, mean_eps: 0.100000\n",
      " 196491/200000: episode: 1085, duration: 0.635s, episode steps: 143, steps per second: 225, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.022042, mae: 31.252949, mean_q: -45.972327, mean_eps: 0.100000\n",
      " 196632/200000: episode: 1086, duration: 0.655s, episode steps: 141, steps per second: 215, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.901 [0.000, 2.000],  loss: 0.024755, mae: 31.178208, mean_q: -45.884749, mean_eps: 0.100000\n",
      " 196779/200000: episode: 1087, duration: 0.660s, episode steps: 147, steps per second: 223, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.029833, mae: 31.240942, mean_q: -45.919533, mean_eps: 0.100000\n",
      " 196924/200000: episode: 1088, duration: 0.645s, episode steps: 145, steps per second: 225, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.025151, mae: 31.132064, mean_q: -45.757608, mean_eps: 0.100000\n",
      " 197033/200000: episode: 1089, duration: 0.493s, episode steps: 109, steps per second: 221, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.030619, mae: 30.771905, mean_q: -45.173293, mean_eps: 0.100000\n",
      " 197186/200000: episode: 1090, duration: 0.694s, episode steps: 153, steps per second: 220, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.029158, mae: 30.546395, mean_q: -44.784466, mean_eps: 0.100000\n",
      " 197337/200000: episode: 1091, duration: 0.668s, episode steps: 151, steps per second: 226, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.038610, mae: 30.047598, mean_q: -44.047841, mean_eps: 0.100000\n",
      " 197483/200000: episode: 1092, duration: 0.654s, episode steps: 146, steps per second: 223, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.037024, mae: 29.926361, mean_q: -43.854662, mean_eps: 0.100000\n",
      " 197630/200000: episode: 1093, duration: 0.669s, episode steps: 147, steps per second: 220, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.034807, mae: 29.933424, mean_q: -43.838315, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 197786/200000: episode: 1094, duration: 0.697s, episode steps: 156, steps per second: 224, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000],  loss: 0.030907, mae: 30.048374, mean_q: -44.014176, mean_eps: 0.100000\n",
      " 197925/200000: episode: 1095, duration: 0.618s, episode steps: 139, steps per second: 225, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.024404, mae: 30.708426, mean_q: -45.061633, mean_eps: 0.100000\n",
      " 198082/200000: episode: 1096, duration: 0.702s, episode steps: 157, steps per second: 224, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.025025, mae: 30.561782, mean_q: -44.870010, mean_eps: 0.100000\n",
      " 198230/200000: episode: 1097, duration: 0.663s, episode steps: 148, steps per second: 223, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000],  loss: 0.018962, mae: 30.508352, mean_q: -44.773085, mean_eps: 0.100000\n",
      " 198430/200000: episode: 1098, duration: 0.888s, episode steps: 200, steps per second: 225, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.020932, mae: 29.994803, mean_q: -44.049334, mean_eps: 0.100000\n",
      " 198572/200000: episode: 1099, duration: 0.648s, episode steps: 142, steps per second: 219, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.021 [0.000, 2.000],  loss: 0.030375, mae: 29.692365, mean_q: -43.625320, mean_eps: 0.100000\n",
      " 198683/200000: episode: 1100, duration: 0.493s, episode steps: 111, steps per second: 225, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.029360, mae: 29.865369, mean_q: -43.893272, mean_eps: 0.100000\n",
      " 198824/200000: episode: 1101, duration: 0.626s, episode steps: 141, steps per second: 225, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.053823, mae: 29.584881, mean_q: -43.433952, mean_eps: 0.100000\n",
      " 198960/200000: episode: 1102, duration: 0.617s, episode steps: 136, steps per second: 220, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.059173, mae: 28.932251, mean_q: -42.431182, mean_eps: 0.100000\n",
      " 199104/200000: episode: 1103, duration: 0.637s, episode steps: 144, steps per second: 226, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.218711, mae: 29.650752, mean_q: -43.518234, mean_eps: 0.100000\n",
      " 199253/200000: episode: 1104, duration: 0.675s, episode steps: 149, steps per second: 221, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.326201, mae: 29.509567, mean_q: -43.216622, mean_eps: 0.100000\n",
      " 199354/200000: episode: 1105, duration: 0.460s, episode steps: 101, steps per second: 220, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.125598, mae: 29.673039, mean_q: -43.451243, mean_eps: 0.100000\n",
      " 199509/200000: episode: 1106, duration: 0.698s, episode steps: 155, steps per second: 222, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.446312, mae: 30.236998, mean_q: -44.326055, mean_eps: 0.100000\n",
      " 199656/200000: episode: 1107, duration: 0.660s, episode steps: 147, steps per second: 223, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.399743, mae: 29.891183, mean_q: -43.809899, mean_eps: 0.100000\n",
      " 199822/200000: episode: 1108, duration: 0.755s, episode steps: 166, steps per second: 220, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.287063, mae: 29.472800, mean_q: -43.137837, mean_eps: 0.100000\n",
      " 199970/200000: episode: 1109, duration: 0.655s, episode steps: 148, steps per second: 226, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.309360, mae: 29.153057, mean_q: -42.506763, mean_eps: 0.100000\n",
      "done, took 898.495 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the build_agent function to traing the agent\n",
    "agent = build_agent(model, actions)                      # used build_agent to setup a dqn model\n",
    "\n",
    "agent.compile(Adam(learning_rate = 1e-4),                # use Adam optimisation with learning rate 0.0001\n",
    "            metrics = ['mae']                            # use mean absolute error to evaluate the metric\n",
    "           )      \n",
    "\n",
    "history = agent.fit(env, \n",
    "        nb_steps = 200000,                               # number of timesteps \n",
    "        visualize = False,                               # visualize during the training\n",
    "        verbose = 2                                      # how to show the training output\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent's Performance after 200k timesteps with 1e-4 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -175.000, steps: 175\n",
      "Episode 2: reward: -148.000, steps: 148\n",
      "Episode 3: reward: -108.000, steps: 108\n",
      "Episode 4: reward: -105.000, steps: 105\n",
      "Episode 5: reward: -121.000, steps: 121\n",
      "-131.4\n"
     ]
    }
   ],
   "source": [
    "scores = agent.test(env,                             # pass our environment into the DQNagent.test agent\n",
    "                  nb_episodes = 5,                    # number of episodes\n",
    "                  visualize = True                     # set True if we want to visualize it\n",
    "                 )\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, ***The car reaches the flag on the right with 131.4 timesteps spend on average***, but such performance is not better than the simple policy without any learning.\n",
    "- ***loss: 0.309*** in the last episode means that we are still able to improve the performance. \n",
    "\n",
    "Therefore let's try to train the second Agent with learing rate 1e-3 for 500k training steps to see if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Train the Second Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "    200/500000: episode: 1, duration: 0.202s, episode steps: 200, steps per second: 991, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    400/500000: episode: 2, duration: 0.096s, episode steps: 200, steps per second: 2082, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    600/500000: episode: 3, duration: 0.098s, episode steps: 200, steps per second: 2036, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    800/500000: episode: 4, duration: 0.094s, episode steps: 200, steps per second: 2117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1000/500000: episode: 5, duration: 0.093s, episode steps: 200, steps per second: 2142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1200/500000: episode: 6, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.062921, mae: 1.320444, mean_q: 2.046916, mean_eps: 0.901000\n",
      "   1400/500000: episode: 7, duration: 0.928s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.002553, mae: 0.368392, mean_q: -0.263486, mean_eps: 0.883045\n",
      "   1600/500000: episode: 8, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.005025, mae: 1.437446, mean_q: -2.115613, mean_eps: 0.865045\n",
      "   1800/500000: episode: 9, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.020013, mae: 2.627759, mean_q: -3.865960, mean_eps: 0.847045\n",
      "   2000/500000: episode: 10, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.064955, mae: 3.791435, mean_q: -5.565804, mean_eps: 0.829045\n",
      "   2200/500000: episode: 11, duration: 0.928s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.096533, mae: 4.919347, mean_q: -7.239074, mean_eps: 0.811045\n",
      "   2400/500000: episode: 12, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.184324, mae: 6.036823, mean_q: -8.854133, mean_eps: 0.793045\n",
      "   2600/500000: episode: 13, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.250849, mae: 7.120420, mean_q: -10.429071, mean_eps: 0.775045\n",
      "   2800/500000: episode: 14, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.252668, mae: 8.115528, mean_q: -11.894385, mean_eps: 0.757045\n",
      "   3000/500000: episode: 15, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.262081, mae: 9.060277, mean_q: -13.323318, mean_eps: 0.739045\n",
      "   3200/500000: episode: 16, duration: 0.979s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.443747, mae: 9.995867, mean_q: -14.679176, mean_eps: 0.721045\n",
      "   3400/500000: episode: 17, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.519788, mae: 10.884048, mean_q: -15.939830, mean_eps: 0.703045\n",
      "   3600/500000: episode: 18, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.611539, mae: 11.751755, mean_q: -17.300535, mean_eps: 0.685045\n",
      "   3800/500000: episode: 19, duration: 0.980s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.734891, mae: 12.668134, mean_q: -18.635752, mean_eps: 0.667045\n",
      "   4000/500000: episode: 20, duration: 1.012s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.954486, mae: 13.454615, mean_q: -19.759274, mean_eps: 0.649045\n",
      "   4200/500000: episode: 21, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 1.216438, mae: 14.231635, mean_q: -20.849676, mean_eps: 0.631045\n",
      "   4400/500000: episode: 22, duration: 1.025s, episode steps: 200, steps per second: 195, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 1.064799, mae: 15.002351, mean_q: -22.076332, mean_eps: 0.613045\n",
      "   4600/500000: episode: 23, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 1.334817, mae: 15.746666, mean_q: -23.190878, mean_eps: 0.595045\n",
      "   4800/500000: episode: 24, duration: 1.010s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 1.530653, mae: 16.512006, mean_q: -24.307542, mean_eps: 0.577045\n",
      "   5000/500000: episode: 25, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 1.898860, mae: 17.253470, mean_q: -25.423329, mean_eps: 0.559045\n",
      "   5200/500000: episode: 26, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.746607, mae: 17.867936, mean_q: -26.335596, mean_eps: 0.541045\n",
      "   5400/500000: episode: 27, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 1.450395, mae: 18.342981, mean_q: -26.962120, mean_eps: 0.523045\n",
      "   5600/500000: episode: 28, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.450790, mae: 18.790144, mean_q: -27.717220, mean_eps: 0.505045\n",
      "   5800/500000: episode: 29, duration: 1.043s, episode steps: 200, steps per second: 192, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 1.578422, mae: 19.326609, mean_q: -28.479861, mean_eps: 0.487045\n",
      "   6000/500000: episode: 30, duration: 0.969s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 1.395435, mae: 19.960512, mean_q: -29.506351, mean_eps: 0.469045\n",
      "   6200/500000: episode: 31, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.123634, mae: 20.635663, mean_q: -30.449519, mean_eps: 0.451045\n",
      "   6400/500000: episode: 32, duration: 0.987s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.777778, mae: 21.387348, mean_q: -31.432980, mean_eps: 0.433045\n",
      "   6600/500000: episode: 33, duration: 0.964s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 2.522126, mae: 21.941399, mean_q: -32.361484, mean_eps: 0.415045\n",
      "   6800/500000: episode: 34, duration: 1.017s, episode steps: 200, steps per second: 197, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 2.223922, mae: 22.399773, mean_q: -33.048891, mean_eps: 0.397045\n",
      "   7000/500000: episode: 35, duration: 0.971s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.949058, mae: 23.024225, mean_q: -33.991592, mean_eps: 0.379045\n",
      "   7200/500000: episode: 36, duration: 0.950s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.695 [0.000, 2.000],  loss: 2.685399, mae: 23.595752, mean_q: -34.780453, mean_eps: 0.361045\n",
      "   7400/500000: episode: 37, duration: 0.964s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.455 [0.000, 2.000],  loss: 3.302608, mae: 23.937210, mean_q: -35.156371, mean_eps: 0.343045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7600/500000: episode: 38, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.535 [0.000, 2.000],  loss: 2.653542, mae: 24.231779, mean_q: -35.767329, mean_eps: 0.325045\n",
      "   7800/500000: episode: 39, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 3.533686, mae: 24.787217, mean_q: -36.521745, mean_eps: 0.307045\n",
      "   8000/500000: episode: 40, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.540 [0.000, 2.000],  loss: 3.935366, mae: 25.031509, mean_q: -36.828702, mean_eps: 0.289045\n",
      "   8200/500000: episode: 41, duration: 0.973s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 3.315998, mae: 25.165562, mean_q: -37.152200, mean_eps: 0.271045\n",
      "   8400/500000: episode: 42, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 3.228343, mae: 25.710395, mean_q: -38.004545, mean_eps: 0.253045\n",
      "   8600/500000: episode: 43, duration: 0.978s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.571109, mae: 26.212186, mean_q: -38.751183, mean_eps: 0.235045\n",
      "   8800/500000: episode: 44, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.670 [0.000, 2.000],  loss: 3.324663, mae: 26.526303, mean_q: -39.137142, mean_eps: 0.217045\n",
      "   9000/500000: episode: 45, duration: 0.962s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.365 [0.000, 2.000],  loss: 3.334210, mae: 26.862263, mean_q: -39.538782, mean_eps: 0.199045\n",
      "   9200/500000: episode: 46, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.405 [0.000, 2.000],  loss: 3.562009, mae: 27.280520, mean_q: -40.114916, mean_eps: 0.181045\n",
      "   9400/500000: episode: 47, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.550 [0.000, 2.000],  loss: 3.653454, mae: 27.597360, mean_q: -40.523987, mean_eps: 0.163045\n",
      "   9600/500000: episode: 48, duration: 0.962s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.320 [0.000, 2.000],  loss: 3.107437, mae: 28.062505, mean_q: -41.301331, mean_eps: 0.145045\n",
      "   9800/500000: episode: 49, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.345 [0.000, 2.000],  loss: 4.221082, mae: 28.614617, mean_q: -42.085847, mean_eps: 0.127045\n",
      "  10000/500000: episode: 50, duration: 0.968s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000],  loss: 5.054101, mae: 29.027175, mean_q: -42.687079, mean_eps: 0.109045\n",
      "  10200/500000: episode: 51, duration: 1.007s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.710 [0.000, 2.000],  loss: 4.054601, mae: 29.265893, mean_q: -43.217712, mean_eps: 0.100000\n",
      "  10400/500000: episode: 52, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 5.635206, mae: 29.545507, mean_q: -43.505884, mean_eps: 0.100000\n",
      "  10600/500000: episode: 53, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.390103, mae: 29.728407, mean_q: -43.725134, mean_eps: 0.100000\n",
      "  10800/500000: episode: 54, duration: 0.997s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.630 [0.000, 2.000],  loss: 3.059915, mae: 30.143516, mean_q: -44.503936, mean_eps: 0.100000\n",
      "  11000/500000: episode: 55, duration: 1.068s, episode steps: 200, steps per second: 187, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.670 [0.000, 2.000],  loss: 3.955114, mae: 30.503576, mean_q: -45.006970, mean_eps: 0.100000\n",
      "  11200/500000: episode: 56, duration: 0.979s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 5.454831, mae: 30.876414, mean_q: -45.421517, mean_eps: 0.100000\n",
      "  11400/500000: episode: 57, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000],  loss: 4.217298, mae: 31.092113, mean_q: -45.861959, mean_eps: 0.100000\n",
      "  11600/500000: episode: 58, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.420 [0.000, 2.000],  loss: 4.915726, mae: 31.246544, mean_q: -46.028705, mean_eps: 0.100000\n",
      "  11800/500000: episode: 59, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.220 [0.000, 2.000],  loss: 6.899922, mae: 31.395130, mean_q: -46.080991, mean_eps: 0.100000\n",
      "  12000/500000: episode: 60, duration: 0.946s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.325 [0.000, 2.000],  loss: 6.180174, mae: 31.451616, mean_q: -46.274812, mean_eps: 0.100000\n",
      "  12200/500000: episode: 61, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.365 [0.000, 2.000],  loss: 6.848506, mae: 31.573114, mean_q: -46.327699, mean_eps: 0.100000\n",
      "  12400/500000: episode: 62, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.265 [0.000, 2.000],  loss: 5.528557, mae: 31.836472, mean_q: -46.796054, mean_eps: 0.100000\n",
      "  12600/500000: episode: 63, duration: 0.937s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.525 [0.000, 2.000],  loss: 7.135586, mae: 31.984251, mean_q: -46.939398, mean_eps: 0.100000\n",
      "  12800/500000: episode: 64, duration: 0.946s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.640 [0.000, 2.000],  loss: 6.463280, mae: 31.997050, mean_q: -46.955759, mean_eps: 0.100000\n",
      "  13000/500000: episode: 65, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.550 [0.000, 2.000],  loss: 5.707757, mae: 32.038278, mean_q: -47.184681, mean_eps: 0.100000\n",
      "  13200/500000: episode: 66, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 4.741372, mae: 32.318934, mean_q: -47.804071, mean_eps: 0.100000\n",
      "  13400/500000: episode: 67, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 4.853608, mae: 32.608701, mean_q: -48.195139, mean_eps: 0.100000\n",
      "  13600/500000: episode: 68, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 4.797318, mae: 32.908163, mean_q: -48.542197, mean_eps: 0.100000\n",
      "  13800/500000: episode: 69, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000],  loss: 5.293053, mae: 33.179971, mean_q: -48.958512, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14000/500000: episode: 70, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.560 [0.000, 2.000],  loss: 6.958573, mae: 33.444235, mean_q: -49.198969, mean_eps: 0.100000\n",
      "  14200/500000: episode: 71, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000],  loss: 5.677504, mae: 33.668473, mean_q: -49.742187, mean_eps: 0.100000\n",
      "  14400/500000: episode: 72, duration: 0.981s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.600 [0.000, 2.000],  loss: 4.082977, mae: 33.934174, mean_q: -50.185240, mean_eps: 0.100000\n",
      "  14600/500000: episode: 73, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.440 [0.000, 2.000],  loss: 6.298755, mae: 34.181815, mean_q: -50.401690, mean_eps: 0.100000\n",
      "  14800/500000: episode: 74, duration: 0.981s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.450 [0.000, 2.000],  loss: 4.947290, mae: 34.361123, mean_q: -50.710504, mean_eps: 0.100000\n",
      "  15000/500000: episode: 75, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.560 [0.000, 2.000],  loss: 6.007210, mae: 34.499488, mean_q: -50.889872, mean_eps: 0.100000\n",
      "  15200/500000: episode: 76, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.595 [0.000, 2.000],  loss: 6.429256, mae: 34.652499, mean_q: -51.125285, mean_eps: 0.100000\n",
      "  15400/500000: episode: 77, duration: 0.977s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 5.618931, mae: 34.218963, mean_q: -50.455344, mean_eps: 0.100000\n",
      "  15600/500000: episode: 78, duration: 0.963s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 6.538560, mae: 33.995884, mean_q: -50.229575, mean_eps: 0.100000\n",
      "  15800/500000: episode: 79, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 6.393450, mae: 33.849448, mean_q: -49.956981, mean_eps: 0.100000\n",
      "  16000/500000: episode: 80, duration: 0.968s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 8.171654, mae: 33.718829, mean_q: -49.809647, mean_eps: 0.100000\n",
      "  16200/500000: episode: 81, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 5.974237, mae: 33.727907, mean_q: -49.787727, mean_eps: 0.100000\n",
      "  16400/500000: episode: 82, duration: 1.010s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.243991, mae: 33.795226, mean_q: -49.769831, mean_eps: 0.100000\n",
      "  16600/500000: episode: 83, duration: 0.982s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 4.726185, mae: 33.892391, mean_q: -49.950211, mean_eps: 0.100000\n",
      "  16800/500000: episode: 84, duration: 0.988s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 7.195276, mae: 33.840683, mean_q: -49.509984, mean_eps: 0.100000\n",
      "  17000/500000: episode: 85, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 4.762885, mae: 33.812163, mean_q: -49.626859, mean_eps: 0.100000\n",
      "  17200/500000: episode: 86, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 2.000],  loss: 6.251698, mae: 34.016609, mean_q: -49.793919, mean_eps: 0.100000\n",
      "  17400/500000: episode: 87, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 5.060495, mae: 34.002049, mean_q: -49.874356, mean_eps: 0.100000\n",
      "  17600/500000: episode: 88, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 5.145199, mae: 34.442541, mean_q: -50.583305, mean_eps: 0.100000\n",
      "  17800/500000: episode: 89, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.755081, mae: 34.710934, mean_q: -51.093740, mean_eps: 0.100000\n",
      "  18000/500000: episode: 90, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 5.336713, mae: 35.174234, mean_q: -51.828397, mean_eps: 0.100000\n",
      "  18200/500000: episode: 91, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.783202, mae: 35.355129, mean_q: -52.114824, mean_eps: 0.100000\n",
      "  18400/500000: episode: 92, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 5.927852, mae: 35.458226, mean_q: -52.252788, mean_eps: 0.100000\n",
      "  18600/500000: episode: 93, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.485 [0.000, 2.000],  loss: 7.203272, mae: 35.517224, mean_q: -52.152778, mean_eps: 0.100000\n",
      "  18800/500000: episode: 94, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 7.164715, mae: 35.530579, mean_q: -52.102199, mean_eps: 0.100000\n",
      "  19000/500000: episode: 95, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 7.510577, mae: 35.655428, mean_q: -52.389336, mean_eps: 0.100000\n",
      "  19200/500000: episode: 96, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.445 [0.000, 2.000],  loss: 6.894643, mae: 35.738976, mean_q: -52.551940, mean_eps: 0.100000\n",
      "  19400/500000: episode: 97, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.134196, mae: 35.836715, mean_q: -52.714323, mean_eps: 0.100000\n",
      "  19600/500000: episode: 98, duration: 0.935s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 6.092403, mae: 36.068718, mean_q: -53.160739, mean_eps: 0.100000\n",
      "  19800/500000: episode: 99, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.286387, mae: 36.185888, mean_q: -53.416931, mean_eps: 0.100000\n",
      "  20000/500000: episode: 100, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 5.614543, mae: 36.287325, mean_q: -53.542488, mean_eps: 0.100000\n",
      "  20200/500000: episode: 101, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.625 [0.000, 2.000],  loss: 5.824887, mae: 36.369337, mean_q: -53.682329, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20400/500000: episode: 102, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 6.810746, mae: 36.544037, mean_q: -53.955815, mean_eps: 0.100000\n",
      "  20600/500000: episode: 103, duration: 0.941s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.620 [0.000, 2.000],  loss: 7.144424, mae: 36.560036, mean_q: -53.941327, mean_eps: 0.100000\n",
      "  20800/500000: episode: 104, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.740463, mae: 36.595586, mean_q: -53.914429, mean_eps: 0.100000\n",
      "  21000/500000: episode: 105, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 8.117012, mae: 36.545831, mean_q: -53.784770, mean_eps: 0.100000\n",
      "  21200/500000: episode: 106, duration: 0.963s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 6.665258, mae: 36.289516, mean_q: -53.371849, mean_eps: 0.100000\n",
      "  21400/500000: episode: 107, duration: 0.971s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 6.360220, mae: 36.088030, mean_q: -53.071184, mean_eps: 0.100000\n",
      "  21600/500000: episode: 108, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.200 [0.000, 2.000],  loss: 6.753659, mae: 36.142671, mean_q: -53.217747, mean_eps: 0.100000\n",
      "  21800/500000: episode: 109, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.220 [0.000, 2.000],  loss: 8.246607, mae: 36.231159, mean_q: -53.293679, mean_eps: 0.100000\n",
      "  22000/500000: episode: 110, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.375 [0.000, 2.000],  loss: 7.587056, mae: 36.286002, mean_q: -53.521582, mean_eps: 0.100000\n",
      "  22200/500000: episode: 111, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.265 [0.000, 2.000],  loss: 8.015963, mae: 36.620525, mean_q: -53.827562, mean_eps: 0.100000\n",
      "  22400/500000: episode: 112, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 2.000],  loss: 6.772094, mae: 36.866221, mean_q: -54.316130, mean_eps: 0.100000\n",
      "  22600/500000: episode: 113, duration: 0.997s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.055 [0.000, 2.000],  loss: 5.151911, mae: 37.015481, mean_q: -54.709524, mean_eps: 0.100000\n",
      "  22800/500000: episode: 114, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.145 [0.000, 2.000],  loss: 8.560469, mae: 37.291462, mean_q: -54.784440, mean_eps: 0.100000\n",
      "  23000/500000: episode: 115, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.081196, mae: 36.963129, mean_q: -54.366619, mean_eps: 0.100000\n",
      "  23200/500000: episode: 116, duration: 0.933s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 7.819078, mae: 36.614215, mean_q: -53.853684, mean_eps: 0.100000\n",
      "  23400/500000: episode: 117, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.065536, mae: 36.271043, mean_q: -53.474956, mean_eps: 0.100000\n",
      "  23600/500000: episode: 118, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 5.873522, mae: 36.235974, mean_q: -53.402633, mean_eps: 0.100000\n",
      "  23800/500000: episode: 119, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.458371, mae: 36.297180, mean_q: -53.522499, mean_eps: 0.100000\n",
      "  24000/500000: episode: 120, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 6.231480, mae: 36.364473, mean_q: -53.557762, mean_eps: 0.100000\n",
      "  24200/500000: episode: 121, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.400 [0.000, 2.000],  loss: 6.860078, mae: 36.551523, mean_q: -53.895449, mean_eps: 0.100000\n",
      "  24400/500000: episode: 122, duration: 1.026s, episode steps: 200, steps per second: 195, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.325 [0.000, 2.000],  loss: 7.975199, mae: 36.950975, mean_q: -54.415695, mean_eps: 0.100000\n",
      "  24600/500000: episode: 123, duration: 0.964s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 4.393972, mae: 36.972071, mean_q: -54.724489, mean_eps: 0.100000\n",
      "  24800/500000: episode: 124, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.152162, mae: 36.982444, mean_q: -54.598499, mean_eps: 0.100000\n",
      "  25000/500000: episode: 125, duration: 0.933s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.980852, mae: 37.092147, mean_q: -54.737684, mean_eps: 0.100000\n",
      "  25200/500000: episode: 126, duration: 0.999s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 6.073262, mae: 37.068632, mean_q: -54.751471, mean_eps: 0.100000\n",
      "  25400/500000: episode: 127, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 7.041019, mae: 36.706861, mean_q: -54.212306, mean_eps: 0.100000\n",
      "  25600/500000: episode: 128, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.485 [0.000, 2.000],  loss: 9.016968, mae: 36.560522, mean_q: -53.670187, mean_eps: 0.100000\n",
      "  25800/500000: episode: 129, duration: 0.964s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.335 [0.000, 2.000],  loss: 5.454384, mae: 36.326569, mean_q: -53.495193, mean_eps: 0.100000\n",
      "  26000/500000: episode: 130, duration: 0.989s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.600 [0.000, 2.000],  loss: 6.491203, mae: 36.295194, mean_q: -53.304446, mean_eps: 0.100000\n",
      "  26200/500000: episode: 131, duration: 0.975s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 4.525751, mae: 36.374981, mean_q: -53.570433, mean_eps: 0.100000\n",
      "  26400/500000: episode: 132, duration: 0.960s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 7.306982, mae: 36.479949, mean_q: -53.454384, mean_eps: 0.100000\n",
      "  26600/500000: episode: 133, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 6.523968, mae: 35.854540, mean_q: -52.414591, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  26769/500000: episode: 134, duration: 0.834s, episode steps: 169, steps per second: 203, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.861243, mae: 34.880357, mean_q: -50.847658, mean_eps: 0.100000\n",
      "  26922/500000: episode: 135, duration: 0.743s, episode steps: 153, steps per second: 206, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 3.460604, mae: 33.522970, mean_q: -48.756205, mean_eps: 0.100000\n",
      "  27038/500000: episode: 136, duration: 0.570s, episode steps: 116, steps per second: 204, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 2.585557, mae: 32.500594, mean_q: -47.295221, mean_eps: 0.100000\n",
      "  27193/500000: episode: 137, duration: 0.776s, episode steps: 155, steps per second: 200, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 2.632509, mae: 31.475340, mean_q: -45.755325, mean_eps: 0.100000\n",
      "  27363/500000: episode: 138, duration: 0.848s, episode steps: 170, steps per second: 201, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.903233, mae: 30.697054, mean_q: -44.856885, mean_eps: 0.100000\n",
      "  27534/500000: episode: 139, duration: 0.811s, episode steps: 171, steps per second: 211, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.906370, mae: 29.722843, mean_q: -43.475197, mean_eps: 0.100000\n",
      "  27726/500000: episode: 140, duration: 0.924s, episode steps: 192, steps per second: 208, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.356259, mae: 29.474178, mean_q: -43.187263, mean_eps: 0.100000\n",
      "  27842/500000: episode: 141, duration: 0.534s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.552 [0.000, 2.000],  loss: 0.118653, mae: 29.032638, mean_q: -42.687254, mean_eps: 0.100000\n",
      "  28029/500000: episode: 142, duration: 0.957s, episode steps: 187, steps per second: 195, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.168059, mae: 29.421531, mean_q: -43.267263, mean_eps: 0.100000\n",
      "  28229/500000: episode: 143, duration: 0.989s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.229949, mae: 29.339393, mean_q: -43.004031, mean_eps: 0.100000\n",
      "  28369/500000: episode: 144, duration: 0.697s, episode steps: 140, steps per second: 201, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.257816, mae: 29.624349, mean_q: -43.440162, mean_eps: 0.100000\n",
      "  28533/500000: episode: 145, duration: 0.792s, episode steps: 164, steps per second: 207, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.304010, mae: 30.095890, mean_q: -44.133091, mean_eps: 0.100000\n",
      "  28684/500000: episode: 146, duration: 0.727s, episode steps: 151, steps per second: 208, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.295715, mae: 30.323145, mean_q: -44.348932, mean_eps: 0.100000\n",
      "  28819/500000: episode: 147, duration: 0.647s, episode steps: 135, steps per second: 209, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.407052, mae: 30.801687, mean_q: -44.830766, mean_eps: 0.100000\n",
      "  29019/500000: episode: 148, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.275964, mae: 31.548984, mean_q: -45.923313, mean_eps: 0.100000\n",
      "  29219/500000: episode: 149, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.241216, mae: 32.098750, mean_q: -46.751597, mean_eps: 0.100000\n",
      "  29419/500000: episode: 150, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.175656, mae: 32.765738, mean_q: -47.851899, mean_eps: 0.100000\n",
      "  29619/500000: episode: 151, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 1.642734, mae: 33.981857, mean_q: -49.433059, mean_eps: 0.100000\n",
      "  29819/500000: episode: 152, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.927632, mae: 35.622737, mean_q: -52.076822, mean_eps: 0.100000\n",
      "  30019/500000: episode: 153, duration: 0.994s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 3.735941, mae: 36.848762, mean_q: -53.844440, mean_eps: 0.100000\n",
      "  30219/500000: episode: 154, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 5.191182, mae: 38.384042, mean_q: -56.128184, mean_eps: 0.100000\n",
      "  30419/500000: episode: 155, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.258167, mae: 38.636544, mean_q: -56.465136, mean_eps: 0.100000\n",
      "  30585/500000: episode: 156, duration: 0.788s, episode steps: 166, steps per second: 211, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 3.323398, mae: 38.059208, mean_q: -55.585588, mean_eps: 0.100000\n",
      "  30785/500000: episode: 157, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 3.276782, mae: 36.710263, mean_q: -53.550110, mean_eps: 0.100000\n",
      "  30985/500000: episode: 158, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.625 [0.000, 2.000],  loss: 3.415254, mae: 36.058573, mean_q: -52.512062, mean_eps: 0.100000\n",
      "  31185/500000: episode: 159, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 4.493032, mae: 35.493347, mean_q: -51.650474, mean_eps: 0.100000\n",
      "  31385/500000: episode: 160, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 2.000],  loss: 1.795297, mae: 34.926645, mean_q: -51.104322, mean_eps: 0.100000\n",
      "  31585/500000: episode: 161, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000],  loss: 5.047920, mae: 36.207596, mean_q: -52.906104, mean_eps: 0.100000\n",
      "  31785/500000: episode: 162, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.510 [0.000, 2.000],  loss: 6.161584, mae: 37.329739, mean_q: -54.656560, mean_eps: 0.100000\n",
      "  31985/500000: episode: 163, duration: 0.956s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.365 [0.000, 2.000],  loss: 8.884945, mae: 37.585848, mean_q: -54.806084, mean_eps: 0.100000\n",
      "  32185/500000: episode: 164, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 7.306784, mae: 37.833888, mean_q: -55.440578, mean_eps: 0.100000\n",
      "  32350/500000: episode: 165, duration: 0.767s, episode steps: 165, steps per second: 215, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.933 [0.000, 2.000],  loss: 7.595951, mae: 38.364660, mean_q: -56.359875, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32509/500000: episode: 166, duration: 0.768s, episode steps: 159, steps per second: 207, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.893 [0.000, 2.000],  loss: 7.053425, mae: 35.988905, mean_q: -52.325072, mean_eps: 0.100000\n",
      "  32709/500000: episode: 167, duration: 0.977s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.690 [0.000, 2.000],  loss: 5.348475, mae: 34.530011, mean_q: -49.813957, mean_eps: 0.100000\n",
      "  32906/500000: episode: 168, duration: 0.957s, episode steps: 197, steps per second: 206, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 2.831537, mae: 32.936283, mean_q: -47.440212, mean_eps: 0.100000\n",
      "  33074/500000: episode: 169, duration: 0.814s, episode steps: 168, steps per second: 206, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.887 [0.000, 2.000],  loss: 4.061818, mae: 31.328996, mean_q: -44.805882, mean_eps: 0.100000\n",
      "  33224/500000: episode: 170, duration: 0.699s, episode steps: 150, steps per second: 215, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.893 [0.000, 2.000],  loss: 0.940360, mae: 29.872202, mean_q: -42.626669, mean_eps: 0.100000\n",
      "  33424/500000: episode: 171, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 2.000],  loss: 0.482171, mae: 30.322365, mean_q: -43.509904, mean_eps: 0.100000\n",
      "  33576/500000: episode: 172, duration: 0.716s, episode steps: 152, steps per second: 212, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.995947, mae: 32.571115, mean_q: -47.267714, mean_eps: 0.100000\n",
      "  33715/500000: episode: 173, duration: 0.648s, episode steps: 139, steps per second: 215, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 2.336078, mae: 31.131518, mean_q: -44.682515, mean_eps: 0.100000\n",
      "  33856/500000: episode: 174, duration: 0.698s, episode steps: 141, steps per second: 202, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 2.444350, mae: 29.935695, mean_q: -42.630979, mean_eps: 0.100000\n",
      "  34004/500000: episode: 175, duration: 0.712s, episode steps: 148, steps per second: 208, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 1.517381, mae: 29.249585, mean_q: -41.768220, mean_eps: 0.100000\n",
      "  34146/500000: episode: 176, duration: 0.709s, episode steps: 142, steps per second: 200, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.951 [0.000, 2.000],  loss: 1.470276, mae: 28.422308, mean_q: -40.672955, mean_eps: 0.100000\n",
      "  34297/500000: episode: 177, duration: 0.755s, episode steps: 151, steps per second: 200, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.906412, mae: 27.259011, mean_q: -39.066289, mean_eps: 0.100000\n",
      "  34440/500000: episode: 178, duration: 0.707s, episode steps: 143, steps per second: 202, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.793174, mae: 25.092383, mean_q: -36.165967, mean_eps: 0.100000\n",
      "  34579/500000: episode: 179, duration: 0.693s, episode steps: 139, steps per second: 200, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.266222, mae: 24.810994, mean_q: -35.852953, mean_eps: 0.100000\n",
      "  34719/500000: episode: 180, duration: 0.683s, episode steps: 140, steps per second: 205, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.169983, mae: 25.317769, mean_q: -36.712821, mean_eps: 0.100000\n",
      "  34860/500000: episode: 181, duration: 0.681s, episode steps: 141, steps per second: 207, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.177106, mae: 25.513861, mean_q: -37.052350, mean_eps: 0.100000\n",
      "  35060/500000: episode: 182, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.122141, mae: 27.014663, mean_q: -39.261159, mean_eps: 0.100000\n",
      "  35260/500000: episode: 183, duration: 1.007s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.150 [0.000, 2.000],  loss: 0.103804, mae: 28.558705, mean_q: -41.697197, mean_eps: 0.100000\n",
      "  35403/500000: episode: 184, duration: 0.687s, episode steps: 143, steps per second: 208, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.804 [0.000, 2.000],  loss: 2.943963, mae: 31.046357, mean_q: -45.187030, mean_eps: 0.100000\n",
      "  35603/500000: episode: 185, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.435 [0.000, 2.000],  loss: 1.820330, mae: 31.200424, mean_q: -45.486628, mean_eps: 0.100000\n",
      "  35757/500000: episode: 186, duration: 0.758s, episode steps: 154, steps per second: 203, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 2.816006, mae: 32.497165, mean_q: -47.408216, mean_eps: 0.100000\n",
      "  35873/500000: episode: 187, duration: 0.639s, episode steps: 116, steps per second: 182, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 1.735542, mae: 32.277037, mean_q: -47.223815, mean_eps: 0.100000\n",
      "  35976/500000: episode: 188, duration: 0.496s, episode steps: 103, steps per second: 208, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 2.404420, mae: 31.726463, mean_q: -46.210213, mean_eps: 0.100000\n",
      "  36094/500000: episode: 189, duration: 0.572s, episode steps: 118, steps per second: 206, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 3.775977, mae: 31.389593, mean_q: -45.590275, mean_eps: 0.100000\n",
      "  36294/500000: episode: 190, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.315 [0.000, 2.000],  loss: 1.596507, mae: 30.463291, mean_q: -44.345024, mean_eps: 0.100000\n",
      "  36454/500000: episode: 191, duration: 0.764s, episode steps: 160, steps per second: 209, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.480490, mae: 30.363397, mean_q: -44.254346, mean_eps: 0.100000\n",
      "  36619/500000: episode: 192, duration: 0.784s, episode steps: 165, steps per second: 210, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.891 [0.000, 2.000],  loss: 0.565226, mae: 28.809115, mean_q: -41.732048, mean_eps: 0.100000\n",
      "  36800/500000: episode: 193, duration: 0.876s, episode steps: 181, steps per second: 207, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.781330, mae: 28.613788, mean_q: -41.458088, mean_eps: 0.100000\n",
      "  36958/500000: episode: 194, duration: 0.741s, episode steps: 158, steps per second: 213, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000],  loss: 0.892409, mae: 28.551559, mean_q: -41.346153, mean_eps: 0.100000\n",
      "  37128/500000: episode: 195, duration: 0.824s, episode steps: 170, steps per second: 206, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.818758, mae: 28.233657, mean_q: -41.012506, mean_eps: 0.100000\n",
      "  37300/500000: episode: 196, duration: 0.909s, episode steps: 172, steps per second: 189, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 0.784205, mae: 26.861544, mean_q: -39.121634, mean_eps: 0.100000\n",
      "  37452/500000: episode: 197, duration: 0.751s, episode steps: 152, steps per second: 202, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.343843, mae: 25.618781, mean_q: -37.360627, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  37598/500000: episode: 198, duration: 0.712s, episode steps: 146, steps per second: 205, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.153444, mae: 25.754512, mean_q: -37.594276, mean_eps: 0.100000\n",
      "  37739/500000: episode: 199, duration: 0.699s, episode steps: 141, steps per second: 202, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.119220, mae: 25.132234, mean_q: -36.636756, mean_eps: 0.100000\n",
      "  37889/500000: episode: 200, duration: 0.719s, episode steps: 150, steps per second: 209, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.080576, mae: 24.742673, mean_q: -36.065971, mean_eps: 0.100000\n",
      "  38089/500000: episode: 201, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 0.081688, mae: 25.648353, mean_q: -37.509828, mean_eps: 0.100000\n",
      "  38233/500000: episode: 202, duration: 0.697s, episode steps: 144, steps per second: 207, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.154887, mae: 26.846055, mean_q: -39.294048, mean_eps: 0.100000\n",
      "  38375/500000: episode: 203, duration: 0.681s, episode steps: 142, steps per second: 208, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.108757, mae: 26.558483, mean_q: -38.887793, mean_eps: 0.100000\n",
      "  38524/500000: episode: 204, duration: 0.736s, episode steps: 149, steps per second: 202, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.093892, mae: 27.171723, mean_q: -39.935042, mean_eps: 0.100000\n",
      "  38670/500000: episode: 205, duration: 0.714s, episode steps: 146, steps per second: 205, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.091921, mae: 27.127809, mean_q: -39.869354, mean_eps: 0.100000\n",
      "  38815/500000: episode: 206, duration: 0.697s, episode steps: 145, steps per second: 208, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.100494, mae: 27.504038, mean_q: -40.422106, mean_eps: 0.100000\n",
      "  38958/500000: episode: 207, duration: 0.699s, episode steps: 143, steps per second: 205, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.951 [0.000, 2.000],  loss: 0.139104, mae: 27.907203, mean_q: -40.976161, mean_eps: 0.100000\n",
      "  39103/500000: episode: 208, duration: 0.716s, episode steps: 145, steps per second: 203, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.903 [0.000, 2.000],  loss: 0.105644, mae: 26.269232, mean_q: -38.512396, mean_eps: 0.100000\n",
      "  39243/500000: episode: 209, duration: 0.682s, episode steps: 140, steps per second: 205, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.082837, mae: 25.940520, mean_q: -37.980052, mean_eps: 0.100000\n",
      "  39392/500000: episode: 210, duration: 0.737s, episode steps: 149, steps per second: 202, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.094849, mae: 25.685925, mean_q: -37.572649, mean_eps: 0.100000\n",
      "  39558/500000: episode: 211, duration: 0.808s, episode steps: 166, steps per second: 206, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.922 [0.000, 2.000],  loss: 0.085536, mae: 25.628746, mean_q: -37.489604, mean_eps: 0.100000\n",
      "  39701/500000: episode: 212, duration: 0.667s, episode steps: 143, steps per second: 215, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.096093, mae: 26.191400, mean_q: -38.317519, mean_eps: 0.100000\n",
      "  39849/500000: episode: 213, duration: 0.687s, episode steps: 148, steps per second: 215, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000],  loss: 0.077644, mae: 26.551638, mean_q: -38.922307, mean_eps: 0.100000\n",
      "  39997/500000: episode: 214, duration: 0.718s, episode steps: 148, steps per second: 206, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.066298, mae: 26.767489, mean_q: -39.314542, mean_eps: 0.100000\n",
      "  40138/500000: episode: 215, duration: 0.663s, episode steps: 141, steps per second: 213, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.957 [0.000, 2.000],  loss: 0.038579, mae: 27.206587, mean_q: -39.977151, mean_eps: 0.100000\n",
      "  40311/500000: episode: 216, duration: 0.827s, episode steps: 173, steps per second: 209, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.047882, mae: 26.746679, mean_q: -39.311827, mean_eps: 0.100000\n",
      "  40445/500000: episode: 217, duration: 0.672s, episode steps: 134, steps per second: 199, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.075594, mae: 26.271928, mean_q: -38.628499, mean_eps: 0.100000\n",
      "  40560/500000: episode: 218, duration: 0.547s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.076636, mae: 26.099416, mean_q: -38.341098, mean_eps: 0.100000\n",
      "  40680/500000: episode: 219, duration: 0.591s, episode steps: 120, steps per second: 203, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.367 [0.000, 2.000],  loss: 0.053800, mae: 26.226247, mean_q: -38.524091, mean_eps: 0.100000\n",
      "  40799/500000: episode: 220, duration: 0.579s, episode steps: 119, steps per second: 206, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.053016, mae: 26.213582, mean_q: -38.538792, mean_eps: 0.100000\n",
      "  40915/500000: episode: 221, duration: 0.579s, episode steps: 116, steps per second: 200, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.048165, mae: 26.017368, mean_q: -38.184008, mean_eps: 0.100000\n",
      "  41035/500000: episode: 222, duration: 0.583s, episode steps: 120, steps per second: 206, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.041802, mae: 25.758509, mean_q: -37.773021, mean_eps: 0.100000\n",
      "  41150/500000: episode: 223, duration: 0.556s, episode steps: 115, steps per second: 207, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.063337, mae: 26.058801, mean_q: -38.216934, mean_eps: 0.100000\n",
      "  41279/500000: episode: 224, duration: 0.622s, episode steps: 129, steps per second: 207, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.038507, mae: 26.185279, mean_q: -38.435257, mean_eps: 0.100000\n",
      "  41391/500000: episode: 225, duration: 0.545s, episode steps: 112, steps per second: 206, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.068835, mae: 26.850948, mean_q: -39.351102, mean_eps: 0.100000\n",
      "  41509/500000: episode: 226, duration: 0.583s, episode steps: 118, steps per second: 202, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.076206, mae: 26.611358, mean_q: -38.850227, mean_eps: 0.100000\n",
      "  41625/500000: episode: 227, duration: 0.560s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.036736, mae: 26.742200, mean_q: -39.102520, mean_eps: 0.100000\n",
      "  41745/500000: episode: 228, duration: 0.576s, episode steps: 120, steps per second: 208, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.050965, mae: 26.894553, mean_q: -39.324005, mean_eps: 0.100000\n",
      "  41885/500000: episode: 229, duration: 0.669s, episode steps: 140, steps per second: 209, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.457 [0.000, 2.000],  loss: 0.032972, mae: 27.645511, mean_q: -40.463928, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41999/500000: episode: 230, duration: 0.575s, episode steps: 114, steps per second: 198, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.066102, mae: 27.726819, mean_q: -40.542489, mean_eps: 0.100000\n",
      "  42112/500000: episode: 231, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.069380, mae: 28.003146, mean_q: -40.894150, mean_eps: 0.100000\n",
      "  42312/500000: episode: 232, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.099446, mae: 27.995627, mean_q: -40.733039, mean_eps: 0.100000\n",
      "  42462/500000: episode: 233, duration: 0.741s, episode steps: 150, steps per second: 202, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.133526, mae: 27.633592, mean_q: -40.267438, mean_eps: 0.100000\n",
      "  42578/500000: episode: 234, duration: 0.560s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.156297, mae: 28.034719, mean_q: -40.889093, mean_eps: 0.100000\n",
      "  42686/500000: episode: 235, duration: 0.533s, episode steps: 108, steps per second: 203, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.152222, mae: 27.750663, mean_q: -40.386747, mean_eps: 0.100000\n",
      "  42822/500000: episode: 236, duration: 0.655s, episode steps: 136, steps per second: 207, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.434 [0.000, 2.000],  loss: 0.112919, mae: 27.594098, mean_q: -40.173639, mean_eps: 0.100000\n",
      "  42931/500000: episode: 237, duration: 0.554s, episode steps: 109, steps per second: 197, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.195758, mae: 27.285419, mean_q: -39.562502, mean_eps: 0.100000\n",
      "  43105/500000: episode: 238, duration: 0.831s, episode steps: 174, steps per second: 209, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.145084, mae: 27.453402, mean_q: -39.886060, mean_eps: 0.100000\n",
      "  43223/500000: episode: 239, duration: 0.560s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.167658, mae: 27.698876, mean_q: -40.238282, mean_eps: 0.100000\n",
      "  43354/500000: episode: 240, duration: 0.641s, episode steps: 131, steps per second: 204, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.359 [0.000, 2.000],  loss: 0.159705, mae: 27.502272, mean_q: -39.875503, mean_eps: 0.100000\n",
      "  43466/500000: episode: 241, duration: 0.555s, episode steps: 112, steps per second: 202, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.107761, mae: 27.560757, mean_q: -40.017024, mean_eps: 0.100000\n",
      "  43579/500000: episode: 242, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.130469, mae: 27.858308, mean_q: -40.301817, mean_eps: 0.100000\n",
      "  43690/500000: episode: 243, duration: 0.542s, episode steps: 111, steps per second: 205, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.116472, mae: 27.459647, mean_q: -39.805948, mean_eps: 0.100000\n",
      "  43854/500000: episode: 244, duration: 0.806s, episode steps: 164, steps per second: 203, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.110152, mae: 27.839693, mean_q: -40.499548, mean_eps: 0.100000\n",
      "  44003/500000: episode: 245, duration: 0.743s, episode steps: 149, steps per second: 201, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.953 [0.000, 2.000],  loss: 0.134365, mae: 27.685546, mean_q: -40.237930, mean_eps: 0.100000\n",
      "  44111/500000: episode: 246, duration: 0.525s, episode steps: 108, steps per second: 206, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.112390, mae: 27.184855, mean_q: -39.551534, mean_eps: 0.100000\n",
      "  44305/500000: episode: 247, duration: 0.940s, episode steps: 194, steps per second: 206, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.094004, mae: 26.701623, mean_q: -38.895220, mean_eps: 0.100000\n",
      "  44419/500000: episode: 248, duration: 0.563s, episode steps: 114, steps per second: 202, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.097386, mae: 26.159094, mean_q: -38.099132, mean_eps: 0.100000\n",
      "  44515/500000: episode: 249, duration: 0.474s, episode steps:  96, steps per second: 202, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.098236, mae: 26.162774, mean_q: -38.132461, mean_eps: 0.100000\n",
      "  44667/500000: episode: 250, duration: 0.734s, episode steps: 152, steps per second: 207, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.967 [0.000, 2.000],  loss: 0.096060, mae: 26.193350, mean_q: -38.121049, mean_eps: 0.100000\n",
      "  44810/500000: episode: 251, duration: 0.664s, episode steps: 143, steps per second: 215, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.100083, mae: 26.087001, mean_q: -37.889767, mean_eps: 0.100000\n",
      "  44944/500000: episode: 252, duration: 0.638s, episode steps: 134, steps per second: 210, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000],  loss: 0.119114, mae: 26.001214, mean_q: -37.752952, mean_eps: 0.100000\n",
      "  45093/500000: episode: 253, duration: 0.685s, episode steps: 149, steps per second: 217, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.919 [0.000, 2.000],  loss: 0.114462, mae: 26.426352, mean_q: -38.369123, mean_eps: 0.100000\n",
      "  45239/500000: episode: 254, duration: 0.686s, episode steps: 146, steps per second: 213, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.089650, mae: 26.770931, mean_q: -39.049360, mean_eps: 0.100000\n",
      "  45349/500000: episode: 255, duration: 0.536s, episode steps: 110, steps per second: 205, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.116500, mae: 26.884315, mean_q: -39.199200, mean_eps: 0.100000\n",
      "  45527/500000: episode: 256, duration: 0.842s, episode steps: 178, steps per second: 211, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.075169, mae: 27.144135, mean_q: -39.615627, mean_eps: 0.100000\n",
      "  45673/500000: episode: 257, duration: 0.689s, episode steps: 146, steps per second: 212, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.027 [0.000, 2.000],  loss: 0.058827, mae: 26.843460, mean_q: -39.334421, mean_eps: 0.100000\n",
      "  45835/500000: episode: 258, duration: 0.773s, episode steps: 162, steps per second: 209, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.065431, mae: 26.831311, mean_q: -39.301492, mean_eps: 0.100000\n",
      "  45994/500000: episode: 259, duration: 0.889s, episode steps: 159, steps per second: 179, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.055672, mae: 26.761575, mean_q: -39.230327, mean_eps: 0.100000\n",
      "  46143/500000: episode: 260, duration: 0.854s, episode steps: 149, steps per second: 174, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.056245, mae: 27.425957, mean_q: -40.255086, mean_eps: 0.100000\n",
      "  46230/500000: episode: 261, duration: 0.457s, episode steps:  87, steps per second: 190, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.052527, mae: 26.860023, mean_q: -39.444577, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  46377/500000: episode: 262, duration: 0.689s, episode steps: 147, steps per second: 213, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.078760, mae: 27.598057, mean_q: -40.436446, mean_eps: 0.100000\n",
      "  46524/500000: episode: 263, duration: 0.702s, episode steps: 147, steps per second: 209, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.056563, mae: 27.284057, mean_q: -40.000619, mean_eps: 0.100000\n",
      "  46623/500000: episode: 264, duration: 0.470s, episode steps:  99, steps per second: 211, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.070812, mae: 27.244844, mean_q: -39.892527, mean_eps: 0.100000\n",
      "  46789/500000: episode: 265, duration: 0.772s, episode steps: 166, steps per second: 215, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.155854, mae: 28.004756, mean_q: -40.966150, mean_eps: 0.100000\n",
      "  46882/500000: episode: 266, duration: 0.446s, episode steps:  93, steps per second: 209, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.097455, mae: 27.516369, mean_q: -40.258599, mean_eps: 0.100000\n",
      "  47031/500000: episode: 267, duration: 0.710s, episode steps: 149, steps per second: 210, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.123075, mae: 28.167256, mean_q: -41.124432, mean_eps: 0.100000\n",
      "  47181/500000: episode: 268, duration: 0.700s, episode steps: 150, steps per second: 214, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.162828, mae: 27.715594, mean_q: -40.389477, mean_eps: 0.100000\n",
      "  47341/500000: episode: 269, duration: 0.753s, episode steps: 160, steps per second: 213, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.147309, mae: 28.081542, mean_q: -40.952579, mean_eps: 0.100000\n",
      "  47487/500000: episode: 270, duration: 0.691s, episode steps: 146, steps per second: 211, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.136451, mae: 28.329303, mean_q: -41.404293, mean_eps: 0.100000\n",
      "  47622/500000: episode: 271, duration: 0.631s, episode steps: 135, steps per second: 214, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.112124, mae: 28.683237, mean_q: -42.045586, mean_eps: 0.100000\n",
      "  47777/500000: episode: 272, duration: 0.731s, episode steps: 155, steps per second: 212, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.098563, mae: 28.552916, mean_q: -41.849782, mean_eps: 0.100000\n",
      "  47940/500000: episode: 273, duration: 0.787s, episode steps: 163, steps per second: 207, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.076915, mae: 28.867237, mean_q: -42.446037, mean_eps: 0.100000\n",
      "  48086/500000: episode: 274, duration: 0.770s, episode steps: 146, steps per second: 190, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.065777, mae: 28.955140, mean_q: -42.639598, mean_eps: 0.100000\n",
      "  48234/500000: episode: 275, duration: 0.776s, episode steps: 148, steps per second: 191, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.939 [0.000, 2.000],  loss: 0.071060, mae: 29.501041, mean_q: -43.453052, mean_eps: 0.100000\n",
      "  48394/500000: episode: 276, duration: 0.824s, episode steps: 160, steps per second: 194, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000],  loss: 0.070873, mae: 28.561423, mean_q: -41.996886, mean_eps: 0.100000\n",
      "  48561/500000: episode: 277, duration: 0.824s, episode steps: 167, steps per second: 203, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.892 [0.000, 2.000],  loss: 0.087381, mae: 28.757419, mean_q: -42.200217, mean_eps: 0.100000\n",
      "  48703/500000: episode: 278, duration: 0.684s, episode steps: 142, steps per second: 208, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.080173, mae: 28.842928, mean_q: -42.377267, mean_eps: 0.100000\n",
      "  48853/500000: episode: 279, duration: 0.705s, episode steps: 150, steps per second: 213, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.073786, mae: 29.047115, mean_q: -42.753301, mean_eps: 0.100000\n",
      "  49003/500000: episode: 280, duration: 0.716s, episode steps: 150, steps per second: 209, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.065256, mae: 28.983487, mean_q: -42.645173, mean_eps: 0.100000\n",
      "  49150/500000: episode: 281, duration: 0.685s, episode steps: 147, steps per second: 215, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.063060, mae: 29.034635, mean_q: -42.734846, mean_eps: 0.100000\n",
      "  49292/500000: episode: 282, duration: 0.663s, episode steps: 142, steps per second: 214, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.046254, mae: 28.763744, mean_q: -42.351086, mean_eps: 0.100000\n",
      "  49449/500000: episode: 283, duration: 0.779s, episode steps: 157, steps per second: 202, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.042240, mae: 29.429816, mean_q: -43.362143, mean_eps: 0.100000\n",
      "  49595/500000: episode: 284, duration: 0.720s, episode steps: 146, steps per second: 203, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.863 [0.000, 2.000],  loss: 0.044740, mae: 28.538069, mean_q: -41.988972, mean_eps: 0.100000\n",
      "  49741/500000: episode: 285, duration: 0.730s, episode steps: 146, steps per second: 200, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.057203, mae: 28.470720, mean_q: -41.853094, mean_eps: 0.100000\n",
      "  49858/500000: episode: 286, duration: 0.586s, episode steps: 117, steps per second: 200, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.048368, mae: 28.125265, mean_q: -41.335533, mean_eps: 0.100000\n",
      "  50025/500000: episode: 287, duration: 0.858s, episode steps: 167, steps per second: 195, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.050161, mae: 28.676595, mean_q: -42.123187, mean_eps: 0.100000\n",
      "  50177/500000: episode: 288, duration: 0.749s, episode steps: 152, steps per second: 203, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.888 [0.000, 2.000],  loss: 0.062820, mae: 28.531202, mean_q: -41.875060, mean_eps: 0.100000\n",
      "  50293/500000: episode: 289, duration: 0.575s, episode steps: 116, steps per second: 202, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.397 [0.000, 2.000],  loss: 0.080898, mae: 28.340414, mean_q: -41.577545, mean_eps: 0.100000\n",
      "  50404/500000: episode: 290, duration: 0.572s, episode steps: 111, steps per second: 194, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.063672, mae: 28.303819, mean_q: -41.556974, mean_eps: 0.100000\n",
      "  50514/500000: episode: 291, duration: 0.542s, episode steps: 110, steps per second: 203, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.071070, mae: 28.382939, mean_q: -41.596876, mean_eps: 0.100000\n",
      "  50662/500000: episode: 292, duration: 0.728s, episode steps: 148, steps per second: 203, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.069112, mae: 28.589333, mean_q: -41.834321, mean_eps: 0.100000\n",
      "  50778/500000: episode: 293, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.082618, mae: 28.774229, mean_q: -42.104945, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50928/500000: episode: 294, duration: 0.721s, episode steps: 150, steps per second: 208, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.101116, mae: 29.508202, mean_q: -43.116065, mean_eps: 0.100000\n",
      "  51074/500000: episode: 295, duration: 0.694s, episode steps: 146, steps per second: 210, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.060596, mae: 28.987454, mean_q: -42.396111, mean_eps: 0.100000\n",
      "  51217/500000: episode: 296, duration: 0.670s, episode steps: 143, steps per second: 213, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.063965, mae: 28.953830, mean_q: -42.353545, mean_eps: 0.100000\n",
      "  51334/500000: episode: 297, duration: 0.544s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.479 [0.000, 2.000],  loss: 0.051883, mae: 29.184830, mean_q: -42.706996, mean_eps: 0.100000\n",
      "  51484/500000: episode: 298, duration: 0.714s, episode steps: 150, steps per second: 210, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.953 [0.000, 2.000],  loss: 0.055180, mae: 29.181272, mean_q: -42.715615, mean_eps: 0.100000\n",
      "  51597/500000: episode: 299, duration: 0.530s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.061783, mae: 28.445353, mean_q: -41.663601, mean_eps: 0.100000\n",
      "  51709/500000: episode: 300, duration: 0.520s, episode steps: 112, steps per second: 215, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000],  loss: 0.075149, mae: 28.593000, mean_q: -41.806930, mean_eps: 0.100000\n",
      "  51820/500000: episode: 301, duration: 0.528s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000],  loss: 0.059238, mae: 28.391645, mean_q: -41.524965, mean_eps: 0.100000\n",
      "  51940/500000: episode: 302, duration: 0.604s, episode steps: 120, steps per second: 199, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000],  loss: 0.070062, mae: 28.740770, mean_q: -42.031291, mean_eps: 0.100000\n",
      "  52055/500000: episode: 303, duration: 0.577s, episode steps: 115, steps per second: 199, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.043489, mae: 28.137273, mean_q: -41.168542, mean_eps: 0.100000\n",
      "  52202/500000: episode: 304, duration: 0.743s, episode steps: 147, steps per second: 198, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.053112, mae: 28.459537, mean_q: -41.606737, mean_eps: 0.100000\n",
      "  52314/500000: episode: 305, duration: 0.539s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.076265, mae: 28.814074, mean_q: -42.152438, mean_eps: 0.100000\n",
      "  52436/500000: episode: 306, duration: 0.639s, episode steps: 122, steps per second: 191, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.443 [0.000, 2.000],  loss: 0.052617, mae: 28.583180, mean_q: -41.811491, mean_eps: 0.100000\n",
      "  52556/500000: episode: 307, duration: 0.591s, episode steps: 120, steps per second: 203, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.049954, mae: 28.791990, mean_q: -42.137104, mean_eps: 0.100000\n",
      "  52673/500000: episode: 308, duration: 0.580s, episode steps: 117, steps per second: 202, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.088393, mae: 28.800082, mean_q: -42.078336, mean_eps: 0.100000\n",
      "  52788/500000: episode: 309, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.066340, mae: 28.616940, mean_q: -41.729065, mean_eps: 0.100000\n",
      "  52895/500000: episode: 310, duration: 0.503s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.073497, mae: 28.498010, mean_q: -41.553910, mean_eps: 0.100000\n",
      "  53095/500000: episode: 311, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.072081, mae: 28.479758, mean_q: -41.478872, mean_eps: 0.100000\n",
      "  53209/500000: episode: 312, duration: 0.564s, episode steps: 114, steps per second: 202, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.201728, mae: 28.299180, mean_q: -41.140745, mean_eps: 0.100000\n",
      "  53326/500000: episode: 313, duration: 0.594s, episode steps: 117, steps per second: 197, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.651193, mae: 28.969510, mean_q: -42.003684, mean_eps: 0.100000\n",
      "  53440/500000: episode: 314, duration: 0.540s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.396159, mae: 28.720987, mean_q: -41.625679, mean_eps: 0.100000\n",
      "  53601/500000: episode: 315, duration: 0.769s, episode steps: 161, steps per second: 209, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.242830, mae: 28.475866, mean_q: -41.314999, mean_eps: 0.100000\n",
      "  53715/500000: episode: 316, duration: 0.533s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.304418, mae: 28.508676, mean_q: -41.332548, mean_eps: 0.100000\n",
      "  53829/500000: episode: 317, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.133534, mae: 28.424554, mean_q: -41.355994, mean_eps: 0.100000\n",
      "  53948/500000: episode: 318, duration: 0.572s, episode steps: 119, steps per second: 208, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.159881, mae: 28.473568, mean_q: -41.419468, mean_eps: 0.100000\n",
      "  54106/500000: episode: 319, duration: 0.741s, episode steps: 158, steps per second: 213, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.146014, mae: 28.558981, mean_q: -41.689655, mean_eps: 0.100000\n",
      "  54284/500000: episode: 320, duration: 0.846s, episode steps: 178, steps per second: 210, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.081395, mae: 27.651890, mean_q: -40.355954, mean_eps: 0.100000\n",
      "  54380/500000: episode: 321, duration: 0.465s, episode steps:  96, steps per second: 207, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.074363, mae: 27.222139, mean_q: -39.709927, mean_eps: 0.100000\n",
      "  54562/500000: episode: 322, duration: 0.847s, episode steps: 182, steps per second: 215, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.080083, mae: 27.278370, mean_q: -39.785090, mean_eps: 0.100000\n",
      "  54686/500000: episode: 323, duration: 0.599s, episode steps: 124, steps per second: 207, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.121 [0.000, 2.000],  loss: 0.092959, mae: 27.036496, mean_q: -39.417407, mean_eps: 0.100000\n",
      "  54800/500000: episode: 324, duration: 0.556s, episode steps: 114, steps per second: 205, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.124041, mae: 26.934945, mean_q: -39.234141, mean_eps: 0.100000\n",
      "  54899/500000: episode: 325, duration: 0.503s, episode steps:  99, steps per second: 197, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.169625, mae: 27.579884, mean_q: -40.034360, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  55060/500000: episode: 326, duration: 0.760s, episode steps: 161, steps per second: 212, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.105842, mae: 27.061928, mean_q: -39.242058, mean_eps: 0.100000\n",
      "  55175/500000: episode: 327, duration: 0.609s, episode steps: 115, steps per second: 189, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.103819, mae: 27.585192, mean_q: -39.904981, mean_eps: 0.100000\n",
      "  55339/500000: episode: 328, duration: 0.813s, episode steps: 164, steps per second: 202, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.090503, mae: 27.502101, mean_q: -39.869278, mean_eps: 0.100000\n",
      "  55516/500000: episode: 329, duration: 0.851s, episode steps: 177, steps per second: 208, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.109607, mae: 26.935817, mean_q: -39.135701, mean_eps: 0.100000\n",
      "  55633/500000: episode: 330, duration: 0.585s, episode steps: 117, steps per second: 200, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.134576, mae: 27.512770, mean_q: -39.868796, mean_eps: 0.100000\n",
      "  55754/500000: episode: 331, duration: 0.633s, episode steps: 121, steps per second: 191, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.119976, mae: 27.572024, mean_q: -39.899047, mean_eps: 0.100000\n",
      "  55870/500000: episode: 332, duration: 0.553s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.135604, mae: 27.062987, mean_q: -39.195765, mean_eps: 0.100000\n",
      "  56030/500000: episode: 333, duration: 0.766s, episode steps: 160, steps per second: 209, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.881 [0.000, 2.000],  loss: 0.122953, mae: 27.236286, mean_q: -39.545827, mean_eps: 0.100000\n",
      "  56212/500000: episode: 334, duration: 0.861s, episode steps: 182, steps per second: 211, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.112194, mae: 27.581293, mean_q: -40.097606, mean_eps: 0.100000\n",
      "  56327/500000: episode: 335, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.121091, mae: 26.781362, mean_q: -38.849756, mean_eps: 0.100000\n",
      "  56444/500000: episode: 336, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.256 [0.000, 2.000],  loss: 0.151014, mae: 27.603790, mean_q: -40.114409, mean_eps: 0.100000\n",
      "  56607/500000: episode: 337, duration: 0.769s, episode steps: 163, steps per second: 212, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.957 [0.000, 2.000],  loss: 0.104247, mae: 27.559382, mean_q: -40.117588, mean_eps: 0.100000\n",
      "  56772/500000: episode: 338, duration: 0.777s, episode steps: 165, steps per second: 212, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000],  loss: 0.105938, mae: 27.282886, mean_q: -39.699121, mean_eps: 0.100000\n",
      "  56929/500000: episode: 339, duration: 0.751s, episode steps: 157, steps per second: 209, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.084120, mae: 27.157937, mean_q: -39.631711, mean_eps: 0.100000\n",
      "  57047/500000: episode: 340, duration: 0.560s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.052587, mae: 27.181005, mean_q: -39.799649, mean_eps: 0.100000\n",
      "  57201/500000: episode: 341, duration: 0.739s, episode steps: 154, steps per second: 208, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.058043, mae: 27.089108, mean_q: -39.706781, mean_eps: 0.100000\n",
      "  57348/500000: episode: 342, duration: 0.707s, episode steps: 147, steps per second: 208, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.051523, mae: 27.773029, mean_q: -40.793486, mean_eps: 0.100000\n",
      "  57500/500000: episode: 343, duration: 0.724s, episode steps: 152, steps per second: 210, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.063503, mae: 27.981468, mean_q: -41.095977, mean_eps: 0.100000\n",
      "  57615/500000: episode: 344, duration: 0.544s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.061656, mae: 28.111445, mean_q: -41.283580, mean_eps: 0.100000\n",
      "  57769/500000: episode: 345, duration: 0.728s, episode steps: 154, steps per second: 212, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.068700, mae: 27.936257, mean_q: -41.054499, mean_eps: 0.100000\n",
      "  57882/500000: episode: 346, duration: 0.538s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.076573, mae: 27.513461, mean_q: -40.418629, mean_eps: 0.100000\n",
      "  58055/500000: episode: 347, duration: 0.861s, episode steps: 173, steps per second: 201, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.142564, mae: 28.107741, mean_q: -41.221873, mean_eps: 0.100000\n",
      "  58220/500000: episode: 348, duration: 0.791s, episode steps: 165, steps per second: 209, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.094342, mae: 27.739643, mean_q: -40.645286, mean_eps: 0.100000\n",
      "  58373/500000: episode: 349, duration: 0.718s, episode steps: 153, steps per second: 213, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.069520, mae: 27.516199, mean_q: -40.348488, mean_eps: 0.100000\n",
      "  58484/500000: episode: 350, duration: 0.529s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.066503, mae: 27.551417, mean_q: -40.397314, mean_eps: 0.100000\n",
      "  58615/500000: episode: 351, duration: 0.626s, episode steps: 131, steps per second: 209, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.056750, mae: 28.220533, mean_q: -41.405058, mean_eps: 0.100000\n",
      "  58724/500000: episode: 352, duration: 0.512s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.054108, mae: 27.595833, mean_q: -40.462571, mean_eps: 0.100000\n",
      "  58834/500000: episode: 353, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.083312, mae: 28.459011, mean_q: -41.540419, mean_eps: 0.100000\n",
      "  58943/500000: episode: 354, duration: 0.525s, episode steps: 109, steps per second: 207, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.072249, mae: 27.980141, mean_q: -40.776713, mean_eps: 0.100000\n",
      "  59061/500000: episode: 355, duration: 0.551s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.061489, mae: 28.446912, mean_q: -41.501264, mean_eps: 0.100000\n",
      "  59215/500000: episode: 356, duration: 0.731s, episode steps: 154, steps per second: 211, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.336580, mae: 29.083772, mean_q: -42.268427, mean_eps: 0.100000\n",
      "  59409/500000: episode: 357, duration: 0.919s, episode steps: 194, steps per second: 211, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000],  loss: 0.307728, mae: 29.414062, mean_q: -42.719968, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  59564/500000: episode: 358, duration: 0.721s, episode steps: 155, steps per second: 215, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.143703, mae: 29.997470, mean_q: -43.653446, mean_eps: 0.100000\n",
      "  59712/500000: episode: 359, duration: 0.699s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.158786, mae: 29.836671, mean_q: -43.360060, mean_eps: 0.100000\n",
      "  59860/500000: episode: 360, duration: 0.707s, episode steps: 148, steps per second: 209, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.899 [0.000, 2.000],  loss: 0.077819, mae: 30.184907, mean_q: -44.178477, mean_eps: 0.100000\n",
      "  60009/500000: episode: 361, duration: 0.695s, episode steps: 149, steps per second: 214, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.933 [0.000, 2.000],  loss: 0.100418, mae: 30.191890, mean_q: -44.264473, mean_eps: 0.100000\n",
      "  60171/500000: episode: 362, duration: 0.771s, episode steps: 162, steps per second: 210, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.046696, mae: 29.954992, mean_q: -44.085061, mean_eps: 0.100000\n",
      "  60270/500000: episode: 363, duration: 0.470s, episode steps:  99, steps per second: 210, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.041021, mae: 29.788025, mean_q: -43.854511, mean_eps: 0.100000\n",
      "  60423/500000: episode: 364, duration: 0.710s, episode steps: 153, steps per second: 215, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.876 [0.000, 2.000],  loss: 0.047699, mae: 29.877694, mean_q: -43.898971, mean_eps: 0.100000\n",
      "  60532/500000: episode: 365, duration: 0.510s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.044782, mae: 29.724524, mean_q: -43.653839, mean_eps: 0.100000\n",
      "  60653/500000: episode: 366, duration: 0.581s, episode steps: 121, steps per second: 208, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.438 [0.000, 2.000],  loss: 0.102888, mae: 30.149309, mean_q: -44.189412, mean_eps: 0.100000\n",
      "  60768/500000: episode: 367, duration: 0.550s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 0.145523, mae: 29.891810, mean_q: -43.693677, mean_eps: 0.100000\n",
      "  60926/500000: episode: 368, duration: 0.742s, episode steps: 158, steps per second: 213, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.155833, mae: 29.919347, mean_q: -43.596336, mean_eps: 0.100000\n",
      "  61025/500000: episode: 369, duration: 0.513s, episode steps:  99, steps per second: 193, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.144216, mae: 29.646318, mean_q: -43.195625, mean_eps: 0.100000\n",
      "  61138/500000: episode: 370, duration: 0.551s, episode steps: 113, steps per second: 205, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.126993, mae: 29.204633, mean_q: -42.503457, mean_eps: 0.100000\n",
      "  61254/500000: episode: 371, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000],  loss: 0.123184, mae: 29.711786, mean_q: -43.211782, mean_eps: 0.100000\n",
      "  61366/500000: episode: 372, duration: 0.559s, episode steps: 112, steps per second: 200, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.104233, mae: 28.675456, mean_q: -41.831936, mean_eps: 0.100000\n",
      "  61558/500000: episode: 373, duration: 0.939s, episode steps: 192, steps per second: 205, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.097976, mae: 29.034156, mean_q: -42.288589, mean_eps: 0.100000\n",
      "  61674/500000: episode: 374, duration: 0.591s, episode steps: 116, steps per second: 196, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.071397, mae: 28.324985, mean_q: -41.290321, mean_eps: 0.100000\n",
      "  61792/500000: episode: 375, duration: 0.596s, episode steps: 118, steps per second: 198, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.087173, mae: 28.321220, mean_q: -41.221093, mean_eps: 0.100000\n",
      "  61900/500000: episode: 376, duration: 0.534s, episode steps: 108, steps per second: 202, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.107099, mae: 28.570105, mean_q: -41.568482, mean_eps: 0.100000\n",
      "  62017/500000: episode: 377, duration: 0.566s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.071667, mae: 28.646324, mean_q: -41.747110, mean_eps: 0.100000\n",
      "  62109/500000: episode: 378, duration: 0.454s, episode steps:  92, steps per second: 202, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.064187, mae: 27.778450, mean_q: -40.522229, mean_eps: 0.100000\n",
      "  62229/500000: episode: 379, duration: 0.599s, episode steps: 120, steps per second: 200, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.084846, mae: 28.916313, mean_q: -42.118440, mean_eps: 0.100000\n",
      "  62382/500000: episode: 380, duration: 0.760s, episode steps: 153, steps per second: 201, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.077367, mae: 28.692648, mean_q: -41.760382, mean_eps: 0.100000\n",
      "  62497/500000: episode: 381, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.083147, mae: 28.274314, mean_q: -41.182269, mean_eps: 0.100000\n",
      "  62631/500000: episode: 382, duration: 0.668s, episode steps: 134, steps per second: 201, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.085586, mae: 28.474848, mean_q: -41.395999, mean_eps: 0.100000\n",
      "  62786/500000: episode: 383, duration: 0.789s, episode steps: 155, steps per second: 196, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.961 [0.000, 2.000],  loss: 0.118188, mae: 28.673626, mean_q: -41.683643, mean_eps: 0.100000\n",
      "  62900/500000: episode: 384, duration: 0.543s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.073686, mae: 27.981142, mean_q: -40.689660, mean_eps: 0.100000\n",
      "  63007/500000: episode: 385, duration: 0.520s, episode steps: 107, steps per second: 206, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.070465, mae: 28.164170, mean_q: -40.995257, mean_eps: 0.100000\n",
      "  63120/500000: episode: 386, duration: 0.588s, episode steps: 113, steps per second: 192, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.068191, mae: 28.784598, mean_q: -41.813367, mean_eps: 0.100000\n",
      "  63278/500000: episode: 387, duration: 0.762s, episode steps: 158, steps per second: 207, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.066904, mae: 28.428643, mean_q: -41.355472, mean_eps: 0.100000\n",
      "  63405/500000: episode: 388, duration: 0.640s, episode steps: 127, steps per second: 199, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.076120, mae: 28.525381, mean_q: -41.361070, mean_eps: 0.100000\n",
      "  63520/500000: episode: 389, duration: 0.571s, episode steps: 115, steps per second: 201, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.090944, mae: 28.534375, mean_q: -41.324918, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  63638/500000: episode: 390, duration: 0.550s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.114540, mae: 28.178948, mean_q: -40.875925, mean_eps: 0.100000\n",
      "  63749/500000: episode: 391, duration: 0.532s, episode steps: 111, steps per second: 208, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.069592, mae: 27.628658, mean_q: -40.185131, mean_eps: 0.100000\n",
      "  63862/500000: episode: 392, duration: 0.555s, episode steps: 113, steps per second: 204, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.089527, mae: 27.779881, mean_q: -40.348694, mean_eps: 0.100000\n",
      "  63984/500000: episode: 393, duration: 0.581s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.074697, mae: 27.551324, mean_q: -40.019327, mean_eps: 0.100000\n",
      "  64077/500000: episode: 394, duration: 0.437s, episode steps:  93, steps per second: 213, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.112143, mae: 26.810033, mean_q: -38.919721, mean_eps: 0.100000\n",
      "  64188/500000: episode: 395, duration: 0.530s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.071555, mae: 26.543809, mean_q: -38.574205, mean_eps: 0.100000\n",
      "  64335/500000: episode: 396, duration: 0.697s, episode steps: 147, steps per second: 211, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.056785, mae: 26.930238, mean_q: -39.177537, mean_eps: 0.100000\n",
      "  64439/500000: episode: 397, duration: 0.485s, episode steps: 104, steps per second: 214, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.394 [0.000, 2.000],  loss: 0.066619, mae: 27.022985, mean_q: -39.344390, mean_eps: 0.100000\n",
      "  64602/500000: episode: 398, duration: 0.773s, episode steps: 163, steps per second: 211, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.963 [0.000, 2.000],  loss: 0.094333, mae: 27.110816, mean_q: -39.414227, mean_eps: 0.100000\n",
      "  64719/500000: episode: 399, duration: 0.569s, episode steps: 117, steps per second: 206, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.063189, mae: 27.419184, mean_q: -39.876415, mean_eps: 0.100000\n",
      "  64828/500000: episode: 400, duration: 0.520s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.115152, mae: 27.373549, mean_q: -39.716546, mean_eps: 0.100000\n",
      "  64942/500000: episode: 401, duration: 0.598s, episode steps: 114, steps per second: 191, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.081743, mae: 27.173587, mean_q: -39.439145, mean_eps: 0.100000\n",
      "  65055/500000: episode: 402, duration: 0.595s, episode steps: 113, steps per second: 190, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.077361, mae: 28.012529, mean_q: -40.761011, mean_eps: 0.100000\n",
      "  65177/500000: episode: 403, duration: 0.634s, episode steps: 122, steps per second: 192, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000],  loss: 0.066373, mae: 27.674453, mean_q: -40.265353, mean_eps: 0.100000\n",
      "  65293/500000: episode: 404, duration: 0.587s, episode steps: 116, steps per second: 198, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.068201, mae: 27.029634, mean_q: -39.387396, mean_eps: 0.100000\n",
      "  65488/500000: episode: 405, duration: 0.941s, episode steps: 195, steps per second: 207, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.087895, mae: 27.492996, mean_q: -40.034974, mean_eps: 0.100000\n",
      "  65577/500000: episode: 406, duration: 0.442s, episode steps:  89, steps per second: 202, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000],  loss: 0.057437, mae: 26.237534, mean_q: -38.178467, mean_eps: 0.100000\n",
      "  65777/500000: episode: 407, duration: 0.974s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.079771, mae: 26.632591, mean_q: -38.578102, mean_eps: 0.100000\n",
      "  65891/500000: episode: 408, duration: 0.579s, episode steps: 114, steps per second: 197, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.068690, mae: 26.405932, mean_q: -38.202985, mean_eps: 0.100000\n",
      "  66002/500000: episode: 409, duration: 0.544s, episode steps: 111, steps per second: 204, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.087293, mae: 26.520001, mean_q: -38.231774, mean_eps: 0.100000\n",
      "  66115/500000: episode: 410, duration: 0.568s, episode steps: 113, steps per second: 199, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.059559, mae: 26.725334, mean_q: -38.661199, mean_eps: 0.100000\n",
      "  66239/500000: episode: 411, duration: 0.587s, episode steps: 124, steps per second: 211, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.371 [0.000, 2.000],  loss: 0.099829, mae: 27.219761, mean_q: -39.291431, mean_eps: 0.100000\n",
      "  66364/500000: episode: 412, duration: 0.622s, episode steps: 125, steps per second: 201, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 0.115554, mae: 26.625450, mean_q: -38.481772, mean_eps: 0.100000\n",
      "  66483/500000: episode: 413, duration: 0.602s, episode steps: 119, steps per second: 198, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.095785, mae: 26.855365, mean_q: -39.005471, mean_eps: 0.100000\n",
      "  66639/500000: episode: 414, duration: 0.740s, episode steps: 156, steps per second: 211, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.093900, mae: 26.850919, mean_q: -39.137985, mean_eps: 0.100000\n",
      "  66754/500000: episode: 415, duration: 0.556s, episode steps: 115, steps per second: 207, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.087152, mae: 26.762878, mean_q: -39.040230, mean_eps: 0.100000\n",
      "  66875/500000: episode: 416, duration: 0.604s, episode steps: 121, steps per second: 200, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.080385, mae: 26.665669, mean_q: -39.015008, mean_eps: 0.100000\n",
      "  66989/500000: episode: 417, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.456 [0.000, 2.000],  loss: 0.058214, mae: 26.419834, mean_q: -38.781390, mean_eps: 0.100000\n",
      "  67077/500000: episode: 418, duration: 0.429s, episode steps:  88, steps per second: 205, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.086039, mae: 26.148469, mean_q: -38.394677, mean_eps: 0.100000\n",
      "  67193/500000: episode: 419, duration: 0.583s, episode steps: 116, steps per second: 199, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.084067, mae: 26.214263, mean_q: -38.455367, mean_eps: 0.100000\n",
      "  67377/500000: episode: 420, duration: 0.881s, episode steps: 184, steps per second: 209, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.090295, mae: 26.229285, mean_q: -38.365926, mean_eps: 0.100000\n",
      "  67552/500000: episode: 421, duration: 0.916s, episode steps: 175, steps per second: 191, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.113025, mae: 26.570033, mean_q: -38.668788, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  67641/500000: episode: 422, duration: 0.444s, episode steps:  89, steps per second: 200, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.107125, mae: 26.510517, mean_q: -38.558124, mean_eps: 0.100000\n",
      "  67750/500000: episode: 423, duration: 0.553s, episode steps: 109, steps per second: 197, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.367 [0.000, 2.000],  loss: 0.111753, mae: 26.427539, mean_q: -38.332736, mean_eps: 0.100000\n",
      "  67863/500000: episode: 424, duration: 0.568s, episode steps: 113, steps per second: 199, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.082197, mae: 26.761976, mean_q: -38.759370, mean_eps: 0.100000\n",
      "  67973/500000: episode: 425, duration: 0.541s, episode steps: 110, steps per second: 204, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.142193, mae: 26.644586, mean_q: -38.475771, mean_eps: 0.100000\n",
      "  68083/500000: episode: 426, duration: 0.535s, episode steps: 110, steps per second: 206, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.082943, mae: 27.086805, mean_q: -39.244732, mean_eps: 0.100000\n",
      "  68207/500000: episode: 427, duration: 0.605s, episode steps: 124, steps per second: 205, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.100723, mae: 26.959212, mean_q: -39.016216, mean_eps: 0.100000\n",
      "  68362/500000: episode: 428, duration: 0.780s, episode steps: 155, steps per second: 199, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.100812, mae: 27.141830, mean_q: -39.339095, mean_eps: 0.100000\n",
      "  68516/500000: episode: 429, duration: 0.730s, episode steps: 154, steps per second: 211, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.064524, mae: 26.928246, mean_q: -39.180718, mean_eps: 0.100000\n",
      "  68675/500000: episode: 430, duration: 0.758s, episode steps: 159, steps per second: 210, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000],  loss: 0.052454, mae: 27.581170, mean_q: -40.304433, mean_eps: 0.100000\n",
      "  68785/500000: episode: 431, duration: 0.532s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.048720, mae: 27.635971, mean_q: -40.440530, mean_eps: 0.100000\n",
      "  68950/500000: episode: 432, duration: 0.772s, episode steps: 165, steps per second: 214, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.049679, mae: 28.369723, mean_q: -41.611086, mean_eps: 0.100000\n",
      "  69106/500000: episode: 433, duration: 0.738s, episode steps: 156, steps per second: 211, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.859 [0.000, 2.000],  loss: 0.048236, mae: 28.985629, mean_q: -42.600570, mean_eps: 0.100000\n",
      "  69219/500000: episode: 434, duration: 0.549s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.043840, mae: 29.194571, mean_q: -42.971202, mean_eps: 0.100000\n",
      "  69372/500000: episode: 435, duration: 0.793s, episode steps: 153, steps per second: 193, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.037488, mae: 28.781883, mean_q: -42.402894, mean_eps: 0.100000\n",
      "  69484/500000: episode: 436, duration: 0.568s, episode steps: 112, steps per second: 197, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.045857, mae: 28.819574, mean_q: -42.457441, mean_eps: 0.100000\n",
      "  69646/500000: episode: 437, duration: 0.786s, episode steps: 162, steps per second: 206, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000],  loss: 0.050985, mae: 28.733946, mean_q: -42.383992, mean_eps: 0.100000\n",
      "  69767/500000: episode: 438, duration: 0.623s, episode steps: 121, steps per second: 194, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.051626, mae: 28.600225, mean_q: -42.217037, mean_eps: 0.100000\n",
      "  69865/500000: episode: 439, duration: 0.513s, episode steps:  98, steps per second: 191, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.062497, mae: 28.702265, mean_q: -42.379746, mean_eps: 0.100000\n",
      "  69986/500000: episode: 440, duration: 0.629s, episode steps: 121, steps per second: 193, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.094857, mae: 28.058784, mean_q: -41.304997, mean_eps: 0.100000\n",
      "  70100/500000: episode: 441, duration: 0.590s, episode steps: 114, steps per second: 193, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.087214, mae: 27.636401, mean_q: -40.675808, mean_eps: 0.100000\n",
      "  70251/500000: episode: 442, duration: 0.769s, episode steps: 151, steps per second: 196, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.199 [0.000, 2.000],  loss: 0.085291, mae: 28.195851, mean_q: -41.339481, mean_eps: 0.100000\n",
      "  70368/500000: episode: 443, duration: 0.605s, episode steps: 117, steps per second: 193, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.063636, mae: 27.769472, mean_q: -40.676154, mean_eps: 0.100000\n",
      "  70522/500000: episode: 444, duration: 0.779s, episode steps: 154, steps per second: 198, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.039 [0.000, 2.000],  loss: 0.086588, mae: 27.509892, mean_q: -40.189748, mean_eps: 0.100000\n",
      "  70638/500000: episode: 445, duration: 0.565s, episode steps: 116, steps per second: 205, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.080314, mae: 27.389062, mean_q: -39.924623, mean_eps: 0.100000\n",
      "  70803/500000: episode: 446, duration: 0.794s, episode steps: 165, steps per second: 208, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.075509, mae: 27.627898, mean_q: -40.278990, mean_eps: 0.100000\n",
      "  70918/500000: episode: 447, duration: 0.546s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.088653, mae: 27.498592, mean_q: -40.167464, mean_eps: 0.100000\n",
      "  71031/500000: episode: 448, duration: 0.568s, episode steps: 113, steps per second: 199, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.078797, mae: 27.250813, mean_q: -39.760493, mean_eps: 0.100000\n",
      "  71143/500000: episode: 449, duration: 0.589s, episode steps: 112, steps per second: 190, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.074704, mae: 27.431109, mean_q: -40.077723, mean_eps: 0.100000\n",
      "  71263/500000: episode: 450, duration: 0.619s, episode steps: 120, steps per second: 194, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.088829, mae: 26.981135, mean_q: -39.421121, mean_eps: 0.100000\n",
      "  71371/500000: episode: 451, duration: 0.552s, episode steps: 108, steps per second: 196, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.108618, mae: 27.082944, mean_q: -39.405468, mean_eps: 0.100000\n",
      "  71483/500000: episode: 452, duration: 0.572s, episode steps: 112, steps per second: 196, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.107481, mae: 27.631870, mean_q: -40.120242, mean_eps: 0.100000\n",
      "  71659/500000: episode: 453, duration: 0.897s, episode steps: 176, steps per second: 196, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.095499, mae: 27.410463, mean_q: -39.774169, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  71769/500000: episode: 454, duration: 0.556s, episode steps: 110, steps per second: 198, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.089155, mae: 26.982573, mean_q: -39.188975, mean_eps: 0.100000\n",
      "  71891/500000: episode: 455, duration: 0.588s, episode steps: 122, steps per second: 207, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.080471, mae: 27.058348, mean_q: -39.312880, mean_eps: 0.100000\n",
      "  72063/500000: episode: 456, duration: 0.835s, episode steps: 172, steps per second: 206, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.100014, mae: 27.441614, mean_q: -39.689430, mean_eps: 0.100000\n",
      "  72154/500000: episode: 457, duration: 0.424s, episode steps:  91, steps per second: 215, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.071722, mae: 27.778401, mean_q: -40.113588, mean_eps: 0.100000\n",
      "  72265/500000: episode: 458, duration: 0.544s, episode steps: 111, steps per second: 204, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.086231, mae: 27.826522, mean_q: -40.119697, mean_eps: 0.100000\n",
      "  72385/500000: episode: 459, duration: 0.603s, episode steps: 120, steps per second: 199, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.092703, mae: 27.929166, mean_q: -40.323009, mean_eps: 0.100000\n",
      "  72500/500000: episode: 460, duration: 0.562s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.098445, mae: 28.114316, mean_q: -40.763716, mean_eps: 0.100000\n",
      "  72656/500000: episode: 461, duration: 0.777s, episode steps: 156, steps per second: 201, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.104000, mae: 27.812644, mean_q: -40.398144, mean_eps: 0.100000\n",
      "  72775/500000: episode: 462, duration: 0.603s, episode steps: 119, steps per second: 197, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.086147, mae: 27.960567, mean_q: -40.584347, mean_eps: 0.100000\n",
      "  72890/500000: episode: 463, duration: 0.546s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000],  loss: 0.137079, mae: 26.924336, mean_q: -39.063163, mean_eps: 0.100000\n",
      "  73011/500000: episode: 464, duration: 0.563s, episode steps: 121, steps per second: 215, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000],  loss: 0.085239, mae: 26.993968, mean_q: -39.328408, mean_eps: 0.100000\n",
      "  73122/500000: episode: 465, duration: 0.572s, episode steps: 111, steps per second: 194, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.414 [0.000, 2.000],  loss: 0.046766, mae: 26.881514, mean_q: -39.342554, mean_eps: 0.100000\n",
      "  73272/500000: episode: 466, duration: 0.723s, episode steps: 150, steps per second: 208, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.050184, mae: 26.956823, mean_q: -39.494760, mean_eps: 0.100000\n",
      "  73426/500000: episode: 467, duration: 0.717s, episode steps: 154, steps per second: 215, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.044113, mae: 27.307623, mean_q: -40.088156, mean_eps: 0.100000\n",
      "  73585/500000: episode: 468, duration: 0.756s, episode steps: 159, steps per second: 210, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.056671, mae: 27.940365, mean_q: -40.996706, mean_eps: 0.100000\n",
      "  73697/500000: episode: 469, duration: 0.528s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.384 [0.000, 2.000],  loss: 0.076168, mae: 27.460352, mean_q: -40.231684, mean_eps: 0.100000\n",
      "  73877/500000: episode: 470, duration: 0.834s, episode steps: 180, steps per second: 216, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.072565, mae: 28.063180, mean_q: -41.136890, mean_eps: 0.100000\n",
      "  74044/500000: episode: 471, duration: 0.837s, episode steps: 167, steps per second: 199, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000],  loss: 0.128205, mae: 28.721277, mean_q: -42.120025, mean_eps: 0.100000\n",
      "  74220/500000: episode: 472, duration: 0.874s, episode steps: 176, steps per second: 201, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.086634, mae: 28.932961, mean_q: -42.470983, mean_eps: 0.100000\n",
      "  74308/500000: episode: 473, duration: 0.459s, episode steps:  88, steps per second: 192, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.103225, mae: 28.347133, mean_q: -41.555639, mean_eps: 0.100000\n",
      "  74410/500000: episode: 474, duration: 0.520s, episode steps: 102, steps per second: 196, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.095431, mae: 28.286958, mean_q: -41.357342, mean_eps: 0.100000\n",
      "  74520/500000: episode: 475, duration: 0.548s, episode steps: 110, steps per second: 201, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.091410, mae: 28.628731, mean_q: -41.692975, mean_eps: 0.100000\n",
      "  74671/500000: episode: 476, duration: 0.761s, episode steps: 151, steps per second: 198, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.095400, mae: 28.885018, mean_q: -42.008205, mean_eps: 0.100000\n",
      "  74764/500000: episode: 477, duration: 0.452s, episode steps:  93, steps per second: 206, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.098408, mae: 29.413032, mean_q: -42.835375, mean_eps: 0.100000\n",
      "  74956/500000: episode: 478, duration: 0.921s, episode steps: 192, steps per second: 209, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.093874, mae: 28.873235, mean_q: -41.768010, mean_eps: 0.100000\n",
      "  75067/500000: episode: 479, duration: 0.523s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.083580, mae: 29.140411, mean_q: -42.172840, mean_eps: 0.100000\n",
      "  75181/500000: episode: 480, duration: 0.547s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.103237, mae: 28.500221, mean_q: -41.112814, mean_eps: 0.100000\n",
      "  75344/500000: episode: 481, duration: 0.800s, episode steps: 163, steps per second: 204, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.095941, mae: 29.316345, mean_q: -42.273292, mean_eps: 0.100000\n",
      "  75456/500000: episode: 482, duration: 0.525s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.086794, mae: 28.641418, mean_q: -41.525584, mean_eps: 0.100000\n",
      "  75576/500000: episode: 483, duration: 0.624s, episode steps: 120, steps per second: 192, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.101109, mae: 28.208236, mean_q: -40.976461, mean_eps: 0.100000\n",
      "  75689/500000: episode: 484, duration: 0.568s, episode steps: 113, steps per second: 199, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.094106, mae: 28.213434, mean_q: -40.984834, mean_eps: 0.100000\n",
      "  75781/500000: episode: 485, duration: 0.474s, episode steps:  92, steps per second: 194, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.078336, mae: 27.644268, mean_q: -40.237408, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75897/500000: episode: 486, duration: 0.587s, episode steps: 116, steps per second: 198, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.079233, mae: 27.122192, mean_q: -39.481812, mean_eps: 0.100000\n",
      "  76012/500000: episode: 487, duration: 0.588s, episode steps: 115, steps per second: 196, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.067654, mae: 27.057131, mean_q: -39.381423, mean_eps: 0.100000\n",
      "  76126/500000: episode: 488, duration: 0.583s, episode steps: 114, steps per second: 195, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.072327, mae: 27.077725, mean_q: -39.355659, mean_eps: 0.100000\n",
      "  76236/500000: episode: 489, duration: 0.548s, episode steps: 110, steps per second: 201, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.056674, mae: 26.857721, mean_q: -39.101484, mean_eps: 0.100000\n",
      "  76350/500000: episode: 490, duration: 0.580s, episode steps: 114, steps per second: 197, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 0.038178, mae: 26.035478, mean_q: -37.876663, mean_eps: 0.100000\n",
      "  76470/500000: episode: 491, duration: 0.615s, episode steps: 120, steps per second: 195, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.045638, mae: 26.746794, mean_q: -38.908359, mean_eps: 0.100000\n",
      "  76580/500000: episode: 492, duration: 0.552s, episode steps: 110, steps per second: 199, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.079455, mae: 27.381814, mean_q: -39.812382, mean_eps: 0.100000\n",
      "  76714/500000: episode: 493, duration: 0.650s, episode steps: 134, steps per second: 206, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.067833, mae: 27.295785, mean_q: -39.600588, mean_eps: 0.100000\n",
      "  76833/500000: episode: 494, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.133878, mae: 27.614224, mean_q: -39.906571, mean_eps: 0.100000\n",
      "  76923/500000: episode: 495, duration: 0.456s, episode steps:  90, steps per second: 197, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 0.155012, mae: 27.434869, mean_q: -39.431714, mean_eps: 0.100000\n",
      "  77031/500000: episode: 496, duration: 0.512s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.103268, mae: 27.253122, mean_q: -39.282670, mean_eps: 0.100000\n",
      "  77164/500000: episode: 497, duration: 0.657s, episode steps: 133, steps per second: 202, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.100537, mae: 27.312726, mean_q: -39.340274, mean_eps: 0.100000\n",
      "  77278/500000: episode: 498, duration: 0.576s, episode steps: 114, steps per second: 198, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.123673, mae: 27.375469, mean_q: -39.300310, mean_eps: 0.100000\n",
      "  77386/500000: episode: 499, duration: 0.560s, episode steps: 108, steps per second: 193, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.150706, mae: 27.163951, mean_q: -38.874857, mean_eps: 0.100000\n",
      "  77475/500000: episode: 500, duration: 0.450s, episode steps:  89, steps per second: 198, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.119643, mae: 26.932749, mean_q: -38.438507, mean_eps: 0.100000\n",
      "  77599/500000: episode: 501, duration: 0.618s, episode steps: 124, steps per second: 201, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.120032, mae: 27.000443, mean_q: -38.521939, mean_eps: 0.100000\n",
      "  77717/500000: episode: 502, duration: 0.566s, episode steps: 118, steps per second: 208, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.087239, mae: 26.595080, mean_q: -38.101257, mean_eps: 0.100000\n",
      "  77828/500000: episode: 503, duration: 0.547s, episode steps: 111, steps per second: 203, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.089249, mae: 26.651300, mean_q: -38.249379, mean_eps: 0.100000\n",
      "  77919/500000: episode: 504, duration: 0.434s, episode steps:  91, steps per second: 210, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.066103, mae: 25.936717, mean_q: -37.336615, mean_eps: 0.100000\n",
      "  78014/500000: episode: 505, duration: 0.489s, episode steps:  95, steps per second: 194, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.056186, mae: 26.115120, mean_q: -37.546591, mean_eps: 0.100000\n",
      "  78123/500000: episode: 506, duration: 0.515s, episode steps: 109, steps per second: 212, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.064271, mae: 25.470284, mean_q: -36.460252, mean_eps: 0.100000\n",
      "  78293/500000: episode: 507, duration: 0.816s, episode steps: 170, steps per second: 208, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.894 [0.000, 2.000],  loss: 0.083793, mae: 25.765336, mean_q: -37.057432, mean_eps: 0.100000\n",
      "  78487/500000: episode: 508, duration: 0.948s, episode steps: 194, steps per second: 205, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.082081, mae: 26.563456, mean_q: -38.343389, mean_eps: 0.100000\n",
      "  78593/500000: episode: 509, duration: 0.520s, episode steps: 106, steps per second: 204, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.087395, mae: 26.390463, mean_q: -38.169725, mean_eps: 0.100000\n",
      "  78698/500000: episode: 510, duration: 0.500s, episode steps: 105, steps per second: 210, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.080413, mae: 26.367792, mean_q: -38.054823, mean_eps: 0.100000\n",
      "  78809/500000: episode: 511, duration: 0.557s, episode steps: 111, steps per second: 199, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.136918, mae: 26.349415, mean_q: -38.044116, mean_eps: 0.100000\n",
      "  78921/500000: episode: 512, duration: 0.542s, episode steps: 112, steps per second: 207, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.113064, mae: 26.206953, mean_q: -37.863263, mean_eps: 0.100000\n",
      "  79039/500000: episode: 513, duration: 0.555s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.110515, mae: 26.322000, mean_q: -38.131614, mean_eps: 0.100000\n",
      "  79147/500000: episode: 514, duration: 0.541s, episode steps: 108, steps per second: 200, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.129144, mae: 26.299678, mean_q: -37.962582, mean_eps: 0.100000\n",
      "  79325/500000: episode: 515, duration: 0.871s, episode steps: 178, steps per second: 204, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.154543, mae: 26.698506, mean_q: -38.513473, mean_eps: 0.100000\n",
      "  79437/500000: episode: 516, duration: 0.555s, episode steps: 112, steps per second: 202, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.137607, mae: 25.923318, mean_q: -37.398600, mean_eps: 0.100000\n",
      "  79612/500000: episode: 517, duration: 0.860s, episode steps: 175, steps per second: 204, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000],  loss: 0.121260, mae: 26.340490, mean_q: -38.012710, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79723/500000: episode: 518, duration: 0.546s, episode steps: 111, steps per second: 203, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.114863, mae: 26.693617, mean_q: -38.336067, mean_eps: 0.100000\n",
      "  79840/500000: episode: 519, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.133589, mae: 26.783760, mean_q: -38.462784, mean_eps: 0.100000\n",
      "  79956/500000: episode: 520, duration: 0.569s, episode steps: 116, steps per second: 204, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.145108, mae: 26.897982, mean_q: -38.554693, mean_eps: 0.100000\n",
      "  80063/500000: episode: 521, duration: 0.533s, episode steps: 107, steps per second: 201, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.091796, mae: 26.573911, mean_q: -38.175737, mean_eps: 0.100000\n",
      "  80161/500000: episode: 522, duration: 0.486s, episode steps:  98, steps per second: 202, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.070789, mae: 26.959363, mean_q: -38.755433, mean_eps: 0.100000\n",
      "  80299/500000: episode: 523, duration: 0.667s, episode steps: 138, steps per second: 207, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.442 [0.000, 2.000],  loss: 0.081934, mae: 25.951789, mean_q: -37.043093, mean_eps: 0.100000\n",
      "  80407/500000: episode: 524, duration: 0.553s, episode steps: 108, steps per second: 195, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.116812, mae: 26.399946, mean_q: -37.469026, mean_eps: 0.100000\n",
      "  80495/500000: episode: 525, duration: 0.467s, episode steps:  88, steps per second: 189, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.089585, mae: 25.810584, mean_q: -36.578426, mean_eps: 0.100000\n",
      "  80630/500000: episode: 526, duration: 0.701s, episode steps: 135, steps per second: 193, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.100345, mae: 26.007283, mean_q: -36.954544, mean_eps: 0.100000\n",
      "  80742/500000: episode: 527, duration: 0.520s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.072173, mae: 25.647282, mean_q: -36.493588, mean_eps: 0.100000\n",
      "  80907/500000: episode: 528, duration: 0.785s, episode steps: 165, steps per second: 210, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.092377, mae: 26.165607, mean_q: -37.261994, mean_eps: 0.100000\n",
      "  81099/500000: episode: 529, duration: 0.884s, episode steps: 192, steps per second: 217, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.107469, mae: 26.898235, mean_q: -38.532171, mean_eps: 0.100000\n",
      "  81211/500000: episode: 530, duration: 0.529s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.073164, mae: 27.625123, mean_q: -39.951904, mean_eps: 0.100000\n",
      "  81320/500000: episode: 531, duration: 0.527s, episode steps: 109, steps per second: 207, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.061330, mae: 27.093175, mean_q: -39.416789, mean_eps: 0.100000\n",
      "  81520/500000: episode: 532, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.069378, mae: 27.746320, mean_q: -40.439094, mean_eps: 0.100000\n",
      "  81637/500000: episode: 533, duration: 0.566s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.436 [0.000, 2.000],  loss: 0.463967, mae: 27.791313, mean_q: -40.307223, mean_eps: 0.100000\n",
      "  81764/500000: episode: 534, duration: 0.605s, episode steps: 127, steps per second: 210, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.417 [0.000, 2.000],  loss: 0.313036, mae: 27.717282, mean_q: -40.345360, mean_eps: 0.100000\n",
      "  81924/500000: episode: 535, duration: 0.759s, episode steps: 160, steps per second: 211, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.551921, mae: 27.292555, mean_q: -39.466094, mean_eps: 0.100000\n",
      "  82023/500000: episode: 536, duration: 0.500s, episode steps:  99, steps per second: 198, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.307428, mae: 26.668340, mean_q: -38.525701, mean_eps: 0.100000\n",
      "  82129/500000: episode: 537, duration: 0.529s, episode steps: 106, steps per second: 200, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.288799, mae: 26.492942, mean_q: -38.191527, mean_eps: 0.100000\n",
      "  82245/500000: episode: 538, duration: 0.580s, episode steps: 116, steps per second: 200, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.368931, mae: 27.006817, mean_q: -38.887245, mean_eps: 0.100000\n",
      "  82363/500000: episode: 539, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.300157, mae: 27.282246, mean_q: -39.302362, mean_eps: 0.100000\n",
      "  82471/500000: episode: 540, duration: 0.642s, episode steps: 108, steps per second: 168, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.307202, mae: 27.099901, mean_q: -38.969650, mean_eps: 0.100000\n",
      "  82588/500000: episode: 541, duration: 0.667s, episode steps: 117, steps per second: 176, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.200382, mae: 26.249306, mean_q: -37.830326, mean_eps: 0.100000\n",
      "  82702/500000: episode: 542, duration: 0.566s, episode steps: 114, steps per second: 201, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.046386, mae: 26.287770, mean_q: -38.038398, mean_eps: 0.100000\n",
      "  82813/500000: episode: 543, duration: 0.580s, episode steps: 111, steps per second: 191, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.047962, mae: 26.118158, mean_q: -37.753852, mean_eps: 0.100000\n",
      "  82929/500000: episode: 544, duration: 0.600s, episode steps: 116, steps per second: 193, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000],  loss: 0.043365, mae: 25.994456, mean_q: -37.622947, mean_eps: 0.100000\n",
      "  83043/500000: episode: 545, duration: 0.591s, episode steps: 114, steps per second: 193, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.041037, mae: 25.563328, mean_q: -37.008906, mean_eps: 0.100000\n",
      "  83155/500000: episode: 546, duration: 0.572s, episode steps: 112, steps per second: 196, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.042434, mae: 26.084768, mean_q: -37.874557, mean_eps: 0.100000\n",
      "  83264/500000: episode: 547, duration: 0.553s, episode steps: 109, steps per second: 197, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.033484, mae: 25.687698, mean_q: -37.358539, mean_eps: 0.100000\n",
      "  83378/500000: episode: 548, duration: 0.581s, episode steps: 114, steps per second: 196, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.029297, mae: 25.710919, mean_q: -37.453639, mean_eps: 0.100000\n",
      "  83485/500000: episode: 549, duration: 0.546s, episode steps: 107, steps per second: 196, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.031118, mae: 25.240001, mean_q: -36.844074, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  83595/500000: episode: 550, duration: 0.558s, episode steps: 110, steps per second: 197, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.032774, mae: 25.463075, mean_q: -37.119617, mean_eps: 0.100000\n",
      "  83709/500000: episode: 551, duration: 0.569s, episode steps: 114, steps per second: 200, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.044866, mae: 25.522109, mean_q: -37.225617, mean_eps: 0.100000\n",
      "  83820/500000: episode: 552, duration: 0.545s, episode steps: 111, steps per second: 204, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.036374, mae: 25.499770, mean_q: -37.157387, mean_eps: 0.100000\n",
      "  83993/500000: episode: 553, duration: 0.843s, episode steps: 173, steps per second: 205, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.061120, mae: 25.908500, mean_q: -37.739897, mean_eps: 0.100000\n",
      "  84193/500000: episode: 554, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.083174, mae: 25.908908, mean_q: -37.498056, mean_eps: 0.100000\n",
      "  84300/500000: episode: 555, duration: 0.502s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.375189, mae: 26.493526, mean_q: -37.979836, mean_eps: 0.100000\n",
      "  84410/500000: episode: 556, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.257760, mae: 26.778425, mean_q: -38.502311, mean_eps: 0.100000\n",
      "  84524/500000: episode: 557, duration: 0.545s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.113873, mae: 26.630449, mean_q: -38.527909, mean_eps: 0.100000\n",
      "  84722/500000: episode: 558, duration: 0.918s, episode steps: 198, steps per second: 216, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.343 [0.000, 2.000],  loss: 0.236273, mae: 26.739435, mean_q: -38.579862, mean_eps: 0.100000\n",
      "  84838/500000: episode: 559, duration: 0.544s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.371 [0.000, 2.000],  loss: 0.182260, mae: 26.551026, mean_q: -38.499045, mean_eps: 0.100000\n",
      "  84952/500000: episode: 560, duration: 0.547s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.214063, mae: 26.322902, mean_q: -38.150610, mean_eps: 0.100000\n",
      "  85073/500000: episode: 561, duration: 0.569s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.143360, mae: 26.868860, mean_q: -38.903315, mean_eps: 0.100000\n",
      "  85190/500000: episode: 562, duration: 0.544s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.189385, mae: 26.585504, mean_q: -38.517901, mean_eps: 0.100000\n",
      "  85302/500000: episode: 563, duration: 0.531s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.059298, mae: 25.705187, mean_q: -37.296003, mean_eps: 0.100000\n",
      "  85413/500000: episode: 564, duration: 0.530s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.069469, mae: 25.883463, mean_q: -37.491748, mean_eps: 0.100000\n",
      "  85520/500000: episode: 565, duration: 0.510s, episode steps: 107, steps per second: 210, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.085139, mae: 26.028937, mean_q: -37.705690, mean_eps: 0.100000\n",
      "  85629/500000: episode: 566, duration: 0.516s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.066267, mae: 25.876327, mean_q: -37.517810, mean_eps: 0.100000\n",
      "  85808/500000: episode: 567, duration: 0.888s, episode steps: 179, steps per second: 202, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.056089, mae: 25.791293, mean_q: -37.406093, mean_eps: 0.100000\n",
      "  85971/500000: episode: 568, duration: 0.824s, episode steps: 163, steps per second: 198, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.071539, mae: 26.186448, mean_q: -37.985628, mean_eps: 0.100000\n",
      "  86131/500000: episode: 569, duration: 0.785s, episode steps: 160, steps per second: 204, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.086095, mae: 26.781777, mean_q: -39.013050, mean_eps: 0.100000\n",
      "  86309/500000: episode: 570, duration: 0.933s, episode steps: 178, steps per second: 191, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.085069, mae: 27.538952, mean_q: -40.250085, mean_eps: 0.100000\n",
      "  86467/500000: episode: 571, duration: 0.802s, episode steps: 158, steps per second: 197, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.086948, mae: 28.051352, mean_q: -41.117855, mean_eps: 0.100000\n",
      "  86628/500000: episode: 572, duration: 0.821s, episode steps: 161, steps per second: 196, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.074320, mae: 28.914722, mean_q: -42.509142, mean_eps: 0.100000\n",
      "  86790/500000: episode: 573, duration: 0.817s, episode steps: 162, steps per second: 198, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.064812, mae: 29.454570, mean_q: -43.410348, mean_eps: 0.100000\n",
      "  86955/500000: episode: 574, duration: 0.840s, episode steps: 165, steps per second: 196, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.069992, mae: 29.470301, mean_q: -43.470107, mean_eps: 0.100000\n",
      "  87131/500000: episode: 575, duration: 0.888s, episode steps: 176, steps per second: 198, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.046893, mae: 29.538246, mean_q: -43.623810, mean_eps: 0.100000\n",
      "  87249/500000: episode: 576, duration: 0.591s, episode steps: 118, steps per second: 200, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000],  loss: 0.051930, mae: 28.892959, mean_q: -42.623496, mean_eps: 0.100000\n",
      "  87425/500000: episode: 577, duration: 0.852s, episode steps: 176, steps per second: 206, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.256 [0.000, 2.000],  loss: 0.084913, mae: 28.454425, mean_q: -41.824452, mean_eps: 0.100000\n",
      "  87625/500000: episode: 578, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000],  loss: 0.080076, mae: 28.896711, mean_q: -42.379118, mean_eps: 0.100000\n",
      "  87728/500000: episode: 579, duration: 0.521s, episode steps: 103, steps per second: 198, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.794602, mae: 29.143669, mean_q: -42.280071, mean_eps: 0.100000\n",
      "  87839/500000: episode: 580, duration: 0.541s, episode steps: 111, steps per second: 205, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.354649, mae: 29.079961, mean_q: -42.231221, mean_eps: 0.100000\n",
      "  87958/500000: episode: 581, duration: 0.594s, episode steps: 119, steps per second: 200, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.203199, mae: 29.089217, mean_q: -42.059848, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  88071/500000: episode: 582, duration: 0.528s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.292105, mae: 28.779813, mean_q: -41.551529, mean_eps: 0.100000\n",
      "  88188/500000: episode: 583, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.255159, mae: 28.261227, mean_q: -40.779841, mean_eps: 0.100000\n",
      "  88357/500000: episode: 584, duration: 0.793s, episode steps: 169, steps per second: 213, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.600476, mae: 28.706470, mean_q: -41.362592, mean_eps: 0.100000\n",
      "  88477/500000: episode: 585, duration: 0.555s, episode steps: 120, steps per second: 216, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.270274, mae: 28.818471, mean_q: -41.506932, mean_eps: 0.100000\n",
      "  88590/500000: episode: 586, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.297618, mae: 28.379976, mean_q: -40.869668, mean_eps: 0.100000\n",
      "  88698/500000: episode: 587, duration: 0.520s, episode steps: 108, steps per second: 208, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.098680, mae: 27.377225, mean_q: -39.442205, mean_eps: 0.100000\n",
      "  88811/500000: episode: 588, duration: 0.522s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.070148, mae: 26.968881, mean_q: -39.026313, mean_eps: 0.100000\n",
      "  88980/500000: episode: 589, duration: 0.793s, episode steps: 169, steps per second: 213, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.044057, mae: 27.213435, mean_q: -39.637073, mean_eps: 0.100000\n",
      "  89102/500000: episode: 590, duration: 0.578s, episode steps: 122, steps per second: 211, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.043303, mae: 27.470730, mean_q: -40.095998, mean_eps: 0.100000\n",
      "  89212/500000: episode: 591, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.052204, mae: 27.394188, mean_q: -39.979849, mean_eps: 0.100000\n",
      "  89384/500000: episode: 592, duration: 0.793s, episode steps: 172, steps per second: 217, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.849 [0.000, 2.000],  loss: 0.042730, mae: 27.029050, mean_q: -39.464929, mean_eps: 0.100000\n",
      "  89500/500000: episode: 593, duration: 0.553s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.059735, mae: 27.116913, mean_q: -39.666419, mean_eps: 0.100000\n",
      "  89664/500000: episode: 594, duration: 0.762s, episode steps: 164, steps per second: 215, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.939 [0.000, 2.000],  loss: 0.054722, mae: 27.396252, mean_q: -40.203504, mean_eps: 0.100000\n",
      "  89815/500000: episode: 595, duration: 0.705s, episode steps: 151, steps per second: 214, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.050953, mae: 28.327041, mean_q: -41.726530, mean_eps: 0.100000\n",
      "  89992/500000: episode: 596, duration: 0.860s, episode steps: 177, steps per second: 206, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.143265, mae: 28.010417, mean_q: -41.125901, mean_eps: 0.100000\n",
      "  90105/500000: episode: 597, duration: 0.560s, episode steps: 113, steps per second: 202, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.092585, mae: 27.955692, mean_q: -40.980847, mean_eps: 0.100000\n",
      "  90277/500000: episode: 598, duration: 0.807s, episode steps: 172, steps per second: 213, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.108358, mae: 29.045615, mean_q: -42.535008, mean_eps: 0.100000\n",
      "  90394/500000: episode: 599, duration: 0.559s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.061730, mae: 28.124433, mean_q: -41.213854, mean_eps: 0.100000\n",
      "  90508/500000: episode: 600, duration: 0.567s, episode steps: 114, steps per second: 201, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.101789, mae: 28.965344, mean_q: -42.252635, mean_eps: 0.100000\n",
      "  90675/500000: episode: 601, duration: 0.811s, episode steps: 167, steps per second: 206, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.141357, mae: 29.198856, mean_q: -42.391588, mean_eps: 0.100000\n",
      "  90850/500000: episode: 602, duration: 0.886s, episode steps: 175, steps per second: 197, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.063 [0.000, 2.000],  loss: 0.126968, mae: 29.037962, mean_q: -42.038227, mean_eps: 0.100000\n",
      "  90990/500000: episode: 603, duration: 0.701s, episode steps: 140, steps per second: 200, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.150300, mae: 29.463323, mean_q: -42.586928, mean_eps: 0.100000\n",
      "  91099/500000: episode: 604, duration: 0.532s, episode steps: 109, steps per second: 205, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.404 [0.000, 2.000],  loss: 0.224069, mae: 29.139787, mean_q: -42.070216, mean_eps: 0.100000\n",
      "  91211/500000: episode: 605, duration: 0.581s, episode steps: 112, steps per second: 193, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.411 [0.000, 2.000],  loss: 0.107854, mae: 28.776792, mean_q: -41.610647, mean_eps: 0.100000\n",
      "  91331/500000: episode: 606, duration: 0.619s, episode steps: 120, steps per second: 194, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.102283, mae: 28.376293, mean_q: -40.922100, mean_eps: 0.100000\n",
      "  91507/500000: episode: 607, duration: 0.868s, episode steps: 176, steps per second: 203, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.140726, mae: 28.758773, mean_q: -41.374644, mean_eps: 0.100000\n",
      "  91625/500000: episode: 608, duration: 0.572s, episode steps: 118, steps per second: 206, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.132997, mae: 28.580073, mean_q: -41.101635, mean_eps: 0.100000\n",
      "  91740/500000: episode: 609, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.147685, mae: 27.731898, mean_q: -39.940271, mean_eps: 0.100000\n",
      "  91852/500000: episode: 610, duration: 0.564s, episode steps: 112, steps per second: 199, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.163060, mae: 27.618524, mean_q: -39.716430, mean_eps: 0.100000\n",
      "  91993/500000: episode: 611, duration: 0.686s, episode steps: 141, steps per second: 206, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.151258, mae: 27.267039, mean_q: -39.382925, mean_eps: 0.100000\n",
      "  92178/500000: episode: 612, duration: 0.891s, episode steps: 185, steps per second: 208, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.127691, mae: 27.385990, mean_q: -39.591339, mean_eps: 0.100000\n",
      "  92290/500000: episode: 613, duration: 0.554s, episode steps: 112, steps per second: 202, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.154799, mae: 27.684638, mean_q: -39.954117, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  92407/500000: episode: 614, duration: 0.586s, episode steps: 117, steps per second: 200, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.108286, mae: 27.151942, mean_q: -39.383625, mean_eps: 0.100000\n",
      "  92517/500000: episode: 615, duration: 0.571s, episode steps: 110, steps per second: 193, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.083894, mae: 27.108885, mean_q: -39.426836, mean_eps: 0.100000\n",
      "  92627/500000: episode: 616, duration: 0.567s, episode steps: 110, steps per second: 194, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.096573, mae: 26.970681, mean_q: -39.188912, mean_eps: 0.100000\n",
      "  92739/500000: episode: 617, duration: 0.584s, episode steps: 112, steps per second: 192, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.100588, mae: 26.980174, mean_q: -39.135423, mean_eps: 0.100000\n",
      "  92848/500000: episode: 618, duration: 0.529s, episode steps: 109, steps per second: 206, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.080473, mae: 27.529793, mean_q: -39.857434, mean_eps: 0.100000\n",
      "  92976/500000: episode: 619, duration: 0.598s, episode steps: 128, steps per second: 214, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.091933, mae: 26.995282, mean_q: -38.934973, mean_eps: 0.100000\n",
      "  93087/500000: episode: 620, duration: 0.549s, episode steps: 111, steps per second: 202, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.423 [0.000, 2.000],  loss: 0.107650, mae: 26.727073, mean_q: -38.489278, mean_eps: 0.100000\n",
      "  93280/500000: episode: 621, duration: 0.951s, episode steps: 193, steps per second: 203, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.097403, mae: 26.571578, mean_q: -38.360208, mean_eps: 0.100000\n",
      "  93391/500000: episode: 622, duration: 0.551s, episode steps: 111, steps per second: 201, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.378 [0.000, 2.000],  loss: 0.080892, mae: 26.778551, mean_q: -38.755456, mean_eps: 0.100000\n",
      "  93529/500000: episode: 623, duration: 0.687s, episode steps: 138, steps per second: 201, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.090948, mae: 27.150991, mean_q: -39.365445, mean_eps: 0.100000\n",
      "  93654/500000: episode: 624, duration: 0.593s, episode steps: 125, steps per second: 211, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.102796, mae: 26.984766, mean_q: -39.142410, mean_eps: 0.100000\n",
      "  93833/500000: episode: 625, duration: 0.869s, episode steps: 179, steps per second: 206, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.073699, mae: 27.027598, mean_q: -39.395491, mean_eps: 0.100000\n",
      "  93945/500000: episode: 626, duration: 0.619s, episode steps: 112, steps per second: 181, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.134707, mae: 27.302301, mean_q: -39.753113, mean_eps: 0.100000\n",
      "  94052/500000: episode: 627, duration: 0.520s, episode steps: 107, steps per second: 206, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.085040, mae: 27.200859, mean_q: -39.672110, mean_eps: 0.100000\n",
      "  94160/500000: episode: 628, duration: 0.533s, episode steps: 108, steps per second: 202, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.076076, mae: 27.408940, mean_q: -39.908684, mean_eps: 0.100000\n",
      "  94276/500000: episode: 629, duration: 0.600s, episode steps: 116, steps per second: 193, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.070530, mae: 26.721697, mean_q: -38.923429, mean_eps: 0.100000\n",
      "  94391/500000: episode: 630, duration: 0.562s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.091224, mae: 27.101861, mean_q: -39.388597, mean_eps: 0.100000\n",
      "  94506/500000: episode: 631, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.086678, mae: 26.946947, mean_q: -39.003343, mean_eps: 0.100000\n",
      "  94706/500000: episode: 632, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.112822, mae: 27.623287, mean_q: -39.844847, mean_eps: 0.100000\n",
      "  94824/500000: episode: 633, duration: 0.564s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.259578, mae: 27.354229, mean_q: -39.267700, mean_eps: 0.100000\n",
      "  94960/500000: episode: 634, duration: 0.660s, episode steps: 136, steps per second: 206, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.243496, mae: 27.581523, mean_q: -39.776828, mean_eps: 0.100000\n",
      "  95076/500000: episode: 635, duration: 0.550s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.247059, mae: 27.412493, mean_q: -39.498603, mean_eps: 0.100000\n",
      "  95189/500000: episode: 636, duration: 0.542s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.178754, mae: 27.284440, mean_q: -39.316640, mean_eps: 0.100000\n",
      "  95306/500000: episode: 637, duration: 0.556s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.232849, mae: 27.798581, mean_q: -40.096091, mean_eps: 0.100000\n",
      "  95481/500000: episode: 638, duration: 0.807s, episode steps: 175, steps per second: 217, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000],  loss: 0.219608, mae: 27.943093, mean_q: -40.325877, mean_eps: 0.100000\n",
      "  95681/500000: episode: 639, duration: 0.946s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.215579, mae: 27.945408, mean_q: -40.308634, mean_eps: 0.100000\n",
      "  95849/500000: episode: 640, duration: 0.784s, episode steps: 168, steps per second: 214, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.601 [0.000, 2.000],  loss: 0.438822, mae: 28.394261, mean_q: -40.594363, mean_eps: 0.100000\n",
      "  95962/500000: episode: 641, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000],  loss: 0.174629, mae: 28.309795, mean_q: -40.419060, mean_eps: 0.100000\n",
      "  96079/500000: episode: 642, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.336983, mae: 28.396368, mean_q: -40.423418, mean_eps: 0.100000\n",
      "  96227/500000: episode: 643, duration: 0.689s, episode steps: 148, steps per second: 215, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.271293, mae: 28.604604, mean_q: -40.830056, mean_eps: 0.100000\n",
      "  96399/500000: episode: 644, duration: 0.802s, episode steps: 172, steps per second: 214, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.341908, mae: 28.980823, mean_q: -41.597226, mean_eps: 0.100000\n",
      "  96516/500000: episode: 645, duration: 0.559s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.195962, mae: 28.758252, mean_q: -41.297996, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  96716/500000: episode: 646, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 0.227847, mae: 28.651766, mean_q: -41.149659, mean_eps: 0.100000\n",
      "  96909/500000: episode: 647, duration: 0.905s, episode steps: 193, steps per second: 213, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.161524, mae: 28.876137, mean_q: -41.660148, mean_eps: 0.100000\n",
      "  97019/500000: episode: 648, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.181765, mae: 28.964140, mean_q: -41.869376, mean_eps: 0.100000\n",
      "  97135/500000: episode: 649, duration: 0.541s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 0.173339, mae: 28.956719, mean_q: -41.737002, mean_eps: 0.100000\n",
      "  97250/500000: episode: 650, duration: 0.538s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.173022, mae: 28.040727, mean_q: -40.166677, mean_eps: 0.100000\n",
      "  97360/500000: episode: 651, duration: 0.529s, episode steps: 110, steps per second: 208, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.169371, mae: 27.831433, mean_q: -39.858643, mean_eps: 0.100000\n",
      "  97477/500000: episode: 652, duration: 0.542s, episode steps: 117, steps per second: 216, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.123510, mae: 27.617490, mean_q: -39.522133, mean_eps: 0.100000\n",
      "  97612/500000: episode: 653, duration: 0.629s, episode steps: 135, steps per second: 215, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 0.153026, mae: 27.780574, mean_q: -39.892654, mean_eps: 0.100000\n",
      "  97730/500000: episode: 654, duration: 0.564s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.115815, mae: 27.246777, mean_q: -39.134443, mean_eps: 0.100000\n",
      "  97925/500000: episode: 655, duration: 0.903s, episode steps: 195, steps per second: 216, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.123 [0.000, 2.000],  loss: 0.067200, mae: 27.506808, mean_q: -39.643234, mean_eps: 0.100000\n",
      "  98040/500000: episode: 656, duration: 0.536s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.091631, mae: 27.014819, mean_q: -39.035092, mean_eps: 0.100000\n",
      "  98150/500000: episode: 657, duration: 0.526s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.070722, mae: 27.477959, mean_q: -39.827024, mean_eps: 0.100000\n",
      "  98263/500000: episode: 658, duration: 0.531s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.077835, mae: 26.673828, mean_q: -38.770773, mean_eps: 0.100000\n",
      "  98358/500000: episode: 659, duration: 0.443s, episode steps:  95, steps per second: 214, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.069243, mae: 26.726904, mean_q: -38.886789, mean_eps: 0.100000\n",
      "  98467/500000: episode: 660, duration: 0.511s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.082520, mae: 26.941119, mean_q: -39.148648, mean_eps: 0.100000\n",
      "  98580/500000: episode: 661, duration: 0.542s, episode steps: 113, steps per second: 208, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.073891, mae: 26.722186, mean_q: -38.829531, mean_eps: 0.100000\n",
      "  98695/500000: episode: 662, duration: 0.543s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.092754, mae: 26.326788, mean_q: -38.210933, mean_eps: 0.100000\n",
      "  98816/500000: episode: 663, duration: 0.561s, episode steps: 121, steps per second: 216, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.137092, mae: 25.521080, mean_q: -37.021432, mean_eps: 0.100000\n",
      "  98931/500000: episode: 664, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.042981, mae: 25.057342, mean_q: -36.378362, mean_eps: 0.100000\n",
      "  99035/500000: episode: 665, duration: 0.493s, episode steps: 104, steps per second: 211, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.065193, mae: 25.333024, mean_q: -36.678479, mean_eps: 0.100000\n",
      "  99145/500000: episode: 666, duration: 0.518s, episode steps: 110, steps per second: 212, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.062005, mae: 25.314782, mean_q: -36.646237, mean_eps: 0.100000\n",
      "  99261/500000: episode: 667, duration: 0.536s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.114510, mae: 25.270231, mean_q: -36.568801, mean_eps: 0.100000\n",
      "  99382/500000: episode: 668, duration: 0.568s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.173944, mae: 25.787221, mean_q: -37.194493, mean_eps: 0.100000\n",
      "  99492/500000: episode: 669, duration: 0.519s, episode steps: 110, steps per second: 212, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.053013, mae: 25.236126, mean_q: -36.392079, mean_eps: 0.100000\n",
      "  99610/500000: episode: 670, duration: 0.553s, episode steps: 118, steps per second: 213, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.054618, mae: 25.505790, mean_q: -36.837337, mean_eps: 0.100000\n",
      "  99810/500000: episode: 671, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.105941, mae: 26.115543, mean_q: -37.522913, mean_eps: 0.100000\n",
      "  99987/500000: episode: 672, duration: 0.832s, episode steps: 177, steps per second: 213, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.017 [0.000, 2.000],  loss: 0.096476, mae: 26.601495, mean_q: -38.110950, mean_eps: 0.100000\n",
      " 100172/500000: episode: 673, duration: 0.851s, episode steps: 185, steps per second: 217, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.155594, mae: 27.960585, mean_q: -40.199279, mean_eps: 0.100000\n",
      " 100286/500000: episode: 674, duration: 0.538s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.118883, mae: 27.963318, mean_q: -40.313350, mean_eps: 0.100000\n",
      " 100453/500000: episode: 675, duration: 0.778s, episode steps: 167, steps per second: 215, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.142870, mae: 29.100296, mean_q: -41.851343, mean_eps: 0.100000\n",
      " 100562/500000: episode: 676, duration: 0.503s, episode steps: 109, steps per second: 217, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.136562, mae: 29.266928, mean_q: -42.041113, mean_eps: 0.100000\n",
      " 100674/500000: episode: 677, duration: 0.527s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.161987, mae: 29.707432, mean_q: -42.705475, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100771/500000: episode: 678, duration: 0.462s, episode steps:  97, steps per second: 210, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.206 [0.000, 2.000],  loss: 0.126945, mae: 28.804437, mean_q: -41.367316, mean_eps: 0.100000\n",
      " 100860/500000: episode: 679, duration: 0.412s, episode steps:  89, steps per second: 216, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.139664, mae: 28.914821, mean_q: -41.299583, mean_eps: 0.100000\n",
      " 100982/500000: episode: 680, duration: 0.562s, episode steps: 122, steps per second: 217, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.105717, mae: 28.271309, mean_q: -40.424785, mean_eps: 0.100000\n",
      " 101085/500000: episode: 681, duration: 0.484s, episode steps: 103, steps per second: 213, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.116160, mae: 27.925840, mean_q: -39.615370, mean_eps: 0.100000\n",
      " 101196/500000: episode: 682, duration: 0.527s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.123553, mae: 27.366327, mean_q: -38.554514, mean_eps: 0.100000\n",
      " 101295/500000: episode: 683, duration: 0.462s, episode steps:  99, steps per second: 214, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.091840, mae: 26.821270, mean_q: -37.842772, mean_eps: 0.100000\n",
      " 101417/500000: episode: 684, duration: 0.563s, episode steps: 122, steps per second: 217, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 0.108169, mae: 26.269316, mean_q: -37.086047, mean_eps: 0.100000\n",
      " 101583/500000: episode: 685, duration: 0.783s, episode steps: 166, steps per second: 212, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.121408, mae: 26.865751, mean_q: -37.726460, mean_eps: 0.100000\n",
      " 101687/500000: episode: 686, duration: 0.489s, episode steps: 104, steps per second: 213, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.150405, mae: 26.492436, mean_q: -37.267854, mean_eps: 0.100000\n",
      " 101849/500000: episode: 687, duration: 0.746s, episode steps: 162, steps per second: 217, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.125068, mae: 27.302022, mean_q: -38.654267, mean_eps: 0.100000\n",
      " 101960/500000: episode: 688, duration: 0.525s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.117305, mae: 26.759188, mean_q: -38.011900, mean_eps: 0.100000\n",
      " 102117/500000: episode: 689, duration: 0.740s, episode steps: 157, steps per second: 212, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.130290, mae: 27.846367, mean_q: -39.826083, mean_eps: 0.100000\n",
      " 102240/500000: episode: 690, duration: 0.573s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.134022, mae: 27.909059, mean_q: -40.183584, mean_eps: 0.100000\n",
      " 102357/500000: episode: 691, duration: 0.547s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.129306, mae: 27.804800, mean_q: -40.185078, mean_eps: 0.100000\n",
      " 102537/500000: episode: 692, duration: 0.848s, episode steps: 180, steps per second: 212, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.141300, mae: 27.716910, mean_q: -40.075101, mean_eps: 0.100000\n",
      " 102647/500000: episode: 693, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.114231, mae: 27.703741, mean_q: -40.154653, mean_eps: 0.100000\n",
      " 102755/500000: episode: 694, duration: 0.511s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.115507, mae: 27.707113, mean_q: -40.298178, mean_eps: 0.100000\n",
      " 102876/500000: episode: 695, duration: 0.573s, episode steps: 121, steps per second: 211, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.105268, mae: 26.727644, mean_q: -38.782723, mean_eps: 0.100000\n",
      " 102988/500000: episode: 696, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 0.119381, mae: 26.725266, mean_q: -38.562251, mean_eps: 0.100000\n",
      " 103085/500000: episode: 697, duration: 0.453s, episode steps:  97, steps per second: 214, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.120302, mae: 25.733836, mean_q: -36.968630, mean_eps: 0.100000\n",
      " 103190/500000: episode: 698, duration: 0.488s, episode steps: 105, steps per second: 215, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.094543, mae: 25.822954, mean_q: -36.880301, mean_eps: 0.100000\n",
      " 103301/500000: episode: 699, duration: 0.533s, episode steps: 111, steps per second: 208, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.104696, mae: 25.924409, mean_q: -37.119795, mean_eps: 0.100000\n",
      " 103428/500000: episode: 700, duration: 0.596s, episode steps: 127, steps per second: 213, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.394 [0.000, 2.000],  loss: 0.096472, mae: 25.630517, mean_q: -36.706547, mean_eps: 0.100000\n",
      " 103527/500000: episode: 701, duration: 0.459s, episode steps:  99, steps per second: 216, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.078370, mae: 25.221935, mean_q: -36.125059, mean_eps: 0.100000\n",
      " 103636/500000: episode: 702, duration: 0.516s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.059602, mae: 24.859639, mean_q: -35.559991, mean_eps: 0.100000\n",
      " 103744/500000: episode: 703, duration: 0.515s, episode steps: 108, steps per second: 210, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.061735, mae: 24.526671, mean_q: -35.083738, mean_eps: 0.100000\n",
      " 103855/500000: episode: 704, duration: 0.538s, episode steps: 111, steps per second: 206, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.052832, mae: 25.245763, mean_q: -36.227090, mean_eps: 0.100000\n",
      " 104055/500000: episode: 705, duration: 0.922s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.063335, mae: 25.655772, mean_q: -36.889497, mean_eps: 0.100000\n",
      " 104255/500000: episode: 706, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.092930, mae: 26.371425, mean_q: -37.910774, mean_eps: 0.100000\n",
      " 104366/500000: episode: 707, duration: 0.512s, episode steps: 111, steps per second: 217, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.142091, mae: 26.558018, mean_q: -38.103087, mean_eps: 0.100000\n",
      " 104475/500000: episode: 708, duration: 0.517s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.129420, mae: 27.254114, mean_q: -39.158312, mean_eps: 0.100000\n",
      " 104601/500000: episode: 709, duration: 0.598s, episode steps: 126, steps per second: 211, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.108067, mae: 27.138470, mean_q: -38.910112, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 104714/500000: episode: 710, duration: 0.527s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.092984, mae: 27.678444, mean_q: -39.738195, mean_eps: 0.100000\n",
      " 104830/500000: episode: 711, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.123740, mae: 27.539168, mean_q: -39.488921, mean_eps: 0.100000\n",
      " 104956/500000: episode: 712, duration: 0.596s, episode steps: 126, steps per second: 211, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.111747, mae: 26.953183, mean_q: -38.727581, mean_eps: 0.100000\n",
      " 105069/500000: episode: 713, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.155192, mae: 26.589790, mean_q: -37.866792, mean_eps: 0.100000\n",
      " 105182/500000: episode: 714, duration: 0.520s, episode steps: 113, steps per second: 217, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.451 [0.000, 2.000],  loss: 0.124825, mae: 26.420520, mean_q: -37.722981, mean_eps: 0.100000\n",
      " 105382/500000: episode: 715, duration: 0.932s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 0.083458, mae: 25.647353, mean_q: -36.570188, mean_eps: 0.100000\n",
      " 105525/500000: episode: 716, duration: 0.668s, episode steps: 143, steps per second: 214, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.797954, mae: 26.563804, mean_q: -37.648731, mean_eps: 0.100000\n",
      " 105651/500000: episode: 717, duration: 0.584s, episode steps: 126, steps per second: 216, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.809725, mae: 26.154918, mean_q: -37.102528, mean_eps: 0.100000\n",
      " 105772/500000: episode: 718, duration: 0.568s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.368734, mae: 25.992219, mean_q: -37.173782, mean_eps: 0.100000\n",
      " 105865/500000: episode: 719, duration: 0.449s, episode steps:  93, steps per second: 207, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.638144, mae: 26.003791, mean_q: -36.921070, mean_eps: 0.100000\n",
      " 105979/500000: episode: 720, duration: 0.533s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.474 [0.000, 2.000],  loss: 0.344303, mae: 26.000246, mean_q: -37.008485, mean_eps: 0.100000\n",
      " 106103/500000: episode: 721, duration: 0.571s, episode steps: 124, steps per second: 217, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000],  loss: 0.591640, mae: 25.620643, mean_q: -36.365840, mean_eps: 0.100000\n",
      " 106303/500000: episode: 722, duration: 0.946s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.757313, mae: 26.263580, mean_q: -37.131178, mean_eps: 0.100000\n",
      " 106443/500000: episode: 723, duration: 0.641s, episode steps: 140, steps per second: 218, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.373365, mae: 26.512671, mean_q: -37.615409, mean_eps: 0.100000\n",
      " 106643/500000: episode: 724, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.207435, mae: 26.766686, mean_q: -37.734934, mean_eps: 0.100000\n",
      " 106742/500000: episode: 725, duration: 0.495s, episode steps:  99, steps per second: 200, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.233424, mae: 26.814507, mean_q: -37.690234, mean_eps: 0.100000\n",
      " 106923/500000: episode: 726, duration: 0.848s, episode steps: 181, steps per second: 213, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.337 [0.000, 2.000],  loss: 0.217302, mae: 27.535562, mean_q: -38.764430, mean_eps: 0.100000\n",
      " 107094/500000: episode: 727, duration: 0.807s, episode steps: 171, steps per second: 212, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.386 [0.000, 2.000],  loss: 0.370784, mae: 28.474782, mean_q: -40.162153, mean_eps: 0.100000\n",
      " 107212/500000: episode: 728, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000],  loss: 0.253903, mae: 28.847152, mean_q: -40.872557, mean_eps: 0.100000\n",
      " 107328/500000: episode: 729, duration: 0.535s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.237567, mae: 28.307846, mean_q: -39.960802, mean_eps: 0.100000\n",
      " 107444/500000: episode: 730, duration: 0.542s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.205454, mae: 28.140837, mean_q: -39.833216, mean_eps: 0.100000\n",
      " 107555/500000: episode: 731, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000],  loss: 0.150725, mae: 27.472745, mean_q: -39.130385, mean_eps: 0.100000\n",
      " 107665/500000: episode: 732, duration: 0.515s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.164 [0.000, 2.000],  loss: 0.140491, mae: 26.957960, mean_q: -38.431881, mean_eps: 0.100000\n",
      " 107787/500000: episode: 733, duration: 0.558s, episode steps: 122, steps per second: 219, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.109125, mae: 27.074552, mean_q: -38.827671, mean_eps: 0.100000\n",
      " 107917/500000: episode: 734, duration: 0.613s, episode steps: 130, steps per second: 212, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.072341, mae: 26.691742, mean_q: -38.419481, mean_eps: 0.100000\n",
      " 108109/500000: episode: 735, duration: 0.902s, episode steps: 192, steps per second: 213, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.069932, mae: 26.434645, mean_q: -38.096459, mean_eps: 0.100000\n",
      " 108218/500000: episode: 736, duration: 0.504s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.081099, mae: 26.093036, mean_q: -37.602546, mean_eps: 0.100000\n",
      " 108329/500000: episode: 737, duration: 0.518s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.066896, mae: 26.506187, mean_q: -38.340469, mean_eps: 0.100000\n",
      " 108438/500000: episode: 738, duration: 0.525s, episode steps: 109, steps per second: 208, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.060531, mae: 26.055678, mean_q: -37.769020, mean_eps: 0.100000\n",
      " 108549/500000: episode: 739, duration: 0.518s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.044458, mae: 26.260939, mean_q: -38.115679, mean_eps: 0.100000\n",
      " 108658/500000: episode: 740, duration: 0.506s, episode steps: 109, steps per second: 215, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.051196, mae: 25.794360, mean_q: -37.477669, mean_eps: 0.100000\n",
      " 108770/500000: episode: 741, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.045037, mae: 26.414403, mean_q: -38.369938, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 108943/500000: episode: 742, duration: 0.818s, episode steps: 173, steps per second: 211, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.051978, mae: 26.725200, mean_q: -38.745358, mean_eps: 0.100000\n",
      " 109052/500000: episode: 743, duration: 0.505s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.076860, mae: 26.336462, mean_q: -38.191929, mean_eps: 0.100000\n",
      " 109161/500000: episode: 744, duration: 0.511s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.046330, mae: 25.759232, mean_q: -37.345106, mean_eps: 0.100000\n",
      " 109327/500000: episode: 745, duration: 0.784s, episode steps: 166, steps per second: 212, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.058357, mae: 26.628508, mean_q: -38.695532, mean_eps: 0.100000\n",
      " 109497/500000: episode: 746, duration: 0.788s, episode steps: 170, steps per second: 216, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.941 [0.000, 2.000],  loss: 0.055564, mae: 27.339852, mean_q: -39.793337, mean_eps: 0.100000\n",
      " 109603/500000: episode: 747, duration: 0.500s, episode steps: 106, steps per second: 212, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.077864, mae: 27.761750, mean_q: -40.407968, mean_eps: 0.100000\n",
      " 109778/500000: episode: 748, duration: 0.823s, episode steps: 175, steps per second: 213, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.145617, mae: 28.152402, mean_q: -40.739457, mean_eps: 0.100000\n",
      " 109889/500000: episode: 749, duration: 0.511s, episode steps: 111, steps per second: 217, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.189627, mae: 27.820116, mean_q: -40.076768, mean_eps: 0.100000\n",
      " 110076/500000: episode: 750, duration: 0.882s, episode steps: 187, steps per second: 212, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.251 [0.000, 2.000],  loss: 0.210647, mae: 27.903235, mean_q: -40.037822, mean_eps: 0.100000\n",
      " 110179/500000: episode: 751, duration: 0.489s, episode steps: 103, steps per second: 211, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.303959, mae: 28.317787, mean_q: -40.385611, mean_eps: 0.100000\n",
      " 110310/500000: episode: 752, duration: 0.606s, episode steps: 131, steps per second: 216, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.255852, mae: 28.197502, mean_q: -40.070258, mean_eps: 0.100000\n",
      " 110417/500000: episode: 753, duration: 0.501s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.258123, mae: 28.515746, mean_q: -40.314927, mean_eps: 0.100000\n",
      " 110531/500000: episode: 754, duration: 0.545s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.404 [0.000, 2.000],  loss: 0.179120, mae: 28.009035, mean_q: -39.629357, mean_eps: 0.100000\n",
      " 110720/500000: episode: 755, duration: 0.881s, episode steps: 189, steps per second: 215, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.181942, mae: 28.222229, mean_q: -39.892615, mean_eps: 0.100000\n",
      " 110916/500000: episode: 756, duration: 0.914s, episode steps: 196, steps per second: 214, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.139071, mae: 28.702211, mean_q: -40.981278, mean_eps: 0.100000\n",
      " 111032/500000: episode: 757, duration: 0.544s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.102002, mae: 28.547707, mean_q: -41.038901, mean_eps: 0.100000\n",
      " 111197/500000: episode: 758, duration: 0.757s, episode steps: 165, steps per second: 218, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.101069, mae: 28.987341, mean_q: -41.758433, mean_eps: 0.100000\n",
      " 111378/500000: episode: 759, duration: 0.859s, episode steps: 181, steps per second: 211, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.103437, mae: 29.004406, mean_q: -42.057150, mean_eps: 0.100000\n",
      " 111486/500000: episode: 760, duration: 0.512s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.096329, mae: 29.078578, mean_q: -42.234019, mean_eps: 0.100000\n",
      " 111613/500000: episode: 761, duration: 0.591s, episode steps: 127, steps per second: 215, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.179164, mae: 29.610182, mean_q: -42.883894, mean_eps: 0.100000\n",
      " 111723/500000: episode: 762, duration: 0.515s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.116803, mae: 28.723762, mean_q: -41.417397, mean_eps: 0.100000\n",
      " 111816/500000: episode: 763, duration: 0.446s, episode steps:  93, steps per second: 208, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.098518, mae: 28.670858, mean_q: -41.201382, mean_eps: 0.100000\n",
      " 111925/500000: episode: 764, duration: 0.516s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.158284, mae: 27.251456, mean_q: -39.093259, mean_eps: 0.100000\n",
      " 112118/500000: episode: 765, duration: 0.893s, episode steps: 193, steps per second: 216, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.180583, mae: 27.954189, mean_q: -40.115605, mean_eps: 0.100000\n",
      " 112242/500000: episode: 766, duration: 0.594s, episode steps: 124, steps per second: 209, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.128692, mae: 27.296336, mean_q: -39.004621, mean_eps: 0.100000\n",
      " 112438/500000: episode: 767, duration: 0.903s, episode steps: 196, steps per second: 217, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.124623, mae: 27.230379, mean_q: -38.794144, mean_eps: 0.100000\n",
      " 112585/500000: episode: 768, duration: 0.690s, episode steps: 147, steps per second: 213, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.456 [0.000, 2.000],  loss: 0.178178, mae: 27.529895, mean_q: -39.068798, mean_eps: 0.100000\n",
      " 112694/500000: episode: 769, duration: 0.528s, episode steps: 109, steps per second: 207, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.201210, mae: 27.636607, mean_q: -39.147856, mean_eps: 0.100000\n",
      " 112804/500000: episode: 770, duration: 0.506s, episode steps: 110, steps per second: 217, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.212884, mae: 27.430580, mean_q: -38.748816, mean_eps: 0.100000\n",
      " 112981/500000: episode: 771, duration: 0.823s, episode steps: 177, steps per second: 215, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000],  loss: 0.189603, mae: 28.379523, mean_q: -40.369181, mean_eps: 0.100000\n",
      " 113086/500000: episode: 772, duration: 0.497s, episode steps: 105, steps per second: 211, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.154479, mae: 27.412625, mean_q: -39.008165, mean_eps: 0.100000\n",
      " 113282/500000: episode: 773, duration: 0.910s, episode steps: 196, steps per second: 215, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.159498, mae: 28.530596, mean_q: -40.813501, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 113482/500000: episode: 774, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.159926, mae: 28.872410, mean_q: -41.371890, mean_eps: 0.100000\n",
      " 113607/500000: episode: 775, duration: 0.594s, episode steps: 125, steps per second: 210, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.512968, mae: 29.488938, mean_q: -42.180061, mean_eps: 0.100000\n",
      " 113695/500000: episode: 776, duration: 0.412s, episode steps:  88, steps per second: 213, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.419624, mae: 29.349260, mean_q: -41.806977, mean_eps: 0.100000\n",
      " 113802/500000: episode: 777, duration: 0.503s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.439542, mae: 29.530587, mean_q: -41.930679, mean_eps: 0.100000\n",
      " 113911/500000: episode: 778, duration: 0.521s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.431 [0.000, 2.000],  loss: 0.526294, mae: 28.810994, mean_q: -40.816549, mean_eps: 0.100000\n",
      " 114111/500000: episode: 779, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.415004, mae: 29.205446, mean_q: -41.192986, mean_eps: 0.100000\n",
      " 114239/500000: episode: 780, duration: 0.596s, episode steps: 128, steps per second: 215, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.344 [0.000, 2.000],  loss: 0.407853, mae: 28.807859, mean_q: -40.746531, mean_eps: 0.100000\n",
      " 114412/500000: episode: 781, duration: 0.818s, episode steps: 173, steps per second: 211, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.477937, mae: 28.655559, mean_q: -40.305424, mean_eps: 0.100000\n",
      " 114603/500000: episode: 782, duration: 0.879s, episode steps: 191, steps per second: 217, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.296443, mae: 27.967052, mean_q: -39.326636, mean_eps: 0.100000\n",
      " 114716/500000: episode: 783, duration: 0.538s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.442 [0.000, 2.000],  loss: 0.286202, mae: 28.219223, mean_q: -40.068186, mean_eps: 0.100000\n",
      " 114833/500000: episode: 784, duration: 0.561s, episode steps: 117, steps per second: 208, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.325677, mae: 27.588023, mean_q: -39.138209, mean_eps: 0.100000\n",
      " 114949/500000: episode: 785, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.279590, mae: 27.311401, mean_q: -38.709247, mean_eps: 0.100000\n",
      " 115059/500000: episode: 786, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.271101, mae: 26.597431, mean_q: -37.903334, mean_eps: 0.100000\n",
      " 115209/500000: episode: 787, duration: 0.711s, episode steps: 150, steps per second: 211, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.205177, mae: 26.383553, mean_q: -37.669763, mean_eps: 0.100000\n",
      " 115409/500000: episode: 788, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.158814, mae: 26.708683, mean_q: -38.295440, mean_eps: 0.100000\n",
      " 115526/500000: episode: 789, duration: 0.543s, episode steps: 117, steps per second: 216, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.117309, mae: 26.970180, mean_q: -38.736246, mean_eps: 0.100000\n",
      " 115649/500000: episode: 790, duration: 0.587s, episode steps: 123, steps per second: 210, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000],  loss: 0.097861, mae: 26.239616, mean_q: -37.709655, mean_eps: 0.100000\n",
      " 115759/500000: episode: 791, duration: 0.510s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.080937, mae: 26.113330, mean_q: -37.591263, mean_eps: 0.100000\n",
      " 115866/500000: episode: 792, duration: 0.498s, episode steps: 107, steps per second: 215, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.078595, mae: 25.894203, mean_q: -37.390980, mean_eps: 0.100000\n",
      " 115998/500000: episode: 793, duration: 0.621s, episode steps: 132, steps per second: 213, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.104269, mae: 26.436877, mean_q: -38.172533, mean_eps: 0.100000\n",
      " 116114/500000: episode: 794, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.135145, mae: 26.397769, mean_q: -37.957755, mean_eps: 0.100000\n",
      " 116225/500000: episode: 795, duration: 0.512s, episode steps: 111, steps per second: 217, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.089961, mae: 25.983833, mean_q: -37.178681, mean_eps: 0.100000\n",
      " 116338/500000: episode: 796, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.090383, mae: 25.146355, mean_q: -36.110070, mean_eps: 0.100000\n",
      " 116538/500000: episode: 797, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.109147, mae: 25.835709, mean_q: -37.171953, mean_eps: 0.100000\n",
      " 116648/500000: episode: 798, duration: 0.522s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.101862, mae: 25.973624, mean_q: -37.313198, mean_eps: 0.100000\n",
      " 116761/500000: episode: 799, duration: 0.523s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.105586, mae: 25.896058, mean_q: -37.261452, mean_eps: 0.100000\n",
      " 116929/500000: episode: 800, duration: 0.793s, episode steps: 168, steps per second: 212, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.124003, mae: 26.703176, mean_q: -38.404476, mean_eps: 0.100000\n",
      " 117039/500000: episode: 801, duration: 0.522s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.080281, mae: 26.571359, mean_q: -38.511940, mean_eps: 0.100000\n",
      " 117151/500000: episode: 802, duration: 0.519s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.100399, mae: 26.227640, mean_q: -38.172530, mean_eps: 0.100000\n",
      " 117306/500000: episode: 803, duration: 0.727s, episode steps: 155, steps per second: 213, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000],  loss: 0.101744, mae: 26.973440, mean_q: -39.272056, mean_eps: 0.100000\n",
      " 117462/500000: episode: 804, duration: 0.737s, episode steps: 156, steps per second: 212, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.073180, mae: 27.592356, mean_q: -40.275151, mean_eps: 0.100000\n",
      " 117572/500000: episode: 805, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.051883, mae: 27.641493, mean_q: -40.564463, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 117744/500000: episode: 806, duration: 0.801s, episode steps: 172, steps per second: 215, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.094850, mae: 28.084543, mean_q: -41.140468, mean_eps: 0.100000\n",
      " 117862/500000: episode: 807, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.082623, mae: 28.028863, mean_q: -40.985328, mean_eps: 0.100000\n",
      " 118027/500000: episode: 808, duration: 0.761s, episode steps: 165, steps per second: 217, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.166114, mae: 27.813456, mean_q: -40.455413, mean_eps: 0.100000\n",
      " 118188/500000: episode: 809, duration: 0.762s, episode steps: 161, steps per second: 211, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.919 [0.000, 2.000],  loss: 0.120991, mae: 28.141572, mean_q: -41.004290, mean_eps: 0.100000\n",
      " 118279/500000: episode: 810, duration: 0.430s, episode steps:  91, steps per second: 212, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.120923, mae: 27.100019, mean_q: -39.386322, mean_eps: 0.100000\n",
      " 118393/500000: episode: 811, duration: 0.531s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.181006, mae: 27.428028, mean_q: -39.692175, mean_eps: 0.100000\n",
      " 118507/500000: episode: 812, duration: 0.538s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.160098, mae: 27.327028, mean_q: -39.425849, mean_eps: 0.100000\n",
      " 118623/500000: episode: 813, duration: 0.549s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.110691, mae: 26.500628, mean_q: -38.218581, mean_eps: 0.100000\n",
      " 118738/500000: episode: 814, duration: 0.541s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.113932, mae: 26.306290, mean_q: -37.912862, mean_eps: 0.100000\n",
      " 118853/500000: episode: 815, duration: 0.532s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.108208, mae: 25.889183, mean_q: -37.225517, mean_eps: 0.100000\n",
      " 118969/500000: episode: 816, duration: 0.545s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.094460, mae: 26.148548, mean_q: -37.714982, mean_eps: 0.100000\n",
      " 119059/500000: episode: 817, duration: 0.428s, episode steps:  90, steps per second: 210, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.073290, mae: 25.388381, mean_q: -36.665352, mean_eps: 0.100000\n",
      " 119175/500000: episode: 818, duration: 0.548s, episode steps: 116, steps per second: 212, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.068416, mae: 25.099586, mean_q: -36.220130, mean_eps: 0.100000\n",
      " 119293/500000: episode: 819, duration: 0.548s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.053069, mae: 24.798342, mean_q: -35.967800, mean_eps: 0.100000\n",
      " 119418/500000: episode: 820, duration: 0.586s, episode steps: 125, steps per second: 213, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.065231, mae: 24.879111, mean_q: -36.114995, mean_eps: 0.100000\n",
      " 119531/500000: episode: 821, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.067211, mae: 24.466550, mean_q: -35.507166, mean_eps: 0.100000\n",
      " 119644/500000: episode: 822, duration: 0.530s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.057729, mae: 24.834435, mean_q: -36.067334, mean_eps: 0.100000\n",
      " 119754/500000: episode: 823, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.057579, mae: 24.640808, mean_q: -35.799216, mean_eps: 0.100000\n",
      " 119867/500000: episode: 824, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.090561, mae: 24.551138, mean_q: -35.722934, mean_eps: 0.100000\n",
      " 120067/500000: episode: 825, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.092495, mae: 25.204868, mean_q: -36.650905, mean_eps: 0.100000\n",
      " 120191/500000: episode: 826, duration: 0.576s, episode steps: 124, steps per second: 215, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.131938, mae: 25.989683, mean_q: -37.834368, mean_eps: 0.100000\n",
      " 120281/500000: episode: 827, duration: 0.430s, episode steps:  90, steps per second: 209, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.097353, mae: 25.693503, mean_q: -37.236511, mean_eps: 0.100000\n",
      " 120394/500000: episode: 828, duration: 0.541s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.087802, mae: 26.178504, mean_q: -37.896192, mean_eps: 0.100000\n",
      " 120510/500000: episode: 829, duration: 0.543s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.093571, mae: 25.995684, mean_q: -37.683744, mean_eps: 0.100000\n",
      " 120602/500000: episode: 830, duration: 0.433s, episode steps:  92, steps per second: 213, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.211444, mae: 26.597381, mean_q: -38.346708, mean_eps: 0.100000\n",
      " 120737/500000: episode: 831, duration: 0.653s, episode steps: 135, steps per second: 207, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.100463, mae: 26.863264, mean_q: -38.810855, mean_eps: 0.100000\n",
      " 120924/500000: episode: 832, duration: 0.874s, episode steps: 187, steps per second: 214, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.104080, mae: 27.613026, mean_q: -39.837910, mean_eps: 0.100000\n",
      " 121027/500000: episode: 833, duration: 0.488s, episode steps: 103, steps per second: 211, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.254346, mae: 26.866137, mean_q: -38.533154, mean_eps: 0.100000\n",
      " 121134/500000: episode: 834, duration: 0.515s, episode steps: 107, steps per second: 208, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.411 [0.000, 2.000],  loss: 0.298360, mae: 27.126951, mean_q: -38.720817, mean_eps: 0.100000\n",
      " 121240/500000: episode: 835, duration: 0.509s, episode steps: 106, steps per second: 208, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.133863, mae: 26.786363, mean_q: -38.262648, mean_eps: 0.100000\n",
      " 121346/500000: episode: 836, duration: 0.497s, episode steps: 106, steps per second: 213, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.155154, mae: 26.972763, mean_q: -38.597151, mean_eps: 0.100000\n",
      " 121458/500000: episode: 837, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.384 [0.000, 2.000],  loss: 0.222783, mae: 26.965340, mean_q: -38.653600, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 121569/500000: episode: 838, duration: 0.535s, episode steps: 111, steps per second: 208, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.115995, mae: 26.598902, mean_q: -38.316954, mean_eps: 0.100000\n",
      " 121677/500000: episode: 839, duration: 0.513s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.115614, mae: 26.305834, mean_q: -37.897369, mean_eps: 0.100000\n",
      " 121839/500000: episode: 840, duration: 0.752s, episode steps: 162, steps per second: 215, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.134332, mae: 26.361813, mean_q: -37.828353, mean_eps: 0.100000\n",
      " 121960/500000: episode: 841, duration: 0.576s, episode steps: 121, steps per second: 210, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.148780, mae: 25.943348, mean_q: -37.247488, mean_eps: 0.100000\n",
      " 122082/500000: episode: 842, duration: 0.582s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.101193, mae: 25.808909, mean_q: -37.253602, mean_eps: 0.100000\n",
      " 122211/500000: episode: 843, duration: 0.605s, episode steps: 129, steps per second: 213, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000],  loss: 0.092343, mae: 25.894116, mean_q: -37.498806, mean_eps: 0.100000\n",
      " 122323/500000: episode: 844, duration: 0.521s, episode steps: 112, steps per second: 215, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.092298, mae: 25.779235, mean_q: -37.224995, mean_eps: 0.100000\n",
      " 122420/500000: episode: 845, duration: 0.463s, episode steps:  97, steps per second: 209, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.114441, mae: 25.690127, mean_q: -37.019027, mean_eps: 0.100000\n",
      " 122534/500000: episode: 846, duration: 0.539s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.101870, mae: 25.872945, mean_q: -37.260165, mean_eps: 0.100000\n",
      " 122733/500000: episode: 847, duration: 0.921s, episode steps: 199, steps per second: 216, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.206 [0.000, 2.000],  loss: 0.153761, mae: 26.051300, mean_q: -37.531712, mean_eps: 0.100000\n",
      " 122857/500000: episode: 848, duration: 0.589s, episode steps: 124, steps per second: 211, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.120924, mae: 25.740900, mean_q: -37.068120, mean_eps: 0.100000\n",
      " 123042/500000: episode: 849, duration: 0.858s, episode steps: 185, steps per second: 216, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.132730, mae: 25.524463, mean_q: -36.570319, mean_eps: 0.100000\n",
      " 123226/500000: episode: 850, duration: 0.859s, episode steps: 184, steps per second: 214, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.161472, mae: 26.677408, mean_q: -38.096888, mean_eps: 0.100000\n",
      " 123350/500000: episode: 851, duration: 0.589s, episode steps: 124, steps per second: 211, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 2.000],  loss: 0.163033, mae: 27.259747, mean_q: -38.976609, mean_eps: 0.100000\n",
      " 123550/500000: episode: 852, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 0.204668, mae: 27.365721, mean_q: -39.286371, mean_eps: 0.100000\n",
      " 123660/500000: episode: 853, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.748004, mae: 28.203755, mean_q: -40.277271, mean_eps: 0.100000\n",
      " 123781/500000: episode: 854, duration: 0.572s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.277931, mae: 27.803348, mean_q: -39.869828, mean_eps: 0.100000\n",
      " 123875/500000: episode: 855, duration: 0.442s, episode steps:  94, steps per second: 213, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.532824, mae: 27.579488, mean_q: -39.655254, mean_eps: 0.100000\n",
      " 123991/500000: episode: 856, duration: 0.535s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.422 [0.000, 2.000],  loss: 0.328002, mae: 27.178355, mean_q: -39.224159, mean_eps: 0.100000\n",
      " 124087/500000: episode: 857, duration: 0.460s, episode steps:  96, steps per second: 209, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.749324, mae: 26.908389, mean_q: -38.564620, mean_eps: 0.100000\n",
      " 124202/500000: episode: 858, duration: 0.540s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.158267, mae: 25.930410, mean_q: -37.319291, mean_eps: 0.100000\n",
      " 124325/500000: episode: 859, duration: 0.571s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.142426, mae: 26.069831, mean_q: -37.458599, mean_eps: 0.100000\n",
      " 124438/500000: episode: 860, duration: 0.532s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.230231, mae: 25.920166, mean_q: -37.125723, mean_eps: 0.100000\n",
      " 124638/500000: episode: 861, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.138031, mae: 25.167146, mean_q: -36.088561, mean_eps: 0.100000\n",
      " 124761/500000: episode: 862, duration: 0.574s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.376737, mae: 25.578013, mean_q: -36.624988, mean_eps: 0.100000\n",
      " 124961/500000: episode: 863, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.358454, mae: 26.569488, mean_q: -37.585032, mean_eps: 0.100000\n",
      " 125151/500000: episode: 864, duration: 0.883s, episode steps: 190, steps per second: 215, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.731129, mae: 27.041843, mean_q: -37.926156, mean_eps: 0.100000\n",
      " 125240/500000: episode: 865, duration: 0.414s, episode steps:  89, steps per second: 215, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.645224, mae: 27.154545, mean_q: -38.154031, mean_eps: 0.100000\n",
      " 125352/500000: episode: 866, duration: 0.529s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.474709, mae: 26.782629, mean_q: -37.546165, mean_eps: 0.100000\n",
      " 125462/500000: episode: 867, duration: 0.528s, episode steps: 110, steps per second: 208, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.402313, mae: 26.972732, mean_q: -37.909279, mean_eps: 0.100000\n",
      " 125578/500000: episode: 868, duration: 0.544s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.535893, mae: 26.704897, mean_q: -37.492620, mean_eps: 0.100000\n",
      " 125691/500000: episode: 869, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.261315, mae: 26.096756, mean_q: -36.711423, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 125799/500000: episode: 870, duration: 0.517s, episode steps: 108, steps per second: 209, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.212041, mae: 25.382067, mean_q: -35.913545, mean_eps: 0.100000\n",
      " 125913/500000: episode: 871, duration: 0.538s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.266722, mae: 24.954422, mean_q: -35.710646, mean_eps: 0.100000\n",
      " 126029/500000: episode: 872, duration: 0.537s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.113783, mae: 24.754737, mean_q: -35.517404, mean_eps: 0.100000\n",
      " 126229/500000: episode: 873, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.068459, mae: 25.085446, mean_q: -36.241811, mean_eps: 0.100000\n",
      " 126429/500000: episode: 874, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.187078, mae: 26.135414, mean_q: -37.655124, mean_eps: 0.100000\n",
      " 126535/500000: episode: 875, duration: 0.489s, episode steps: 106, steps per second: 217, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.166966, mae: 26.280520, mean_q: -37.782862, mean_eps: 0.100000\n",
      " 126625/500000: episode: 876, duration: 0.423s, episode steps:  90, steps per second: 213, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.175885, mae: 26.017668, mean_q: -37.340427, mean_eps: 0.100000\n",
      " 126812/500000: episode: 877, duration: 0.883s, episode steps: 187, steps per second: 212, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.235656, mae: 26.590228, mean_q: -37.963403, mean_eps: 0.100000\n",
      " 126971/500000: episode: 878, duration: 0.727s, episode steps: 159, steps per second: 219, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.818 [0.000, 2.000],  loss: 0.242973, mae: 27.352454, mean_q: -39.071884, mean_eps: 0.100000\n",
      " 127087/500000: episode: 879, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.222497, mae: 27.731915, mean_q: -39.616661, mean_eps: 0.100000\n",
      " 127265/500000: episode: 880, duration: 0.827s, episode steps: 178, steps per second: 215, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.175015, mae: 26.930582, mean_q: -38.554153, mean_eps: 0.100000\n",
      " 127427/500000: episode: 881, duration: 0.754s, episode steps: 162, steps per second: 215, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.094064, mae: 27.522248, mean_q: -39.546167, mean_eps: 0.100000\n",
      " 127521/500000: episode: 882, duration: 0.443s, episode steps:  94, steps per second: 212, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.089380, mae: 27.656927, mean_q: -39.691825, mean_eps: 0.100000\n",
      " 127631/500000: episode: 883, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.097094, mae: 27.473962, mean_q: -39.450606, mean_eps: 0.100000\n",
      " 127727/500000: episode: 884, duration: 0.448s, episode steps:  96, steps per second: 214, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.120636, mae: 27.692194, mean_q: -39.718305, mean_eps: 0.100000\n",
      " 127841/500000: episode: 885, duration: 0.533s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.143848, mae: 27.432971, mean_q: -39.205676, mean_eps: 0.100000\n",
      " 127963/500000: episode: 886, duration: 0.584s, episode steps: 122, steps per second: 209, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.215524, mae: 27.210399, mean_q: -38.687748, mean_eps: 0.100000\n",
      " 128151/500000: episode: 887, duration: 0.870s, episode steps: 188, steps per second: 216, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.548 [0.000, 2.000],  loss: 0.204772, mae: 28.044347, mean_q: -39.547362, mean_eps: 0.100000\n",
      " 128348/500000: episode: 888, duration: 0.925s, episode steps: 197, steps per second: 213, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.255502, mae: 27.894766, mean_q: -39.066435, mean_eps: 0.100000\n",
      " 128548/500000: episode: 889, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.281967, mae: 28.224896, mean_q: -39.393365, mean_eps: 0.100000\n",
      " 128662/500000: episode: 890, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.455919, mae: 28.518491, mean_q: -40.028399, mean_eps: 0.100000\n",
      " 128849/500000: episode: 891, duration: 0.885s, episode steps: 187, steps per second: 211, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.461214, mae: 28.929149, mean_q: -40.802377, mean_eps: 0.100000\n",
      " 128960/500000: episode: 892, duration: 0.520s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.461809, mae: 28.670806, mean_q: -40.620042, mean_eps: 0.100000\n",
      " 129072/500000: episode: 893, duration: 0.523s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000],  loss: 0.330423, mae: 28.314531, mean_q: -40.208441, mean_eps: 0.100000\n",
      " 129190/500000: episode: 894, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000],  loss: 0.285162, mae: 27.712984, mean_q: -39.243565, mean_eps: 0.100000\n",
      " 129329/500000: episode: 895, duration: 0.664s, episode steps: 139, steps per second: 209, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.417 [0.000, 2.000],  loss: 0.243235, mae: 26.959083, mean_q: -38.197699, mean_eps: 0.100000\n",
      " 129447/500000: episode: 896, duration: 0.547s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.211776, mae: 27.435627, mean_q: -38.719616, mean_eps: 0.100000\n",
      " 129560/500000: episode: 897, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.202830, mae: 26.921133, mean_q: -37.919542, mean_eps: 0.100000\n",
      " 129760/500000: episode: 898, duration: 0.950s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.152895, mae: 27.002741, mean_q: -37.925733, mean_eps: 0.100000\n",
      " 129950/500000: episode: 899, duration: 0.879s, episode steps: 190, steps per second: 216, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.534387, mae: 28.108625, mean_q: -38.961475, mean_eps: 0.100000\n",
      " 130062/500000: episode: 900, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.392837, mae: 27.908249, mean_q: -38.893254, mean_eps: 0.100000\n",
      " 130231/500000: episode: 901, duration: 0.794s, episode steps: 169, steps per second: 213, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.572950, mae: 28.098259, mean_q: -39.467787, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 130387/500000: episode: 902, duration: 0.722s, episode steps: 156, steps per second: 216, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.345565, mae: 28.208078, mean_q: -40.104366, mean_eps: 0.100000\n",
      " 130499/500000: episode: 903, duration: 0.529s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.338576, mae: 27.645841, mean_q: -39.550472, mean_eps: 0.100000\n",
      " 130625/500000: episode: 904, duration: 0.600s, episode steps: 126, steps per second: 210, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.276320, mae: 28.064255, mean_q: -40.221433, mean_eps: 0.100000\n",
      " 130747/500000: episode: 905, duration: 0.566s, episode steps: 122, steps per second: 215, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.419975, mae: 28.149483, mean_q: -40.291571, mean_eps: 0.100000\n",
      " 130859/500000: episode: 906, duration: 0.527s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.100012, mae: 27.234292, mean_q: -38.974917, mean_eps: 0.100000\n",
      " 130976/500000: episode: 907, duration: 0.563s, episode steps: 117, steps per second: 208, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.094118, mae: 26.642122, mean_q: -38.106474, mean_eps: 0.100000\n",
      " 131087/500000: episode: 908, duration: 0.524s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.084377, mae: 26.997634, mean_q: -38.627514, mean_eps: 0.100000\n",
      " 131262/500000: episode: 909, duration: 0.808s, episode steps: 175, steps per second: 216, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000],  loss: 0.131213, mae: 26.417754, mean_q: -37.613837, mean_eps: 0.100000\n",
      " 131379/500000: episode: 910, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.081432, mae: 26.027700, mean_q: -36.912315, mean_eps: 0.100000\n",
      " 131473/500000: episode: 911, duration: 0.442s, episode steps:  94, steps per second: 213, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.098225, mae: 25.835564, mean_q: -36.574637, mean_eps: 0.100000\n",
      " 131605/500000: episode: 912, duration: 0.615s, episode steps: 132, steps per second: 215, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.100407, mae: 25.805728, mean_q: -36.498245, mean_eps: 0.100000\n",
      " 131805/500000: episode: 913, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.134985, mae: 26.030765, mean_q: -36.916681, mean_eps: 0.100000\n",
      " 131950/500000: episode: 914, duration: 0.673s, episode steps: 145, steps per second: 215, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.159780, mae: 26.344446, mean_q: -37.313219, mean_eps: 0.100000\n",
      " 132093/500000: episode: 915, duration: 0.677s, episode steps: 143, steps per second: 211, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.392 [0.000, 2.000],  loss: 0.268678, mae: 26.768034, mean_q: -37.658506, mean_eps: 0.100000\n",
      " 132257/500000: episode: 916, duration: 0.776s, episode steps: 164, steps per second: 211, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.256542, mae: 26.655796, mean_q: -37.372262, mean_eps: 0.100000\n",
      " 132369/500000: episode: 917, duration: 0.535s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.238946, mae: 26.557907, mean_q: -37.271486, mean_eps: 0.100000\n",
      " 132506/500000: episode: 918, duration: 0.644s, episode steps: 137, steps per second: 213, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.197744, mae: 26.647073, mean_q: -37.708019, mean_eps: 0.100000\n",
      " 132706/500000: episode: 919, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.246682, mae: 27.107488, mean_q: -38.221782, mean_eps: 0.100000\n",
      " 132853/500000: episode: 920, duration: 0.690s, episode steps: 147, steps per second: 213, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.228818, mae: 26.077343, mean_q: -36.703145, mean_eps: 0.100000\n",
      " 132970/500000: episode: 921, duration: 0.549s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.208970, mae: 25.264839, mean_q: -35.691059, mean_eps: 0.100000\n",
      " 133090/500000: episode: 922, duration: 0.584s, episode steps: 120, steps per second: 206, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.161407, mae: 25.406271, mean_q: -36.227778, mean_eps: 0.100000\n",
      " 133215/500000: episode: 923, duration: 0.583s, episode steps: 125, steps per second: 214, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.344 [0.000, 2.000],  loss: 0.181979, mae: 24.562079, mean_q: -35.312821, mean_eps: 0.100000\n",
      " 133337/500000: episode: 924, duration: 0.571s, episode steps: 122, steps per second: 214, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.492 [0.000, 2.000],  loss: 0.168696, mae: 24.625775, mean_q: -35.468097, mean_eps: 0.100000\n",
      " 133457/500000: episode: 925, duration: 0.581s, episode steps: 120, steps per second: 207, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.171345, mae: 24.535299, mean_q: -35.229775, mean_eps: 0.100000\n",
      " 133571/500000: episode: 926, duration: 0.547s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.165606, mae: 24.140263, mean_q: -34.512235, mean_eps: 0.100000\n",
      " 133687/500000: episode: 927, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.110252, mae: 23.695506, mean_q: -34.111649, mean_eps: 0.100000\n",
      " 133810/500000: episode: 928, duration: 0.576s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.124723, mae: 23.920109, mean_q: -34.422114, mean_eps: 0.100000\n",
      " 133982/500000: episode: 929, duration: 0.813s, episode steps: 172, steps per second: 212, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.079703, mae: 25.200767, mean_q: -36.346973, mean_eps: 0.100000\n",
      " 134101/500000: episode: 930, duration: 0.553s, episode steps: 119, steps per second: 215, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.378 [0.000, 2.000],  loss: 0.068557, mae: 25.520813, mean_q: -36.960319, mean_eps: 0.100000\n",
      " 134224/500000: episode: 931, duration: 0.586s, episode steps: 123, steps per second: 210, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.060569, mae: 25.291626, mean_q: -36.677726, mean_eps: 0.100000\n",
      " 134339/500000: episode: 932, duration: 0.551s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 0.049359, mae: 25.489047, mean_q: -37.078735, mean_eps: 0.100000\n",
      " 134538/500000: episode: 933, duration: 0.928s, episode steps: 199, steps per second: 214, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.068139, mae: 26.363556, mean_q: -38.433777, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 134629/500000: episode: 934, duration: 0.421s, episode steps:  91, steps per second: 216, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.093620, mae: 26.691887, mean_q: -38.842721, mean_eps: 0.100000\n",
      " 134722/500000: episode: 935, duration: 0.445s, episode steps:  93, steps per second: 209, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.080084, mae: 25.980799, mean_q: -37.709260, mean_eps: 0.100000\n",
      " 134830/500000: episode: 936, duration: 0.511s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.075857, mae: 25.931302, mean_q: -37.591376, mean_eps: 0.100000\n",
      " 134942/500000: episode: 937, duration: 0.527s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.057656, mae: 25.416277, mean_q: -36.863096, mean_eps: 0.100000\n",
      " 135058/500000: episode: 938, duration: 0.542s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.058029, mae: 25.364978, mean_q: -36.823288, mean_eps: 0.100000\n",
      " 135167/500000: episode: 939, duration: 0.525s, episode steps: 109, steps per second: 208, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.404 [0.000, 2.000],  loss: 0.058908, mae: 25.446272, mean_q: -36.969990, mean_eps: 0.100000\n",
      " 135284/500000: episode: 940, duration: 0.553s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.436 [0.000, 2.000],  loss: 0.043693, mae: 25.831641, mean_q: -37.576143, mean_eps: 0.100000\n",
      " 135394/500000: episode: 941, duration: 0.518s, episode steps: 110, steps per second: 212, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.084315, mae: 25.798416, mean_q: -37.385367, mean_eps: 0.100000\n",
      " 135554/500000: episode: 942, duration: 0.742s, episode steps: 160, steps per second: 216, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 0.074656, mae: 25.282414, mean_q: -36.633536, mean_eps: 0.100000\n",
      " 135746/500000: episode: 943, duration: 0.906s, episode steps: 192, steps per second: 212, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.158311, mae: 26.009831, mean_q: -37.575027, mean_eps: 0.100000\n",
      " 135862/500000: episode: 944, duration: 0.539s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.397 [0.000, 2.000],  loss: 0.151339, mae: 25.799307, mean_q: -37.315779, mean_eps: 0.100000\n",
      " 136023/500000: episode: 945, duration: 0.750s, episode steps: 161, steps per second: 215, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.143795, mae: 26.312978, mean_q: -38.025632, mean_eps: 0.100000\n",
      " 136199/500000: episode: 946, duration: 0.826s, episode steps: 176, steps per second: 213, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.183057, mae: 26.902607, mean_q: -38.819248, mean_eps: 0.100000\n",
      " 136357/500000: episode: 947, duration: 0.731s, episode steps: 158, steps per second: 216, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.119952, mae: 26.706929, mean_q: -38.577673, mean_eps: 0.100000\n",
      " 136464/500000: episode: 948, duration: 0.506s, episode steps: 107, steps per second: 211, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.108897, mae: 26.982499, mean_q: -39.094969, mean_eps: 0.100000\n",
      " 136618/500000: episode: 949, duration: 0.717s, episode steps: 154, steps per second: 215, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.111013, mae: 27.566325, mean_q: -39.962365, mean_eps: 0.100000\n",
      " 136735/500000: episode: 950, duration: 0.539s, episode steps: 117, steps per second: 217, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.073563, mae: 27.145536, mean_q: -39.534618, mean_eps: 0.100000\n",
      " 136852/500000: episode: 951, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.479 [0.000, 2.000],  loss: 0.075538, mae: 27.162930, mean_q: -39.640866, mean_eps: 0.100000\n",
      " 136965/500000: episode: 952, duration: 0.545s, episode steps: 113, steps per second: 207, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.469 [0.000, 2.000],  loss: 0.074854, mae: 27.010083, mean_q: -39.386627, mean_eps: 0.100000\n",
      " 137085/500000: episode: 953, duration: 0.568s, episode steps: 120, steps per second: 211, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.058315, mae: 26.992201, mean_q: -39.405226, mean_eps: 0.100000\n",
      " 137209/500000: episode: 954, duration: 0.574s, episode steps: 124, steps per second: 216, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.067770, mae: 27.076465, mean_q: -39.638686, mean_eps: 0.100000\n",
      " 137324/500000: episode: 955, duration: 0.547s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.061006, mae: 26.380694, mean_q: -38.705225, mean_eps: 0.100000\n",
      " 137443/500000: episode: 956, duration: 0.559s, episode steps: 119, steps per second: 213, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.047666, mae: 26.450675, mean_q: -38.846057, mean_eps: 0.100000\n",
      " 137595/500000: episode: 957, duration: 0.700s, episode steps: 152, steps per second: 217, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.041610, mae: 26.257704, mean_q: -38.535721, mean_eps: 0.100000\n",
      " 137708/500000: episode: 958, duration: 0.530s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.033123, mae: 26.034041, mean_q: -38.231847, mean_eps: 0.100000\n",
      " 137818/500000: episode: 959, duration: 0.526s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.427 [0.000, 2.000],  loss: 0.025807, mae: 25.619747, mean_q: -37.661613, mean_eps: 0.100000\n",
      " 137933/500000: episode: 960, duration: 0.535s, episode steps: 115, steps per second: 215, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 0.024235, mae: 25.558505, mean_q: -37.563948, mean_eps: 0.100000\n",
      " 138101/500000: episode: 961, duration: 0.785s, episode steps: 168, steps per second: 214, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.061014, mae: 26.495162, mean_q: -38.826326, mean_eps: 0.100000\n",
      " 138219/500000: episode: 962, duration: 0.560s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.071807, mae: 26.497090, mean_q: -38.809708, mean_eps: 0.100000\n",
      " 138327/500000: episode: 963, duration: 0.503s, episode steps: 108, steps per second: 215, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.463 [0.000, 2.000],  loss: 0.049150, mae: 26.660670, mean_q: -39.061087, mean_eps: 0.100000\n",
      " 138440/500000: episode: 964, duration: 0.522s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.044520, mae: 26.591192, mean_q: -38.955651, mean_eps: 0.100000\n",
      " 138563/500000: episode: 965, duration: 0.589s, episode steps: 123, steps per second: 209, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 0.033775, mae: 26.621177, mean_q: -39.093308, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 138748/500000: episode: 966, duration: 0.872s, episode steps: 185, steps per second: 212, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.080786, mae: 26.916931, mean_q: -39.431113, mean_eps: 0.100000\n",
      " 138900/500000: episode: 967, duration: 0.707s, episode steps: 152, steps per second: 215, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.091523, mae: 27.436664, mean_q: -40.065912, mean_eps: 0.100000\n",
      " 139016/500000: episode: 968, duration: 0.563s, episode steps: 116, steps per second: 206, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.422 [0.000, 2.000],  loss: 0.071461, mae: 27.601546, mean_q: -40.410870, mean_eps: 0.100000\n",
      " 139135/500000: episode: 969, duration: 0.569s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.112544, mae: 27.156795, mean_q: -39.675551, mean_eps: 0.100000\n",
      " 139241/500000: episode: 970, duration: 0.497s, episode steps: 106, steps per second: 213, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.089336, mae: 26.935005, mean_q: -39.483552, mean_eps: 0.100000\n",
      " 139396/500000: episode: 971, duration: 0.721s, episode steps: 155, steps per second: 215, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.046832, mae: 27.512141, mean_q: -40.340788, mean_eps: 0.100000\n",
      " 139506/500000: episode: 972, duration: 0.526s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.047362, mae: 27.428154, mean_q: -40.092360, mean_eps: 0.100000\n",
      " 139666/500000: episode: 973, duration: 0.743s, episode steps: 160, steps per second: 215, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.124894, mae: 28.312264, mean_q: -41.309334, mean_eps: 0.100000\n",
      " 139866/500000: episode: 974, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.105005, mae: 28.150422, mean_q: -41.023762, mean_eps: 0.100000\n",
      " 139989/500000: episode: 975, duration: 0.578s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.190981, mae: 28.310782, mean_q: -40.959408, mean_eps: 0.100000\n",
      " 140106/500000: episode: 976, duration: 0.546s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.217641, mae: 28.358448, mean_q: -40.906789, mean_eps: 0.100000\n",
      " 140221/500000: episode: 977, duration: 0.545s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.203738, mae: 28.483483, mean_q: -41.008463, mean_eps: 0.100000\n",
      " 140375/500000: episode: 978, duration: 0.738s, episode steps: 154, steps per second: 209, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.139502, mae: 28.446180, mean_q: -40.968464, mean_eps: 0.100000\n",
      " 140489/500000: episode: 979, duration: 0.529s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.146935, mae: 28.361512, mean_q: -40.904612, mean_eps: 0.100000\n",
      " 140605/500000: episode: 980, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.110197, mae: 27.153824, mean_q: -39.318244, mean_eps: 0.100000\n",
      " 140711/500000: episode: 981, duration: 0.503s, episode steps: 106, steps per second: 211, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.131114, mae: 27.207038, mean_q: -39.275278, mean_eps: 0.100000\n",
      " 140820/500000: episode: 982, duration: 0.514s, episode steps: 109, steps per second: 212, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.095631, mae: 26.811719, mean_q: -38.915817, mean_eps: 0.100000\n",
      " 140942/500000: episode: 983, duration: 0.568s, episode steps: 122, steps per second: 215, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.086839, mae: 26.773969, mean_q: -38.550830, mean_eps: 0.100000\n",
      " 141032/500000: episode: 984, duration: 0.429s, episode steps:  90, steps per second: 210, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.103196, mae: 27.035552, mean_q: -38.838557, mean_eps: 0.100000\n",
      " 141142/500000: episode: 985, duration: 0.530s, episode steps: 110, steps per second: 208, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.090136, mae: 26.988047, mean_q: -38.622719, mean_eps: 0.100000\n",
      " 141247/500000: episode: 986, duration: 0.496s, episode steps: 105, steps per second: 212, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.115771, mae: 27.360957, mean_q: -39.009383, mean_eps: 0.100000\n",
      " 141359/500000: episode: 987, duration: 0.524s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.069343, mae: 26.529636, mean_q: -37.807118, mean_eps: 0.100000\n",
      " 141539/500000: episode: 988, duration: 0.852s, episode steps: 180, steps per second: 211, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 0.076903, mae: 27.014295, mean_q: -38.570895, mean_eps: 0.100000\n",
      " 141650/500000: episode: 989, duration: 0.529s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.076337, mae: 27.199201, mean_q: -38.858643, mean_eps: 0.100000\n",
      " 141747/500000: episode: 990, duration: 0.453s, episode steps:  97, steps per second: 214, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.073843, mae: 27.199704, mean_q: -38.742085, mean_eps: 0.100000\n",
      " 141856/500000: episode: 991, duration: 0.506s, episode steps: 109, steps per second: 215, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.064248, mae: 27.353395, mean_q: -39.014632, mean_eps: 0.100000\n",
      " 141967/500000: episode: 992, duration: 0.531s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.044440, mae: 26.987691, mean_q: -38.755912, mean_eps: 0.100000\n",
      " 142082/500000: episode: 993, duration: 0.560s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.034160, mae: 27.240498, mean_q: -39.486021, mean_eps: 0.100000\n",
      " 142242/500000: episode: 994, duration: 0.741s, episode steps: 160, steps per second: 216, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.963 [0.000, 2.000],  loss: 0.033587, mae: 27.336055, mean_q: -39.787549, mean_eps: 0.100000\n",
      " 142330/500000: episode: 995, duration: 0.420s, episode steps:  88, steps per second: 209, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.022758, mae: 27.155208, mean_q: -39.414938, mean_eps: 0.100000\n",
      " 142437/500000: episode: 996, duration: 0.513s, episode steps: 107, steps per second: 209, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.031390, mae: 27.136603, mean_q: -39.362468, mean_eps: 0.100000\n",
      " 142545/500000: episode: 997, duration: 0.515s, episode steps: 108, steps per second: 210, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.028114, mae: 26.177827, mean_q: -38.031920, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 142670/500000: episode: 998, duration: 0.584s, episode steps: 125, steps per second: 214, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000],  loss: 0.025148, mae: 25.747090, mean_q: -37.371114, mean_eps: 0.100000\n",
      " 142776/500000: episode: 999, duration: 0.496s, episode steps: 106, steps per second: 214, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.034145, mae: 26.329198, mean_q: -38.368954, mean_eps: 0.100000\n",
      " 142883/500000: episode: 1000, duration: 0.509s, episode steps: 107, steps per second: 210, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.032322, mae: 26.132091, mean_q: -38.165027, mean_eps: 0.100000\n",
      " 142973/500000: episode: 1001, duration: 0.427s, episode steps:  90, steps per second: 211, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.031731, mae: 26.660271, mean_q: -38.826813, mean_eps: 0.100000\n",
      " 143173/500000: episode: 1002, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.050334, mae: 26.309194, mean_q: -38.196837, mean_eps: 0.100000\n",
      " 143290/500000: episode: 1003, duration: 0.564s, episode steps: 117, steps per second: 208, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.461514, mae: 25.846512, mean_q: -37.258165, mean_eps: 0.100000\n",
      " 143400/500000: episode: 1004, duration: 0.518s, episode steps: 110, steps per second: 212, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.387743, mae: 26.089452, mean_q: -37.771505, mean_eps: 0.100000\n",
      " 143600/500000: episode: 1005, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 0.243089, mae: 26.350746, mean_q: -37.987513, mean_eps: 0.100000\n",
      " 143724/500000: episode: 1006, duration: 0.588s, episode steps: 124, steps per second: 211, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.215101, mae: 26.550120, mean_q: -38.261501, mean_eps: 0.100000\n",
      " 143845/500000: episode: 1007, duration: 0.566s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.303069, mae: 26.671715, mean_q: -38.358051, mean_eps: 0.100000\n",
      " 143961/500000: episode: 1008, duration: 0.537s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.191228, mae: 26.622750, mean_q: -38.384281, mean_eps: 0.100000\n",
      " 144070/500000: episode: 1009, duration: 0.517s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.404 [0.000, 2.000],  loss: 0.161726, mae: 26.404198, mean_q: -38.125865, mean_eps: 0.100000\n",
      " 144185/500000: episode: 1010, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.143119, mae: 26.693656, mean_q: -38.758647, mean_eps: 0.100000\n",
      " 144293/500000: episode: 1011, duration: 0.502s, episode steps: 108, steps per second: 215, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.062269, mae: 25.816316, mean_q: -37.452427, mean_eps: 0.100000\n",
      " 144410/500000: episode: 1012, duration: 0.546s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.427 [0.000, 2.000],  loss: 0.044499, mae: 25.879766, mean_q: -37.614864, mean_eps: 0.100000\n",
      " 144528/500000: episode: 1013, duration: 0.564s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000],  loss: 0.042842, mae: 25.615235, mean_q: -37.368905, mean_eps: 0.100000\n",
      " 144646/500000: episode: 1014, duration: 0.562s, episode steps: 118, steps per second: 210, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.036836, mae: 24.738089, mean_q: -36.103374, mean_eps: 0.100000\n",
      " 144763/500000: episode: 1015, duration: 0.539s, episode steps: 117, steps per second: 217, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.025642, mae: 25.913206, mean_q: -37.843667, mean_eps: 0.100000\n",
      " 144884/500000: episode: 1016, duration: 0.567s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.446 [0.000, 2.000],  loss: 0.021770, mae: 25.410884, mean_q: -37.092412, mean_eps: 0.100000\n",
      " 145030/500000: episode: 1017, duration: 0.690s, episode steps: 146, steps per second: 212, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.548 [0.000, 2.000],  loss: 0.022011, mae: 25.752252, mean_q: -37.457857, mean_eps: 0.100000\n",
      " 145146/500000: episode: 1018, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.027875, mae: 26.263554, mean_q: -38.013922, mean_eps: 0.100000\n",
      " 145254/500000: episode: 1019, duration: 0.512s, episode steps: 108, steps per second: 211, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.042627, mae: 25.811666, mean_q: -37.295873, mean_eps: 0.100000\n",
      " 145454/500000: episode: 1020, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.039941, mae: 26.594836, mean_q: -38.419407, mean_eps: 0.100000\n",
      " 145601/500000: episode: 1021, duration: 0.693s, episode steps: 147, steps per second: 212, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.189618, mae: 27.205371, mean_q: -39.175354, mean_eps: 0.100000\n",
      " 145785/500000: episode: 1022, duration: 0.875s, episode steps: 184, steps per second: 210, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.273224, mae: 27.704177, mean_q: -39.687858, mean_eps: 0.100000\n",
      " 145905/500000: episode: 1023, duration: 0.567s, episode steps: 120, steps per second: 212, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.097719, mae: 27.504211, mean_q: -39.401606, mean_eps: 0.100000\n",
      " 146017/500000: episode: 1024, duration: 0.515s, episode steps: 112, steps per second: 218, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.217107, mae: 26.765831, mean_q: -38.290229, mean_eps: 0.100000\n",
      " 146121/500000: episode: 1025, duration: 0.489s, episode steps: 104, steps per second: 213, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.107516, mae: 26.933554, mean_q: -38.509073, mean_eps: 0.100000\n",
      " 146284/500000: episode: 1026, duration: 0.781s, episode steps: 163, steps per second: 209, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.147675, mae: 27.371153, mean_q: -38.966594, mean_eps: 0.100000\n",
      " 146443/500000: episode: 1027, duration: 0.736s, episode steps: 159, steps per second: 216, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.169114, mae: 27.356189, mean_q: -38.957532, mean_eps: 0.100000\n",
      " 146540/500000: episode: 1028, duration: 0.453s, episode steps:  97, steps per second: 214, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000],  loss: 0.159794, mae: 26.851395, mean_q: -38.275493, mean_eps: 0.100000\n",
      " 146655/500000: episode: 1029, duration: 0.547s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.094979, mae: 26.472746, mean_q: -37.796660, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 146809/500000: episode: 1030, duration: 0.723s, episode steps: 154, steps per second: 213, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.086980, mae: 26.512261, mean_q: -38.093193, mean_eps: 0.100000\n",
      " 146930/500000: episode: 1031, duration: 0.560s, episode steps: 121, steps per second: 216, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.256 [0.000, 2.000],  loss: 0.109008, mae: 26.145295, mean_q: -37.634413, mean_eps: 0.100000\n",
      " 147048/500000: episode: 1032, duration: 0.555s, episode steps: 118, steps per second: 213, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.094120, mae: 26.859927, mean_q: -38.834449, mean_eps: 0.100000\n",
      " 147156/500000: episode: 1033, duration: 0.517s, episode steps: 108, steps per second: 209, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.105783, mae: 27.165210, mean_q: -39.406956, mean_eps: 0.100000\n",
      " 147264/500000: episode: 1034, duration: 0.506s, episode steps: 108, steps per second: 213, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.343 [0.000, 2.000],  loss: 0.074401, mae: 26.001770, mean_q: -37.778975, mean_eps: 0.100000\n",
      " 147425/500000: episode: 1035, duration: 0.747s, episode steps: 161, steps per second: 215, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.071494, mae: 26.407059, mean_q: -38.445399, mean_eps: 0.100000\n",
      " 147533/500000: episode: 1036, duration: 0.521s, episode steps: 108, steps per second: 207, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.057255, mae: 26.502471, mean_q: -38.646203, mean_eps: 0.100000\n",
      " 147688/500000: episode: 1037, duration: 0.714s, episode steps: 155, steps per second: 217, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.180241, mae: 26.883476, mean_q: -39.086326, mean_eps: 0.100000\n",
      " 147845/500000: episode: 1038, duration: 0.731s, episode steps: 157, steps per second: 215, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.101298, mae: 26.244474, mean_q: -38.093200, mean_eps: 0.100000\n",
      " 148045/500000: episode: 1039, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.159713, mae: 26.430883, mean_q: -38.191723, mean_eps: 0.100000\n",
      " 148155/500000: episode: 1040, duration: 0.509s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.151816, mae: 26.509748, mean_q: -38.331588, mean_eps: 0.100000\n",
      " 148273/500000: episode: 1041, duration: 0.554s, episode steps: 118, steps per second: 213, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.159330, mae: 26.687990, mean_q: -38.447213, mean_eps: 0.100000\n",
      " 148438/500000: episode: 1042, duration: 0.784s, episode steps: 165, steps per second: 211, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.129763, mae: 26.550551, mean_q: -38.092069, mean_eps: 0.100000\n",
      " 148604/500000: episode: 1043, duration: 0.765s, episode steps: 166, steps per second: 217, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.136752, mae: 27.090870, mean_q: -38.540210, mean_eps: 0.100000\n",
      " 148750/500000: episode: 1044, duration: 0.690s, episode steps: 146, steps per second: 212, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.282221, mae: 27.365810, mean_q: -38.921340, mean_eps: 0.100000\n",
      " 148879/500000: episode: 1045, duration: 0.610s, episode steps: 129, steps per second: 211, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.170815, mae: 26.903434, mean_q: -38.570121, mean_eps: 0.100000\n",
      " 149006/500000: episode: 1046, duration: 0.590s, episode steps: 127, steps per second: 215, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.145568, mae: 26.803742, mean_q: -38.335510, mean_eps: 0.100000\n",
      " 149161/500000: episode: 1047, duration: 0.724s, episode steps: 155, steps per second: 214, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.083979, mae: 27.557160, mean_q: -39.530631, mean_eps: 0.100000\n",
      " 149341/500000: episode: 1048, duration: 0.850s, episode steps: 180, steps per second: 212, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.113649, mae: 26.927494, mean_q: -38.625438, mean_eps: 0.100000\n",
      " 149459/500000: episode: 1049, duration: 0.549s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.152639, mae: 26.496810, mean_q: -37.741397, mean_eps: 0.100000\n",
      " 149659/500000: episode: 1050, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.133104, mae: 26.672497, mean_q: -37.821017, mean_eps: 0.100000\n",
      " 149831/500000: episode: 1051, duration: 0.800s, episode steps: 172, steps per second: 215, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.924 [0.000, 2.000],  loss: 0.227831, mae: 26.677710, mean_q: -37.584831, mean_eps: 0.100000\n",
      " 150009/500000: episode: 1052, duration: 0.830s, episode steps: 178, steps per second: 214, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.904 [0.000, 2.000],  loss: 0.258352, mae: 26.824361, mean_q: -37.675489, mean_eps: 0.100000\n",
      " 150121/500000: episode: 1053, duration: 0.536s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.178930, mae: 25.599827, mean_q: -36.081663, mean_eps: 0.100000\n",
      " 150228/500000: episode: 1054, duration: 0.501s, episode steps: 107, steps per second: 214, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.132777, mae: 26.209699, mean_q: -36.828649, mean_eps: 0.100000\n",
      " 150404/500000: episode: 1055, duration: 0.827s, episode steps: 176, steps per second: 213, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.142024, mae: 26.529124, mean_q: -37.503696, mean_eps: 0.100000\n",
      " 150518/500000: episode: 1056, duration: 0.544s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.149829, mae: 26.394017, mean_q: -37.696169, mean_eps: 0.100000\n",
      " 150680/500000: episode: 1057, duration: 0.759s, episode steps: 162, steps per second: 213, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.864 [0.000, 2.000],  loss: 0.095180, mae: 26.198885, mean_q: -37.578590, mean_eps: 0.100000\n",
      " 150842/500000: episode: 1058, duration: 0.757s, episode steps: 162, steps per second: 214, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.098320, mae: 26.111309, mean_q: -37.473695, mean_eps: 0.100000\n",
      " 151006/500000: episode: 1059, duration: 0.766s, episode steps: 164, steps per second: 214, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.111744, mae: 26.127660, mean_q: -37.469133, mean_eps: 0.100000\n",
      " 151116/500000: episode: 1060, duration: 0.513s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.087194, mae: 26.986842, mean_q: -38.857972, mean_eps: 0.100000\n",
      " 151232/500000: episode: 1061, duration: 0.546s, episode steps: 116, steps per second: 212, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.069052, mae: 26.673622, mean_q: -38.558000, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 151344/500000: episode: 1062, duration: 0.535s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.079462, mae: 26.444541, mean_q: -38.215960, mean_eps: 0.100000\n",
      " 151440/500000: episode: 1063, duration: 0.456s, episode steps:  96, steps per second: 211, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.090986, mae: 26.849559, mean_q: -38.631436, mean_eps: 0.100000\n",
      " 151632/500000: episode: 1064, duration: 0.892s, episode steps: 192, steps per second: 215, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.144092, mae: 26.265806, mean_q: -37.427689, mean_eps: 0.100000\n",
      " 151739/500000: episode: 1065, duration: 0.511s, episode steps: 107, steps per second: 209, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.159871, mae: 26.516548, mean_q: -37.441829, mean_eps: 0.100000\n",
      " 151852/500000: episode: 1066, duration: 0.537s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.113994, mae: 26.537506, mean_q: -37.442109, mean_eps: 0.100000\n",
      " 151970/500000: episode: 1067, duration: 0.550s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.106238, mae: 25.281165, mean_q: -35.534796, mean_eps: 0.100000\n",
      " 152123/500000: episode: 1068, duration: 0.711s, episode steps: 153, steps per second: 215, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.114251, mae: 25.317215, mean_q: -35.665966, mean_eps: 0.100000\n",
      " 152238/500000: episode: 1069, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000],  loss: 0.194917, mae: 25.544183, mean_q: -35.871719, mean_eps: 0.100000\n",
      " 152352/500000: episode: 1070, duration: 0.531s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.195591, mae: 25.711768, mean_q: -36.160870, mean_eps: 0.100000\n",
      " 152459/500000: episode: 1071, duration: 0.498s, episode steps: 107, steps per second: 215, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.161702, mae: 25.469147, mean_q: -36.112716, mean_eps: 0.100000\n",
      " 152644/500000: episode: 1072, duration: 0.870s, episode steps: 185, steps per second: 213, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.101041, mae: 25.676697, mean_q: -36.695688, mean_eps: 0.100000\n",
      " 152793/500000: episode: 1073, duration: 0.685s, episode steps: 149, steps per second: 218, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.919 [0.000, 2.000],  loss: 0.126486, mae: 26.268367, mean_q: -37.741832, mean_eps: 0.100000\n",
      " 152903/500000: episode: 1074, duration: 0.525s, episode steps: 110, steps per second: 210, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.080495, mae: 26.402419, mean_q: -38.111665, mean_eps: 0.100000\n",
      " 153018/500000: episode: 1075, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000],  loss: 0.072641, mae: 26.799820, mean_q: -38.721121, mean_eps: 0.100000\n",
      " 153172/500000: episode: 1076, duration: 0.717s, episode steps: 154, steps per second: 215, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 0.057739, mae: 26.285759, mean_q: -38.017110, mean_eps: 0.100000\n",
      " 153321/500000: episode: 1077, duration: 0.690s, episode steps: 149, steps per second: 216, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.839 [0.000, 2.000],  loss: 0.055887, mae: 26.940829, mean_q: -39.048470, mean_eps: 0.100000\n",
      " 153472/500000: episode: 1078, duration: 0.719s, episode steps: 151, steps per second: 210, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.868 [0.000, 2.000],  loss: 0.067728, mae: 26.873822, mean_q: -38.948642, mean_eps: 0.100000\n",
      " 153622/500000: episode: 1079, duration: 0.706s, episode steps: 150, steps per second: 212, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 0.082213, mae: 26.941009, mean_q: -39.018905, mean_eps: 0.100000\n",
      " 153728/500000: episode: 1080, duration: 0.494s, episode steps: 106, steps per second: 215, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.059500, mae: 26.644635, mean_q: -38.684594, mean_eps: 0.100000\n",
      " 153839/500000: episode: 1081, duration: 0.526s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.060163, mae: 26.186470, mean_q: -37.897630, mean_eps: 0.100000\n",
      " 153993/500000: episode: 1082, duration: 0.723s, episode steps: 154, steps per second: 213, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.078089, mae: 26.172045, mean_q: -37.885827, mean_eps: 0.100000\n",
      " 154193/500000: episode: 1083, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.145137, mae: 27.037727, mean_q: -39.020469, mean_eps: 0.100000\n",
      " 154347/500000: episode: 1084, duration: 0.734s, episode steps: 154, steps per second: 210, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 0.153951, mae: 27.227073, mean_q: -39.363066, mean_eps: 0.100000\n",
      " 154547/500000: episode: 1085, duration: 0.930s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.194777, mae: 27.564720, mean_q: -39.815298, mean_eps: 0.100000\n",
      " 154659/500000: episode: 1086, duration: 0.529s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.189085, mae: 27.422994, mean_q: -39.442435, mean_eps: 0.100000\n",
      " 154749/500000: episode: 1087, duration: 0.438s, episode steps:  90, steps per second: 205, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.144 [0.000, 2.000],  loss: 0.192230, mae: 27.058345, mean_q: -38.863327, mean_eps: 0.100000\n",
      " 154925/500000: episode: 1088, duration: 0.821s, episode steps: 176, steps per second: 214, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.983 [0.000, 2.000],  loss: 0.160825, mae: 28.102655, mean_q: -40.376587, mean_eps: 0.100000\n",
      " 155049/500000: episode: 1089, duration: 0.584s, episode steps: 124, steps per second: 212, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.199334, mae: 28.264084, mean_q: -40.585501, mean_eps: 0.100000\n",
      " 155169/500000: episode: 1090, duration: 0.571s, episode steps: 120, steps per second: 210, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.131845, mae: 28.069555, mean_q: -40.290464, mean_eps: 0.100000\n",
      " 155283/500000: episode: 1091, duration: 0.541s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.139998, mae: 27.914704, mean_q: -39.703365, mean_eps: 0.100000\n",
      " 155392/500000: episode: 1092, duration: 0.505s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.196167, mae: 27.774102, mean_q: -39.485281, mean_eps: 0.100000\n",
      " 155559/500000: episode: 1093, duration: 0.794s, episode steps: 167, steps per second: 210, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.138890, mae: 28.168216, mean_q: -40.012721, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 155684/500000: episode: 1094, duration: 0.588s, episode steps: 125, steps per second: 213, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.154423, mae: 28.129790, mean_q: -40.249260, mean_eps: 0.100000\n",
      " 155853/500000: episode: 1095, duration: 0.782s, episode steps: 169, steps per second: 216, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.106254, mae: 28.561613, mean_q: -40.927251, mean_eps: 0.100000\n",
      " 155952/500000: episode: 1096, duration: 0.470s, episode steps:  99, steps per second: 210, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.167606, mae: 27.969210, mean_q: -39.797750, mean_eps: 0.100000\n",
      " 156070/500000: episode: 1097, duration: 0.558s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.132706, mae: 28.329153, mean_q: -40.317562, mean_eps: 0.100000\n",
      " 156270/500000: episode: 1098, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.133429, mae: 28.111839, mean_q: -39.937868, mean_eps: 0.100000\n",
      " 156460/500000: episode: 1099, duration: 0.903s, episode steps: 190, steps per second: 210, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.089 [0.000, 2.000],  loss: 0.188880, mae: 28.835446, mean_q: -41.019383, mean_eps: 0.100000\n",
      " 156610/500000: episode: 1100, duration: 0.696s, episode steps: 150, steps per second: 216, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.191064, mae: 28.873897, mean_q: -41.066705, mean_eps: 0.100000\n",
      " 156720/500000: episode: 1101, duration: 0.508s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.089599, mae: 29.096450, mean_q: -41.508448, mean_eps: 0.100000\n",
      " 156858/500000: episode: 1102, duration: 0.660s, episode steps: 138, steps per second: 209, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.428 [0.000, 2.000],  loss: 0.190561, mae: 27.899114, mean_q: -39.481820, mean_eps: 0.100000\n",
      " 156961/500000: episode: 1103, duration: 0.488s, episode steps: 103, steps per second: 211, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.318786, mae: 28.333655, mean_q: -39.909020, mean_eps: 0.100000\n",
      " 157136/500000: episode: 1104, duration: 0.805s, episode steps: 175, steps per second: 217, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.165274, mae: 29.015986, mean_q: -40.975488, mean_eps: 0.100000\n",
      " 157311/500000: episode: 1105, duration: 0.832s, episode steps: 175, steps per second: 210, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.063 [0.000, 2.000],  loss: 0.195599, mae: 28.871826, mean_q: -40.956335, mean_eps: 0.100000\n",
      " 157487/500000: episode: 1106, duration: 0.832s, episode steps: 176, steps per second: 212, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.109028, mae: 27.869285, mean_q: -39.487275, mean_eps: 0.100000\n",
      " 157687/500000: episode: 1107, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.166285, mae: 27.564326, mean_q: -38.795204, mean_eps: 0.100000\n",
      " 157797/500000: episode: 1108, duration: 0.532s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.278217, mae: 27.818326, mean_q: -39.219876, mean_eps: 0.100000\n",
      " 157915/500000: episode: 1109, duration: 0.547s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.360783, mae: 28.573308, mean_q: -40.493076, mean_eps: 0.100000\n",
      " 158079/500000: episode: 1110, duration: 0.776s, episode steps: 164, steps per second: 211, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.191744, mae: 28.240082, mean_q: -40.073818, mean_eps: 0.100000\n",
      " 158192/500000: episode: 1111, duration: 0.544s, episode steps: 113, steps per second: 208, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.302343, mae: 27.511186, mean_q: -38.859424, mean_eps: 0.100000\n",
      " 158358/500000: episode: 1112, duration: 0.773s, episode steps: 166, steps per second: 215, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000],  loss: 0.262687, mae: 27.698997, mean_q: -39.030286, mean_eps: 0.100000\n",
      " 158534/500000: episode: 1113, duration: 0.826s, episode steps: 176, steps per second: 213, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.291493, mae: 27.892577, mean_q: -39.512550, mean_eps: 0.100000\n",
      " 158706/500000: episode: 1114, duration: 0.810s, episode steps: 172, steps per second: 212, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.226226, mae: 28.012242, mean_q: -39.721218, mean_eps: 0.100000\n",
      " 158820/500000: episode: 1115, duration: 0.523s, episode steps: 114, steps per second: 218, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.104247, mae: 27.746779, mean_q: -39.322010, mean_eps: 0.100000\n",
      " 158938/500000: episode: 1116, duration: 0.555s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.099158, mae: 27.422290, mean_q: -38.985495, mean_eps: 0.100000\n",
      " 159138/500000: episode: 1117, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.111385, mae: 27.061989, mean_q: -38.479168, mean_eps: 0.100000\n",
      " 159303/500000: episode: 1118, duration: 0.770s, episode steps: 165, steps per second: 214, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.118750, mae: 27.813937, mean_q: -39.695904, mean_eps: 0.100000\n",
      " 159399/500000: episode: 1119, duration: 0.456s, episode steps:  96, steps per second: 210, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.096728, mae: 27.485215, mean_q: -39.142773, mean_eps: 0.100000\n",
      " 159559/500000: episode: 1120, duration: 0.751s, episode steps: 160, steps per second: 213, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.125708, mae: 27.362661, mean_q: -38.842255, mean_eps: 0.100000\n",
      " 159673/500000: episode: 1121, duration: 0.534s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.111614, mae: 27.018122, mean_q: -38.373009, mean_eps: 0.100000\n",
      " 159837/500000: episode: 1122, duration: 0.775s, episode steps: 164, steps per second: 212, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.116281, mae: 27.592111, mean_q: -39.159322, mean_eps: 0.100000\n",
      " 159955/500000: episode: 1123, duration: 0.559s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.441 [0.000, 2.000],  loss: 0.152815, mae: 27.685253, mean_q: -39.487837, mean_eps: 0.100000\n",
      " 160117/500000: episode: 1124, duration: 0.748s, episode steps: 162, steps per second: 217, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.125436, mae: 27.829579, mean_q: -39.651672, mean_eps: 0.100000\n",
      " 160226/500000: episode: 1125, duration: 0.522s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.169437, mae: 27.582407, mean_q: -38.946270, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 160330/500000: episode: 1126, duration: 0.494s, episode steps: 104, steps per second: 210, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.158505, mae: 27.635139, mean_q: -38.803653, mean_eps: 0.100000\n",
      " 160439/500000: episode: 1127, duration: 0.508s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.168474, mae: 27.575250, mean_q: -38.876611, mean_eps: 0.100000\n",
      " 160553/500000: episode: 1128, duration: 0.526s, episode steps: 114, steps per second: 217, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.114497, mae: 27.204625, mean_q: -38.392264, mean_eps: 0.100000\n",
      " 160667/500000: episode: 1129, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.106354, mae: 27.700779, mean_q: -39.037681, mean_eps: 0.100000\n",
      " 160778/500000: episode: 1130, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.107014, mae: 26.727650, mean_q: -37.553523, mean_eps: 0.100000\n",
      " 160976/500000: episode: 1131, duration: 0.911s, episode steps: 198, steps per second: 217, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.165950, mae: 26.967777, mean_q: -37.576406, mean_eps: 0.100000\n",
      " 161172/500000: episode: 1132, duration: 0.925s, episode steps: 196, steps per second: 212, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.213921, mae: 26.786185, mean_q: -37.218382, mean_eps: 0.100000\n",
      " 161287/500000: episode: 1133, duration: 0.545s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.149691, mae: 27.017328, mean_q: -37.819585, mean_eps: 0.100000\n",
      " 161396/500000: episode: 1134, duration: 0.508s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.172875, mae: 26.782745, mean_q: -37.726838, mean_eps: 0.100000\n",
      " 161515/500000: episode: 1135, duration: 0.568s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.218 [0.000, 2.000],  loss: 0.113749, mae: 26.832090, mean_q: -37.879641, mean_eps: 0.100000\n",
      " 161662/500000: episode: 1136, duration: 0.686s, episode steps: 147, steps per second: 214, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.871 [0.000, 2.000],  loss: 0.123364, mae: 27.523928, mean_q: -39.102462, mean_eps: 0.100000\n",
      " 161809/500000: episode: 1137, duration: 0.681s, episode steps: 147, steps per second: 216, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.857 [0.000, 2.000],  loss: 0.167273, mae: 27.471863, mean_q: -39.231609, mean_eps: 0.100000\n",
      " 161898/500000: episode: 1138, duration: 0.426s, episode steps:  89, steps per second: 209, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.169 [0.000, 2.000],  loss: 0.136986, mae: 27.524920, mean_q: -39.472012, mean_eps: 0.100000\n",
      " 162098/500000: episode: 1139, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.098665, mae: 27.406021, mean_q: -39.288196, mean_eps: 0.100000\n",
      " 162285/500000: episode: 1140, duration: 0.883s, episode steps: 187, steps per second: 212, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.121956, mae: 27.843441, mean_q: -39.881326, mean_eps: 0.100000\n",
      " 162399/500000: episode: 1141, duration: 0.573s, episode steps: 114, steps per second: 199, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.097867, mae: 28.328617, mean_q: -40.441964, mean_eps: 0.100000\n",
      " 162516/500000: episode: 1142, duration: 0.566s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.128079, mae: 28.350773, mean_q: -40.349343, mean_eps: 0.100000\n",
      " 162633/500000: episode: 1143, duration: 0.605s, episode steps: 117, steps per second: 193, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.094058, mae: 27.743985, mean_q: -39.429410, mean_eps: 0.100000\n",
      " 162754/500000: episode: 1144, duration: 0.640s, episode steps: 121, steps per second: 189, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.100742, mae: 27.810757, mean_q: -39.320439, mean_eps: 0.100000\n",
      " 162910/500000: episode: 1145, duration: 0.799s, episode steps: 156, steps per second: 195, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.139618, mae: 28.065801, mean_q: -39.487755, mean_eps: 0.100000\n",
      " 163083/500000: episode: 1146, duration: 0.871s, episode steps: 173, steps per second: 199, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.171627, mae: 28.049006, mean_q: -39.564035, mean_eps: 0.100000\n",
      " 163283/500000: episode: 1147, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.222832, mae: 27.562105, mean_q: -38.911658, mean_eps: 0.100000\n",
      " 163474/500000: episode: 1148, duration: 0.951s, episode steps: 191, steps per second: 201, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.525351, mae: 28.584308, mean_q: -40.556078, mean_eps: 0.100000\n",
      " 163674/500000: episode: 1149, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.528517, mae: 29.244841, mean_q: -41.633259, mean_eps: 0.100000\n",
      " 163833/500000: episode: 1150, duration: 0.750s, episode steps: 159, steps per second: 212, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.943 [0.000, 2.000],  loss: 0.513124, mae: 29.389189, mean_q: -41.995900, mean_eps: 0.100000\n",
      " 163954/500000: episode: 1151, duration: 0.618s, episode steps: 121, steps per second: 196, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.443615, mae: 29.114343, mean_q: -41.748245, mean_eps: 0.100000\n",
      " 164066/500000: episode: 1152, duration: 0.569s, episode steps: 112, steps per second: 197, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.330316, mae: 28.751169, mean_q: -40.997148, mean_eps: 0.100000\n",
      " 164248/500000: episode: 1153, duration: 0.878s, episode steps: 182, steps per second: 207, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.294894, mae: 29.515666, mean_q: -41.965197, mean_eps: 0.100000\n",
      " 164365/500000: episode: 1154, duration: 0.574s, episode steps: 117, steps per second: 204, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.243562, mae: 28.496217, mean_q: -40.498224, mean_eps: 0.100000\n",
      " 164487/500000: episode: 1155, duration: 0.618s, episode steps: 122, steps per second: 197, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.105198, mae: 27.506597, mean_q: -38.821846, mean_eps: 0.100000\n",
      " 164679/500000: episode: 1156, duration: 0.938s, episode steps: 192, steps per second: 205, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.114452, mae: 28.404770, mean_q: -40.203434, mean_eps: 0.100000\n",
      " 164796/500000: episode: 1157, duration: 0.561s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.107458, mae: 28.078288, mean_q: -39.510150, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 164915/500000: episode: 1158, duration: 0.559s, episode steps: 119, steps per second: 213, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.109209, mae: 28.156662, mean_q: -39.510092, mean_eps: 0.100000\n",
      " 165019/500000: episode: 1159, duration: 0.482s, episode steps: 104, steps per second: 216, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.107710, mae: 27.775230, mean_q: -38.810099, mean_eps: 0.100000\n",
      " 165137/500000: episode: 1160, duration: 0.552s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.175149, mae: 27.698785, mean_q: -38.723077, mean_eps: 0.100000\n",
      " 165250/500000: episode: 1161, duration: 0.546s, episode steps: 113, steps per second: 207, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.143786, mae: 27.650212, mean_q: -38.531753, mean_eps: 0.100000\n",
      " 165450/500000: episode: 1162, duration: 0.924s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.108542, mae: 27.289248, mean_q: -37.951085, mean_eps: 0.100000\n",
      " 165573/500000: episode: 1163, duration: 0.580s, episode steps: 123, steps per second: 212, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.099554, mae: 26.167364, mean_q: -36.655760, mean_eps: 0.100000\n",
      " 165773/500000: episode: 1164, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.143979, mae: 26.308581, mean_q: -36.781377, mean_eps: 0.100000\n",
      " 165896/500000: episode: 1165, duration: 0.565s, episode steps: 123, steps per second: 218, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.126890, mae: 26.287801, mean_q: -36.825358, mean_eps: 0.100000\n",
      " 166087/500000: episode: 1166, duration: 0.896s, episode steps: 191, steps per second: 213, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.140544, mae: 26.378573, mean_q: -37.241740, mean_eps: 0.100000\n",
      " 166247/500000: episode: 1167, duration: 0.742s, episode steps: 160, steps per second: 216, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.193228, mae: 26.439382, mean_q: -37.432567, mean_eps: 0.100000\n",
      " 166416/500000: episode: 1168, duration: 0.791s, episode steps: 169, steps per second: 214, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.763 [0.000, 2.000],  loss: 0.107672, mae: 26.149289, mean_q: -37.282257, mean_eps: 0.100000\n",
      " 166531/500000: episode: 1169, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 0.115376, mae: 26.558283, mean_q: -37.922222, mean_eps: 0.100000\n",
      " 166638/500000: episode: 1170, duration: 0.500s, episode steps: 107, steps per second: 214, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.110883, mae: 26.624516, mean_q: -38.113059, mean_eps: 0.100000\n",
      " 166822/500000: episode: 1171, duration: 0.855s, episode steps: 184, steps per second: 215, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.098504, mae: 26.492798, mean_q: -37.857682, mean_eps: 0.100000\n",
      " 166912/500000: episode: 1172, duration: 0.433s, episode steps:  90, steps per second: 208, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.105325, mae: 26.256815, mean_q: -37.541221, mean_eps: 0.100000\n",
      " 167062/500000: episode: 1173, duration: 0.695s, episode steps: 150, steps per second: 216, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.467 [0.000, 2.000],  loss: 0.075744, mae: 26.275594, mean_q: -37.352384, mean_eps: 0.100000\n",
      " 167257/500000: episode: 1174, duration: 0.900s, episode steps: 195, steps per second: 217, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.112703, mae: 26.852921, mean_q: -38.057542, mean_eps: 0.100000\n",
      " 167419/500000: episode: 1175, duration: 0.762s, episode steps: 162, steps per second: 213, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.143894, mae: 26.888518, mean_q: -37.951160, mean_eps: 0.100000\n",
      " 167582/500000: episode: 1176, duration: 0.782s, episode steps: 163, steps per second: 209, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.140631, mae: 26.993538, mean_q: -38.217240, mean_eps: 0.100000\n",
      " 167701/500000: episode: 1177, duration: 0.554s, episode steps: 119, steps per second: 215, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.231558, mae: 27.445256, mean_q: -39.064747, mean_eps: 0.100000\n",
      " 167820/500000: episode: 1178, duration: 0.568s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.271259, mae: 27.453238, mean_q: -38.799561, mean_eps: 0.100000\n",
      " 168001/500000: episode: 1179, duration: 0.830s, episode steps: 181, steps per second: 218, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.189123, mae: 28.420089, mean_q: -40.218441, mean_eps: 0.100000\n",
      " 168145/500000: episode: 1180, duration: 0.679s, episode steps: 144, steps per second: 212, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.403 [0.000, 2.000],  loss: 0.165206, mae: 28.203384, mean_q: -39.966454, mean_eps: 0.100000\n",
      " 168345/500000: episode: 1181, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.169923, mae: 27.690302, mean_q: -38.972480, mean_eps: 0.100000\n",
      " 168466/500000: episode: 1182, duration: 0.570s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.128447, mae: 27.608752, mean_q: -38.779698, mean_eps: 0.100000\n",
      " 168587/500000: episode: 1183, duration: 0.597s, episode steps: 121, steps per second: 203, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.120460, mae: 26.696909, mean_q: -37.448727, mean_eps: 0.100000\n",
      " 168752/500000: episode: 1184, duration: 0.814s, episode steps: 165, steps per second: 203, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000],  loss: 0.098980, mae: 26.314847, mean_q: -36.963391, mean_eps: 0.100000\n",
      " 168865/500000: episode: 1185, duration: 0.541s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.176973, mae: 25.923454, mean_q: -36.725289, mean_eps: 0.100000\n",
      " 169059/500000: episode: 1186, duration: 0.957s, episode steps: 194, steps per second: 203, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.095183, mae: 25.657441, mean_q: -36.315045, mean_eps: 0.100000\n",
      " 169256/500000: episode: 1187, duration: 0.969s, episode steps: 197, steps per second: 203, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.848 [0.000, 2.000],  loss: 0.100024, mae: 26.594632, mean_q: -38.076321, mean_eps: 0.100000\n",
      " 169438/500000: episode: 1188, duration: 0.905s, episode steps: 182, steps per second: 201, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.083401, mae: 26.848336, mean_q: -38.643384, mean_eps: 0.100000\n",
      " 169578/500000: episode: 1189, duration: 0.668s, episode steps: 140, steps per second: 209, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.414 [0.000, 2.000],  loss: 0.095878, mae: 26.937600, mean_q: -38.658481, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 169672/500000: episode: 1190, duration: 0.491s, episode steps:  94, steps per second: 191, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.120078, mae: 27.316754, mean_q: -39.019789, mean_eps: 0.100000\n",
      " 169778/500000: episode: 1191, duration: 0.542s, episode steps: 106, steps per second: 196, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.112166, mae: 26.512109, mean_q: -37.683324, mean_eps: 0.100000\n",
      " 169896/500000: episode: 1192, duration: 0.574s, episode steps: 118, steps per second: 206, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.129698, mae: 27.300556, mean_q: -38.831598, mean_eps: 0.100000\n",
      " 170051/500000: episode: 1193, duration: 0.793s, episode steps: 155, steps per second: 196, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.103929, mae: 27.479025, mean_q: -39.109812, mean_eps: 0.100000\n",
      " 170197/500000: episode: 1194, duration: 0.737s, episode steps: 146, steps per second: 198, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.418 [0.000, 2.000],  loss: 0.085209, mae: 26.833529, mean_q: -38.276557, mean_eps: 0.100000\n",
      " 170315/500000: episode: 1195, duration: 0.590s, episode steps: 118, steps per second: 200, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.130467, mae: 26.022041, mean_q: -36.787621, mean_eps: 0.100000\n",
      " 170404/500000: episode: 1196, duration: 0.434s, episode steps:  89, steps per second: 205, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.065365, mae: 25.143839, mean_q: -35.732665, mean_eps: 0.100000\n",
      " 170604/500000: episode: 1197, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.066860, mae: 25.721512, mean_q: -36.583981, mean_eps: 0.100000\n",
      " 170802/500000: episode: 1198, duration: 0.982s, episode steps: 198, steps per second: 202, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.313948, mae: 26.629471, mean_q: -38.038164, mean_eps: 0.100000\n",
      " 170910/500000: episode: 1199, duration: 0.546s, episode steps: 108, steps per second: 198, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.166007, mae: 26.714888, mean_q: -38.254456, mean_eps: 0.100000\n",
      " 171097/500000: episode: 1200, duration: 0.895s, episode steps: 187, steps per second: 209, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.209849, mae: 26.482602, mean_q: -37.675814, mean_eps: 0.100000\n",
      " 171208/500000: episode: 1201, duration: 0.523s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.246069, mae: 26.071953, mean_q: -37.114378, mean_eps: 0.100000\n",
      " 171333/500000: episode: 1202, duration: 0.593s, episode steps: 125, steps per second: 211, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.210266, mae: 26.456179, mean_q: -37.723638, mean_eps: 0.100000\n",
      " 171422/500000: episode: 1203, duration: 0.422s, episode steps:  89, steps per second: 211, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.160428, mae: 27.432899, mean_q: -38.987200, mean_eps: 0.100000\n",
      " 171607/500000: episode: 1204, duration: 0.876s, episode steps: 185, steps per second: 211, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.140303, mae: 27.315932, mean_q: -38.625905, mean_eps: 0.100000\n",
      " 171702/500000: episode: 1205, duration: 0.449s, episode steps:  95, steps per second: 211, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.078188, mae: 26.349224, mean_q: -36.980661, mean_eps: 0.100000\n",
      " 171891/500000: episode: 1206, duration: 0.925s, episode steps: 189, steps per second: 204, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.122125, mae: 26.556682, mean_q: -37.034247, mean_eps: 0.100000\n",
      " 172050/500000: episode: 1207, duration: 0.760s, episode steps: 159, steps per second: 209, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000],  loss: 0.167800, mae: 26.747387, mean_q: -37.336658, mean_eps: 0.100000\n",
      " 172200/500000: episode: 1208, duration: 0.734s, episode steps: 150, steps per second: 204, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.102509, mae: 27.020957, mean_q: -37.816392, mean_eps: 0.100000\n",
      " 172313/500000: episode: 1209, duration: 0.558s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.156210, mae: 27.153494, mean_q: -38.237416, mean_eps: 0.100000\n",
      " 172427/500000: episode: 1210, duration: 0.537s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.098077, mae: 26.839798, mean_q: -38.070835, mean_eps: 0.100000\n",
      " 172577/500000: episode: 1211, duration: 0.701s, episode steps: 150, steps per second: 214, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.953 [0.000, 2.000],  loss: 0.109893, mae: 26.531088, mean_q: -37.769585, mean_eps: 0.100000\n",
      " 172728/500000: episode: 1212, duration: 0.720s, episode steps: 151, steps per second: 210, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.371 [0.000, 2.000],  loss: 0.147336, mae: 27.419011, mean_q: -38.895491, mean_eps: 0.100000\n",
      " 172829/500000: episode: 1213, duration: 0.501s, episode steps: 101, steps per second: 202, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.083506, mae: 27.337038, mean_q: -38.804728, mean_eps: 0.100000\n",
      " 172938/500000: episode: 1214, duration: 0.513s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.086464, mae: 27.267481, mean_q: -38.700581, mean_eps: 0.100000\n",
      " 173091/500000: episode: 1215, duration: 0.727s, episode steps: 153, steps per second: 211, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.882 [0.000, 2.000],  loss: 0.099364, mae: 27.304302, mean_q: -38.659735, mean_eps: 0.100000\n",
      " 173250/500000: episode: 1216, duration: 0.767s, episode steps: 159, steps per second: 207, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.969 [0.000, 2.000],  loss: 0.077205, mae: 27.375043, mean_q: -38.879166, mean_eps: 0.100000\n",
      " 173446/500000: episode: 1217, duration: 0.927s, episode steps: 196, steps per second: 211, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.194382, mae: 28.247327, mean_q: -39.969238, mean_eps: 0.100000\n",
      " 173558/500000: episode: 1218, duration: 0.576s, episode steps: 112, steps per second: 195, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.107449, mae: 28.231477, mean_q: -40.059474, mean_eps: 0.100000\n",
      " 173731/500000: episode: 1219, duration: 0.823s, episode steps: 173, steps per second: 210, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 0.102807, mae: 28.777308, mean_q: -40.883441, mean_eps: 0.100000\n",
      " 173845/500000: episode: 1220, duration: 0.607s, episode steps: 114, steps per second: 188, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.111005, mae: 28.545070, mean_q: -40.886953, mean_eps: 0.100000\n",
      " 173961/500000: episode: 1221, duration: 0.564s, episode steps: 116, steps per second: 206, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.170062, mae: 28.213897, mean_q: -40.329627, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 174075/500000: episode: 1222, duration: 0.559s, episode steps: 114, steps per second: 204, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.191105, mae: 27.681035, mean_q: -39.631793, mean_eps: 0.100000\n",
      " 174275/500000: episode: 1223, duration: 0.991s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.122802, mae: 27.540202, mean_q: -38.974598, mean_eps: 0.100000\n",
      " 174445/500000: episode: 1224, duration: 0.814s, episode steps: 170, steps per second: 209, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.174559, mae: 27.751333, mean_q: -39.245502, mean_eps: 0.100000\n",
      " 174645/500000: episode: 1225, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.228173, mae: 27.983301, mean_q: -39.649208, mean_eps: 0.100000\n",
      " 174837/500000: episode: 1226, duration: 0.922s, episode steps: 192, steps per second: 208, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.168385, mae: 28.092314, mean_q: -39.722496, mean_eps: 0.100000\n",
      " 174954/500000: episode: 1227, duration: 0.544s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.261029, mae: 28.554689, mean_q: -40.486149, mean_eps: 0.100000\n",
      " 175071/500000: episode: 1228, duration: 0.552s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.230219, mae: 28.166437, mean_q: -39.723581, mean_eps: 0.100000\n",
      " 175182/500000: episode: 1229, duration: 0.537s, episode steps: 111, steps per second: 207, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.200908, mae: 28.363456, mean_q: -40.109402, mean_eps: 0.100000\n",
      " 175316/500000: episode: 1230, duration: 0.645s, episode steps: 134, steps per second: 208, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.133332, mae: 27.809287, mean_q: -39.333977, mean_eps: 0.100000\n",
      " 175496/500000: episode: 1231, duration: 0.858s, episode steps: 180, steps per second: 210, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.202860, mae: 27.089518, mean_q: -38.044676, mean_eps: 0.100000\n",
      " 175617/500000: episode: 1232, duration: 0.597s, episode steps: 121, steps per second: 203, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.176158, mae: 27.338217, mean_q: -38.432882, mean_eps: 0.100000\n",
      " 175733/500000: episode: 1233, duration: 0.557s, episode steps: 116, steps per second: 208, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.188051, mae: 26.858093, mean_q: -37.614451, mean_eps: 0.100000\n",
      " 175850/500000: episode: 1234, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.119318, mae: 25.854099, mean_q: -36.366110, mean_eps: 0.100000\n",
      " 175968/500000: episode: 1235, duration: 0.562s, episode steps: 118, steps per second: 210, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.091228, mae: 26.280684, mean_q: -36.784442, mean_eps: 0.100000\n",
      " 176128/500000: episode: 1236, duration: 0.784s, episode steps: 160, steps per second: 204, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.931 [0.000, 2.000],  loss: 0.104792, mae: 26.871630, mean_q: -37.859723, mean_eps: 0.100000\n",
      " 176263/500000: episode: 1237, duration: 0.638s, episode steps: 135, steps per second: 212, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 0.119487, mae: 27.044648, mean_q: -38.142620, mean_eps: 0.100000\n",
      " 176385/500000: episode: 1238, duration: 0.604s, episode steps: 122, steps per second: 202, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.112045, mae: 27.198944, mean_q: -38.047582, mean_eps: 0.100000\n",
      " 176495/500000: episode: 1239, duration: 0.571s, episode steps: 110, steps per second: 193, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.093134, mae: 26.677459, mean_q: -37.077261, mean_eps: 0.100000\n",
      " 176594/500000: episode: 1240, duration: 0.469s, episode steps:  99, steps per second: 211, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.112046, mae: 26.771136, mean_q: -37.066319, mean_eps: 0.100000\n",
      " 176711/500000: episode: 1241, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.142856, mae: 26.603045, mean_q: -36.520490, mean_eps: 0.100000\n",
      " 176827/500000: episode: 1242, duration: 0.568s, episode steps: 116, steps per second: 204, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.053648, mae: 27.027727, mean_q: -37.206266, mean_eps: 0.100000\n",
      " 176983/500000: episode: 1243, duration: 0.749s, episode steps: 156, steps per second: 208, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.077197, mae: 27.323816, mean_q: -37.786632, mean_eps: 0.100000\n",
      " 177102/500000: episode: 1244, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.084921, mae: 26.221329, mean_q: -36.333957, mean_eps: 0.100000\n",
      " 177233/500000: episode: 1245, duration: 0.629s, episode steps: 131, steps per second: 208, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.107035, mae: 26.317708, mean_q: -36.409610, mean_eps: 0.100000\n",
      " 177350/500000: episode: 1246, duration: 0.566s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.106672, mae: 26.542970, mean_q: -36.915049, mean_eps: 0.100000\n",
      " 177550/500000: episode: 1247, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.100224, mae: 26.584199, mean_q: -37.345918, mean_eps: 0.100000\n",
      " 177750/500000: episode: 1248, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.267296, mae: 27.466950, mean_q: -38.735171, mean_eps: 0.100000\n",
      " 177907/500000: episode: 1249, duration: 0.772s, episode steps: 157, steps per second: 203, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.232198, mae: 28.297186, mean_q: -40.160371, mean_eps: 0.100000\n",
      " 178095/500000: episode: 1250, duration: 0.928s, episode steps: 188, steps per second: 203, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.280410, mae: 28.082176, mean_q: -39.890835, mean_eps: 0.100000\n",
      " 178289/500000: episode: 1251, duration: 0.925s, episode steps: 194, steps per second: 210, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.244667, mae: 28.447275, mean_q: -40.539419, mean_eps: 0.100000\n",
      " 178441/500000: episode: 1252, duration: 0.720s, episode steps: 152, steps per second: 211, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.053 [0.000, 2.000],  loss: 0.260073, mae: 29.238434, mean_q: -41.771321, mean_eps: 0.100000\n",
      " 178614/500000: episode: 1253, duration: 0.811s, episode steps: 173, steps per second: 213, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.247915, mae: 29.279294, mean_q: -42.104646, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 178814/500000: episode: 1254, duration: 0.969s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.186528, mae: 28.942846, mean_q: -41.709711, mean_eps: 0.100000\n",
      " 178971/500000: episode: 1255, duration: 0.767s, episode steps: 157, steps per second: 205, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.943 [0.000, 2.000],  loss: 0.269972, mae: 29.123449, mean_q: -41.852011, mean_eps: 0.100000\n",
      " 179152/500000: episode: 1256, duration: 0.843s, episode steps: 181, steps per second: 215, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.176288, mae: 28.900314, mean_q: -41.630503, mean_eps: 0.100000\n",
      " 179305/500000: episode: 1257, duration: 0.725s, episode steps: 153, steps per second: 211, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.444 [0.000, 2.000],  loss: 0.147968, mae: 28.425959, mean_q: -40.697274, mean_eps: 0.100000\n",
      " 179419/500000: episode: 1258, duration: 0.557s, episode steps: 114, steps per second: 205, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.160818, mae: 28.490530, mean_q: -40.407454, mean_eps: 0.100000\n",
      " 179522/500000: episode: 1259, duration: 0.480s, episode steps: 103, steps per second: 215, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.132257, mae: 28.904661, mean_q: -40.658383, mean_eps: 0.100000\n",
      " 179701/500000: episode: 1260, duration: 0.852s, episode steps: 179, steps per second: 210, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.508 [0.000, 2.000],  loss: 0.130941, mae: 28.979543, mean_q: -40.293449, mean_eps: 0.100000\n",
      " 179895/500000: episode: 1261, duration: 0.924s, episode steps: 194, steps per second: 210, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.137902, mae: 28.377943, mean_q: -39.242952, mean_eps: 0.100000\n",
      " 180004/500000: episode: 1262, duration: 0.508s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.141535, mae: 28.090557, mean_q: -38.728197, mean_eps: 0.100000\n",
      " 180120/500000: episode: 1263, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.149615, mae: 28.044684, mean_q: -38.703807, mean_eps: 0.100000\n",
      " 180239/500000: episode: 1264, duration: 0.575s, episode steps: 119, steps per second: 207, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.164117, mae: 28.253082, mean_q: -38.825041, mean_eps: 0.100000\n",
      " 180355/500000: episode: 1265, duration: 0.556s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.125747, mae: 27.708041, mean_q: -38.076318, mean_eps: 0.100000\n",
      " 180468/500000: episode: 1266, duration: 0.529s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.105639, mae: 27.577906, mean_q: -37.995042, mean_eps: 0.100000\n",
      " 180647/500000: episode: 1267, duration: 0.844s, episode steps: 179, steps per second: 212, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.804 [0.000, 2.000],  loss: 0.146901, mae: 27.670857, mean_q: -38.389282, mean_eps: 0.100000\n",
      " 180833/500000: episode: 1268, duration: 0.867s, episode steps: 186, steps per second: 214, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.093431, mae: 27.425270, mean_q: -38.419320, mean_eps: 0.100000\n",
      " 181008/500000: episode: 1269, duration: 0.826s, episode steps: 175, steps per second: 212, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.869 [0.000, 2.000],  loss: 0.086945, mae: 27.205548, mean_q: -38.320592, mean_eps: 0.100000\n",
      " 181130/500000: episode: 1270, duration: 0.574s, episode steps: 122, steps per second: 213, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.098397, mae: 28.005048, mean_q: -39.663973, mean_eps: 0.100000\n",
      " 181245/500000: episode: 1271, duration: 0.533s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.107698, mae: 27.450220, mean_q: -38.774749, mean_eps: 0.100000\n",
      " 181368/500000: episode: 1272, duration: 0.577s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.090591, mae: 27.542994, mean_q: -39.090218, mean_eps: 0.100000\n",
      " 181481/500000: episode: 1273, duration: 0.542s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.077444, mae: 27.092554, mean_q: -38.419073, mean_eps: 0.100000\n",
      " 181647/500000: episode: 1274, duration: 0.771s, episode steps: 166, steps per second: 215, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.112401, mae: 27.003880, mean_q: -38.160388, mean_eps: 0.100000\n",
      " 181808/500000: episode: 1275, duration: 0.759s, episode steps: 161, steps per second: 212, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.121735, mae: 27.210601, mean_q: -38.351654, mean_eps: 0.100000\n",
      " 181991/500000: episode: 1276, duration: 0.874s, episode steps: 183, steps per second: 209, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.185426, mae: 27.219619, mean_q: -38.511416, mean_eps: 0.100000\n",
      " 182140/500000: episode: 1277, duration: 0.693s, episode steps: 149, steps per second: 215, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.933 [0.000, 2.000],  loss: 0.152199, mae: 26.753751, mean_q: -37.730155, mean_eps: 0.100000\n",
      " 182287/500000: episode: 1278, duration: 0.706s, episode steps: 147, steps per second: 208, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.871 [0.000, 2.000],  loss: 0.162459, mae: 27.186188, mean_q: -38.573359, mean_eps: 0.100000\n",
      " 182406/500000: episode: 1279, duration: 0.577s, episode steps: 119, steps per second: 206, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.149953, mae: 27.235003, mean_q: -38.834071, mean_eps: 0.100000\n",
      " 182582/500000: episode: 1280, duration: 0.889s, episode steps: 176, steps per second: 198, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.154412, mae: 27.571039, mean_q: -39.370758, mean_eps: 0.100000\n",
      " 182714/500000: episode: 1281, duration: 0.666s, episode steps: 132, steps per second: 198, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.394 [0.000, 2.000],  loss: 0.131676, mae: 28.002439, mean_q: -40.159711, mean_eps: 0.100000\n",
      " 182889/500000: episode: 1282, duration: 0.869s, episode steps: 175, steps per second: 201, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.129964, mae: 27.727844, mean_q: -39.370566, mean_eps: 0.100000\n",
      " 183006/500000: episode: 1283, duration: 0.561s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.122548, mae: 27.731388, mean_q: -39.397992, mean_eps: 0.100000\n",
      " 183206/500000: episode: 1284, duration: 1.032s, episode steps: 200, steps per second: 194, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.123240, mae: 28.321397, mean_q: -40.011175, mean_eps: 0.100000\n",
      " 183366/500000: episode: 1285, duration: 0.775s, episode steps: 160, steps per second: 206, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.217157, mae: 28.294853, mean_q: -39.896392, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 183461/500000: episode: 1286, duration: 0.468s, episode steps:  95, steps per second: 203, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.295521, mae: 27.896242, mean_q: -39.543575, mean_eps: 0.100000\n",
      " 183614/500000: episode: 1287, duration: 0.757s, episode steps: 153, steps per second: 202, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.212826, mae: 27.230867, mean_q: -38.267993, mean_eps: 0.100000\n",
      " 183814/500000: episode: 1288, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.162226, mae: 26.904269, mean_q: -38.015352, mean_eps: 0.100000\n",
      " 183923/500000: episode: 1289, duration: 0.518s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.229156, mae: 26.769233, mean_q: -37.856762, mean_eps: 0.100000\n",
      " 184047/500000: episode: 1290, duration: 0.579s, episode steps: 124, steps per second: 214, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.323 [0.000, 2.000],  loss: 0.233584, mae: 27.009079, mean_q: -38.220915, mean_eps: 0.100000\n",
      " 184168/500000: episode: 1291, duration: 0.569s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.166475, mae: 26.689388, mean_q: -37.602836, mean_eps: 0.100000\n",
      " 184293/500000: episode: 1292, duration: 0.639s, episode steps: 125, steps per second: 196, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.127746, mae: 26.646597, mean_q: -37.405435, mean_eps: 0.100000\n",
      " 184460/500000: episode: 1293, duration: 0.871s, episode steps: 167, steps per second: 192, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.142166, mae: 26.853588, mean_q: -37.556863, mean_eps: 0.100000\n",
      " 184571/500000: episode: 1294, duration: 0.547s, episode steps: 111, steps per second: 203, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.203830, mae: 26.882963, mean_q: -37.665347, mean_eps: 0.100000\n",
      " 184686/500000: episode: 1295, duration: 0.545s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.127436, mae: 27.066189, mean_q: -37.638823, mean_eps: 0.100000\n",
      " 184857/500000: episode: 1296, duration: 0.831s, episode steps: 171, steps per second: 206, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.135404, mae: 26.828597, mean_q: -37.683563, mean_eps: 0.100000\n",
      " 185014/500000: episode: 1297, duration: 0.725s, episode steps: 157, steps per second: 217, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.924 [0.000, 2.000],  loss: 0.082908, mae: 26.915369, mean_q: -37.877913, mean_eps: 0.100000\n",
      " 185184/500000: episode: 1298, duration: 0.811s, episode steps: 170, steps per second: 210, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.081345, mae: 27.323172, mean_q: -38.896986, mean_eps: 0.100000\n",
      " 185296/500000: episode: 1299, duration: 0.532s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.081397, mae: 27.255493, mean_q: -38.927425, mean_eps: 0.100000\n",
      " 185447/500000: episode: 1300, duration: 0.734s, episode steps: 151, steps per second: 206, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.801 [0.000, 2.000],  loss: 0.140379, mae: 27.316866, mean_q: -38.846231, mean_eps: 0.100000\n",
      " 185563/500000: episode: 1301, duration: 0.664s, episode steps: 116, steps per second: 175, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.162422, mae: 27.653301, mean_q: -39.575736, mean_eps: 0.100000\n",
      " 185668/500000: episode: 1302, duration: 0.514s, episode steps: 105, steps per second: 204, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.080467, mae: 27.295850, mean_q: -39.136879, mean_eps: 0.100000\n",
      " 185840/500000: episode: 1303, duration: 0.797s, episode steps: 172, steps per second: 216, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.080637, mae: 27.292606, mean_q: -39.041943, mean_eps: 0.100000\n",
      " 185955/500000: episode: 1304, duration: 0.578s, episode steps: 115, steps per second: 199, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.080237, mae: 27.296536, mean_q: -38.906698, mean_eps: 0.100000\n",
      " 186109/500000: episode: 1305, duration: 0.747s, episode steps: 154, steps per second: 206, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.857 [0.000, 2.000],  loss: 0.086335, mae: 27.498672, mean_q: -39.011813, mean_eps: 0.100000\n",
      " 186267/500000: episode: 1306, duration: 0.774s, episode steps: 158, steps per second: 204, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.072294, mae: 27.313677, mean_q: -38.775847, mean_eps: 0.100000\n",
      " 186384/500000: episode: 1307, duration: 0.574s, episode steps: 117, steps per second: 204, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.059262, mae: 27.575432, mean_q: -39.445765, mean_eps: 0.100000\n",
      " 186501/500000: episode: 1308, duration: 0.604s, episode steps: 117, steps per second: 194, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.073806, mae: 26.906395, mean_q: -38.313073, mean_eps: 0.100000\n",
      " 186613/500000: episode: 1309, duration: 0.524s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.156160, mae: 27.648859, mean_q: -39.148700, mean_eps: 0.100000\n",
      " 186768/500000: episode: 1310, duration: 0.758s, episode steps: 155, steps per second: 204, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.114489, mae: 27.287054, mean_q: -38.702069, mean_eps: 0.100000\n",
      " 186962/500000: episode: 1311, duration: 0.922s, episode steps: 194, steps per second: 210, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.086545, mae: 27.037893, mean_q: -38.431095, mean_eps: 0.100000\n",
      " 187105/500000: episode: 1312, duration: 0.687s, episode steps: 143, steps per second: 208, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000],  loss: 0.080880, mae: 27.057829, mean_q: -38.569544, mean_eps: 0.100000\n",
      " 187219/500000: episode: 1313, duration: 0.663s, episode steps: 114, steps per second: 172, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.074961, mae: 26.652816, mean_q: -37.604902, mean_eps: 0.100000\n",
      " 187338/500000: episode: 1314, duration: 0.677s, episode steps: 119, steps per second: 176, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.107495, mae: 26.761002, mean_q: -37.589108, mean_eps: 0.100000\n",
      " 187507/500000: episode: 1315, duration: 0.838s, episode steps: 169, steps per second: 202, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 0.103664, mae: 26.919940, mean_q: -37.874066, mean_eps: 0.100000\n",
      " 187657/500000: episode: 1316, duration: 0.748s, episode steps: 150, steps per second: 200, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.887 [0.000, 2.000],  loss: 0.184449, mae: 27.675186, mean_q: -39.201532, mean_eps: 0.100000\n",
      " 187781/500000: episode: 1317, duration: 0.616s, episode steps: 124, steps per second: 201, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.129202, mae: 27.222408, mean_q: -38.724045, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 187874/500000: episode: 1318, duration: 0.459s, episode steps:  93, steps per second: 203, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.108644, mae: 27.430985, mean_q: -38.889961, mean_eps: 0.100000\n",
      " 188064/500000: episode: 1319, duration: 0.938s, episode steps: 190, steps per second: 203, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.109023, mae: 27.014166, mean_q: -38.268303, mean_eps: 0.100000\n",
      " 188182/500000: episode: 1320, duration: 0.555s, episode steps: 118, steps per second: 213, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.091570, mae: 27.149382, mean_q: -38.581405, mean_eps: 0.100000\n",
      " 188357/500000: episode: 1321, duration: 0.858s, episode steps: 175, steps per second: 204, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.079989, mae: 27.915117, mean_q: -39.664495, mean_eps: 0.100000\n",
      " 188468/500000: episode: 1322, duration: 0.521s, episode steps: 111, steps per second: 213, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.130729, mae: 27.097164, mean_q: -38.606593, mean_eps: 0.100000\n",
      " 188625/500000: episode: 1323, duration: 0.723s, episode steps: 157, steps per second: 217, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.089 [0.000, 2.000],  loss: 0.120805, mae: 27.414002, mean_q: -39.006253, mean_eps: 0.100000\n",
      " 188797/500000: episode: 1324, duration: 0.815s, episode steps: 172, steps per second: 211, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.103896, mae: 27.718496, mean_q: -39.370921, mean_eps: 0.100000\n",
      " 188951/500000: episode: 1325, duration: 0.722s, episode steps: 154, steps per second: 213, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.154387, mae: 28.394552, mean_q: -40.649080, mean_eps: 0.100000\n",
      " 189064/500000: episode: 1326, duration: 0.537s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.110689, mae: 27.843355, mean_q: -39.959654, mean_eps: 0.100000\n",
      " 189176/500000: episode: 1327, duration: 0.548s, episode steps: 112, steps per second: 204, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.132484, mae: 28.028495, mean_q: -40.229726, mean_eps: 0.100000\n",
      " 189288/500000: episode: 1328, duration: 0.543s, episode steps: 112, steps per second: 206, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.099612, mae: 28.033608, mean_q: -40.219611, mean_eps: 0.100000\n",
      " 189395/500000: episode: 1329, duration: 0.504s, episode steps: 107, steps per second: 212, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.060685, mae: 27.167367, mean_q: -39.010658, mean_eps: 0.100000\n",
      " 189511/500000: episode: 1330, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.096122, mae: 27.602982, mean_q: -39.784077, mean_eps: 0.100000\n",
      " 189633/500000: episode: 1331, duration: 0.594s, episode steps: 122, steps per second: 205, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.077747, mae: 27.142660, mean_q: -38.936323, mean_eps: 0.100000\n",
      " 189748/500000: episode: 1332, duration: 0.617s, episode steps: 115, steps per second: 186, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.066923, mae: 26.229580, mean_q: -37.555802, mean_eps: 0.100000\n",
      " 189866/500000: episode: 1333, duration: 0.606s, episode steps: 118, steps per second: 195, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.057985, mae: 26.143545, mean_q: -37.308385, mean_eps: 0.100000\n",
      " 189985/500000: episode: 1334, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.032711, mae: 25.512240, mean_q: -36.477293, mean_eps: 0.100000\n",
      " 190123/500000: episode: 1335, duration: 0.722s, episode steps: 138, steps per second: 191, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000],  loss: 0.031316, mae: 25.688744, mean_q: -36.809401, mean_eps: 0.100000\n",
      " 190247/500000: episode: 1336, duration: 0.601s, episode steps: 124, steps per second: 206, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.082682, mae: 25.728622, mean_q: -36.647887, mean_eps: 0.100000\n",
      " 190356/500000: episode: 1337, duration: 0.516s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.070353, mae: 26.072943, mean_q: -36.994017, mean_eps: 0.100000\n",
      " 190479/500000: episode: 1338, duration: 0.603s, episode steps: 123, steps per second: 204, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.077963, mae: 26.424330, mean_q: -37.130097, mean_eps: 0.100000\n",
      " 190595/500000: episode: 1339, duration: 0.551s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.048583, mae: 26.329439, mean_q: -37.158610, mean_eps: 0.100000\n",
      " 190714/500000: episode: 1340, duration: 0.555s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.053221, mae: 26.357687, mean_q: -37.264085, mean_eps: 0.100000\n",
      " 190825/500000: episode: 1341, duration: 0.542s, episode steps: 111, steps per second: 205, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.034039, mae: 26.127878, mean_q: -36.957687, mean_eps: 0.100000\n",
      " 190928/500000: episode: 1342, duration: 0.488s, episode steps: 103, steps per second: 211, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.041427, mae: 26.250656, mean_q: -37.115006, mean_eps: 0.100000\n",
      " 191051/500000: episode: 1343, duration: 0.578s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.237989, mae: 26.290717, mean_q: -36.860936, mean_eps: 0.100000\n",
      " 191178/500000: episode: 1344, duration: 0.647s, episode steps: 127, steps per second: 196, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.036038, mae: 25.904042, mean_q: -36.461860, mean_eps: 0.100000\n",
      " 191286/500000: episode: 1345, duration: 0.613s, episode steps: 108, steps per second: 176, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.060965, mae: 25.970703, mean_q: -36.422986, mean_eps: 0.100000\n",
      " 191399/500000: episode: 1346, duration: 0.600s, episode steps: 113, steps per second: 188, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.020750, mae: 25.827508, mean_q: -36.324797, mean_eps: 0.100000\n",
      " 191571/500000: episode: 1347, duration: 0.847s, episode steps: 172, steps per second: 203, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.033435, mae: 26.266127, mean_q: -37.062904, mean_eps: 0.100000\n",
      " 191668/500000: episode: 1348, duration: 0.473s, episode steps:  97, steps per second: 205, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000],  loss: 0.036053, mae: 25.912581, mean_q: -36.535195, mean_eps: 0.100000\n",
      " 191832/500000: episode: 1349, duration: 0.793s, episode steps: 164, steps per second: 207, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.043964, mae: 26.718357, mean_q: -37.467029, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 192003/500000: episode: 1350, duration: 0.859s, episode steps: 171, steps per second: 199, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.077021, mae: 26.386958, mean_q: -37.071814, mean_eps: 0.100000\n",
      " 192178/500000: episode: 1351, duration: 0.938s, episode steps: 175, steps per second: 187, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.166 [0.000, 2.000],  loss: 0.126828, mae: 26.747528, mean_q: -37.652353, mean_eps: 0.100000\n",
      " 192344/500000: episode: 1352, duration: 0.831s, episode steps: 166, steps per second: 200, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.141888, mae: 27.055067, mean_q: -38.218041, mean_eps: 0.100000\n",
      " 192458/500000: episode: 1353, duration: 0.556s, episode steps: 114, steps per second: 205, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.181820, mae: 27.250347, mean_q: -38.596199, mean_eps: 0.100000\n",
      " 192658/500000: episode: 1354, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.106516, mae: 26.877707, mean_q: -37.982237, mean_eps: 0.100000\n",
      " 192782/500000: episode: 1355, duration: 0.643s, episode steps: 124, steps per second: 193, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.204364, mae: 27.055853, mean_q: -38.557946, mean_eps: 0.100000\n",
      " 192897/500000: episode: 1356, duration: 0.595s, episode steps: 115, steps per second: 193, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.195816, mae: 27.001342, mean_q: -38.381207, mean_eps: 0.100000\n",
      " 193015/500000: episode: 1357, duration: 0.586s, episode steps: 118, steps per second: 201, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.144676, mae: 27.053408, mean_q: -38.491454, mean_eps: 0.100000\n",
      " 193134/500000: episode: 1358, duration: 0.590s, episode steps: 119, steps per second: 202, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.094214, mae: 26.891504, mean_q: -38.375500, mean_eps: 0.100000\n",
      " 193334/500000: episode: 1359, duration: 0.968s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.092015, mae: 26.953953, mean_q: -38.497205, mean_eps: 0.100000\n",
      " 193517/500000: episode: 1360, duration: 0.912s, episode steps: 183, steps per second: 201, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.096737, mae: 27.535897, mean_q: -39.300195, mean_eps: 0.100000\n",
      " 193641/500000: episode: 1361, duration: 0.596s, episode steps: 124, steps per second: 208, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.091071, mae: 27.582793, mean_q: -39.551881, mean_eps: 0.100000\n",
      " 193833/500000: episode: 1362, duration: 0.924s, episode steps: 192, steps per second: 208, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.060024, mae: 28.064178, mean_q: -40.152648, mean_eps: 0.100000\n",
      " 193947/500000: episode: 1363, duration: 0.570s, episode steps: 114, steps per second: 200, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.092746, mae: 28.709783, mean_q: -41.280679, mean_eps: 0.100000\n",
      " 194073/500000: episode: 1364, duration: 0.607s, episode steps: 126, steps per second: 208, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.508 [0.000, 2.000],  loss: 0.057155, mae: 28.759406, mean_q: -41.451425, mean_eps: 0.100000\n",
      " 194177/500000: episode: 1365, duration: 0.495s, episode steps: 104, steps per second: 210, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000],  loss: 0.064313, mae: 28.711685, mean_q: -41.319953, mean_eps: 0.100000\n",
      " 194342/500000: episode: 1366, duration: 0.761s, episode steps: 165, steps per second: 217, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.078012, mae: 28.205450, mean_q: -40.339599, mean_eps: 0.100000\n",
      " 194497/500000: episode: 1367, duration: 0.740s, episode steps: 155, steps per second: 210, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.084979, mae: 28.294292, mean_q: -40.546770, mean_eps: 0.100000\n",
      " 194697/500000: episode: 1368, duration: 0.926s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.139373, mae: 28.352381, mean_q: -40.592750, mean_eps: 0.100000\n",
      " 194816/500000: episode: 1369, duration: 0.553s, episode steps: 119, steps per second: 215, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.129319, mae: 27.406278, mean_q: -39.234553, mean_eps: 0.100000\n",
      " 194907/500000: episode: 1370, duration: 0.439s, episode steps:  91, steps per second: 207, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.205982, mae: 27.377204, mean_q: -38.858160, mean_eps: 0.100000\n",
      " 195036/500000: episode: 1371, duration: 0.591s, episode steps: 129, steps per second: 218, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000],  loss: 0.210138, mae: 27.509396, mean_q: -38.952520, mean_eps: 0.100000\n",
      " 195140/500000: episode: 1372, duration: 0.481s, episode steps: 104, steps per second: 216, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.251590, mae: 27.301605, mean_q: -38.497894, mean_eps: 0.100000\n",
      " 195312/500000: episode: 1373, duration: 0.816s, episode steps: 172, steps per second: 211, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.152118, mae: 27.914826, mean_q: -39.036386, mean_eps: 0.100000\n",
      " 195496/500000: episode: 1374, duration: 0.931s, episode steps: 184, steps per second: 198, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.283000, mae: 27.707431, mean_q: -38.397414, mean_eps: 0.100000\n",
      " 195621/500000: episode: 1375, duration: 0.606s, episode steps: 125, steps per second: 206, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.087506, mae: 27.360559, mean_q: -37.832664, mean_eps: 0.100000\n",
      " 195727/500000: episode: 1376, duration: 0.531s, episode steps: 106, steps per second: 200, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.090228, mae: 27.181457, mean_q: -37.505428, mean_eps: 0.100000\n",
      " 195902/500000: episode: 1377, duration: 0.844s, episode steps: 175, steps per second: 207, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.115442, mae: 27.704888, mean_q: -38.076829, mean_eps: 0.100000\n",
      " 196026/500000: episode: 1378, duration: 0.592s, episode steps: 124, steps per second: 210, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.157081, mae: 27.983459, mean_q: -38.670664, mean_eps: 0.100000\n",
      " 196222/500000: episode: 1379, duration: 0.965s, episode steps: 196, steps per second: 203, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.239807, mae: 28.197279, mean_q: -39.092426, mean_eps: 0.100000\n",
      " 196356/500000: episode: 1380, duration: 0.662s, episode steps: 134, steps per second: 202, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000],  loss: 0.083654, mae: 27.790449, mean_q: -38.766780, mean_eps: 0.100000\n",
      " 196552/500000: episode: 1381, duration: 0.955s, episode steps: 196, steps per second: 205, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.112549, mae: 27.845895, mean_q: -38.850929, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 196640/500000: episode: 1382, duration: 0.420s, episode steps:  88, steps per second: 209, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.151969, mae: 27.620436, mean_q: -38.309738, mean_eps: 0.100000\n",
      " 196740/500000: episode: 1383, duration: 0.503s, episode steps: 100, steps per second: 199, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.130956, mae: 27.045850, mean_q: -37.680374, mean_eps: 0.100000\n",
      " 196928/500000: episode: 1384, duration: 0.899s, episode steps: 188, steps per second: 209, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.122042, mae: 26.965381, mean_q: -37.386337, mean_eps: 0.100000\n",
      " 197100/500000: episode: 1385, duration: 0.814s, episode steps: 172, steps per second: 211, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.130172, mae: 27.116295, mean_q: -37.613851, mean_eps: 0.100000\n",
      " 197300/500000: episode: 1386, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.093985, mae: 27.306794, mean_q: -38.141118, mean_eps: 0.100000\n",
      " 197464/500000: episode: 1387, duration: 0.773s, episode steps: 164, steps per second: 212, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.216482, mae: 27.942986, mean_q: -39.142452, mean_eps: 0.100000\n",
      " 197649/500000: episode: 1388, duration: 0.891s, episode steps: 185, steps per second: 208, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000],  loss: 0.222522, mae: 28.785681, mean_q: -40.548565, mean_eps: 0.100000\n",
      " 197813/500000: episode: 1389, duration: 0.818s, episode steps: 164, steps per second: 201, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.241962, mae: 29.990064, mean_q: -42.626981, mean_eps: 0.100000\n",
      " 197963/500000: episode: 1390, duration: 0.745s, episode steps: 150, steps per second: 201, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.893 [0.000, 2.000],  loss: 0.158804, mae: 30.531528, mean_q: -43.759612, mean_eps: 0.100000\n",
      " 198087/500000: episode: 1391, duration: 0.611s, episode steps: 124, steps per second: 203, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000],  loss: 0.288896, mae: 29.728759, mean_q: -42.678661, mean_eps: 0.100000\n",
      " 198206/500000: episode: 1392, duration: 0.606s, episode steps: 119, steps per second: 196, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.230236, mae: 29.613869, mean_q: -42.606422, mean_eps: 0.100000\n",
      " 198379/500000: episode: 1393, duration: 0.849s, episode steps: 173, steps per second: 204, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.148557, mae: 29.020993, mean_q: -42.024119, mean_eps: 0.100000\n",
      " 198494/500000: episode: 1394, duration: 0.543s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.123569, mae: 28.369923, mean_q: -41.094665, mean_eps: 0.100000\n",
      " 198644/500000: episode: 1395, duration: 0.751s, episode steps: 150, steps per second: 200, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.143236, mae: 27.739619, mean_q: -39.953910, mean_eps: 0.100000\n",
      " 198734/500000: episode: 1396, duration: 0.453s, episode steps:  90, steps per second: 199, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.118135, mae: 26.936268, mean_q: -38.787913, mean_eps: 0.100000\n",
      " 198902/500000: episode: 1397, duration: 0.824s, episode steps: 168, steps per second: 204, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.113974, mae: 26.429110, mean_q: -37.978741, mean_eps: 0.100000\n",
      " 199013/500000: episode: 1398, duration: 0.532s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.084210, mae: 26.514221, mean_q: -38.118231, mean_eps: 0.100000\n",
      " 199213/500000: episode: 1399, duration: 0.989s, episode steps: 200, steps per second: 202, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.082136, mae: 27.135140, mean_q: -39.183348, mean_eps: 0.100000\n",
      " 199329/500000: episode: 1400, duration: 0.565s, episode steps: 116, steps per second: 205, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.364035, mae: 27.477837, mean_q: -39.417785, mean_eps: 0.100000\n",
      " 199529/500000: episode: 1401, duration: 0.973s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.255906, mae: 27.394247, mean_q: -39.225113, mean_eps: 0.100000\n",
      " 199664/500000: episode: 1402, duration: 0.631s, episode steps: 135, steps per second: 214, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.273274, mae: 27.349213, mean_q: -39.220345, mean_eps: 0.100000\n",
      " 199812/500000: episode: 1403, duration: 0.698s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000],  loss: 0.297349, mae: 28.132757, mean_q: -40.034352, mean_eps: 0.100000\n",
      " 199944/500000: episode: 1404, duration: 0.628s, episode steps: 132, steps per second: 210, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.386 [0.000, 2.000],  loss: 0.187093, mae: 28.174015, mean_q: -39.996384, mean_eps: 0.100000\n",
      " 200063/500000: episode: 1405, duration: 0.555s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.162825, mae: 28.457234, mean_q: -40.086552, mean_eps: 0.100000\n",
      " 200234/500000: episode: 1406, duration: 0.799s, episode steps: 171, steps per second: 214, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.971 [0.000, 2.000],  loss: 0.159435, mae: 28.496251, mean_q: -40.113737, mean_eps: 0.100000\n",
      " 200378/500000: episode: 1407, duration: 0.682s, episode steps: 144, steps per second: 211, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000],  loss: 0.153388, mae: 28.572441, mean_q: -40.199782, mean_eps: 0.100000\n",
      " 200561/500000: episode: 1408, duration: 0.839s, episode steps: 183, steps per second: 218, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000],  loss: 0.121755, mae: 29.051815, mean_q: -41.116658, mean_eps: 0.100000\n",
      " 200671/500000: episode: 1409, duration: 0.530s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.170406, mae: 28.674361, mean_q: -40.461489, mean_eps: 0.100000\n",
      " 200786/500000: episode: 1410, duration: 0.543s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.115352, mae: 27.931069, mean_q: -39.667850, mean_eps: 0.100000\n",
      " 200916/500000: episode: 1411, duration: 0.598s, episode steps: 130, steps per second: 218, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.157362, mae: 28.337922, mean_q: -40.256161, mean_eps: 0.100000\n",
      " 201092/500000: episode: 1412, duration: 0.826s, episode steps: 176, steps per second: 213, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.137284, mae: 29.042011, mean_q: -41.298891, mean_eps: 0.100000\n",
      " 201215/500000: episode: 1413, duration: 0.585s, episode steps: 123, steps per second: 210, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.158839, mae: 27.904076, mean_q: -39.768315, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 201331/500000: episode: 1414, duration: 0.541s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.097989, mae: 27.453055, mean_q: -38.968206, mean_eps: 0.100000\n",
      " 201477/500000: episode: 1415, duration: 0.683s, episode steps: 146, steps per second: 214, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.452 [0.000, 2.000],  loss: 0.124765, mae: 27.243377, mean_q: -38.368253, mean_eps: 0.100000\n",
      " 201563/500000: episode: 1416, duration: 0.413s, episode steps:  86, steps per second: 208, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.127600, mae: 27.369192, mean_q: -38.433907, mean_eps: 0.100000\n",
      " 201663/500000: episode: 1417, duration: 0.477s, episode steps: 100, steps per second: 210, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.086216, mae: 26.154451, mean_q: -36.473662, mean_eps: 0.100000\n",
      " 201777/500000: episode: 1418, duration: 0.531s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.412 [0.000, 2.000],  loss: 0.101346, mae: 26.665360, mean_q: -37.341126, mean_eps: 0.100000\n",
      " 201888/500000: episode: 1419, duration: 0.518s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.073964, mae: 26.377423, mean_q: -36.926529, mean_eps: 0.100000\n",
      " 202007/500000: episode: 1420, duration: 0.565s, episode steps: 119, steps per second: 211, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.050001, mae: 26.133958, mean_q: -36.878349, mean_eps: 0.100000\n",
      " 202207/500000: episode: 1421, duration: 0.921s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.077623, mae: 25.714660, mean_q: -36.140585, mean_eps: 0.100000\n",
      " 202320/500000: episode: 1422, duration: 0.544s, episode steps: 113, steps per second: 208, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.082168, mae: 26.006287, mean_q: -36.664469, mean_eps: 0.100000\n",
      " 202444/500000: episode: 1423, duration: 0.593s, episode steps: 124, steps per second: 209, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000],  loss: 0.114761, mae: 25.913023, mean_q: -36.605131, mean_eps: 0.100000\n",
      " 202541/500000: episode: 1424, duration: 0.454s, episode steps:  97, steps per second: 213, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.130809, mae: 25.967508, mean_q: -36.766730, mean_eps: 0.100000\n",
      " 202714/500000: episode: 1425, duration: 0.803s, episode steps: 173, steps per second: 215, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.086456, mae: 26.257757, mean_q: -37.274884, mean_eps: 0.100000\n",
      " 202864/500000: episode: 1426, duration: 0.742s, episode steps: 150, steps per second: 202, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.151494, mae: 26.360026, mean_q: -37.439774, mean_eps: 0.100000\n",
      " 203064/500000: episode: 1427, duration: 1.000s, episode steps: 200, steps per second: 200, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.198606, mae: 26.202250, mean_q: -36.926100, mean_eps: 0.100000\n",
      " 203245/500000: episode: 1428, duration: 0.885s, episode steps: 181, steps per second: 205, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.726312, mae: 26.795448, mean_q: -37.546905, mean_eps: 0.100000\n",
      " 203405/500000: episode: 1429, duration: 0.800s, episode steps: 160, steps per second: 200, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.364926, mae: 27.195868, mean_q: -38.534240, mean_eps: 0.100000\n",
      " 203589/500000: episode: 1430, duration: 0.969s, episode steps: 184, steps per second: 190, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.373350, mae: 27.698915, mean_q: -39.391140, mean_eps: 0.100000\n",
      " 203714/500000: episode: 1431, duration: 0.594s, episode steps: 125, steps per second: 210, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.404409, mae: 27.995526, mean_q: -39.837096, mean_eps: 0.100000\n",
      " 203888/500000: episode: 1432, duration: 0.854s, episode steps: 174, steps per second: 204, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.211739, mae: 28.336410, mean_q: -40.275557, mean_eps: 0.100000\n",
      " 204073/500000: episode: 1433, duration: 0.897s, episode steps: 185, steps per second: 206, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.556913, mae: 28.460516, mean_q: -40.445908, mean_eps: 0.100000\n",
      " 204257/500000: episode: 1434, duration: 0.855s, episode steps: 184, steps per second: 215, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.134635, mae: 27.533560, mean_q: -39.220037, mean_eps: 0.100000\n",
      " 204372/500000: episode: 1435, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.147866, mae: 27.107024, mean_q: -38.496516, mean_eps: 0.100000\n",
      " 204510/500000: episode: 1436, duration: 0.665s, episode steps: 138, steps per second: 207, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000],  loss: 0.156823, mae: 26.914501, mean_q: -37.971477, mean_eps: 0.100000\n",
      " 204614/500000: episode: 1437, duration: 0.499s, episode steps: 104, steps per second: 208, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.110294, mae: 26.822460, mean_q: -37.467681, mean_eps: 0.100000\n",
      " 204767/500000: episode: 1438, duration: 0.714s, episode steps: 153, steps per second: 214, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000],  loss: 0.155047, mae: 27.222392, mean_q: -37.968175, mean_eps: 0.100000\n",
      " 204946/500000: episode: 1439, duration: 0.854s, episode steps: 179, steps per second: 210, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.122696, mae: 27.128964, mean_q: -38.193509, mean_eps: 0.100000\n",
      " 205084/500000: episode: 1440, duration: 0.661s, episode steps: 138, steps per second: 209, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.094484, mae: 27.154751, mean_q: -38.322170, mean_eps: 0.100000\n",
      " 205204/500000: episode: 1441, duration: 0.589s, episode steps: 120, steps per second: 204, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.092857, mae: 27.555350, mean_q: -38.616100, mean_eps: 0.100000\n",
      " 205321/500000: episode: 1442, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.096169, mae: 27.573231, mean_q: -38.519482, mean_eps: 0.100000\n",
      " 205521/500000: episode: 1443, duration: 1.010s, episode steps: 200, steps per second: 198, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.080690, mae: 27.535009, mean_q: -38.493158, mean_eps: 0.100000\n",
      " 205702/500000: episode: 1444, duration: 1.048s, episode steps: 181, steps per second: 173, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.144 [0.000, 2.000],  loss: 0.352518, mae: 27.869883, mean_q: -39.311690, mean_eps: 0.100000\n",
      " 205821/500000: episode: 1445, duration: 0.648s, episode steps: 119, steps per second: 184, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.223292, mae: 27.271374, mean_q: -38.615653, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 205986/500000: episode: 1446, duration: 0.788s, episode steps: 165, steps per second: 209, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.539 [0.000, 2.000],  loss: 0.142631, mae: 27.738161, mean_q: -38.998721, mean_eps: 0.100000\n",
      " 206109/500000: episode: 1447, duration: 0.601s, episode steps: 123, steps per second: 205, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.264893, mae: 27.444581, mean_q: -38.274785, mean_eps: 0.100000\n",
      " 206289/500000: episode: 1448, duration: 0.864s, episode steps: 180, steps per second: 208, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.153089, mae: 27.565875, mean_q: -38.522785, mean_eps: 0.100000\n",
      " 206489/500000: episode: 1449, duration: 0.937s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.152658, mae: 27.356323, mean_q: -38.497009, mean_eps: 0.100000\n",
      " 206689/500000: episode: 1450, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.159196, mae: 27.683678, mean_q: -39.094638, mean_eps: 0.100000\n",
      " 206872/500000: episode: 1451, duration: 0.868s, episode steps: 183, steps per second: 211, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.218328, mae: 28.793397, mean_q: -40.820375, mean_eps: 0.100000\n",
      " 206986/500000: episode: 1452, duration: 0.535s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.210237, mae: 28.721226, mean_q: -40.943869, mean_eps: 0.100000\n",
      " 207186/500000: episode: 1453, duration: 0.959s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.359047, mae: 28.971372, mean_q: -41.138718, mean_eps: 0.100000\n",
      " 207319/500000: episode: 1454, duration: 0.798s, episode steps: 133, steps per second: 167, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.387932, mae: 29.392714, mean_q: -41.809869, mean_eps: 0.100000\n",
      " 207512/500000: episode: 1455, duration: 0.962s, episode steps: 193, steps per second: 201, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.334107, mae: 29.397763, mean_q: -41.680576, mean_eps: 0.100000\n",
      " 207674/500000: episode: 1456, duration: 0.774s, episode steps: 162, steps per second: 209, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.345155, mae: 28.304482, mean_q: -40.030168, mean_eps: 0.100000\n",
      " 207865/500000: episode: 1457, duration: 0.946s, episode steps: 191, steps per second: 202, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.300316, mae: 27.447359, mean_q: -38.761933, mean_eps: 0.100000\n",
      " 208029/500000: episode: 1458, duration: 0.841s, episode steps: 164, steps per second: 195, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000],  loss: 0.173934, mae: 27.251367, mean_q: -38.666194, mean_eps: 0.100000\n",
      " 208196/500000: episode: 1459, duration: 0.806s, episode steps: 167, steps per second: 207, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.245764, mae: 26.813244, mean_q: -38.223444, mean_eps: 0.100000\n",
      " 208352/500000: episode: 1460, duration: 0.740s, episode steps: 156, steps per second: 211, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.098010, mae: 26.809798, mean_q: -38.399347, mean_eps: 0.100000\n",
      " 208552/500000: episode: 1461, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.090594, mae: 26.806518, mean_q: -38.416491, mean_eps: 0.100000\n",
      " 208726/500000: episode: 1462, duration: 0.850s, episode steps: 174, steps per second: 205, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.874 [0.000, 2.000],  loss: 0.203253, mae: 27.655215, mean_q: -39.749745, mean_eps: 0.100000\n",
      " 208838/500000: episode: 1463, duration: 0.553s, episode steps: 112, steps per second: 203, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.321634, mae: 27.597394, mean_q: -39.814818, mean_eps: 0.100000\n",
      " 208948/500000: episode: 1464, duration: 0.524s, episode steps: 110, steps per second: 210, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.164115, mae: 27.975667, mean_q: -40.458020, mean_eps: 0.100000\n",
      " 209045/500000: episode: 1465, duration: 0.455s, episode steps:  97, steps per second: 213, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.219125, mae: 27.469954, mean_q: -39.813624, mean_eps: 0.100000\n",
      " 209159/500000: episode: 1466, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.235413, mae: 27.706322, mean_q: -39.915907, mean_eps: 0.100000\n",
      " 209282/500000: episode: 1467, duration: 0.590s, episode steps: 123, steps per second: 208, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.135590, mae: 27.006472, mean_q: -39.066514, mean_eps: 0.100000\n",
      " 209434/500000: episode: 1468, duration: 0.739s, episode steps: 152, steps per second: 206, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.196330, mae: 27.037141, mean_q: -39.051958, mean_eps: 0.100000\n",
      " 209570/500000: episode: 1469, duration: 0.643s, episode steps: 136, steps per second: 211, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.412 [0.000, 2.000],  loss: 0.178104, mae: 26.670744, mean_q: -38.432583, mean_eps: 0.100000\n",
      " 209722/500000: episode: 1470, duration: 0.725s, episode steps: 152, steps per second: 210, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.098960, mae: 25.710279, mean_q: -36.698609, mean_eps: 0.100000\n",
      " 209842/500000: episode: 1471, duration: 0.573s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.087671, mae: 25.740003, mean_q: -36.612079, mean_eps: 0.100000\n",
      " 209937/500000: episode: 1472, duration: 0.446s, episode steps:  95, steps per second: 213, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.076342, mae: 26.355506, mean_q: -37.224376, mean_eps: 0.100000\n",
      " 210061/500000: episode: 1473, duration: 0.579s, episode steps: 124, steps per second: 214, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000],  loss: 0.135946, mae: 26.718374, mean_q: -37.625597, mean_eps: 0.100000\n",
      " 210167/500000: episode: 1474, duration: 0.507s, episode steps: 106, steps per second: 209, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.092155, mae: 26.242379, mean_q: -36.994427, mean_eps: 0.100000\n",
      " 210254/500000: episode: 1475, duration: 0.413s, episode steps:  87, steps per second: 211, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.218 [0.000, 2.000],  loss: 0.129347, mae: 26.390979, mean_q: -36.851650, mean_eps: 0.100000\n",
      " 210369/500000: episode: 1476, duration: 0.552s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.264317, mae: 26.360000, mean_q: -36.479016, mean_eps: 0.100000\n",
      " 210564/500000: episode: 1477, duration: 0.922s, episode steps: 195, steps per second: 212, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000],  loss: 0.095434, mae: 27.083067, mean_q: -37.690602, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 210681/500000: episode: 1478, duration: 0.570s, episode steps: 117, steps per second: 205, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.062510, mae: 26.871348, mean_q: -37.762871, mean_eps: 0.100000\n",
      " 210869/500000: episode: 1479, duration: 0.889s, episode steps: 188, steps per second: 211, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.098676, mae: 26.845102, mean_q: -37.542825, mean_eps: 0.100000\n",
      " 211047/500000: episode: 1480, duration: 0.847s, episode steps: 178, steps per second: 210, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.120140, mae: 27.595729, mean_q: -38.849782, mean_eps: 0.100000\n",
      " 211163/500000: episode: 1481, duration: 0.540s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.114470, mae: 27.804057, mean_q: -39.391607, mean_eps: 0.100000\n",
      " 211337/500000: episode: 1482, duration: 0.800s, episode steps: 174, steps per second: 218, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.122136, mae: 28.254653, mean_q: -40.132965, mean_eps: 0.100000\n",
      " 211537/500000: episode: 1483, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.098348, mae: 28.646418, mean_q: -41.042776, mean_eps: 0.100000\n",
      " 211692/500000: episode: 1484, duration: 0.709s, episode steps: 155, steps per second: 219, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.805558, mae: 28.634332, mean_q: -40.817629, mean_eps: 0.100000\n",
      " 211853/500000: episode: 1485, duration: 0.764s, episode steps: 161, steps per second: 211, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.863 [0.000, 2.000],  loss: 0.518042, mae: 28.863972, mean_q: -41.481281, mean_eps: 0.100000\n",
      " 212044/500000: episode: 1486, duration: 0.891s, episode steps: 191, steps per second: 214, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.183075, mae: 28.964729, mean_q: -41.830909, mean_eps: 0.100000\n",
      " 212196/500000: episode: 1487, duration: 0.709s, episode steps: 152, steps per second: 214, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.921 [0.000, 2.000],  loss: 0.367385, mae: 28.963775, mean_q: -41.690389, mean_eps: 0.100000\n",
      " 212369/500000: episode: 1488, duration: 0.818s, episode steps: 173, steps per second: 211, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.400567, mae: 28.595708, mean_q: -41.166969, mean_eps: 0.100000\n",
      " 212480/500000: episode: 1489, duration: 0.526s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.351590, mae: 28.346331, mean_q: -40.934665, mean_eps: 0.100000\n",
      " 212581/500000: episode: 1490, duration: 0.527s, episode steps: 101, steps per second: 192, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.074585, mae: 27.516568, mean_q: -39.668613, mean_eps: 0.100000\n",
      " 212710/500000: episode: 1491, duration: 0.657s, episode steps: 129, steps per second: 196, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.093535, mae: 27.674013, mean_q: -39.608218, mean_eps: 0.100000\n",
      " 212826/500000: episode: 1492, duration: 0.561s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.102631, mae: 27.499744, mean_q: -39.066245, mean_eps: 0.100000\n",
      " 212978/500000: episode: 1493, duration: 0.742s, episode steps: 152, steps per second: 205, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.947 [0.000, 2.000],  loss: 0.098658, mae: 27.856271, mean_q: -39.441539, mean_eps: 0.100000\n",
      " 213116/500000: episode: 1494, duration: 0.656s, episode steps: 138, steps per second: 210, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.089407, mae: 27.823965, mean_q: -39.374667, mean_eps: 0.100000\n",
      " 213229/500000: episode: 1495, duration: 0.623s, episode steps: 113, steps per second: 181, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.113178, mae: 27.984630, mean_q: -39.418325, mean_eps: 0.100000\n",
      " 213342/500000: episode: 1496, duration: 0.583s, episode steps: 113, steps per second: 194, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.133546, mae: 27.413304, mean_q: -38.113219, mean_eps: 0.100000\n",
      " 213463/500000: episode: 1497, duration: 0.614s, episode steps: 121, steps per second: 197, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.100391, mae: 27.972660, mean_q: -38.744764, mean_eps: 0.100000\n",
      " 213648/500000: episode: 1498, duration: 0.880s, episode steps: 185, steps per second: 210, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.125854, mae: 28.237644, mean_q: -39.060998, mean_eps: 0.100000\n",
      " 213762/500000: episode: 1499, duration: 0.540s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.089752, mae: 27.418336, mean_q: -38.442726, mean_eps: 0.100000\n",
      " 213962/500000: episode: 1500, duration: 0.964s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.105721, mae: 27.092037, mean_q: -37.811280, mean_eps: 0.100000\n",
      " 214096/500000: episode: 1501, duration: 0.630s, episode steps: 134, steps per second: 213, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.156098, mae: 27.134644, mean_q: -37.881456, mean_eps: 0.100000\n",
      " 214216/500000: episode: 1502, duration: 0.564s, episode steps: 120, steps per second: 213, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.183054, mae: 27.111443, mean_q: -37.868063, mean_eps: 0.100000\n",
      " 214406/500000: episode: 1503, duration: 0.915s, episode steps: 190, steps per second: 208, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.163994, mae: 27.333154, mean_q: -38.122096, mean_eps: 0.100000\n",
      " 214561/500000: episode: 1504, duration: 0.713s, episode steps: 155, steps per second: 217, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.903 [0.000, 2.000],  loss: 0.119610, mae: 27.385019, mean_q: -38.550019, mean_eps: 0.100000\n",
      " 214681/500000: episode: 1505, duration: 0.610s, episode steps: 120, steps per second: 197, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.117 [0.000, 2.000],  loss: 0.185183, mae: 26.761193, mean_q: -37.579307, mean_eps: 0.100000\n",
      " 214857/500000: episode: 1506, duration: 0.875s, episode steps: 176, steps per second: 201, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.023 [0.000, 2.000],  loss: 0.190430, mae: 27.268866, mean_q: -38.526021, mean_eps: 0.100000\n",
      " 214968/500000: episode: 1507, duration: 0.528s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.211000, mae: 27.304984, mean_q: -38.931207, mean_eps: 0.100000\n",
      " 215089/500000: episode: 1508, duration: 0.576s, episode steps: 121, steps per second: 210, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.079980, mae: 27.180386, mean_q: -38.822853, mean_eps: 0.100000\n",
      " 215181/500000: episode: 1509, duration: 0.454s, episode steps:  92, steps per second: 203, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.107687, mae: 26.598480, mean_q: -38.015934, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 215268/500000: episode: 1510, duration: 0.417s, episode steps:  87, steps per second: 209, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.080082, mae: 26.349609, mean_q: -37.597850, mean_eps: 0.100000\n",
      " 215416/500000: episode: 1511, duration: 0.753s, episode steps: 148, steps per second: 197, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: 0.067076, mae: 26.466031, mean_q: -38.094559, mean_eps: 0.100000\n",
      " 215605/500000: episode: 1512, duration: 0.931s, episode steps: 189, steps per second: 203, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.101781, mae: 26.580828, mean_q: -38.088265, mean_eps: 0.100000\n",
      " 215719/500000: episode: 1513, duration: 0.555s, episode steps: 114, steps per second: 206, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.085000, mae: 26.983506, mean_q: -38.647259, mean_eps: 0.100000\n",
      " 215832/500000: episode: 1514, duration: 0.538s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.087786, mae: 26.146090, mean_q: -37.472381, mean_eps: 0.100000\n",
      " 215948/500000: episode: 1515, duration: 0.560s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.097701, mae: 25.941065, mean_q: -37.057108, mean_eps: 0.100000\n",
      " 216111/500000: episode: 1516, duration: 0.802s, episode steps: 163, steps per second: 203, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000],  loss: 0.118305, mae: 26.498064, mean_q: -37.808542, mean_eps: 0.100000\n",
      " 216219/500000: episode: 1517, duration: 0.531s, episode steps: 108, steps per second: 203, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.104593, mae: 27.123298, mean_q: -38.833218, mean_eps: 0.100000\n",
      " 216348/500000: episode: 1518, duration: 0.615s, episode steps: 129, steps per second: 210, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.170902, mae: 27.392085, mean_q: -39.092231, mean_eps: 0.100000\n",
      " 216460/500000: episode: 1519, duration: 0.536s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.101341, mae: 26.891692, mean_q: -38.460674, mean_eps: 0.100000\n",
      " 216616/500000: episode: 1520, duration: 0.724s, episode steps: 156, steps per second: 216, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.089440, mae: 27.526919, mean_q: -39.434919, mean_eps: 0.100000\n",
      " 216766/500000: episode: 1521, duration: 0.709s, episode steps: 150, steps per second: 212, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.075739, mae: 27.915585, mean_q: -40.165947, mean_eps: 0.100000\n",
      " 216889/500000: episode: 1522, duration: 0.584s, episode steps: 123, steps per second: 210, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000],  loss: 0.127742, mae: 28.343613, mean_q: -40.702815, mean_eps: 0.100000\n",
      " 217002/500000: episode: 1523, duration: 0.534s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.111328, mae: 28.047089, mean_q: -40.177341, mean_eps: 0.100000\n",
      " 217202/500000: episode: 1524, duration: 0.924s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.177461, mae: 28.345327, mean_q: -39.988990, mean_eps: 0.100000\n",
      " 217364/500000: episode: 1525, duration: 0.781s, episode steps: 162, steps per second: 207, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.305901, mae: 28.497746, mean_q: -40.207471, mean_eps: 0.100000\n",
      " 217524/500000: episode: 1526, duration: 0.733s, episode steps: 160, steps per second: 218, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.312940, mae: 28.932320, mean_q: -40.798480, mean_eps: 0.100000\n",
      " 217707/500000: episode: 1527, duration: 0.878s, episode steps: 183, steps per second: 208, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.164828, mae: 28.590892, mean_q: -40.184681, mean_eps: 0.100000\n",
      " 217819/500000: episode: 1528, duration: 0.527s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.243034, mae: 28.436820, mean_q: -39.850319, mean_eps: 0.100000\n",
      " 217914/500000: episode: 1529, duration: 0.528s, episode steps:  95, steps per second: 180, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.154476, mae: 28.011197, mean_q: -39.262862, mean_eps: 0.100000\n",
      " 218003/500000: episode: 1530, duration: 0.507s, episode steps:  89, steps per second: 176, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.180722, mae: 27.407481, mean_q: -38.435852, mean_eps: 0.100000\n",
      " 218188/500000: episode: 1531, duration: 1.022s, episode steps: 185, steps per second: 181, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.152578, mae: 27.907141, mean_q: -39.627939, mean_eps: 0.100000\n",
      " 218304/500000: episode: 1532, duration: 0.573s, episode steps: 116, steps per second: 202, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.084227, mae: 27.382291, mean_q: -39.031241, mean_eps: 0.100000\n",
      " 218479/500000: episode: 1533, duration: 0.916s, episode steps: 175, steps per second: 191, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000],  loss: 0.072507, mae: 27.755760, mean_q: -39.444524, mean_eps: 0.100000\n",
      " 218640/500000: episode: 1534, duration: 0.748s, episode steps: 161, steps per second: 215, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.064137, mae: 27.436407, mean_q: -38.940519, mean_eps: 0.100000\n",
      " 218808/500000: episode: 1535, duration: 0.870s, episode steps: 168, steps per second: 193, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.072630, mae: 27.787100, mean_q: -39.485569, mean_eps: 0.100000\n",
      " 218972/500000: episode: 1536, duration: 0.822s, episode steps: 164, steps per second: 199, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.075167, mae: 28.807546, mean_q: -41.159112, mean_eps: 0.100000\n",
      " 219081/500000: episode: 1537, duration: 0.524s, episode steps: 109, steps per second: 208, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.071749, mae: 29.263877, mean_q: -41.921322, mean_eps: 0.100000\n",
      " 219267/500000: episode: 1538, duration: 0.912s, episode steps: 186, steps per second: 204, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.094632, mae: 28.481413, mean_q: -40.562739, mean_eps: 0.100000\n",
      " 219424/500000: episode: 1539, duration: 0.750s, episode steps: 157, steps per second: 209, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.159675, mae: 28.681871, mean_q: -40.768964, mean_eps: 0.100000\n",
      " 219599/500000: episode: 1540, duration: 0.899s, episode steps: 175, steps per second: 195, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.154566, mae: 28.764742, mean_q: -40.817080, mean_eps: 0.100000\n",
      " 219721/500000: episode: 1541, duration: 0.592s, episode steps: 122, steps per second: 206, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.095261, mae: 28.285865, mean_q: -39.896252, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 219890/500000: episode: 1542, duration: 0.834s, episode steps: 169, steps per second: 203, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.116202, mae: 28.244667, mean_q: -39.597624, mean_eps: 0.100000\n",
      " 220006/500000: episode: 1543, duration: 0.611s, episode steps: 116, steps per second: 190, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.115053, mae: 28.005353, mean_q: -39.153632, mean_eps: 0.100000\n",
      " 220129/500000: episode: 1544, duration: 0.580s, episode steps: 123, steps per second: 212, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.125730, mae: 28.270600, mean_q: -39.257858, mean_eps: 0.100000\n",
      " 220262/500000: episode: 1545, duration: 0.673s, episode steps: 133, steps per second: 198, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.108167, mae: 28.129029, mean_q: -39.113338, mean_eps: 0.100000\n",
      " 220371/500000: episode: 1546, duration: 0.517s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.148860, mae: 27.882061, mean_q: -38.432986, mean_eps: 0.100000\n",
      " 220571/500000: episode: 1547, duration: 1.102s, episode steps: 200, steps per second: 182, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.108861, mae: 27.790990, mean_q: -38.601313, mean_eps: 0.100000\n",
      " 220739/500000: episode: 1548, duration: 0.906s, episode steps: 168, steps per second: 186, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.121497, mae: 28.728327, mean_q: -40.464789, mean_eps: 0.100000\n",
      " 220908/500000: episode: 1549, duration: 0.810s, episode steps: 169, steps per second: 209, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.959 [0.000, 2.000],  loss: 0.112802, mae: 28.940246, mean_q: -41.126827, mean_eps: 0.100000\n",
      " 221052/500000: episode: 1550, duration: 0.666s, episode steps: 144, steps per second: 216, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.162558, mae: 28.254844, mean_q: -40.265913, mean_eps: 0.100000\n",
      " 221170/500000: episode: 1551, duration: 0.566s, episode steps: 118, steps per second: 208, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.153792, mae: 28.224345, mean_q: -40.297644, mean_eps: 0.100000\n",
      " 221336/500000: episode: 1552, duration: 0.805s, episode steps: 166, steps per second: 206, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.819 [0.000, 2.000],  loss: 0.141388, mae: 28.814060, mean_q: -41.370521, mean_eps: 0.100000\n",
      " 221505/500000: episode: 1553, duration: 0.811s, episode steps: 169, steps per second: 208, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.083 [0.000, 2.000],  loss: 0.148574, mae: 29.097403, mean_q: -41.837668, mean_eps: 0.100000\n",
      " 221657/500000: episode: 1554, duration: 0.734s, episode steps: 152, steps per second: 207, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000],  loss: 0.214966, mae: 28.963638, mean_q: -41.469667, mean_eps: 0.100000\n",
      " 221815/500000: episode: 1555, duration: 0.753s, episode steps: 158, steps per second: 210, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.227046, mae: 28.826901, mean_q: -40.732331, mean_eps: 0.100000\n",
      " 222015/500000: episode: 1556, duration: 0.963s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.181670, mae: 28.569609, mean_q: -40.332655, mean_eps: 0.100000\n",
      " 222133/500000: episode: 1557, duration: 0.570s, episode steps: 118, steps per second: 207, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.250289, mae: 28.799664, mean_q: -40.731579, mean_eps: 0.100000\n",
      " 222239/500000: episode: 1558, duration: 0.492s, episode steps: 106, steps per second: 216, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.300892, mae: 28.505486, mean_q: -40.338249, mean_eps: 0.100000\n",
      " 222422/500000: episode: 1559, duration: 0.850s, episode steps: 183, steps per second: 215, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.365505, mae: 27.680399, mean_q: -38.955872, mean_eps: 0.100000\n",
      " 222557/500000: episode: 1560, duration: 0.655s, episode steps: 135, steps per second: 206, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.299096, mae: 27.317709, mean_q: -38.483580, mean_eps: 0.100000\n",
      " 222679/500000: episode: 1561, duration: 0.572s, episode steps: 122, steps per second: 213, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.178221, mae: 27.033345, mean_q: -38.076014, mean_eps: 0.100000\n",
      " 222867/500000: episode: 1562, duration: 0.895s, episode steps: 188, steps per second: 210, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.191667, mae: 26.531643, mean_q: -37.401110, mean_eps: 0.100000\n",
      " 223053/500000: episode: 1563, duration: 0.892s, episode steps: 186, steps per second: 208, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.202421, mae: 26.574881, mean_q: -37.632765, mean_eps: 0.100000\n",
      " 223167/500000: episode: 1564, duration: 0.533s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.104829, mae: 26.288360, mean_q: -36.988389, mean_eps: 0.100000\n",
      " 223360/500000: episode: 1565, duration: 0.938s, episode steps: 193, steps per second: 206, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.112850, mae: 27.344140, mean_q: -38.462903, mean_eps: 0.100000\n",
      " 223490/500000: episode: 1566, duration: 0.614s, episode steps: 130, steps per second: 212, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.132120, mae: 27.070286, mean_q: -38.052134, mean_eps: 0.100000\n",
      " 223597/500000: episode: 1567, duration: 0.505s, episode steps: 107, steps per second: 212, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.091074, mae: 26.478285, mean_q: -37.589474, mean_eps: 0.100000\n",
      " 223752/500000: episode: 1568, duration: 0.735s, episode steps: 155, steps per second: 211, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.748 [0.000, 2.000],  loss: 0.131448, mae: 26.880205, mean_q: -38.176151, mean_eps: 0.100000\n",
      " 223921/500000: episode: 1569, duration: 0.800s, episode steps: 169, steps per second: 211, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.155408, mae: 27.017549, mean_q: -38.223010, mean_eps: 0.100000\n",
      " 224093/500000: episode: 1570, duration: 0.824s, episode steps: 172, steps per second: 209, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.140303, mae: 26.935617, mean_q: -37.981368, mean_eps: 0.100000\n",
      " 224293/500000: episode: 1571, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.136830, mae: 27.024706, mean_q: -38.437656, mean_eps: 0.100000\n",
      " 224421/500000: episode: 1572, duration: 0.610s, episode steps: 128, steps per second: 210, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.167242, mae: 28.524163, mean_q: -40.741326, mean_eps: 0.100000\n",
      " 224605/500000: episode: 1573, duration: 0.895s, episode steps: 184, steps per second: 206, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.167053, mae: 28.920807, mean_q: -41.095666, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 224709/500000: episode: 1574, duration: 0.493s, episode steps: 104, steps per second: 211, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.101677, mae: 28.897405, mean_q: -41.000554, mean_eps: 0.100000\n",
      " 224863/500000: episode: 1575, duration: 0.733s, episode steps: 154, steps per second: 210, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.137912, mae: 29.036206, mean_q: -41.054771, mean_eps: 0.100000\n",
      " 225029/500000: episode: 1576, duration: 0.791s, episode steps: 166, steps per second: 210, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.161469, mae: 29.189181, mean_q: -41.385090, mean_eps: 0.100000\n",
      " 225180/500000: episode: 1577, duration: 0.702s, episode steps: 151, steps per second: 215, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.053 [0.000, 2.000],  loss: 0.127849, mae: 29.117170, mean_q: -41.439229, mean_eps: 0.100000\n",
      " 225314/500000: episode: 1578, duration: 0.638s, episode steps: 134, steps per second: 210, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.162250, mae: 28.083865, mean_q: -39.664482, mean_eps: 0.100000\n",
      " 225412/500000: episode: 1579, duration: 0.496s, episode steps:  98, steps per second: 197, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.104983, mae: 27.575888, mean_q: -38.791371, mean_eps: 0.100000\n",
      " 225539/500000: episode: 1580, duration: 0.596s, episode steps: 127, steps per second: 213, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.070875, mae: 27.325360, mean_q: -38.539194, mean_eps: 0.100000\n",
      " 225649/500000: episode: 1581, duration: 0.520s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000],  loss: 0.102022, mae: 27.206159, mean_q: -38.381954, mean_eps: 0.100000\n",
      " 225771/500000: episode: 1582, duration: 0.583s, episode steps: 122, steps per second: 209, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.344 [0.000, 2.000],  loss: 0.111274, mae: 26.796561, mean_q: -37.893890, mean_eps: 0.100000\n",
      " 225860/500000: episode: 1583, duration: 0.444s, episode steps:  89, steps per second: 200, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.064082, mae: 26.472340, mean_q: -37.469515, mean_eps: 0.100000\n",
      " 225975/500000: episode: 1584, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.068801, mae: 26.298748, mean_q: -37.134219, mean_eps: 0.100000\n",
      " 226084/500000: episode: 1585, duration: 0.510s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.073741, mae: 25.724261, mean_q: -36.313745, mean_eps: 0.100000\n",
      " 226242/500000: episode: 1586, duration: 0.748s, episode steps: 158, steps per second: 211, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.061955, mae: 25.820317, mean_q: -36.593936, mean_eps: 0.100000\n",
      " 226357/500000: episode: 1587, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.041101, mae: 26.080835, mean_q: -37.189259, mean_eps: 0.100000\n",
      " 226464/500000: episode: 1588, duration: 0.498s, episode steps: 107, steps per second: 215, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.048333, mae: 25.909860, mean_q: -37.351238, mean_eps: 0.100000\n",
      " 226584/500000: episode: 1589, duration: 0.550s, episode steps: 120, steps per second: 218, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.044822, mae: 26.084889, mean_q: -37.741862, mean_eps: 0.100000\n",
      " 226677/500000: episode: 1590, duration: 0.447s, episode steps:  93, steps per second: 208, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.045344, mae: 25.637007, mean_q: -37.076761, mean_eps: 0.100000\n",
      " 226831/500000: episode: 1591, duration: 0.714s, episode steps: 154, steps per second: 216, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000],  loss: 0.072383, mae: 25.351690, mean_q: -36.608800, mean_eps: 0.100000\n",
      " 226990/500000: episode: 1592, duration: 0.739s, episode steps: 159, steps per second: 215, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.128166, mae: 25.998336, mean_q: -37.529404, mean_eps: 0.100000\n",
      " 227109/500000: episode: 1593, duration: 0.569s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.075399, mae: 26.120689, mean_q: -37.569167, mean_eps: 0.100000\n",
      " 227215/500000: episode: 1594, duration: 0.500s, episode steps: 106, steps per second: 212, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.070738, mae: 25.752646, mean_q: -37.066507, mean_eps: 0.100000\n",
      " 227370/500000: episode: 1595, duration: 0.710s, episode steps: 155, steps per second: 218, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.068165, mae: 25.887873, mean_q: -37.066938, mean_eps: 0.100000\n",
      " 227492/500000: episode: 1596, duration: 0.574s, episode steps: 122, steps per second: 213, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.060242, mae: 25.807632, mean_q: -36.937805, mean_eps: 0.100000\n",
      " 227613/500000: episode: 1597, duration: 0.567s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.061317, mae: 25.956868, mean_q: -37.067015, mean_eps: 0.100000\n",
      " 227700/500000: episode: 1598, duration: 0.403s, episode steps:  87, steps per second: 216, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.055543, mae: 26.215361, mean_q: -37.392150, mean_eps: 0.100000\n",
      " 227855/500000: episode: 1599, duration: 0.715s, episode steps: 155, steps per second: 217, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.903 [0.000, 2.000],  loss: 0.113805, mae: 25.998910, mean_q: -37.149577, mean_eps: 0.100000\n",
      " 228014/500000: episode: 1600, duration: 0.753s, episode steps: 159, steps per second: 211, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.849 [0.000, 2.000],  loss: 0.147308, mae: 26.471498, mean_q: -37.890531, mean_eps: 0.100000\n",
      " 228131/500000: episode: 1601, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.051105, mae: 26.718689, mean_q: -38.454484, mean_eps: 0.100000\n",
      " 228240/500000: episode: 1602, duration: 0.513s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.048968, mae: 27.292191, mean_q: -39.548098, mean_eps: 0.100000\n",
      " 228356/500000: episode: 1603, duration: 0.560s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.043866, mae: 26.774618, mean_q: -39.018983, mean_eps: 0.100000\n",
      " 228549/500000: episode: 1604, duration: 0.916s, episode steps: 193, steps per second: 211, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.156153, mae: 26.886325, mean_q: -38.819869, mean_eps: 0.100000\n",
      " 228737/500000: episode: 1605, duration: 0.881s, episode steps: 188, steps per second: 213, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.096931, mae: 27.655098, mean_q: -39.889608, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 228889/500000: episode: 1606, duration: 0.724s, episode steps: 152, steps per second: 210, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.921 [0.000, 2.000],  loss: 0.063827, mae: 27.731028, mean_q: -40.071625, mean_eps: 0.100000\n",
      " 229002/500000: episode: 1607, duration: 0.532s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.063228, mae: 27.107983, mean_q: -39.070461, mean_eps: 0.100000\n",
      " 229187/500000: episode: 1608, duration: 0.890s, episode steps: 185, steps per second: 208, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.075030, mae: 27.650054, mean_q: -39.828353, mean_eps: 0.100000\n",
      " 229348/500000: episode: 1609, duration: 0.820s, episode steps: 161, steps per second: 196, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 0.133122, mae: 28.154032, mean_q: -40.397469, mean_eps: 0.100000\n",
      " 229474/500000: episode: 1610, duration: 0.690s, episode steps: 126, steps per second: 183, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000],  loss: 0.080610, mae: 28.633801, mean_q: -41.353145, mean_eps: 0.100000\n",
      " 229588/500000: episode: 1611, duration: 0.613s, episode steps: 114, steps per second: 186, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.103644, mae: 28.184837, mean_q: -40.752568, mean_eps: 0.100000\n",
      " 229745/500000: episode: 1612, duration: 0.889s, episode steps: 157, steps per second: 177, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.090430, mae: 27.512506, mean_q: -39.862239, mean_eps: 0.100000\n",
      " 229933/500000: episode: 1613, duration: 0.871s, episode steps: 188, steps per second: 216, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000],  loss: 0.200861, mae: 28.005174, mean_q: -40.117985, mean_eps: 0.100000\n",
      " 230080/500000: episode: 1614, duration: 0.748s, episode steps: 147, steps per second: 197, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: 0.106592, mae: 28.657499, mean_q: -40.999293, mean_eps: 0.100000\n",
      " 230269/500000: episode: 1615, duration: 0.873s, episode steps: 189, steps per second: 217, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.088492, mae: 28.646433, mean_q: -40.894883, mean_eps: 0.100000\n",
      " 230378/500000: episode: 1616, duration: 0.512s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.097468, mae: 28.083238, mean_q: -40.025569, mean_eps: 0.100000\n",
      " 230495/500000: episode: 1617, duration: 0.564s, episode steps: 117, steps per second: 207, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.090723, mae: 27.901048, mean_q: -39.683649, mean_eps: 0.100000\n",
      " 230663/500000: episode: 1618, duration: 0.782s, episode steps: 168, steps per second: 215, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.106212, mae: 28.748920, mean_q: -40.907142, mean_eps: 0.100000\n",
      " 230775/500000: episode: 1619, duration: 0.525s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.084006, mae: 28.363925, mean_q: -40.669235, mean_eps: 0.100000\n",
      " 230885/500000: episode: 1620, duration: 0.526s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.053105, mae: 27.518750, mean_q: -39.714163, mean_eps: 0.100000\n",
      " 231071/500000: episode: 1621, duration: 0.864s, episode steps: 186, steps per second: 215, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.062669, mae: 27.451111, mean_q: -39.555099, mean_eps: 0.100000\n",
      " 231198/500000: episode: 1622, duration: 0.599s, episode steps: 127, steps per second: 212, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.071900, mae: 27.110195, mean_q: -39.214121, mean_eps: 0.100000\n",
      " 231296/500000: episode: 1623, duration: 0.468s, episode steps:  98, steps per second: 209, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.039632, mae: 26.278599, mean_q: -37.950772, mean_eps: 0.100000\n",
      " 231400/500000: episode: 1624, duration: 0.490s, episode steps: 104, steps per second: 212, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.042256, mae: 26.671410, mean_q: -38.315196, mean_eps: 0.100000\n",
      " 231578/500000: episode: 1625, duration: 0.826s, episode steps: 178, steps per second: 216, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.058517, mae: 26.744135, mean_q: -38.242526, mean_eps: 0.100000\n",
      " 231737/500000: episode: 1626, duration: 0.760s, episode steps: 159, steps per second: 209, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.065907, mae: 27.185630, mean_q: -38.897279, mean_eps: 0.100000\n",
      " 231854/500000: episode: 1627, duration: 0.551s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.076473, mae: 27.355215, mean_q: -39.222472, mean_eps: 0.100000\n",
      " 232016/500000: episode: 1628, duration: 0.791s, episode steps: 162, steps per second: 205, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.062 [0.000, 2.000],  loss: 0.130934, mae: 27.487509, mean_q: -39.265335, mean_eps: 0.100000\n",
      " 232199/500000: episode: 1629, duration: 0.892s, episode steps: 183, steps per second: 205, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.923 [0.000, 2.000],  loss: 0.156865, mae: 28.583339, mean_q: -40.917481, mean_eps: 0.100000\n",
      " 232307/500000: episode: 1630, duration: 0.514s, episode steps: 108, steps per second: 210, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.100301, mae: 28.851403, mean_q: -41.715725, mean_eps: 0.100000\n",
      " 232465/500000: episode: 1631, duration: 0.845s, episode steps: 158, steps per second: 187, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.791 [0.000, 2.000],  loss: 0.087256, mae: 29.067668, mean_q: -42.328496, mean_eps: 0.100000\n",
      " 232646/500000: episode: 1632, duration: 0.935s, episode steps: 181, steps per second: 194, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.094149, mae: 28.242495, mean_q: -41.012719, mean_eps: 0.100000\n",
      " 232780/500000: episode: 1633, duration: 0.678s, episode steps: 134, steps per second: 198, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000],  loss: 0.079594, mae: 27.567238, mean_q: -39.717710, mean_eps: 0.100000\n",
      " 232900/500000: episode: 1634, duration: 0.564s, episode steps: 120, steps per second: 213, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.442 [0.000, 2.000],  loss: 0.125010, mae: 28.153154, mean_q: -40.386562, mean_eps: 0.100000\n",
      " 233003/500000: episode: 1635, duration: 0.524s, episode steps: 103, steps per second: 197, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.091488, mae: 27.623772, mean_q: -39.545123, mean_eps: 0.100000\n",
      " 233122/500000: episode: 1636, duration: 0.581s, episode steps: 119, steps per second: 205, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.462 [0.000, 2.000],  loss: 0.117389, mae: 27.354734, mean_q: -39.069032, mean_eps: 0.100000\n",
      " 233255/500000: episode: 1637, duration: 0.644s, episode steps: 133, steps per second: 206, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.081206, mae: 26.596429, mean_q: -37.975028, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 233368/500000: episode: 1638, duration: 0.550s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.076407, mae: 26.638426, mean_q: -37.981234, mean_eps: 0.100000\n",
      " 233480/500000: episode: 1639, duration: 0.544s, episode steps: 112, steps per second: 206, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.068376, mae: 26.028630, mean_q: -37.046812, mean_eps: 0.100000\n",
      " 233595/500000: episode: 1640, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.045615, mae: 26.078244, mean_q: -37.476065, mean_eps: 0.100000\n",
      " 233795/500000: episode: 1641, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.046565, mae: 26.147946, mean_q: -37.663479, mean_eps: 0.100000\n",
      " 233915/500000: episode: 1642, duration: 0.556s, episode steps: 120, steps per second: 216, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.442 [0.000, 2.000],  loss: 0.210002, mae: 26.193091, mean_q: -37.721245, mean_eps: 0.100000\n",
      " 234099/500000: episode: 1643, duration: 0.857s, episode steps: 184, steps per second: 215, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.234143, mae: 27.065279, mean_q: -38.915042, mean_eps: 0.100000\n",
      " 234232/500000: episode: 1644, duration: 0.629s, episode steps: 133, steps per second: 211, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.233526, mae: 27.304323, mean_q: -39.139451, mean_eps: 0.100000\n",
      " 234349/500000: episode: 1645, duration: 0.547s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.209082, mae: 27.389437, mean_q: -39.166426, mean_eps: 0.100000\n",
      " 234471/500000: episode: 1646, duration: 0.563s, episode steps: 122, steps per second: 217, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.234378, mae: 27.563449, mean_q: -39.370501, mean_eps: 0.100000\n",
      " 234582/500000: episode: 1647, duration: 0.524s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.371379, mae: 27.186634, mean_q: -38.615770, mean_eps: 0.100000\n",
      " 234667/500000: episode: 1648, duration: 0.397s, episode steps:  85, steps per second: 214, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.289555, mae: 26.891997, mean_q: -38.228941, mean_eps: 0.100000\n",
      " 234776/500000: episode: 1649, duration: 0.505s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.233524, mae: 26.858826, mean_q: -38.409900, mean_eps: 0.100000\n",
      " 234953/500000: episode: 1650, duration: 0.818s, episode steps: 177, steps per second: 216, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.069100, mae: 26.870587, mean_q: -38.434267, mean_eps: 0.100000\n",
      " 235108/500000: episode: 1651, duration: 0.740s, episode steps: 155, steps per second: 209, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.061820, mae: 26.701255, mean_q: -38.387319, mean_eps: 0.100000\n",
      " 235232/500000: episode: 1652, duration: 0.573s, episode steps: 124, steps per second: 216, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.041595, mae: 26.681920, mean_q: -38.669109, mean_eps: 0.100000\n",
      " 235370/500000: episode: 1653, duration: 0.649s, episode steps: 138, steps per second: 213, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.036998, mae: 26.546234, mean_q: -38.598576, mean_eps: 0.100000\n",
      " 235536/500000: episode: 1654, duration: 0.794s, episode steps: 166, steps per second: 209, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.093635, mae: 27.101344, mean_q: -39.012753, mean_eps: 0.100000\n",
      " 235629/500000: episode: 1655, duration: 0.434s, episode steps:  93, steps per second: 214, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.132177, mae: 26.918057, mean_q: -38.584697, mean_eps: 0.100000\n",
      " 235748/500000: episode: 1656, duration: 0.590s, episode steps: 119, steps per second: 202, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.120319, mae: 27.182927, mean_q: -38.892266, mean_eps: 0.100000\n",
      " 235927/500000: episode: 1657, duration: 0.870s, episode steps: 179, steps per second: 206, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.119438, mae: 26.890285, mean_q: -38.505613, mean_eps: 0.100000\n",
      " 236124/500000: episode: 1658, duration: 0.971s, episode steps: 197, steps per second: 203, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.166640, mae: 26.635065, mean_q: -38.055988, mean_eps: 0.100000\n",
      " 236299/500000: episode: 1659, duration: 0.887s, episode steps: 175, steps per second: 197, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.135571, mae: 26.619807, mean_q: -37.777353, mean_eps: 0.100000\n",
      " 236420/500000: episode: 1660, duration: 0.610s, episode steps: 121, steps per second: 199, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.421 [0.000, 2.000],  loss: 0.116084, mae: 26.591408, mean_q: -37.864630, mean_eps: 0.100000\n",
      " 236513/500000: episode: 1661, duration: 0.474s, episode steps:  93, steps per second: 196, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.106790, mae: 26.297658, mean_q: -37.589361, mean_eps: 0.100000\n",
      " 236660/500000: episode: 1662, duration: 0.739s, episode steps: 147, steps per second: 199, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.422 [0.000, 2.000],  loss: 0.100931, mae: 26.687372, mean_q: -38.136148, mean_eps: 0.100000\n",
      " 236818/500000: episode: 1663, duration: 0.746s, episode steps: 158, steps per second: 212, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.107076, mae: 27.294689, mean_q: -38.832813, mean_eps: 0.100000\n",
      " 236978/500000: episode: 1664, duration: 0.791s, episode steps: 160, steps per second: 202, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.794 [0.000, 2.000],  loss: 0.108906, mae: 27.060543, mean_q: -38.432950, mean_eps: 0.100000\n",
      " 237088/500000: episode: 1665, duration: 0.531s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.106740, mae: 27.066408, mean_q: -38.583481, mean_eps: 0.100000\n",
      " 237204/500000: episode: 1666, duration: 0.541s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.085708, mae: 27.308717, mean_q: -39.028121, mean_eps: 0.100000\n",
      " 237317/500000: episode: 1667, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.062790, mae: 26.616299, mean_q: -38.121220, mean_eps: 0.100000\n",
      " 237450/500000: episode: 1668, duration: 0.653s, episode steps: 133, steps per second: 204, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.060893, mae: 27.007376, mean_q: -38.670140, mean_eps: 0.100000\n",
      " 237571/500000: episode: 1669, duration: 0.576s, episode steps: 121, steps per second: 210, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.070771, mae: 27.053405, mean_q: -38.740475, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 237681/500000: episode: 1670, duration: 0.536s, episode steps: 110, steps per second: 205, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.058188, mae: 26.923478, mean_q: -38.618859, mean_eps: 0.100000\n",
      " 237790/500000: episode: 1671, duration: 0.508s, episode steps: 109, steps per second: 215, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.044472, mae: 26.243784, mean_q: -37.811501, mean_eps: 0.100000\n",
      " 237905/500000: episode: 1672, duration: 0.551s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.034541, mae: 26.260737, mean_q: -37.939208, mean_eps: 0.100000\n",
      " 238021/500000: episode: 1673, duration: 0.553s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.025578, mae: 25.537487, mean_q: -36.904717, mean_eps: 0.100000\n",
      " 238186/500000: episode: 1674, duration: 0.778s, episode steps: 165, steps per second: 212, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.062883, mae: 26.447672, mean_q: -38.010392, mean_eps: 0.100000\n",
      " 238380/500000: episode: 1675, duration: 0.915s, episode steps: 194, steps per second: 212, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.128723, mae: 26.769580, mean_q: -38.325558, mean_eps: 0.100000\n",
      " 238535/500000: episode: 1676, duration: 0.730s, episode steps: 155, steps per second: 212, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.122498, mae: 27.071133, mean_q: -38.777884, mean_eps: 0.100000\n",
      " 238690/500000: episode: 1677, duration: 0.733s, episode steps: 155, steps per second: 212, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000],  loss: 0.134545, mae: 27.508189, mean_q: -39.623726, mean_eps: 0.100000\n",
      " 238849/500000: episode: 1678, duration: 0.761s, episode steps: 159, steps per second: 209, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.146623, mae: 27.988365, mean_q: -40.294032, mean_eps: 0.100000\n",
      " 238996/500000: episode: 1679, duration: 0.689s, episode steps: 147, steps per second: 213, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.135154, mae: 28.361434, mean_q: -40.943151, mean_eps: 0.100000\n",
      " 239171/500000: episode: 1680, duration: 0.827s, episode steps: 175, steps per second: 212, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.124715, mae: 28.644901, mean_q: -41.523094, mean_eps: 0.100000\n",
      " 239302/500000: episode: 1681, duration: 0.609s, episode steps: 131, steps per second: 215, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.344 [0.000, 2.000],  loss: 0.116049, mae: 28.816929, mean_q: -41.811161, mean_eps: 0.100000\n",
      " 239462/500000: episode: 1682, duration: 0.742s, episode steps: 160, steps per second: 216, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.165200, mae: 29.181449, mean_q: -42.244317, mean_eps: 0.100000\n",
      " 239619/500000: episode: 1683, duration: 0.757s, episode steps: 157, steps per second: 207, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.892 [0.000, 2.000],  loss: 0.097790, mae: 29.581118, mean_q: -42.877109, mean_eps: 0.100000\n",
      " 239745/500000: episode: 1684, duration: 0.594s, episode steps: 126, steps per second: 212, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.088388, mae: 29.352207, mean_q: -42.476822, mean_eps: 0.100000\n",
      " 239835/500000: episode: 1685, duration: 0.434s, episode steps:  90, steps per second: 207, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.110040, mae: 29.372067, mean_q: -42.290491, mean_eps: 0.100000\n",
      " 240035/500000: episode: 1686, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.114034, mae: 29.668102, mean_q: -42.585647, mean_eps: 0.100000\n",
      " 240124/500000: episode: 1687, duration: 0.413s, episode steps:  89, steps per second: 215, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.326 [0.000, 2.000],  loss: 0.227656, mae: 29.081343, mean_q: -41.726331, mean_eps: 0.100000\n",
      " 240253/500000: episode: 1688, duration: 0.644s, episode steps: 129, steps per second: 200, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000],  loss: 0.449549, mae: 29.283076, mean_q: -41.743670, mean_eps: 0.100000\n",
      " 240401/500000: episode: 1689, duration: 0.727s, episode steps: 148, steps per second: 204, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.419368, mae: 28.779390, mean_q: -41.003906, mean_eps: 0.100000\n",
      " 240523/500000: episode: 1690, duration: 0.595s, episode steps: 122, steps per second: 205, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.238423, mae: 28.753282, mean_q: -40.956106, mean_eps: 0.100000\n",
      " 240693/500000: episode: 1691, duration: 0.804s, episode steps: 170, steps per second: 211, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.053 [0.000, 2.000],  loss: 0.169820, mae: 28.400084, mean_q: -40.217303, mean_eps: 0.100000\n",
      " 240825/500000: episode: 1692, duration: 0.666s, episode steps: 132, steps per second: 198, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.424 [0.000, 2.000],  loss: 0.219988, mae: 28.768085, mean_q: -40.966160, mean_eps: 0.100000\n",
      " 241009/500000: episode: 1693, duration: 0.877s, episode steps: 184, steps per second: 210, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.244892, mae: 27.658666, mean_q: -39.070165, mean_eps: 0.100000\n",
      " 241181/500000: episode: 1694, duration: 0.804s, episode steps: 172, steps per second: 214, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.092988, mae: 27.295185, mean_q: -38.599654, mean_eps: 0.100000\n",
      " 241293/500000: episode: 1695, duration: 0.534s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.085098, mae: 26.844914, mean_q: -38.081559, mean_eps: 0.100000\n",
      " 241475/500000: episode: 1696, duration: 0.875s, episode steps: 182, steps per second: 208, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.187 [0.000, 2.000],  loss: 0.069223, mae: 27.206212, mean_q: -38.702974, mean_eps: 0.100000\n",
      " 241572/500000: episode: 1697, duration: 0.486s, episode steps:  97, steps per second: 199, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.087771, mae: 27.706064, mean_q: -39.386645, mean_eps: 0.100000\n",
      " 241746/500000: episode: 1698, duration: 0.840s, episode steps: 174, steps per second: 207, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.067653, mae: 27.212545, mean_q: -38.639159, mean_eps: 0.100000\n",
      " 241857/500000: episode: 1699, duration: 0.542s, episode steps: 111, steps per second: 205, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.080485, mae: 27.748083, mean_q: -39.568551, mean_eps: 0.100000\n",
      " 242015/500000: episode: 1700, duration: 0.738s, episode steps: 158, steps per second: 214, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.091344, mae: 27.754508, mean_q: -39.824529, mean_eps: 0.100000\n",
      " 242133/500000: episode: 1701, duration: 0.592s, episode steps: 118, steps per second: 199, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.085645, mae: 27.902616, mean_q: -40.072492, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 242252/500000: episode: 1702, duration: 0.580s, episode steps: 119, steps per second: 205, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.120956, mae: 27.669261, mean_q: -39.559564, mean_eps: 0.100000\n",
      " 242342/500000: episode: 1703, duration: 0.431s, episode steps:  90, steps per second: 209, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.116003, mae: 27.984745, mean_q: -39.757764, mean_eps: 0.100000\n",
      " 242463/500000: episode: 1704, duration: 0.588s, episode steps: 121, steps per second: 206, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.104507, mae: 27.051379, mean_q: -38.376484, mean_eps: 0.100000\n",
      " 242633/500000: episode: 1705, duration: 0.812s, episode steps: 170, steps per second: 209, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.095411, mae: 27.263976, mean_q: -38.610472, mean_eps: 0.100000\n",
      " 242735/500000: episode: 1706, duration: 0.478s, episode steps: 102, steps per second: 213, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.094894, mae: 27.010851, mean_q: -38.231602, mean_eps: 0.100000\n",
      " 242841/500000: episode: 1707, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.115090, mae: 27.147487, mean_q: -38.100732, mean_eps: 0.100000\n",
      " 242954/500000: episode: 1708, duration: 0.557s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.128690, mae: 27.156265, mean_q: -37.932349, mean_eps: 0.100000\n",
      " 243105/500000: episode: 1709, duration: 0.718s, episode steps: 151, steps per second: 210, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.921 [0.000, 2.000],  loss: 0.092130, mae: 27.458707, mean_q: -38.578360, mean_eps: 0.100000\n",
      " 243288/500000: episode: 1710, duration: 0.908s, episode steps: 183, steps per second: 202, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.101559, mae: 27.612677, mean_q: -39.020967, mean_eps: 0.100000\n",
      " 243378/500000: episode: 1711, duration: 0.430s, episode steps:  90, steps per second: 209, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.166167, mae: 27.091258, mean_q: -38.358225, mean_eps: 0.100000\n",
      " 243501/500000: episode: 1712, duration: 0.608s, episode steps: 123, steps per second: 202, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.100831, mae: 27.265746, mean_q: -38.559609, mean_eps: 0.100000\n",
      " 243701/500000: episode: 1713, duration: 0.981s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.077433, mae: 26.703295, mean_q: -37.647372, mean_eps: 0.100000\n",
      " 243807/500000: episode: 1714, duration: 0.505s, episode steps: 106, steps per second: 210, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.098369, mae: 26.969477, mean_q: -38.391048, mean_eps: 0.100000\n",
      " 243968/500000: episode: 1715, duration: 0.755s, episode steps: 161, steps per second: 213, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.124948, mae: 26.753827, mean_q: -38.349547, mean_eps: 0.100000\n",
      " 244152/500000: episode: 1716, duration: 0.864s, episode steps: 184, steps per second: 213, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.951 [0.000, 2.000],  loss: 0.123439, mae: 27.210099, mean_q: -38.889104, mean_eps: 0.100000\n",
      " 244314/500000: episode: 1717, duration: 0.788s, episode steps: 162, steps per second: 206, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.154672, mae: 27.956566, mean_q: -40.043699, mean_eps: 0.100000\n",
      " 244435/500000: episode: 1718, duration: 0.587s, episode steps: 121, steps per second: 206, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.119393, mae: 28.289226, mean_q: -40.496424, mean_eps: 0.100000\n",
      " 244547/500000: episode: 1719, duration: 0.554s, episode steps: 112, steps per second: 202, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.148035, mae: 28.739091, mean_q: -40.975300, mean_eps: 0.100000\n",
      " 244711/500000: episode: 1720, duration: 0.797s, episode steps: 164, steps per second: 206, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.178673, mae: 28.400855, mean_q: -40.355983, mean_eps: 0.100000\n",
      " 244899/500000: episode: 1721, duration: 0.859s, episode steps: 188, steps per second: 219, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.101837, mae: 28.873136, mean_q: -40.812614, mean_eps: 0.100000\n",
      " 245076/500000: episode: 1722, duration: 0.832s, episode steps: 177, steps per second: 213, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.095405, mae: 28.348049, mean_q: -39.938967, mean_eps: 0.100000\n",
      " 245253/500000: episode: 1723, duration: 0.857s, episode steps: 177, steps per second: 207, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.079 [0.000, 2.000],  loss: 0.094104, mae: 27.924326, mean_q: -39.155882, mean_eps: 0.100000\n",
      " 245411/500000: episode: 1724, duration: 0.803s, episode steps: 158, steps per second: 197, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000],  loss: 0.090907, mae: 28.528693, mean_q: -40.273243, mean_eps: 0.100000\n",
      " 245592/500000: episode: 1725, duration: 0.889s, episode steps: 181, steps per second: 204, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.983 [0.000, 2.000],  loss: 0.100570, mae: 28.966268, mean_q: -41.136841, mean_eps: 0.100000\n",
      " 245775/500000: episode: 1726, duration: 0.888s, episode steps: 183, steps per second: 206, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.106370, mae: 29.262915, mean_q: -41.878277, mean_eps: 0.100000\n",
      " 245892/500000: episode: 1727, duration: 0.599s, episode steps: 117, steps per second: 195, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.106178, mae: 29.885219, mean_q: -42.771040, mean_eps: 0.100000\n",
      " 246016/500000: episode: 1728, duration: 0.584s, episode steps: 124, steps per second: 212, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.122855, mae: 29.540641, mean_q: -41.852576, mean_eps: 0.100000\n",
      " 246216/500000: episode: 1729, duration: 0.958s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.146022, mae: 30.285613, mean_q: -42.738006, mean_eps: 0.100000\n",
      " 246412/500000: episode: 1730, duration: 0.908s, episode steps: 196, steps per second: 216, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000],  loss: 1.068271, mae: 30.731311, mean_q: -43.219321, mean_eps: 0.100000\n",
      " 246575/500000: episode: 1731, duration: 0.754s, episode steps: 163, steps per second: 216, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.920523, mae: 29.711034, mean_q: -41.870187, mean_eps: 0.100000\n",
      " 246679/500000: episode: 1732, duration: 0.493s, episode steps: 104, steps per second: 211, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.977807, mae: 29.140329, mean_q: -40.739528, mean_eps: 0.100000\n",
      " 246799/500000: episode: 1733, duration: 0.589s, episode steps: 120, steps per second: 204, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.619270, mae: 28.657987, mean_q: -39.901886, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 246973/500000: episode: 1734, duration: 0.836s, episode steps: 174, steps per second: 208, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.983 [0.000, 2.000],  loss: 0.726437, mae: 28.977653, mean_q: -40.452441, mean_eps: 0.100000\n",
      " 247070/500000: episode: 1735, duration: 0.469s, episode steps:  97, steps per second: 207, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 1.327937, mae: 28.960720, mean_q: -40.500462, mean_eps: 0.100000\n",
      " 247270/500000: episode: 1736, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.699618, mae: 28.055277, mean_q: -39.014155, mean_eps: 0.100000\n",
      " 247447/500000: episode: 1737, duration: 0.824s, episode steps: 177, steps per second: 215, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.135289, mae: 28.163895, mean_q: -39.266861, mean_eps: 0.100000\n",
      " 247647/500000: episode: 1738, duration: 0.957s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.119048, mae: 28.581446, mean_q: -39.971030, mean_eps: 0.100000\n",
      " 247742/500000: episode: 1739, duration: 0.514s, episode steps:  95, steps per second: 185, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.139017, mae: 29.199209, mean_q: -41.099489, mean_eps: 0.100000\n",
      " 247942/500000: episode: 1740, duration: 1.049s, episode steps: 200, steps per second: 191, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.219597, mae: 28.319311, mean_q: -39.908265, mean_eps: 0.100000\n",
      " 248111/500000: episode: 1741, duration: 0.918s, episode steps: 169, steps per second: 184, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.352187, mae: 29.243607, mean_q: -41.253561, mean_eps: 0.100000\n",
      " 248221/500000: episode: 1742, duration: 0.568s, episode steps: 110, steps per second: 194, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.199625, mae: 28.951041, mean_q: -40.945001, mean_eps: 0.100000\n",
      " 248328/500000: episode: 1743, duration: 0.519s, episode steps: 107, steps per second: 206, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.251578, mae: 28.971191, mean_q: -40.778549, mean_eps: 0.100000\n",
      " 248443/500000: episode: 1744, duration: 0.531s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.223664, mae: 28.604832, mean_q: -40.376874, mean_eps: 0.100000\n",
      " 248562/500000: episode: 1745, duration: 0.549s, episode steps: 119, steps per second: 217, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.166596, mae: 28.105732, mean_q: -39.720895, mean_eps: 0.100000\n",
      " 248685/500000: episode: 1746, duration: 0.582s, episode steps: 123, steps per second: 211, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.099302, mae: 27.581210, mean_q: -39.102059, mean_eps: 0.100000\n",
      " 248805/500000: episode: 1747, duration: 0.631s, episode steps: 120, steps per second: 190, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.075381, mae: 27.150198, mean_q: -38.651009, mean_eps: 0.100000\n",
      " 248919/500000: episode: 1748, duration: 0.606s, episode steps: 114, steps per second: 188, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.412 [0.000, 2.000],  loss: 0.092221, mae: 26.742540, mean_q: -38.260904, mean_eps: 0.100000\n",
      " 249034/500000: episode: 1749, duration: 0.605s, episode steps: 115, steps per second: 190, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.037782, mae: 26.348689, mean_q: -37.881199, mean_eps: 0.100000\n",
      " 249165/500000: episode: 1750, duration: 0.710s, episode steps: 131, steps per second: 184, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000],  loss: 0.037122, mae: 25.595447, mean_q: -36.933119, mean_eps: 0.100000\n",
      " 249277/500000: episode: 1751, duration: 0.570s, episode steps: 112, steps per second: 197, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 0.036255, mae: 25.904721, mean_q: -37.438135, mean_eps: 0.100000\n",
      " 249400/500000: episode: 1752, duration: 0.610s, episode steps: 123, steps per second: 202, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.035614, mae: 25.665405, mean_q: -37.161255, mean_eps: 0.100000\n",
      " 249514/500000: episode: 1753, duration: 0.563s, episode steps: 114, steps per second: 203, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.025517, mae: 26.035372, mean_q: -37.622942, mean_eps: 0.100000\n",
      " 249630/500000: episode: 1754, duration: 0.545s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.422 [0.000, 2.000],  loss: 0.029236, mae: 25.992165, mean_q: -37.563648, mean_eps: 0.100000\n",
      " 249810/500000: episode: 1755, duration: 0.841s, episode steps: 180, steps per second: 214, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.058999, mae: 26.012646, mean_q: -37.444179, mean_eps: 0.100000\n",
      " 249990/500000: episode: 1756, duration: 0.849s, episode steps: 180, steps per second: 212, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.079247, mae: 25.817973, mean_q: -36.952963, mean_eps: 0.100000\n",
      " 250103/500000: episode: 1757, duration: 0.526s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.080585, mae: 25.696346, mean_q: -36.684805, mean_eps: 0.100000\n",
      " 250219/500000: episode: 1758, duration: 0.542s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.051415, mae: 25.634469, mean_q: -36.868659, mean_eps: 0.100000\n",
      " 250415/500000: episode: 1759, duration: 0.916s, episode steps: 196, steps per second: 214, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.087223, mae: 25.663932, mean_q: -36.887127, mean_eps: 0.100000\n",
      " 250529/500000: episode: 1760, duration: 0.571s, episode steps: 114, steps per second: 200, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.110956, mae: 25.670919, mean_q: -36.796751, mean_eps: 0.100000\n",
      " 250684/500000: episode: 1761, duration: 0.799s, episode steps: 155, steps per second: 194, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.961 [0.000, 2.000],  loss: 0.163070, mae: 26.363338, mean_q: -37.796978, mean_eps: 0.100000\n",
      " 250777/500000: episode: 1762, duration: 0.448s, episode steps:  93, steps per second: 208, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000],  loss: 0.106579, mae: 25.890890, mean_q: -37.478740, mean_eps: 0.100000\n",
      " 250958/500000: episode: 1763, duration: 0.840s, episode steps: 181, steps per second: 216, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.121784, mae: 26.405258, mean_q: -38.102418, mean_eps: 0.100000\n",
      " 251071/500000: episode: 1764, duration: 0.608s, episode steps: 113, steps per second: 186, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.144353, mae: 26.191714, mean_q: -37.767404, mean_eps: 0.100000\n",
      " 251205/500000: episode: 1765, duration: 0.686s, episode steps: 134, steps per second: 195, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.095181, mae: 26.520503, mean_q: -38.338196, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 251392/500000: episode: 1766, duration: 0.905s, episode steps: 187, steps per second: 207, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.113251, mae: 26.373958, mean_q: -37.760875, mean_eps: 0.100000\n",
      " 251592/500000: episode: 1767, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.089159, mae: 26.867449, mean_q: -38.386898, mean_eps: 0.100000\n",
      " 251717/500000: episode: 1768, duration: 0.579s, episode steps: 125, steps per second: 216, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.118960, mae: 26.615018, mean_q: -37.894586, mean_eps: 0.100000\n",
      " 251844/500000: episode: 1769, duration: 0.596s, episode steps: 127, steps per second: 213, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.226052, mae: 27.278372, mean_q: -38.623297, mean_eps: 0.100000\n",
      " 252019/500000: episode: 1770, duration: 0.825s, episode steps: 175, steps per second: 212, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000],  loss: 0.178010, mae: 27.676796, mean_q: -39.166749, mean_eps: 0.100000\n",
      " 252132/500000: episode: 1771, duration: 0.517s, episode steps: 113, steps per second: 219, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.119543, mae: 27.931789, mean_q: -39.472080, mean_eps: 0.100000\n",
      " 252306/500000: episode: 1772, duration: 0.848s, episode steps: 174, steps per second: 205, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.133327, mae: 27.770010, mean_q: -39.349670, mean_eps: 0.100000\n",
      " 252435/500000: episode: 1773, duration: 0.646s, episode steps: 129, steps per second: 200, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.103710, mae: 27.933355, mean_q: -39.726474, mean_eps: 0.100000\n",
      " 252597/500000: episode: 1774, duration: 0.810s, episode steps: 162, steps per second: 200, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.106319, mae: 28.237968, mean_q: -39.778709, mean_eps: 0.100000\n",
      " 252715/500000: episode: 1775, duration: 0.559s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.134796, mae: 27.582841, mean_q: -38.858548, mean_eps: 0.100000\n",
      " 252876/500000: episode: 1776, duration: 0.758s, episode steps: 161, steps per second: 212, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.851 [0.000, 2.000],  loss: 0.137694, mae: 27.894765, mean_q: -39.728517, mean_eps: 0.100000\n",
      " 253023/500000: episode: 1777, duration: 0.719s, episode steps: 147, steps per second: 204, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: 0.104977, mae: 27.722932, mean_q: -39.729803, mean_eps: 0.100000\n",
      " 253223/500000: episode: 1778, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 0.098473, mae: 28.225089, mean_q: -40.686039, mean_eps: 0.100000\n",
      " 253309/500000: episode: 1779, duration: 0.399s, episode steps:  86, steps per second: 215, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.219252, mae: 28.662395, mean_q: -41.213784, mean_eps: 0.100000\n",
      " 253424/500000: episode: 1780, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.193317, mae: 28.258788, mean_q: -40.746322, mean_eps: 0.100000\n",
      " 253531/500000: episode: 1781, duration: 0.520s, episode steps: 107, steps per second: 206, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.143506, mae: 27.600362, mean_q: -40.159436, mean_eps: 0.100000\n",
      " 253692/500000: episode: 1782, duration: 0.765s, episode steps: 161, steps per second: 211, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.062 [0.000, 2.000],  loss: 0.077971, mae: 28.090582, mean_q: -40.738566, mean_eps: 0.100000\n",
      " 253802/500000: episode: 1783, duration: 0.505s, episode steps: 110, steps per second: 218, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.142543, mae: 27.978386, mean_q: -40.457508, mean_eps: 0.100000\n",
      " 253913/500000: episode: 1784, duration: 0.515s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.106531, mae: 27.444686, mean_q: -39.566396, mean_eps: 0.100000\n",
      " 254073/500000: episode: 1785, duration: 0.761s, episode steps: 160, steps per second: 210, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.088762, mae: 27.555255, mean_q: -39.663478, mean_eps: 0.100000\n",
      " 254226/500000: episode: 1786, duration: 0.715s, episode steps: 153, steps per second: 214, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.882 [0.000, 2.000],  loss: 0.148505, mae: 26.599328, mean_q: -38.204394, mean_eps: 0.100000\n",
      " 254341/500000: episode: 1787, duration: 0.552s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.080644, mae: 27.342590, mean_q: -39.401471, mean_eps: 0.100000\n",
      " 254466/500000: episode: 1788, duration: 0.634s, episode steps: 125, steps per second: 197, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.054049, mae: 26.976138, mean_q: -38.819211, mean_eps: 0.100000\n",
      " 254581/500000: episode: 1789, duration: 0.538s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.083539, mae: 27.185707, mean_q: -39.227464, mean_eps: 0.100000\n",
      " 254680/500000: episode: 1790, duration: 0.458s, episode steps:  99, steps per second: 216, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.050141, mae: 26.516061, mean_q: -38.325136, mean_eps: 0.100000\n",
      " 254789/500000: episode: 1791, duration: 0.513s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.061866, mae: 26.831038, mean_q: -38.556727, mean_eps: 0.100000\n",
      " 254903/500000: episode: 1792, duration: 0.539s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.065019, mae: 26.981082, mean_q: -38.831653, mean_eps: 0.100000\n",
      " 255018/500000: episode: 1793, duration: 0.598s, episode steps: 115, steps per second: 192, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.045681, mae: 26.786394, mean_q: -38.660241, mean_eps: 0.100000\n",
      " 255121/500000: episode: 1794, duration: 0.482s, episode steps: 103, steps per second: 214, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.025610, mae: 26.294990, mean_q: -37.910265, mean_eps: 0.100000\n",
      " 255273/500000: episode: 1795, duration: 0.756s, episode steps: 152, steps per second: 201, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.039 [0.000, 2.000],  loss: 0.033837, mae: 25.762276, mean_q: -36.870942, mean_eps: 0.100000\n",
      " 255397/500000: episode: 1796, duration: 0.627s, episode steps: 124, steps per second: 198, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.058058, mae: 25.492536, mean_q: -36.363559, mean_eps: 0.100000\n",
      " 255518/500000: episode: 1797, duration: 0.590s, episode steps: 121, steps per second: 205, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.067131, mae: 25.522756, mean_q: -36.285397, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 255632/500000: episode: 1798, duration: 0.541s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.074175, mae: 25.472748, mean_q: -36.298009, mean_eps: 0.100000\n",
      " 255743/500000: episode: 1799, duration: 0.537s, episode steps: 111, steps per second: 207, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.061591, mae: 25.715877, mean_q: -36.927404, mean_eps: 0.100000\n",
      " 255859/500000: episode: 1800, duration: 0.542s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.056011, mae: 25.489976, mean_q: -36.679783, mean_eps: 0.100000\n",
      " 255973/500000: episode: 1801, duration: 0.534s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.063512, mae: 25.468747, mean_q: -36.566424, mean_eps: 0.100000\n",
      " 256089/500000: episode: 1802, duration: 0.551s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.070485, mae: 25.824775, mean_q: -37.235205, mean_eps: 0.100000\n",
      " 256202/500000: episode: 1803, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.053676, mae: 25.355716, mean_q: -36.885112, mean_eps: 0.100000\n",
      " 256288/500000: episode: 1804, duration: 0.399s, episode steps:  86, steps per second: 215, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.032363, mae: 25.250746, mean_q: -36.768162, mean_eps: 0.100000\n",
      " 256403/500000: episode: 1805, duration: 0.527s, episode steps: 115, steps per second: 218, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.023255, mae: 25.113854, mean_q: -36.783430, mean_eps: 0.100000\n",
      " 256574/500000: episode: 1806, duration: 0.796s, episode steps: 171, steps per second: 215, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.025680, mae: 25.096396, mean_q: -36.707827, mean_eps: 0.100000\n",
      " 256722/500000: episode: 1807, duration: 0.681s, episode steps: 148, steps per second: 217, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.066409, mae: 25.311444, mean_q: -36.779033, mean_eps: 0.100000\n",
      " 256838/500000: episode: 1808, duration: 0.549s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.046103, mae: 25.288572, mean_q: -36.791716, mean_eps: 0.100000\n",
      " 256957/500000: episode: 1809, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.046262, mae: 25.021405, mean_q: -36.333717, mean_eps: 0.100000\n",
      " 257074/500000: episode: 1810, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.033657, mae: 25.554731, mean_q: -37.132535, mean_eps: 0.100000\n",
      " 257258/500000: episode: 1811, duration: 0.850s, episode steps: 184, steps per second: 217, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.063836, mae: 25.765593, mean_q: -37.257681, mean_eps: 0.100000\n",
      " 257347/500000: episode: 1812, duration: 0.419s, episode steps:  89, steps per second: 213, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.088108, mae: 25.666475, mean_q: -36.998718, mean_eps: 0.100000\n",
      " 257496/500000: episode: 1813, duration: 0.699s, episode steps: 149, steps per second: 213, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.115110, mae: 26.552856, mean_q: -38.275734, mean_eps: 0.100000\n",
      " 257605/500000: episode: 1814, duration: 0.499s, episode steps: 109, steps per second: 219, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.051992, mae: 26.084015, mean_q: -37.714824, mean_eps: 0.100000\n",
      " 257731/500000: episode: 1815, duration: 0.586s, episode steps: 126, steps per second: 215, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.050139, mae: 25.508117, mean_q: -36.858821, mean_eps: 0.100000\n",
      " 257841/500000: episode: 1816, duration: 0.527s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 0.076300, mae: 25.920935, mean_q: -37.352261, mean_eps: 0.100000\n",
      " 258016/500000: episode: 1817, duration: 0.810s, episode steps: 175, steps per second: 216, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.051069, mae: 26.506289, mean_q: -38.182365, mean_eps: 0.100000\n",
      " 258129/500000: episode: 1818, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.034342, mae: 26.209646, mean_q: -37.787199, mean_eps: 0.100000\n",
      " 258244/500000: episode: 1819, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.037263, mae: 26.027874, mean_q: -37.593115, mean_eps: 0.100000\n",
      " 258401/500000: episode: 1820, duration: 0.727s, episode steps: 157, steps per second: 216, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.067809, mae: 26.903394, mean_q: -38.704350, mean_eps: 0.100000\n",
      " 258519/500000: episode: 1821, duration: 0.544s, episode steps: 118, steps per second: 217, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.056333, mae: 26.418984, mean_q: -37.954154, mean_eps: 0.100000\n",
      " 258645/500000: episode: 1822, duration: 0.644s, episode steps: 126, steps per second: 196, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.102707, mae: 26.777432, mean_q: -38.304480, mean_eps: 0.100000\n",
      " 258845/500000: episode: 1823, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000],  loss: 0.077309, mae: 26.810428, mean_q: -38.304908, mean_eps: 0.100000\n",
      " 259003/500000: episode: 1824, duration: 0.764s, episode steps: 158, steps per second: 207, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.281219, mae: 27.130971, mean_q: -38.521070, mean_eps: 0.100000\n",
      " 259155/500000: episode: 1825, duration: 0.728s, episode steps: 152, steps per second: 209, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.251659, mae: 27.555500, mean_q: -39.093855, mean_eps: 0.100000\n",
      " 259272/500000: episode: 1826, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.132705, mae: 27.484657, mean_q: -39.222820, mean_eps: 0.100000\n",
      " 259462/500000: episode: 1827, duration: 0.928s, episode steps: 190, steps per second: 205, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.207449, mae: 27.256449, mean_q: -39.096569, mean_eps: 0.100000\n",
      " 259556/500000: episode: 1828, duration: 0.443s, episode steps:  94, steps per second: 212, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.172712, mae: 27.400431, mean_q: -39.377702, mean_eps: 0.100000\n",
      " 259676/500000: episode: 1829, duration: 0.557s, episode steps: 120, steps per second: 216, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 2.000],  loss: 0.120283, mae: 27.394327, mean_q: -39.346149, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 259834/500000: episode: 1830, duration: 0.742s, episode steps: 158, steps per second: 213, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000],  loss: 0.130471, mae: 27.598187, mean_q: -39.574090, mean_eps: 0.100000\n",
      " 259962/500000: episode: 1831, duration: 0.668s, episode steps: 128, steps per second: 192, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.453 [0.000, 2.000],  loss: 0.047711, mae: 27.370014, mean_q: -39.347711, mean_eps: 0.100000\n",
      " 260084/500000: episode: 1832, duration: 0.582s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.045635, mae: 26.837958, mean_q: -38.508408, mean_eps: 0.100000\n",
      " 260281/500000: episode: 1833, duration: 0.965s, episode steps: 197, steps per second: 204, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.063628, mae: 26.521983, mean_q: -37.894468, mean_eps: 0.100000\n",
      " 260395/500000: episode: 1834, duration: 0.574s, episode steps: 114, steps per second: 199, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.079370, mae: 25.772673, mean_q: -36.639014, mean_eps: 0.100000\n",
      " 260504/500000: episode: 1835, duration: 0.541s, episode steps: 109, steps per second: 202, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.054114, mae: 25.869406, mean_q: -36.908944, mean_eps: 0.100000\n",
      " 260613/500000: episode: 1836, duration: 0.542s, episode steps: 109, steps per second: 201, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.055435, mae: 26.158345, mean_q: -37.537474, mean_eps: 0.100000\n",
      " 260723/500000: episode: 1837, duration: 0.565s, episode steps: 110, steps per second: 195, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.052955, mae: 26.179964, mean_q: -37.704716, mean_eps: 0.100000\n",
      " 260832/500000: episode: 1838, duration: 0.565s, episode steps: 109, steps per second: 193, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.413 [0.000, 2.000],  loss: 0.035227, mae: 25.668894, mean_q: -37.215666, mean_eps: 0.100000\n",
      " 260992/500000: episode: 1839, duration: 0.794s, episode steps: 160, steps per second: 201, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.046826, mae: 26.216563, mean_q: -38.093187, mean_eps: 0.100000\n",
      " 261106/500000: episode: 1840, duration: 0.546s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.055683, mae: 26.098049, mean_q: -38.086631, mean_eps: 0.100000\n",
      " 261237/500000: episode: 1841, duration: 0.630s, episode steps: 131, steps per second: 208, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 2.000],  loss: 0.037113, mae: 26.429237, mean_q: -38.801907, mean_eps: 0.100000\n",
      " 261352/500000: episode: 1842, duration: 0.538s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.038101, mae: 26.339580, mean_q: -38.628172, mean_eps: 0.100000\n",
      " 261469/500000: episode: 1843, duration: 0.560s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.027328, mae: 26.378240, mean_q: -38.651396, mean_eps: 0.100000\n",
      " 261581/500000: episode: 1844, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.028211, mae: 26.283255, mean_q: -38.555367, mean_eps: 0.100000\n",
      " 261698/500000: episode: 1845, duration: 0.548s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.009 [0.000, 2.000],  loss: 0.022275, mae: 26.429428, mean_q: -38.799836, mean_eps: 0.100000\n",
      " 261805/500000: episode: 1846, duration: 0.507s, episode steps: 107, steps per second: 211, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.020104, mae: 26.668766, mean_q: -39.135514, mean_eps: 0.100000\n",
      " 261980/500000: episode: 1847, duration: 0.833s, episode steps: 175, steps per second: 210, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.026625, mae: 26.399493, mean_q: -38.658174, mean_eps: 0.100000\n",
      " 262090/500000: episode: 1848, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.029782, mae: 25.950281, mean_q: -37.928397, mean_eps: 0.100000\n",
      " 262200/500000: episode: 1849, duration: 0.527s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.030925, mae: 25.789265, mean_q: -37.702807, mean_eps: 0.100000\n",
      " 262312/500000: episode: 1850, duration: 0.538s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.023122, mae: 25.997858, mean_q: -38.183407, mean_eps: 0.100000\n",
      " 262471/500000: episode: 1851, duration: 0.760s, episode steps: 159, steps per second: 209, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.074870, mae: 26.319306, mean_q: -38.443768, mean_eps: 0.100000\n",
      " 262584/500000: episode: 1852, duration: 0.547s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.113476, mae: 26.539673, mean_q: -38.663929, mean_eps: 0.100000\n",
      " 262701/500000: episode: 1853, duration: 0.585s, episode steps: 117, steps per second: 200, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.057082, mae: 26.112585, mean_q: -37.992697, mean_eps: 0.100000\n",
      " 262821/500000: episode: 1854, duration: 0.575s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.051456, mae: 26.611010, mean_q: -38.584608, mean_eps: 0.100000\n",
      " 262954/500000: episode: 1855, duration: 0.627s, episode steps: 133, steps per second: 212, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.058465, mae: 26.158782, mean_q: -37.941580, mean_eps: 0.100000\n",
      " 263067/500000: episode: 1856, duration: 0.523s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.076295, mae: 26.045235, mean_q: -37.578658, mean_eps: 0.100000\n",
      " 263202/500000: episode: 1857, duration: 0.649s, episode steps: 135, steps per second: 208, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 0.112787, mae: 26.856761, mean_q: -38.762084, mean_eps: 0.100000\n",
      " 263402/500000: episode: 1858, duration: 1.032s, episode steps: 200, steps per second: 194, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.118861, mae: 26.848839, mean_q: -38.398985, mean_eps: 0.100000\n",
      " 263560/500000: episode: 1859, duration: 0.770s, episode steps: 158, steps per second: 205, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.816 [0.000, 2.000],  loss: 0.153702, mae: 27.257293, mean_q: -38.976802, mean_eps: 0.100000\n",
      " 263668/500000: episode: 1860, duration: 0.521s, episode steps: 108, steps per second: 207, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.257181, mae: 27.571804, mean_q: -39.195180, mean_eps: 0.100000\n",
      " 263798/500000: episode: 1861, duration: 0.608s, episode steps: 130, steps per second: 214, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.171807, mae: 27.819519, mean_q: -39.605118, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 263914/500000: episode: 1862, duration: 0.554s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.155175, mae: 27.790968, mean_q: -39.468579, mean_eps: 0.100000\n",
      " 264034/500000: episode: 1863, duration: 0.578s, episode steps: 120, steps per second: 208, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.223126, mae: 27.612634, mean_q: -39.429497, mean_eps: 0.100000\n",
      " 264190/500000: episode: 1864, duration: 0.763s, episode steps: 156, steps per second: 205, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.051 [0.000, 2.000],  loss: 0.199056, mae: 27.866858, mean_q: -39.571721, mean_eps: 0.100000\n",
      " 264313/500000: episode: 1865, duration: 0.623s, episode steps: 123, steps per second: 197, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.167808, mae: 27.739496, mean_q: -39.625398, mean_eps: 0.100000\n",
      " 264480/500000: episode: 1866, duration: 0.857s, episode steps: 167, steps per second: 195, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000],  loss: 0.108040, mae: 27.453938, mean_q: -39.205493, mean_eps: 0.100000\n",
      " 264578/500000: episode: 1867, duration: 0.459s, episode steps:  98, steps per second: 214, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.113954, mae: 26.495962, mean_q: -37.592060, mean_eps: 0.100000\n",
      " 264691/500000: episode: 1868, duration: 0.526s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.060072, mae: 26.509054, mean_q: -37.830179, mean_eps: 0.100000\n",
      " 264883/500000: episode: 1869, duration: 0.921s, episode steps: 192, steps per second: 208, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.099509, mae: 26.907246, mean_q: -38.300952, mean_eps: 0.100000\n",
      " 265044/500000: episode: 1870, duration: 0.808s, episode steps: 161, steps per second: 199, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 0.070555, mae: 27.460369, mean_q: -39.095217, mean_eps: 0.100000\n",
      " 265172/500000: episode: 1871, duration: 0.630s, episode steps: 128, steps per second: 203, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.064630, mae: 27.176781, mean_q: -38.738214, mean_eps: 0.100000\n",
      " 265272/500000: episode: 1872, duration: 0.481s, episode steps: 100, steps per second: 208, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.073471, mae: 26.933100, mean_q: -38.039865, mean_eps: 0.100000\n",
      " 265382/500000: episode: 1873, duration: 0.552s, episode steps: 110, steps per second: 199, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.072991, mae: 27.103887, mean_q: -38.503840, mean_eps: 0.100000\n",
      " 265532/500000: episode: 1874, duration: 0.732s, episode steps: 150, steps per second: 205, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.973 [0.000, 2.000],  loss: 0.083833, mae: 27.234913, mean_q: -38.619100, mean_eps: 0.100000\n",
      " 265647/500000: episode: 1875, duration: 0.592s, episode steps: 115, steps per second: 194, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.066049, mae: 26.892844, mean_q: -38.519297, mean_eps: 0.100000\n",
      " 265762/500000: episode: 1876, duration: 0.560s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.072761, mae: 27.259697, mean_q: -39.075122, mean_eps: 0.100000\n",
      " 265933/500000: episode: 1877, duration: 0.841s, episode steps: 171, steps per second: 203, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.023 [0.000, 2.000],  loss: 0.045422, mae: 27.461543, mean_q: -39.416576, mean_eps: 0.100000\n",
      " 266090/500000: episode: 1878, duration: 0.757s, episode steps: 157, steps per second: 207, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.854 [0.000, 2.000],  loss: 0.046909, mae: 27.473332, mean_q: -39.555029, mean_eps: 0.100000\n",
      " 266244/500000: episode: 1879, duration: 0.817s, episode steps: 154, steps per second: 189, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.063085, mae: 27.720456, mean_q: -40.108284, mean_eps: 0.100000\n",
      " 266340/500000: episode: 1880, duration: 0.472s, episode steps:  96, steps per second: 203, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.063062, mae: 27.772214, mean_q: -40.430574, mean_eps: 0.100000\n",
      " 266519/500000: episode: 1881, duration: 0.864s, episode steps: 179, steps per second: 207, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.104468, mae: 27.867840, mean_q: -40.466911, mean_eps: 0.100000\n",
      " 266665/500000: episode: 1882, duration: 0.682s, episode steps: 146, steps per second: 214, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.128004, mae: 28.554336, mean_q: -41.429702, mean_eps: 0.100000\n",
      " 266764/500000: episode: 1883, duration: 0.462s, episode steps:  99, steps per second: 214, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.080345, mae: 28.506937, mean_q: -41.121226, mean_eps: 0.100000\n",
      " 266913/500000: episode: 1884, duration: 0.707s, episode steps: 149, steps per second: 211, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 2.000],  loss: 0.130098, mae: 28.005432, mean_q: -40.117149, mean_eps: 0.100000\n",
      " 267028/500000: episode: 1885, duration: 0.545s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.426 [0.000, 2.000],  loss: 0.139855, mae: 27.470004, mean_q: -39.141500, mean_eps: 0.100000\n",
      " 267141/500000: episode: 1886, duration: 0.540s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.081657, mae: 27.536076, mean_q: -39.193672, mean_eps: 0.100000\n",
      " 267255/500000: episode: 1887, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.098637, mae: 26.558847, mean_q: -37.794235, mean_eps: 0.100000\n",
      " 267375/500000: episode: 1888, duration: 0.573s, episode steps: 120, steps per second: 210, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.076701, mae: 26.917536, mean_q: -38.481210, mean_eps: 0.100000\n",
      " 267464/500000: episode: 1889, duration: 0.418s, episode steps:  89, steps per second: 213, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.063556, mae: 25.773529, mean_q: -36.848960, mean_eps: 0.100000\n",
      " 267587/500000: episode: 1890, duration: 0.593s, episode steps: 123, steps per second: 207, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.049495, mae: 25.341201, mean_q: -36.269151, mean_eps: 0.100000\n",
      " 267769/500000: episode: 1891, duration: 0.868s, episode steps: 182, steps per second: 210, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.066239, mae: 25.338715, mean_q: -36.400934, mean_eps: 0.100000\n",
      " 267949/500000: episode: 1892, duration: 0.842s, episode steps: 180, steps per second: 214, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000],  loss: 0.088637, mae: 25.195940, mean_q: -36.162571, mean_eps: 0.100000\n",
      " 268131/500000: episode: 1893, duration: 0.861s, episode steps: 182, steps per second: 211, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.081224, mae: 25.652419, mean_q: -36.746258, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 268281/500000: episode: 1894, duration: 0.725s, episode steps: 150, steps per second: 207, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.065104, mae: 25.993252, mean_q: -37.121817, mean_eps: 0.100000\n",
      " 268466/500000: episode: 1895, duration: 0.914s, episode steps: 185, steps per second: 202, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.071953, mae: 26.658864, mean_q: -38.054716, mean_eps: 0.100000\n",
      " 268587/500000: episode: 1896, duration: 0.609s, episode steps: 121, steps per second: 199, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.085317, mae: 27.037552, mean_q: -38.598954, mean_eps: 0.100000\n",
      " 268787/500000: episode: 1897, duration: 0.950s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.062637, mae: 26.966449, mean_q: -38.511412, mean_eps: 0.100000\n",
      " 268965/500000: episode: 1898, duration: 0.855s, episode steps: 178, steps per second: 208, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.103459, mae: 27.319009, mean_q: -39.185876, mean_eps: 0.100000\n",
      " 269114/500000: episode: 1899, duration: 0.772s, episode steps: 149, steps per second: 193, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.099340, mae: 27.520682, mean_q: -39.529328, mean_eps: 0.100000\n",
      " 269229/500000: episode: 1900, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.077295, mae: 27.676439, mean_q: -39.618889, mean_eps: 0.100000\n",
      " 269352/500000: episode: 1901, duration: 0.612s, episode steps: 123, steps per second: 201, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.109014, mae: 27.180193, mean_q: -38.899933, mean_eps: 0.100000\n",
      " 269517/500000: episode: 1902, duration: 0.789s, episode steps: 165, steps per second: 209, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.080002, mae: 27.090310, mean_q: -38.888169, mean_eps: 0.100000\n",
      " 269619/500000: episode: 1903, duration: 0.485s, episode steps: 102, steps per second: 210, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.086498, mae: 27.217690, mean_q: -38.866799, mean_eps: 0.100000\n",
      " 269782/500000: episode: 1904, duration: 0.827s, episode steps: 163, steps per second: 197, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000],  loss: 0.094211, mae: 27.416384, mean_q: -38.895141, mean_eps: 0.100000\n",
      " 269899/500000: episode: 1905, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.065620, mae: 26.721243, mean_q: -38.083573, mean_eps: 0.100000\n",
      " 270088/500000: episode: 1906, duration: 0.890s, episode steps: 189, steps per second: 212, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.060819, mae: 26.841202, mean_q: -38.237421, mean_eps: 0.100000\n",
      " 270208/500000: episode: 1907, duration: 0.574s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.059540, mae: 26.197120, mean_q: -37.530378, mean_eps: 0.100000\n",
      " 270327/500000: episode: 1908, duration: 0.552s, episode steps: 119, steps per second: 216, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000],  loss: 0.061391, mae: 25.923444, mean_q: -37.119976, mean_eps: 0.100000\n",
      " 270445/500000: episode: 1909, duration: 0.550s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.051438, mae: 26.244582, mean_q: -37.703345, mean_eps: 0.100000\n",
      " 270599/500000: episode: 1910, duration: 0.756s, episode steps: 154, steps per second: 204, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.066647, mae: 26.789795, mean_q: -38.595625, mean_eps: 0.100000\n",
      " 270706/500000: episode: 1911, duration: 0.505s, episode steps: 107, steps per second: 212, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.053616, mae: 26.655135, mean_q: -38.695990, mean_eps: 0.100000\n",
      " 270819/500000: episode: 1912, duration: 0.527s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.051840, mae: 26.822574, mean_q: -39.109414, mean_eps: 0.100000\n",
      " 270991/500000: episode: 1913, duration: 0.823s, episode steps: 172, steps per second: 209, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.866 [0.000, 2.000],  loss: 0.079925, mae: 27.284384, mean_q: -39.761180, mean_eps: 0.100000\n",
      " 271123/500000: episode: 1914, duration: 0.638s, episode steps: 132, steps per second: 207, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.049618, mae: 27.325358, mean_q: -39.902902, mean_eps: 0.100000\n",
      " 271234/500000: episode: 1915, duration: 0.530s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.106209, mae: 28.270360, mean_q: -41.124388, mean_eps: 0.100000\n",
      " 271368/500000: episode: 1916, duration: 0.638s, episode steps: 134, steps per second: 210, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.478 [0.000, 2.000],  loss: 0.102374, mae: 28.025091, mean_q: -40.633420, mean_eps: 0.100000\n",
      " 271517/500000: episode: 1917, duration: 0.750s, episode steps: 149, steps per second: 199, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.083907, mae: 28.366419, mean_q: -40.921011, mean_eps: 0.100000\n",
      " 271640/500000: episode: 1918, duration: 0.598s, episode steps: 123, steps per second: 206, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000],  loss: 0.086016, mae: 28.282330, mean_q: -40.776075, mean_eps: 0.100000\n",
      " 271754/500000: episode: 1919, duration: 0.562s, episode steps: 114, steps per second: 203, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.120964, mae: 28.856333, mean_q: -41.294958, mean_eps: 0.100000\n",
      " 271877/500000: episode: 1920, duration: 0.606s, episode steps: 123, steps per second: 203, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.158978, mae: 28.761727, mean_q: -41.034751, mean_eps: 0.100000\n",
      " 272002/500000: episode: 1921, duration: 0.600s, episode steps: 125, steps per second: 208, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.068153, mae: 27.395844, mean_q: -38.819539, mean_eps: 0.100000\n",
      " 272128/500000: episode: 1922, duration: 0.611s, episode steps: 126, steps per second: 206, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.092712, mae: 27.067628, mean_q: -38.214988, mean_eps: 0.100000\n",
      " 272297/500000: episode: 1923, duration: 0.807s, episode steps: 169, steps per second: 209, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.828 [0.000, 2.000],  loss: 0.093759, mae: 27.570575, mean_q: -38.997635, mean_eps: 0.100000\n",
      " 272497/500000: episode: 1924, duration: 0.937s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 0.090361, mae: 27.702807, mean_q: -39.387411, mean_eps: 0.100000\n",
      " 272688/500000: episode: 1925, duration: 0.945s, episode steps: 191, steps per second: 202, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.475599, mae: 28.258964, mean_q: -39.898174, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 272801/500000: episode: 1926, duration: 0.540s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.451 [0.000, 2.000],  loss: 0.389621, mae: 27.885611, mean_q: -39.498217, mean_eps: 0.100000\n",
      " 272916/500000: episode: 1927, duration: 0.567s, episode steps: 115, steps per second: 203, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 0.369462, mae: 27.985178, mean_q: -39.866709, mean_eps: 0.100000\n",
      " 273021/500000: episode: 1928, duration: 0.502s, episode steps: 105, steps per second: 209, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.354935, mae: 27.793547, mean_q: -39.605788, mean_eps: 0.100000\n",
      " 273139/500000: episode: 1929, duration: 0.569s, episode steps: 118, steps per second: 207, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.307588, mae: 27.421326, mean_q: -39.187245, mean_eps: 0.100000\n",
      " 273324/500000: episode: 1930, duration: 0.903s, episode steps: 185, steps per second: 205, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.258068, mae: 27.393060, mean_q: -39.182762, mean_eps: 0.100000\n",
      " 273434/500000: episode: 1931, duration: 0.582s, episode steps: 110, steps per second: 189, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.293663, mae: 26.705302, mean_q: -38.099791, mean_eps: 0.100000\n",
      " 273565/500000: episode: 1932, duration: 0.623s, episode steps: 131, steps per second: 210, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 0.146320, mae: 26.448472, mean_q: -37.902115, mean_eps: 0.100000\n",
      " 273680/500000: episode: 1933, duration: 0.547s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.073044, mae: 26.239391, mean_q: -37.627663, mean_eps: 0.100000\n",
      " 273790/500000: episode: 1934, duration: 0.521s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.066195, mae: 26.173189, mean_q: -37.541236, mean_eps: 0.100000\n",
      " 273978/500000: episode: 1935, duration: 0.895s, episode steps: 188, steps per second: 210, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.963 [0.000, 2.000],  loss: 0.091767, mae: 26.360588, mean_q: -37.729113, mean_eps: 0.100000\n",
      " 274131/500000: episode: 1936, duration: 0.729s, episode steps: 153, steps per second: 210, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.143587, mae: 27.256806, mean_q: -39.269047, mean_eps: 0.100000\n",
      " 274288/500000: episode: 1937, duration: 0.745s, episode steps: 157, steps per second: 211, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.943 [0.000, 2.000],  loss: 0.183385, mae: 27.023757, mean_q: -38.842033, mean_eps: 0.100000\n",
      " 274400/500000: episode: 1938, duration: 0.535s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.185553, mae: 27.398740, mean_q: -39.492937, mean_eps: 0.100000\n",
      " 274584/500000: episode: 1939, duration: 0.929s, episode steps: 184, steps per second: 198, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.123037, mae: 27.550494, mean_q: -39.759605, mean_eps: 0.100000\n",
      " 274740/500000: episode: 1940, duration: 0.762s, episode steps: 156, steps per second: 205, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.103720, mae: 27.739305, mean_q: -39.893729, mean_eps: 0.100000\n",
      " 274846/500000: episode: 1941, duration: 0.531s, episode steps: 106, steps per second: 199, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.100115, mae: 27.834090, mean_q: -40.005996, mean_eps: 0.100000\n",
      " 274984/500000: episode: 1942, duration: 0.661s, episode steps: 138, steps per second: 209, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.478 [0.000, 2.000],  loss: 0.062266, mae: 27.255332, mean_q: -39.208376, mean_eps: 0.100000\n",
      " 275100/500000: episode: 1943, duration: 0.545s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.082463, mae: 26.803886, mean_q: -38.471830, mean_eps: 0.100000\n",
      " 275212/500000: episode: 1944, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000],  loss: 0.071140, mae: 26.529489, mean_q: -38.182849, mean_eps: 0.100000\n",
      " 275392/500000: episode: 1945, duration: 0.850s, episode steps: 180, steps per second: 212, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.062720, mae: 26.639585, mean_q: -38.366979, mean_eps: 0.100000\n",
      " 275512/500000: episode: 1946, duration: 0.587s, episode steps: 120, steps per second: 205, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.071690, mae: 26.535994, mean_q: -38.199782, mean_eps: 0.100000\n",
      " 275700/500000: episode: 1947, duration: 0.907s, episode steps: 188, steps per second: 207, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.102617, mae: 26.763419, mean_q: -38.620832, mean_eps: 0.100000\n",
      " 275856/500000: episode: 1948, duration: 0.739s, episode steps: 156, steps per second: 211, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.062628, mae: 27.350051, mean_q: -39.630492, mean_eps: 0.100000\n",
      " 275970/500000: episode: 1949, duration: 0.550s, episode steps: 114, steps per second: 207, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.056281, mae: 26.906547, mean_q: -39.009507, mean_eps: 0.100000\n",
      " 276151/500000: episode: 1950, duration: 0.866s, episode steps: 181, steps per second: 209, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.069027, mae: 27.964613, mean_q: -40.622939, mean_eps: 0.100000\n",
      " 276272/500000: episode: 1951, duration: 0.574s, episode steps: 121, steps per second: 211, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.103469, mae: 28.458357, mean_q: -41.243749, mean_eps: 0.100000\n",
      " 276364/500000: episode: 1952, duration: 0.444s, episode steps:  92, steps per second: 207, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.066386, mae: 26.593584, mean_q: -38.504309, mean_eps: 0.100000\n",
      " 276515/500000: episode: 1953, duration: 0.717s, episode steps: 151, steps per second: 211, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.088574, mae: 27.670282, mean_q: -39.832085, mean_eps: 0.100000\n",
      " 276618/500000: episode: 1954, duration: 0.481s, episode steps: 103, steps per second: 214, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.093790, mae: 27.612787, mean_q: -39.625555, mean_eps: 0.100000\n",
      " 276733/500000: episode: 1955, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.121101, mae: 28.121779, mean_q: -40.231516, mean_eps: 0.100000\n",
      " 276890/500000: episode: 1956, duration: 0.750s, episode steps: 157, steps per second: 209, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000],  loss: 0.089044, mae: 27.727056, mean_q: -39.671159, mean_eps: 0.100000\n",
      " 277090/500000: episode: 1957, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.208450, mae: 27.384381, mean_q: -38.753653, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 277254/500000: episode: 1958, duration: 0.799s, episode steps: 164, steps per second: 205, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.273308, mae: 27.804410, mean_q: -39.327648, mean_eps: 0.100000\n",
      " 277401/500000: episode: 1959, duration: 0.692s, episode steps: 147, steps per second: 212, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.878 [0.000, 2.000],  loss: 0.125822, mae: 28.717455, mean_q: -41.032780, mean_eps: 0.100000\n",
      " 277489/500000: episode: 1960, duration: 0.420s, episode steps:  88, steps per second: 210, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.150478, mae: 27.888541, mean_q: -40.019385, mean_eps: 0.100000\n",
      " 277648/500000: episode: 1961, duration: 0.812s, episode steps: 159, steps per second: 196, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.767 [0.000, 2.000],  loss: 0.145306, mae: 28.124647, mean_q: -40.509394, mean_eps: 0.100000\n",
      " 277797/500000: episode: 1962, duration: 0.712s, episode steps: 149, steps per second: 209, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.846 [0.000, 2.000],  loss: 0.182107, mae: 29.009782, mean_q: -41.914971, mean_eps: 0.100000\n",
      " 277911/500000: episode: 1963, duration: 0.575s, episode steps: 114, steps per second: 198, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.162081, mae: 28.875481, mean_q: -41.766924, mean_eps: 0.100000\n",
      " 277999/500000: episode: 1964, duration: 0.419s, episode steps:  88, steps per second: 210, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.114361, mae: 28.450549, mean_q: -41.240984, mean_eps: 0.100000\n",
      " 278110/500000: episode: 1965, duration: 0.531s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.146249, mae: 28.397131, mean_q: -41.253872, mean_eps: 0.100000\n",
      " 278227/500000: episode: 1966, duration: 0.552s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.049667, mae: 27.380637, mean_q: -39.889147, mean_eps: 0.100000\n",
      " 278379/500000: episode: 1967, duration: 0.709s, episode steps: 152, steps per second: 214, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.947 [0.000, 2.000],  loss: 0.051033, mae: 27.083617, mean_q: -39.325422, mean_eps: 0.100000\n",
      " 278494/500000: episode: 1968, duration: 0.546s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.054302, mae: 27.712574, mean_q: -40.321078, mean_eps: 0.100000\n",
      " 278607/500000: episode: 1969, duration: 0.524s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.045625, mae: 27.044131, mean_q: -39.341608, mean_eps: 0.100000\n",
      " 278727/500000: episode: 1970, duration: 0.553s, episode steps: 120, steps per second: 217, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.367 [0.000, 2.000],  loss: 0.048445, mae: 26.210421, mean_q: -37.941560, mean_eps: 0.100000\n",
      " 278927/500000: episode: 1971, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.065131, mae: 26.655717, mean_q: -38.382122, mean_eps: 0.100000\n",
      " 279040/500000: episode: 1972, duration: 0.536s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.106171, mae: 27.328446, mean_q: -39.377391, mean_eps: 0.100000\n",
      " 279240/500000: episode: 1973, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000],  loss: 0.161717, mae: 27.301188, mean_q: -39.320986, mean_eps: 0.100000\n",
      " 279434/500000: episode: 1974, duration: 0.922s, episode steps: 194, steps per second: 210, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000],  loss: 0.144754, mae: 27.219497, mean_q: -39.081929, mean_eps: 0.100000\n",
      " 279597/500000: episode: 1975, duration: 0.755s, episode steps: 163, steps per second: 216, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.172751, mae: 27.482011, mean_q: -39.186575, mean_eps: 0.100000\n",
      " 279719/500000: episode: 1976, duration: 0.573s, episode steps: 122, steps per second: 213, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.492 [0.000, 2.000],  loss: 0.145514, mae: 27.407474, mean_q: -39.147225, mean_eps: 0.100000\n",
      " 279837/500000: episode: 1977, duration: 0.559s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.441 [0.000, 2.000],  loss: 0.093261, mae: 27.289114, mean_q: -39.234451, mean_eps: 0.100000\n",
      " 279956/500000: episode: 1978, duration: 0.557s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.126123, mae: 27.115206, mean_q: -38.827337, mean_eps: 0.100000\n",
      " 280117/500000: episode: 1979, duration: 0.743s, episode steps: 161, steps per second: 217, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.178125, mae: 27.956947, mean_q: -39.846882, mean_eps: 0.100000\n",
      " 280282/500000: episode: 1980, duration: 0.783s, episode steps: 165, steps per second: 211, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.129697, mae: 27.978484, mean_q: -40.080174, mean_eps: 0.100000\n",
      " 280405/500000: episode: 1981, duration: 0.567s, episode steps: 123, steps per second: 217, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.119671, mae: 27.995392, mean_q: -40.122369, mean_eps: 0.100000\n",
      " 280593/500000: episode: 1982, duration: 0.933s, episode steps: 188, steps per second: 202, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.104460, mae: 27.986569, mean_q: -40.138684, mean_eps: 0.100000\n",
      " 280742/500000: episode: 1983, duration: 0.704s, episode steps: 149, steps per second: 212, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.146630, mae: 28.016574, mean_q: -40.043519, mean_eps: 0.100000\n",
      " 280899/500000: episode: 1984, duration: 0.723s, episode steps: 157, steps per second: 217, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.096287, mae: 28.321179, mean_q: -40.380841, mean_eps: 0.100000\n",
      " 280990/500000: episode: 1985, duration: 0.427s, episode steps:  91, steps per second: 213, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.130354, mae: 28.021690, mean_q: -40.083287, mean_eps: 0.100000\n",
      " 281154/500000: episode: 1986, duration: 0.775s, episode steps: 164, steps per second: 212, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.139683, mae: 27.965704, mean_q: -39.914078, mean_eps: 0.100000\n",
      " 281274/500000: episode: 1987, duration: 0.557s, episode steps: 120, steps per second: 215, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.101215, mae: 27.621473, mean_q: -39.392651, mean_eps: 0.100000\n",
      " 281445/500000: episode: 1988, duration: 0.808s, episode steps: 171, steps per second: 212, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.126068, mae: 27.931236, mean_q: -39.605678, mean_eps: 0.100000\n",
      " 281554/500000: episode: 1989, duration: 0.530s, episode steps: 109, steps per second: 206, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.130345, mae: 28.098590, mean_q: -39.877248, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 281737/500000: episode: 1990, duration: 0.891s, episode steps: 183, steps per second: 205, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000],  loss: 0.106416, mae: 28.919009, mean_q: -40.944422, mean_eps: 0.100000\n",
      " 281867/500000: episode: 1991, duration: 0.629s, episode steps: 130, steps per second: 207, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.097197, mae: 28.966717, mean_q: -41.239092, mean_eps: 0.100000\n",
      " 281982/500000: episode: 1992, duration: 0.565s, episode steps: 115, steps per second: 203, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 0.139867, mae: 29.650627, mean_q: -42.020748, mean_eps: 0.100000\n",
      " 282146/500000: episode: 1993, duration: 0.808s, episode steps: 164, steps per second: 203, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.092729, mae: 28.926250, mean_q: -41.001617, mean_eps: 0.100000\n",
      " 282236/500000: episode: 1994, duration: 0.438s, episode steps:  90, steps per second: 206, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.100131, mae: 28.737666, mean_q: -40.558872, mean_eps: 0.100000\n",
      " 282354/500000: episode: 1995, duration: 0.585s, episode steps: 118, steps per second: 202, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.066491, mae: 28.250285, mean_q: -39.911504, mean_eps: 0.100000\n",
      " 282464/500000: episode: 1996, duration: 0.526s, episode steps: 110, steps per second: 209, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.436 [0.000, 2.000],  loss: 0.142329, mae: 28.091515, mean_q: -39.705320, mean_eps: 0.100000\n",
      " 282625/500000: episode: 1997, duration: 0.791s, episode steps: 161, steps per second: 204, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.057495, mae: 28.028715, mean_q: -39.707137, mean_eps: 0.100000\n",
      " 282779/500000: episode: 1998, duration: 0.725s, episode steps: 154, steps per second: 212, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.060824, mae: 27.558038, mean_q: -38.998898, mean_eps: 0.100000\n",
      " 282890/500000: episode: 1999, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.060840, mae: 27.541388, mean_q: -39.119573, mean_eps: 0.100000\n",
      " 283008/500000: episode: 2000, duration: 0.549s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.048844, mae: 26.857889, mean_q: -38.192642, mean_eps: 0.100000\n",
      " 283121/500000: episode: 2001, duration: 0.594s, episode steps: 113, steps per second: 190, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.046614, mae: 26.687598, mean_q: -38.072321, mean_eps: 0.100000\n",
      " 283252/500000: episode: 2002, duration: 0.647s, episode steps: 131, steps per second: 203, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.042940, mae: 27.394518, mean_q: -39.156687, mean_eps: 0.100000\n",
      " 283358/500000: episode: 2003, duration: 0.501s, episode steps: 106, steps per second: 212, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.109932, mae: 27.206663, mean_q: -38.563164, mean_eps: 0.100000\n",
      " 283532/500000: episode: 2004, duration: 0.832s, episode steps: 174, steps per second: 209, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.115643, mae: 27.560671, mean_q: -39.040654, mean_eps: 0.100000\n",
      " 283657/500000: episode: 2005, duration: 0.597s, episode steps: 125, steps per second: 209, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.085468, mae: 26.907297, mean_q: -38.248841, mean_eps: 0.100000\n",
      " 283751/500000: episode: 2006, duration: 0.439s, episode steps:  94, steps per second: 214, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.143903, mae: 25.950759, mean_q: -36.707023, mean_eps: 0.100000\n",
      " 283869/500000: episode: 2007, duration: 0.565s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.090782, mae: 25.843384, mean_q: -36.362245, mean_eps: 0.100000\n",
      " 283983/500000: episode: 2008, duration: 0.569s, episode steps: 114, steps per second: 200, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.079365, mae: 25.899379, mean_q: -36.624660, mean_eps: 0.100000\n",
      " 284154/500000: episode: 2009, duration: 0.858s, episode steps: 171, steps per second: 199, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.140500, mae: 26.305730, mean_q: -36.950094, mean_eps: 0.100000\n",
      " 284307/500000: episode: 2010, duration: 0.734s, episode steps: 153, steps per second: 209, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.128171, mae: 26.906179, mean_q: -37.994203, mean_eps: 0.100000\n",
      " 284417/500000: episode: 2011, duration: 0.512s, episode steps: 110, steps per second: 215, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.091516, mae: 27.228943, mean_q: -38.900715, mean_eps: 0.100000\n",
      " 284590/500000: episode: 2012, duration: 0.882s, episode steps: 173, steps per second: 196, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000],  loss: 0.112697, mae: 26.816948, mean_q: -38.097596, mean_eps: 0.100000\n",
      " 284790/500000: episode: 2013, duration: 0.961s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.145425, mae: 27.421173, mean_q: -39.126496, mean_eps: 0.100000\n",
      " 284956/500000: episode: 2014, duration: 0.779s, episode steps: 166, steps per second: 213, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.861 [0.000, 2.000],  loss: 0.100309, mae: 28.426580, mean_q: -40.780271, mean_eps: 0.100000\n",
      " 285072/500000: episode: 2015, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.086341, mae: 28.304200, mean_q: -40.715504, mean_eps: 0.100000\n",
      " 285187/500000: episode: 2016, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.071548, mae: 27.898508, mean_q: -40.201366, mean_eps: 0.100000\n",
      " 285315/500000: episode: 2017, duration: 0.611s, episode steps: 128, steps per second: 209, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.075715, mae: 27.313524, mean_q: -39.247995, mean_eps: 0.100000\n",
      " 285505/500000: episode: 2018, duration: 0.926s, episode steps: 190, steps per second: 205, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000],  loss: 0.108874, mae: 27.744583, mean_q: -39.720626, mean_eps: 0.100000\n",
      " 285607/500000: episode: 2019, duration: 0.494s, episode steps: 102, steps per second: 206, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.412 [0.000, 2.000],  loss: 0.148648, mae: 27.887203, mean_q: -40.050793, mean_eps: 0.100000\n",
      " 285776/500000: episode: 2020, duration: 0.818s, episode steps: 169, steps per second: 207, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.149876, mae: 27.427327, mean_q: -38.987020, mean_eps: 0.100000\n",
      " 285944/500000: episode: 2021, duration: 0.825s, episode steps: 168, steps per second: 204, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.121931, mae: 27.324744, mean_q: -38.545158, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 286062/500000: episode: 2022, duration: 0.585s, episode steps: 118, steps per second: 202, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.109148, mae: 27.180577, mean_q: -38.517016, mean_eps: 0.100000\n",
      " 286227/500000: episode: 2023, duration: 0.792s, episode steps: 165, steps per second: 208, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.111841, mae: 28.199617, mean_q: -39.800149, mean_eps: 0.100000\n",
      " 286346/500000: episode: 2024, duration: 0.572s, episode steps: 119, steps per second: 208, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.131453, mae: 28.045699, mean_q: -39.659303, mean_eps: 0.100000\n",
      " 286458/500000: episode: 2025, duration: 0.534s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.112501, mae: 27.408603, mean_q: -38.733389, mean_eps: 0.100000\n",
      " 286573/500000: episode: 2026, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 0.098032, mae: 27.625508, mean_q: -38.951152, mean_eps: 0.100000\n",
      " 286692/500000: episode: 2027, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.082588, mae: 27.593882, mean_q: -39.060599, mean_eps: 0.100000\n",
      " 286892/500000: episode: 2028, duration: 0.950s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.093799, mae: 27.158450, mean_q: -38.406856, mean_eps: 0.100000\n",
      " 286988/500000: episode: 2029, duration: 0.450s, episode steps:  96, steps per second: 213, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.597692, mae: 27.475333, mean_q: -38.768962, mean_eps: 0.100000\n",
      " 287101/500000: episode: 2030, duration: 0.527s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.423556, mae: 27.367583, mean_q: -38.585953, mean_eps: 0.100000\n",
      " 287190/500000: episode: 2031, duration: 0.424s, episode steps:  89, steps per second: 210, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000],  loss: 0.155699, mae: 26.317047, mean_q: -37.219514, mean_eps: 0.100000\n",
      " 287324/500000: episode: 2032, duration: 0.648s, episode steps: 134, steps per second: 207, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 0.293634, mae: 26.329718, mean_q: -37.158865, mean_eps: 0.100000\n",
      " 287434/500000: episode: 2033, duration: 0.543s, episode steps: 110, steps per second: 203, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.375426, mae: 26.515734, mean_q: -37.319371, mean_eps: 0.100000\n",
      " 287634/500000: episode: 2034, duration: 0.977s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000],  loss: 0.165697, mae: 26.341895, mean_q: -37.126587, mean_eps: 0.100000\n",
      " 287752/500000: episode: 2035, duration: 0.567s, episode steps: 118, steps per second: 208, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.130349, mae: 26.537076, mean_q: -37.814757, mean_eps: 0.100000\n",
      " 287837/500000: episode: 2036, duration: 0.422s, episode steps:  85, steps per second: 202, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.110299, mae: 25.917937, mean_q: -37.010780, mean_eps: 0.100000\n",
      " 288015/500000: episode: 2037, duration: 0.850s, episode steps: 178, steps per second: 209, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.080434, mae: 26.165433, mean_q: -37.277517, mean_eps: 0.100000\n",
      " 288131/500000: episode: 2038, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.101025, mae: 25.897896, mean_q: -36.990656, mean_eps: 0.100000\n",
      " 288242/500000: episode: 2039, duration: 0.548s, episode steps: 111, steps per second: 203, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.118519, mae: 26.524373, mean_q: -37.980973, mean_eps: 0.100000\n",
      " 288342/500000: episode: 2040, duration: 0.493s, episode steps: 100, steps per second: 203, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.083662, mae: 26.177841, mean_q: -37.535519, mean_eps: 0.100000\n",
      " 288505/500000: episode: 2041, duration: 0.773s, episode steps: 163, steps per second: 211, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.099883, mae: 26.871154, mean_q: -38.358854, mean_eps: 0.100000\n",
      " 288620/500000: episode: 2042, duration: 0.544s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.087254, mae: 26.510864, mean_q: -38.202119, mean_eps: 0.100000\n",
      " 288712/500000: episode: 2043, duration: 0.430s, episode steps:  92, steps per second: 214, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.049674, mae: 26.675053, mean_q: -38.505544, mean_eps: 0.100000\n",
      " 288867/500000: episode: 2044, duration: 0.723s, episode steps: 155, steps per second: 214, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.059940, mae: 26.748786, mean_q: -38.549980, mean_eps: 0.100000\n",
      " 288991/500000: episode: 2045, duration: 0.593s, episode steps: 124, steps per second: 209, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.059615, mae: 26.249240, mean_q: -38.034539, mean_eps: 0.100000\n",
      " 289083/500000: episode: 2046, duration: 0.429s, episode steps:  92, steps per second: 215, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.058416, mae: 25.890668, mean_q: -37.384528, mean_eps: 0.100000\n",
      " 289195/500000: episode: 2047, duration: 0.522s, episode steps: 112, steps per second: 215, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.054269, mae: 26.106158, mean_q: -37.654432, mean_eps: 0.100000\n",
      " 289365/500000: episode: 2048, duration: 0.812s, episode steps: 170, steps per second: 209, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.095076, mae: 27.281169, mean_q: -39.321495, mean_eps: 0.100000\n",
      " 289479/500000: episode: 2049, duration: 0.529s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.087573, mae: 26.303052, mean_q: -38.014136, mean_eps: 0.100000\n",
      " 289579/500000: episode: 2050, duration: 0.462s, episode steps: 100, steps per second: 216, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.067387, mae: 25.796329, mean_q: -37.101393, mean_eps: 0.100000\n",
      " 289695/500000: episode: 2051, duration: 0.550s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.121 [0.000, 2.000],  loss: 0.089644, mae: 26.308759, mean_q: -37.706967, mean_eps: 0.100000\n",
      " 289892/500000: episode: 2052, duration: 0.922s, episode steps: 197, steps per second: 214, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.097349, mae: 26.712994, mean_q: -38.300454, mean_eps: 0.100000\n",
      " 290005/500000: episode: 2053, duration: 0.524s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.145173, mae: 26.436909, mean_q: -37.737722, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 290114/500000: episode: 2054, duration: 0.513s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.164505, mae: 26.604448, mean_q: -38.049740, mean_eps: 0.100000\n",
      " 290207/500000: episode: 2055, duration: 0.448s, episode steps:  93, steps per second: 208, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.129828, mae: 26.477218, mean_q: -37.784975, mean_eps: 0.100000\n",
      " 290325/500000: episode: 2056, duration: 0.581s, episode steps: 118, steps per second: 203, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.110011, mae: 25.938822, mean_q: -37.065587, mean_eps: 0.100000\n",
      " 290438/500000: episode: 2057, duration: 0.528s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.055849, mae: 25.777667, mean_q: -36.751908, mean_eps: 0.100000\n",
      " 290559/500000: episode: 2058, duration: 0.569s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.049244, mae: 25.846383, mean_q: -36.800134, mean_eps: 0.100000\n",
      " 290674/500000: episode: 2059, duration: 0.549s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.034675, mae: 25.732821, mean_q: -36.808286, mean_eps: 0.100000\n",
      " 290838/500000: episode: 2060, duration: 0.766s, episode steps: 164, steps per second: 214, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.045363, mae: 25.787069, mean_q: -36.669826, mean_eps: 0.100000\n",
      " 291008/500000: episode: 2061, duration: 0.795s, episode steps: 170, steps per second: 214, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.055995, mae: 26.184179, mean_q: -37.018339, mean_eps: 0.100000\n",
      " 291151/500000: episode: 2062, duration: 0.666s, episode steps: 143, steps per second: 215, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.371 [0.000, 2.000],  loss: 0.121959, mae: 26.484991, mean_q: -37.230313, mean_eps: 0.100000\n",
      " 291315/500000: episode: 2063, duration: 0.757s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.939 [0.000, 2.000],  loss: 0.198215, mae: 27.110486, mean_q: -37.955974, mean_eps: 0.100000\n",
      " 291502/500000: episode: 2064, duration: 0.888s, episode steps: 187, steps per second: 211, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.147286, mae: 27.568515, mean_q: -38.966243, mean_eps: 0.100000\n",
      " 291702/500000: episode: 2065, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.153793, mae: 27.759865, mean_q: -39.305064, mean_eps: 0.100000\n",
      " 291902/500000: episode: 2066, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.156423, mae: 28.384414, mean_q: -40.406718, mean_eps: 0.100000\n",
      " 292101/500000: episode: 2067, duration: 0.935s, episode steps: 199, steps per second: 213, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.254223, mae: 28.697815, mean_q: -40.715701, mean_eps: 0.100000\n",
      " 292229/500000: episode: 2068, duration: 0.635s, episode steps: 128, steps per second: 202, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.367 [0.000, 2.000],  loss: 0.241554, mae: 28.771627, mean_q: -41.036469, mean_eps: 0.100000\n",
      " 292351/500000: episode: 2069, duration: 0.594s, episode steps: 122, steps per second: 205, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 0.123742, mae: 28.882170, mean_q: -41.307916, mean_eps: 0.100000\n",
      " 292547/500000: episode: 2070, duration: 0.924s, episode steps: 196, steps per second: 212, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.167605, mae: 28.705463, mean_q: -40.856547, mean_eps: 0.100000\n",
      " 292639/500000: episode: 2071, duration: 0.439s, episode steps:  92, steps per second: 210, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.188453, mae: 28.097584, mean_q: -39.967767, mean_eps: 0.100000\n",
      " 292746/500000: episode: 2072, duration: 0.517s, episode steps: 107, steps per second: 207, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.137406, mae: 28.203692, mean_q: -40.135778, mean_eps: 0.100000\n",
      " 292858/500000: episode: 2073, duration: 0.580s, episode steps: 112, steps per second: 193, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.110101, mae: 27.436247, mean_q: -39.125269, mean_eps: 0.100000\n",
      " 292980/500000: episode: 2074, duration: 0.605s, episode steps: 122, steps per second: 202, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.073793, mae: 27.021092, mean_q: -38.448447, mean_eps: 0.100000\n",
      " 293143/500000: episode: 2075, duration: 0.806s, episode steps: 163, steps per second: 202, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.051008, mae: 26.988844, mean_q: -38.458479, mean_eps: 0.100000\n",
      " 293295/500000: episode: 2076, duration: 0.730s, episode steps: 152, steps per second: 208, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.901 [0.000, 2.000],  loss: 0.069457, mae: 27.222026, mean_q: -38.649785, mean_eps: 0.100000\n",
      " 293415/500000: episode: 2077, duration: 0.575s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000],  loss: 0.093297, mae: 26.933114, mean_q: -38.284731, mean_eps: 0.100000\n",
      " 293570/500000: episode: 2078, duration: 0.784s, episode steps: 155, steps per second: 198, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.086056, mae: 26.760226, mean_q: -38.005841, mean_eps: 0.100000\n",
      " 293707/500000: episode: 2079, duration: 0.674s, episode steps: 137, steps per second: 203, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.073207, mae: 27.241375, mean_q: -38.960933, mean_eps: 0.100000\n",
      " 293864/500000: episode: 2080, duration: 0.765s, episode steps: 157, steps per second: 205, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.090552, mae: 27.862441, mean_q: -39.620838, mean_eps: 0.100000\n",
      " 293999/500000: episode: 2081, duration: 0.648s, episode steps: 135, steps per second: 208, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.102068, mae: 27.802038, mean_q: -39.650637, mean_eps: 0.100000\n",
      " 294112/500000: episode: 2082, duration: 0.529s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000],  loss: 0.212719, mae: 28.555114, mean_q: -40.730697, mean_eps: 0.100000\n",
      " 294274/500000: episode: 2083, duration: 0.803s, episode steps: 162, steps per second: 202, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.115343, mae: 28.220058, mean_q: -40.257771, mean_eps: 0.100000\n",
      " 294445/500000: episode: 2084, duration: 0.811s, episode steps: 171, steps per second: 211, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.115112, mae: 28.334577, mean_q: -40.336047, mean_eps: 0.100000\n",
      " 294591/500000: episode: 2085, duration: 0.673s, episode steps: 146, steps per second: 217, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000],  loss: 0.100540, mae: 28.642858, mean_q: -40.924351, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 294727/500000: episode: 2086, duration: 0.633s, episode steps: 136, steps per second: 215, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.456 [0.000, 2.000],  loss: 0.083356, mae: 28.033136, mean_q: -39.995441, mean_eps: 0.100000\n",
      " 294840/500000: episode: 2087, duration: 0.533s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.084255, mae: 27.597745, mean_q: -39.388060, mean_eps: 0.100000\n",
      " 294954/500000: episode: 2088, duration: 0.527s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.088804, mae: 27.539570, mean_q: -39.299156, mean_eps: 0.100000\n",
      " 295070/500000: episode: 2089, duration: 0.535s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.064735, mae: 27.061273, mean_q: -38.901540, mean_eps: 0.100000\n",
      " 295187/500000: episode: 2090, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.069301, mae: 27.306044, mean_q: -39.314280, mean_eps: 0.100000\n",
      " 295295/500000: episode: 2091, duration: 0.558s, episode steps: 108, steps per second: 194, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.058073, mae: 26.434169, mean_q: -37.987864, mean_eps: 0.100000\n",
      " 295405/500000: episode: 2092, duration: 0.510s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000],  loss: 0.033501, mae: 25.934857, mean_q: -37.452653, mean_eps: 0.100000\n",
      " 295533/500000: episode: 2093, duration: 0.606s, episode steps: 128, steps per second: 211, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 0.049844, mae: 25.665503, mean_q: -37.101546, mean_eps: 0.100000\n",
      " 295662/500000: episode: 2094, duration: 0.619s, episode steps: 129, steps per second: 208, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.473 [0.000, 2.000],  loss: 0.065802, mae: 25.886527, mean_q: -37.236354, mean_eps: 0.100000\n",
      " 295786/500000: episode: 2095, duration: 0.582s, episode steps: 124, steps per second: 213, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.419 [0.000, 2.000],  loss: 0.048599, mae: 25.791517, mean_q: -37.326058, mean_eps: 0.100000\n",
      " 295943/500000: episode: 2096, duration: 0.720s, episode steps: 157, steps per second: 218, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.847 [0.000, 2.000],  loss: 0.069872, mae: 26.163849, mean_q: -37.725647, mean_eps: 0.100000\n",
      " 296120/500000: episode: 2097, duration: 0.865s, episode steps: 177, steps per second: 205, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.078283, mae: 26.714583, mean_q: -38.342193, mean_eps: 0.100000\n",
      " 296235/500000: episode: 2098, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.076356, mae: 26.733771, mean_q: -38.443698, mean_eps: 0.100000\n",
      " 296339/500000: episode: 2099, duration: 0.477s, episode steps: 104, steps per second: 218, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.093466, mae: 26.987836, mean_q: -38.610639, mean_eps: 0.100000\n",
      " 296537/500000: episode: 2100, duration: 0.923s, episode steps: 198, steps per second: 215, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.144608, mae: 27.451250, mean_q: -38.980965, mean_eps: 0.100000\n",
      " 296650/500000: episode: 2101, duration: 0.517s, episode steps: 113, steps per second: 218, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.079587, mae: 27.258533, mean_q: -38.843394, mean_eps: 0.100000\n",
      " 296840/500000: episode: 2102, duration: 0.977s, episode steps: 190, steps per second: 194, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.121466, mae: 27.206534, mean_q: -38.808575, mean_eps: 0.100000\n",
      " 296949/500000: episode: 2103, duration: 0.557s, episode steps: 109, steps per second: 196, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.128535, mae: 26.682034, mean_q: -38.065870, mean_eps: 0.100000\n",
      " 297074/500000: episode: 2104, duration: 0.585s, episode steps: 125, steps per second: 214, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.384 [0.000, 2.000],  loss: 0.076424, mae: 26.322257, mean_q: -37.555283, mean_eps: 0.100000\n",
      " 297274/500000: episode: 2105, duration: 0.996s, episode steps: 200, steps per second: 201, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.108045, mae: 26.815694, mean_q: -38.038226, mean_eps: 0.100000\n",
      " 297450/500000: episode: 2106, duration: 0.887s, episode steps: 176, steps per second: 198, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.898 [0.000, 2.000],  loss: 0.135756, mae: 27.645228, mean_q: -39.516071, mean_eps: 0.100000\n",
      " 297626/500000: episode: 2107, duration: 0.857s, episode steps: 176, steps per second: 205, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.139639, mae: 27.938753, mean_q: -39.968451, mean_eps: 0.100000\n",
      " 297738/500000: episode: 2108, duration: 0.538s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.123442, mae: 27.955639, mean_q: -39.988239, mean_eps: 0.100000\n",
      " 297851/500000: episode: 2109, duration: 0.533s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.094231, mae: 28.065625, mean_q: -40.426690, mean_eps: 0.100000\n",
      " 297970/500000: episode: 2110, duration: 0.577s, episode steps: 119, steps per second: 206, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.080693, mae: 27.848787, mean_q: -40.016993, mean_eps: 0.100000\n",
      " 298138/500000: episode: 2111, duration: 0.845s, episode steps: 168, steps per second: 199, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.098829, mae: 28.087583, mean_q: -40.458938, mean_eps: 0.100000\n",
      " 298255/500000: episode: 2112, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.089066, mae: 27.736412, mean_q: -40.038615, mean_eps: 0.100000\n",
      " 298375/500000: episode: 2113, duration: 0.557s, episode steps: 120, steps per second: 215, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.094206, mae: 27.520992, mean_q: -39.537379, mean_eps: 0.100000\n",
      " 298481/500000: episode: 2114, duration: 0.507s, episode steps: 106, steps per second: 209, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.085698, mae: 26.988627, mean_q: -38.646752, mean_eps: 0.100000\n",
      " 298595/500000: episode: 2115, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.076574, mae: 26.398553, mean_q: -37.803234, mean_eps: 0.100000\n",
      " 298692/500000: episode: 2116, duration: 0.456s, episode steps:  97, steps per second: 213, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.073839, mae: 26.147264, mean_q: -37.247088, mean_eps: 0.100000\n",
      " 298844/500000: episode: 2117, duration: 0.704s, episode steps: 152, steps per second: 216, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.941 [0.000, 2.000],  loss: 0.072623, mae: 27.206552, mean_q: -38.788945, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 299002/500000: episode: 2118, duration: 0.753s, episode steps: 158, steps per second: 210, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.091900, mae: 27.013892, mean_q: -38.494046, mean_eps: 0.100000\n",
      " 299120/500000: episode: 2119, duration: 0.547s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 0.060143, mae: 26.923277, mean_q: -38.595279, mean_eps: 0.100000\n",
      " 299223/500000: episode: 2120, duration: 0.484s, episode steps: 103, steps per second: 213, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.057114, mae: 26.879422, mean_q: -38.459738, mean_eps: 0.100000\n",
      " 299336/500000: episode: 2121, duration: 0.538s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.078424, mae: 26.846641, mean_q: -38.304154, mean_eps: 0.100000\n",
      " 299451/500000: episode: 2122, duration: 0.549s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 0.054511, mae: 26.810117, mean_q: -38.383999, mean_eps: 0.100000\n",
      " 299561/500000: episode: 2123, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.050703, mae: 26.929981, mean_q: -38.605342, mean_eps: 0.100000\n",
      " 299670/500000: episode: 2124, duration: 0.511s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.037684, mae: 27.164186, mean_q: -39.119468, mean_eps: 0.100000\n",
      " 299790/500000: episode: 2125, duration: 0.577s, episode steps: 120, steps per second: 208, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.053589, mae: 26.455767, mean_q: -38.024532, mean_eps: 0.100000\n",
      " 299899/500000: episode: 2126, duration: 0.520s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.044447, mae: 26.203548, mean_q: -37.555862, mean_eps: 0.100000\n",
      " 300051/500000: episode: 2127, duration: 0.764s, episode steps: 152, steps per second: 199, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.816 [0.000, 2.000],  loss: 0.066345, mae: 26.404797, mean_q: -37.814390, mean_eps: 0.100000\n",
      " 300209/500000: episode: 2128, duration: 0.783s, episode steps: 158, steps per second: 202, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.791 [0.000, 2.000],  loss: 0.046193, mae: 26.975608, mean_q: -38.774135, mean_eps: 0.100000\n",
      " 300400/500000: episode: 2129, duration: 0.890s, episode steps: 191, steps per second: 215, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.086035, mae: 27.871070, mean_q: -40.161611, mean_eps: 0.100000\n",
      " 300499/500000: episode: 2130, duration: 0.462s, episode steps:  99, steps per second: 214, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.083173, mae: 28.149978, mean_q: -40.464471, mean_eps: 0.100000\n",
      " 300636/500000: episode: 2131, duration: 0.649s, episode steps: 137, steps per second: 211, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000],  loss: 0.104596, mae: 27.487225, mean_q: -39.264525, mean_eps: 0.100000\n",
      " 300817/500000: episode: 2132, duration: 0.841s, episode steps: 181, steps per second: 215, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.085003, mae: 28.030911, mean_q: -40.026453, mean_eps: 0.100000\n",
      " 300914/500000: episode: 2133, duration: 0.448s, episode steps:  97, steps per second: 216, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.107339, mae: 27.641467, mean_q: -39.402725, mean_eps: 0.100000\n",
      " 301030/500000: episode: 2134, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.127479, mae: 27.755479, mean_q: -39.472467, mean_eps: 0.100000\n",
      " 301120/500000: episode: 2135, duration: 0.428s, episode steps:  90, steps per second: 210, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 0.109905, mae: 27.133463, mean_q: -38.563020, mean_eps: 0.100000\n",
      " 301239/500000: episode: 2136, duration: 0.563s, episode steps: 119, steps per second: 212, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.079100, mae: 26.861723, mean_q: -38.123990, mean_eps: 0.100000\n",
      " 301347/500000: episode: 2137, duration: 0.506s, episode steps: 108, steps per second: 214, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.076711, mae: 26.017257, mean_q: -37.038453, mean_eps: 0.100000\n",
      " 301445/500000: episode: 2138, duration: 0.466s, episode steps:  98, steps per second: 210, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.062395, mae: 26.146096, mean_q: -37.281982, mean_eps: 0.100000\n",
      " 301555/500000: episode: 2139, duration: 0.522s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.218 [0.000, 2.000],  loss: 0.077418, mae: 26.180831, mean_q: -37.272865, mean_eps: 0.100000\n",
      " 301670/500000: episode: 2140, duration: 0.541s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.066999, mae: 26.372135, mean_q: -38.043303, mean_eps: 0.100000\n",
      " 301837/500000: episode: 2141, duration: 0.775s, episode steps: 167, steps per second: 215, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.075077, mae: 25.428679, mean_q: -36.617531, mean_eps: 0.100000\n",
      " 301992/500000: episode: 2142, duration: 0.732s, episode steps: 155, steps per second: 212, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.094407, mae: 26.053941, mean_q: -37.511930, mean_eps: 0.100000\n",
      " 302101/500000: episode: 2143, duration: 0.511s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.070841, mae: 26.280657, mean_q: -37.917383, mean_eps: 0.100000\n",
      " 302220/500000: episode: 2144, duration: 0.561s, episode steps: 119, steps per second: 212, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.116950, mae: 26.670421, mean_q: -38.471261, mean_eps: 0.100000\n",
      " 302334/500000: episode: 2145, duration: 0.536s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.052797, mae: 26.371561, mean_q: -38.225536, mean_eps: 0.100000\n",
      " 302482/500000: episode: 2146, duration: 0.697s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.057766, mae: 27.028887, mean_q: -39.294170, mean_eps: 0.100000\n",
      " 302593/500000: episode: 2147, duration: 0.530s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.075403, mae: 27.204621, mean_q: -39.625170, mean_eps: 0.100000\n",
      " 302741/500000: episode: 2148, duration: 0.720s, episode steps: 148, steps per second: 206, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.124161, mae: 27.802542, mean_q: -40.267251, mean_eps: 0.100000\n",
      " 302864/500000: episode: 2149, duration: 0.590s, episode steps: 123, steps per second: 209, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.084593, mae: 27.723497, mean_q: -40.219658, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 303034/500000: episode: 2150, duration: 0.799s, episode steps: 170, steps per second: 213, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.106651, mae: 27.530018, mean_q: -39.504113, mean_eps: 0.100000\n",
      " 303126/500000: episode: 2151, duration: 0.436s, episode steps:  92, steps per second: 211, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.106212, mae: 27.570429, mean_q: -39.402956, mean_eps: 0.100000\n",
      " 303257/500000: episode: 2152, duration: 0.624s, episode steps: 131, steps per second: 210, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.112701, mae: 27.724675, mean_q: -39.448845, mean_eps: 0.100000\n",
      " 303380/500000: episode: 2153, duration: 0.575s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000],  loss: 0.124699, mae: 27.583311, mean_q: -38.958122, mean_eps: 0.100000\n",
      " 303552/500000: episode: 2154, duration: 0.810s, episode steps: 172, steps per second: 212, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.165892, mae: 27.354417, mean_q: -38.622147, mean_eps: 0.100000\n",
      " 303642/500000: episode: 2155, duration: 0.425s, episode steps:  90, steps per second: 212, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.101991, mae: 27.319883, mean_q: -38.388026, mean_eps: 0.100000\n",
      " 303753/500000: episode: 2156, duration: 0.521s, episode steps: 111, steps per second: 213, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.087808, mae: 26.838296, mean_q: -37.881636, mean_eps: 0.100000\n",
      " 303902/500000: episode: 2157, duration: 0.690s, episode steps: 149, steps per second: 216, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.108093, mae: 26.986052, mean_q: -38.162350, mean_eps: 0.100000\n",
      " 304015/500000: episode: 2158, duration: 0.534s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.071041, mae: 26.787750, mean_q: -38.293289, mean_eps: 0.100000\n",
      " 304183/500000: episode: 2159, duration: 0.807s, episode steps: 168, steps per second: 208, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.079413, mae: 26.544037, mean_q: -37.945242, mean_eps: 0.100000\n",
      " 304303/500000: episode: 2160, duration: 0.570s, episode steps: 120, steps per second: 211, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.085662, mae: 26.874175, mean_q: -38.554472, mean_eps: 0.100000\n",
      " 304411/500000: episode: 2161, duration: 0.508s, episode steps: 108, steps per second: 213, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.064437, mae: 26.643493, mean_q: -38.321383, mean_eps: 0.100000\n",
      " 304539/500000: episode: 2162, duration: 0.594s, episode steps: 128, steps per second: 216, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.048011, mae: 26.220241, mean_q: -37.697770, mean_eps: 0.100000\n",
      " 304638/500000: episode: 2163, duration: 0.456s, episode steps:  99, steps per second: 217, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.054325, mae: 26.449219, mean_q: -38.013617, mean_eps: 0.100000\n",
      " 304813/500000: episode: 2164, duration: 0.812s, episode steps: 175, steps per second: 216, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.098311, mae: 27.157150, mean_q: -38.851305, mean_eps: 0.100000\n",
      " 304901/500000: episode: 2165, duration: 0.418s, episode steps:  88, steps per second: 211, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.071424, mae: 26.489942, mean_q: -37.796483, mean_eps: 0.100000\n",
      " 305017/500000: episode: 2166, duration: 0.540s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.063971, mae: 26.476126, mean_q: -37.745273, mean_eps: 0.100000\n",
      " 305118/500000: episode: 2167, duration: 0.467s, episode steps: 101, steps per second: 216, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.067713, mae: 26.920159, mean_q: -38.485865, mean_eps: 0.100000\n",
      " 305224/500000: episode: 2168, duration: 0.497s, episode steps: 106, steps per second: 213, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.055949, mae: 26.814330, mean_q: -38.256419, mean_eps: 0.100000\n",
      " 305338/500000: episode: 2169, duration: 0.536s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.054825, mae: 26.306398, mean_q: -37.442080, mean_eps: 0.100000\n",
      " 305457/500000: episode: 2170, duration: 0.550s, episode steps: 119, steps per second: 216, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.055989, mae: 26.521023, mean_q: -37.818795, mean_eps: 0.100000\n",
      " 305578/500000: episode: 2171, duration: 0.556s, episode steps: 121, steps per second: 218, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000],  loss: 0.050322, mae: 26.427497, mean_q: -37.658414, mean_eps: 0.100000\n",
      " 305738/500000: episode: 2172, duration: 0.756s, episode steps: 160, steps per second: 212, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.107092, mae: 26.887220, mean_q: -38.380376, mean_eps: 0.100000\n",
      " 305845/500000: episode: 2173, duration: 0.503s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.051732, mae: 26.405938, mean_q: -37.848201, mean_eps: 0.100000\n",
      " 305960/500000: episode: 2174, duration: 0.533s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.047297, mae: 26.554700, mean_q: -38.161132, mean_eps: 0.100000\n",
      " 306073/500000: episode: 2175, duration: 0.527s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.048250, mae: 26.532405, mean_q: -38.168842, mean_eps: 0.100000\n",
      " 306195/500000: episode: 2176, duration: 0.580s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.037168, mae: 26.520544, mean_q: -38.482467, mean_eps: 0.100000\n",
      " 306372/500000: episode: 2177, duration: 0.809s, episode steps: 177, steps per second: 219, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.050657, mae: 27.282065, mean_q: -39.604548, mean_eps: 0.100000\n",
      " 306572/500000: episode: 2178, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.091479, mae: 27.091223, mean_q: -39.241792, mean_eps: 0.100000\n",
      " 306744/500000: episode: 2179, duration: 0.791s, episode steps: 172, steps per second: 217, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.168153, mae: 27.424226, mean_q: -39.883751, mean_eps: 0.100000\n",
      " 306917/500000: episode: 2180, duration: 0.821s, episode steps: 173, steps per second: 211, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.162786, mae: 27.527699, mean_q: -40.053599, mean_eps: 0.100000\n",
      " 307112/500000: episode: 2181, duration: 0.924s, episode steps: 195, steps per second: 211, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.165930, mae: 28.261753, mean_q: -40.921896, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 307261/500000: episode: 2182, duration: 0.695s, episode steps: 149, steps per second: 215, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.150786, mae: 28.333115, mean_q: -40.939921, mean_eps: 0.100000\n",
      " 307455/500000: episode: 2183, duration: 0.920s, episode steps: 194, steps per second: 211, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.193502, mae: 28.319060, mean_q: -40.873464, mean_eps: 0.100000\n",
      " 307655/500000: episode: 2184, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.147949, mae: 28.046775, mean_q: -40.442562, mean_eps: 0.100000\n",
      " 307763/500000: episode: 2185, duration: 0.499s, episode steps: 108, steps per second: 216, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.133517, mae: 27.196859, mean_q: -39.032614, mean_eps: 0.100000\n",
      " 307880/500000: episode: 2186, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.094930, mae: 26.719731, mean_q: -38.517354, mean_eps: 0.100000\n",
      " 308051/500000: episode: 2187, duration: 0.794s, episode steps: 171, steps per second: 215, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.602 [0.000, 2.000],  loss: 0.064666, mae: 26.937954, mean_q: -38.810344, mean_eps: 0.100000\n",
      " 308174/500000: episode: 2188, duration: 0.571s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.193359, mae: 27.725958, mean_q: -39.897232, mean_eps: 0.100000\n",
      " 308292/500000: episode: 2189, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.083883, mae: 27.559703, mean_q: -39.809871, mean_eps: 0.100000\n",
      " 308426/500000: episode: 2190, duration: 0.615s, episode steps: 134, steps per second: 218, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.418 [0.000, 2.000],  loss: 0.118199, mae: 27.356841, mean_q: -39.604738, mean_eps: 0.100000\n",
      " 308541/500000: episode: 2191, duration: 0.533s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.097953, mae: 28.267234, mean_q: -40.746395, mean_eps: 0.100000\n",
      " 308710/500000: episode: 2192, duration: 0.811s, episode steps: 169, steps per second: 208, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.201 [0.000, 2.000],  loss: 0.086815, mae: 27.957820, mean_q: -40.361169, mean_eps: 0.100000\n",
      " 308872/500000: episode: 2193, duration: 0.755s, episode steps: 162, steps per second: 214, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.827 [0.000, 2.000],  loss: 0.123297, mae: 28.475685, mean_q: -41.100832, mean_eps: 0.100000\n",
      " 308981/500000: episode: 2194, duration: 0.504s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.108858, mae: 28.091874, mean_q: -40.523235, mean_eps: 0.100000\n",
      " 309131/500000: episode: 2195, duration: 0.712s, episode steps: 150, steps per second: 211, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.413 [0.000, 2.000],  loss: 0.102720, mae: 28.408036, mean_q: -41.098140, mean_eps: 0.100000\n",
      " 309296/500000: episode: 2196, duration: 0.767s, episode steps: 165, steps per second: 215, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.788 [0.000, 2.000],  loss: 0.139502, mae: 29.274818, mean_q: -42.154129, mean_eps: 0.100000\n",
      " 309418/500000: episode: 2197, duration: 0.558s, episode steps: 122, steps per second: 219, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.114619, mae: 29.160332, mean_q: -42.032173, mean_eps: 0.100000\n",
      " 309584/500000: episode: 2198, duration: 0.776s, episode steps: 166, steps per second: 214, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.458 [0.000, 2.000],  loss: 0.238716, mae: 29.374076, mean_q: -42.284948, mean_eps: 0.100000\n",
      " 309722/500000: episode: 2199, duration: 0.637s, episode steps: 138, steps per second: 217, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.219005, mae: 29.685714, mean_q: -42.719797, mean_eps: 0.100000\n",
      " 309871/500000: episode: 2200, duration: 0.692s, episode steps: 149, steps per second: 215, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.150321, mae: 29.599073, mean_q: -42.560474, mean_eps: 0.100000\n",
      " 310071/500000: episode: 2201, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.292054, mae: 30.479138, mean_q: -43.564761, mean_eps: 0.100000\n",
      " 310271/500000: episode: 2202, duration: 0.921s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.157918, mae: 29.773909, mean_q: -42.777886, mean_eps: 0.100000\n",
      " 310428/500000: episode: 2203, duration: 0.744s, episode steps: 157, steps per second: 211, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.170552, mae: 29.493086, mean_q: -42.394301, mean_eps: 0.100000\n",
      " 310541/500000: episode: 2204, duration: 0.527s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.469 [0.000, 2.000],  loss: 0.225797, mae: 29.423982, mean_q: -42.472760, mean_eps: 0.100000\n",
      " 310663/500000: episode: 2205, duration: 0.565s, episode steps: 122, steps per second: 216, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.094786, mae: 28.803587, mean_q: -41.850817, mean_eps: 0.100000\n",
      " 310837/500000: episode: 2206, duration: 0.821s, episode steps: 174, steps per second: 212, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.741 [0.000, 2.000],  loss: 0.093814, mae: 29.076354, mean_q: -42.046437, mean_eps: 0.100000\n",
      " 310951/500000: episode: 2207, duration: 0.535s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.087601, mae: 29.379437, mean_q: -42.378084, mean_eps: 0.100000\n",
      " 311062/500000: episode: 2208, duration: 0.515s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.080113, mae: 28.326584, mean_q: -40.924184, mean_eps: 0.100000\n",
      " 311152/500000: episode: 2209, duration: 0.411s, episode steps:  90, steps per second: 219, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.065624, mae: 28.406863, mean_q: -40.902151, mean_eps: 0.100000\n",
      " 311268/500000: episode: 2210, duration: 0.545s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.047391, mae: 27.569696, mean_q: -39.682751, mean_eps: 0.100000\n",
      " 311413/500000: episode: 2211, duration: 0.671s, episode steps: 145, steps per second: 216, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.042817, mae: 28.165425, mean_q: -40.609133, mean_eps: 0.100000\n",
      " 311547/500000: episode: 2212, duration: 0.612s, episode steps: 134, steps per second: 219, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.037196, mae: 28.302361, mean_q: -40.688171, mean_eps: 0.100000\n",
      " 311720/500000: episode: 2213, duration: 0.815s, episode steps: 173, steps per second: 212, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000],  loss: 0.100135, mae: 28.432053, mean_q: -40.692113, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 311877/500000: episode: 2214, duration: 0.735s, episode steps: 157, steps per second: 214, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.783 [0.000, 2.000],  loss: 0.090846, mae: 27.930703, mean_q: -40.023354, mean_eps: 0.100000\n",
      " 312011/500000: episode: 2215, duration: 0.616s, episode steps: 134, steps per second: 217, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.091296, mae: 28.011153, mean_q: -40.183570, mean_eps: 0.100000\n",
      " 312123/500000: episode: 2216, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.092148, mae: 28.081595, mean_q: -40.370861, mean_eps: 0.100000\n",
      " 312275/500000: episode: 2217, duration: 0.705s, episode steps: 152, steps per second: 215, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.093284, mae: 28.476790, mean_q: -41.042242, mean_eps: 0.100000\n",
      " 312387/500000: episode: 2218, duration: 0.520s, episode steps: 112, steps per second: 215, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.081013, mae: 28.114550, mean_q: -40.604894, mean_eps: 0.100000\n",
      " 312473/500000: episode: 2219, duration: 0.409s, episode steps:  86, steps per second: 210, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.071751, mae: 27.719484, mean_q: -39.984081, mean_eps: 0.100000\n",
      " 312621/500000: episode: 2220, duration: 0.698s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.419 [0.000, 2.000],  loss: 0.089791, mae: 27.288590, mean_q: -39.491652, mean_eps: 0.100000\n",
      " 312737/500000: episode: 2221, duration: 0.538s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.085527, mae: 26.532940, mean_q: -38.238747, mean_eps: 0.100000\n",
      " 312888/500000: episode: 2222, duration: 0.697s, episode steps: 151, steps per second: 217, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.033 [0.000, 2.000],  loss: 0.058564, mae: 26.439067, mean_q: -38.214558, mean_eps: 0.100000\n",
      " 312998/500000: episode: 2223, duration: 0.522s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.051520, mae: 26.674192, mean_q: -38.551660, mean_eps: 0.100000\n",
      " 313109/500000: episode: 2224, duration: 0.523s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000],  loss: 0.065066, mae: 26.459826, mean_q: -38.322652, mean_eps: 0.100000\n",
      " 313221/500000: episode: 2225, duration: 0.515s, episode steps: 112, steps per second: 217, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.078018, mae: 26.298738, mean_q: -38.134715, mean_eps: 0.100000\n",
      " 313336/500000: episode: 2226, duration: 0.538s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.024194, mae: 26.353561, mean_q: -38.403532, mean_eps: 0.100000\n",
      " 313449/500000: episode: 2227, duration: 0.540s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.050924, mae: 26.172884, mean_q: -38.137118, mean_eps: 0.100000\n",
      " 313572/500000: episode: 2228, duration: 0.569s, episode steps: 123, steps per second: 216, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.028993, mae: 25.830221, mean_q: -37.782487, mean_eps: 0.100000\n",
      " 313691/500000: episode: 2229, duration: 0.550s, episode steps: 119, steps per second: 216, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.025297, mae: 26.268065, mean_q: -38.579422, mean_eps: 0.100000\n",
      " 313854/500000: episode: 2230, duration: 0.773s, episode steps: 163, steps per second: 211, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.761 [0.000, 2.000],  loss: 0.025694, mae: 26.454970, mean_q: -38.869404, mean_eps: 0.100000\n",
      " 313986/500000: episode: 2231, duration: 0.620s, episode steps: 132, steps per second: 213, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.417 [0.000, 2.000],  loss: 0.043572, mae: 26.545689, mean_q: -38.933502, mean_eps: 0.100000\n",
      " 314143/500000: episode: 2232, duration: 0.724s, episode steps: 157, steps per second: 217, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 0.099889, mae: 27.058483, mean_q: -39.435045, mean_eps: 0.100000\n",
      " 314267/500000: episode: 2233, duration: 0.585s, episode steps: 124, steps per second: 212, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.127276, mae: 27.450368, mean_q: -39.894354, mean_eps: 0.100000\n",
      " 314374/500000: episode: 2234, duration: 0.510s, episode steps: 107, steps per second: 210, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.090754, mae: 27.612306, mean_q: -40.117297, mean_eps: 0.100000\n",
      " 314481/500000: episode: 2235, duration: 0.494s, episode steps: 107, steps per second: 216, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.059751, mae: 27.601435, mean_q: -40.249705, mean_eps: 0.100000\n",
      " 314642/500000: episode: 2236, duration: 0.734s, episode steps: 161, steps per second: 219, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.069395, mae: 27.964681, mean_q: -40.711684, mean_eps: 0.100000\n",
      " 314827/500000: episode: 2237, duration: 0.869s, episode steps: 185, steps per second: 213, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.103844, mae: 27.641172, mean_q: -40.071214, mean_eps: 0.100000\n",
      " 314997/500000: episode: 2238, duration: 0.779s, episode steps: 170, steps per second: 218, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.218 [0.000, 2.000],  loss: 0.181861, mae: 27.323217, mean_q: -39.324764, mean_eps: 0.100000\n",
      " 315167/500000: episode: 2239, duration: 0.813s, episode steps: 170, steps per second: 209, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.130145, mae: 27.191825, mean_q: -39.092250, mean_eps: 0.100000\n",
      " 315296/500000: episode: 2240, duration: 0.598s, episode steps: 129, steps per second: 216, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.115132, mae: 26.728489, mean_q: -38.461606, mean_eps: 0.100000\n",
      " 315474/500000: episode: 2241, duration: 0.830s, episode steps: 178, steps per second: 214, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.102940, mae: 27.111246, mean_q: -38.823745, mean_eps: 0.100000\n",
      " 315627/500000: episode: 2242, duration: 0.729s, episode steps: 153, steps per second: 210, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.099147, mae: 26.866618, mean_q: -38.425875, mean_eps: 0.100000\n",
      " 315823/500000: episode: 2243, duration: 0.962s, episode steps: 196, steps per second: 204, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.086511, mae: 27.231832, mean_q: -39.029992, mean_eps: 0.100000\n",
      " 315941/500000: episode: 2244, duration: 0.612s, episode steps: 118, steps per second: 193, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000],  loss: 0.060826, mae: 27.198097, mean_q: -39.127500, mean_eps: 0.100000\n",
      " 316045/500000: episode: 2245, duration: 0.501s, episode steps: 104, steps per second: 208, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.081153, mae: 27.113181, mean_q: -38.835648, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 316182/500000: episode: 2246, duration: 0.645s, episode steps: 137, steps per second: 212, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000],  loss: 0.079753, mae: 26.730933, mean_q: -38.188979, mean_eps: 0.100000\n",
      " 316268/500000: episode: 2247, duration: 0.408s, episode steps:  86, steps per second: 211, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.074085, mae: 26.458815, mean_q: -37.524104, mean_eps: 0.100000\n",
      " 316386/500000: episode: 2248, duration: 0.609s, episode steps: 118, steps per second: 194, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.089188, mae: 26.503039, mean_q: -37.665344, mean_eps: 0.100000\n",
      " 316552/500000: episode: 2249, duration: 0.831s, episode steps: 166, steps per second: 200, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.169 [0.000, 2.000],  loss: 0.126584, mae: 25.916023, mean_q: -36.713081, mean_eps: 0.100000\n",
      " 316717/500000: episode: 2250, duration: 0.828s, episode steps: 165, steps per second: 199, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.098703, mae: 25.959783, mean_q: -36.678769, mean_eps: 0.100000\n",
      " 316877/500000: episode: 2251, duration: 0.808s, episode steps: 160, steps per second: 198, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.131 [0.000, 2.000],  loss: 0.120931, mae: 26.389043, mean_q: -37.224085, mean_eps: 0.100000\n",
      " 317002/500000: episode: 2252, duration: 0.582s, episode steps: 125, steps per second: 215, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.108194, mae: 26.640154, mean_q: -37.766350, mean_eps: 0.100000\n",
      " 317126/500000: episode: 2253, duration: 0.590s, episode steps: 124, steps per second: 210, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.127821, mae: 26.633375, mean_q: -38.004271, mean_eps: 0.100000\n",
      " 317320/500000: episode: 2254, duration: 0.918s, episode steps: 194, steps per second: 211, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.111565, mae: 27.092140, mean_q: -38.684968, mean_eps: 0.100000\n",
      " 317491/500000: episode: 2255, duration: 0.827s, episode steps: 171, steps per second: 207, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000],  loss: 0.089514, mae: 27.242664, mean_q: -39.054389, mean_eps: 0.100000\n",
      " 317624/500000: episode: 2256, duration: 0.633s, episode steps: 133, steps per second: 210, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.451 [0.000, 2.000],  loss: 0.054358, mae: 27.786446, mean_q: -39.976726, mean_eps: 0.100000\n",
      " 317738/500000: episode: 2257, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.095604, mae: 27.801727, mean_q: -39.843707, mean_eps: 0.100000\n",
      " 317861/500000: episode: 2258, duration: 0.574s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.082327, mae: 26.948316, mean_q: -38.567092, mean_eps: 0.100000\n",
      " 318061/500000: episode: 2259, duration: 0.940s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.101897, mae: 27.209086, mean_q: -38.775508, mean_eps: 0.100000\n",
      " 318237/500000: episode: 2260, duration: 0.818s, episode steps: 176, steps per second: 215, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.550335, mae: 28.011872, mean_q: -39.624439, mean_eps: 0.100000\n",
      " 318386/500000: episode: 2261, duration: 0.690s, episode steps: 149, steps per second: 216, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.398871, mae: 28.362563, mean_q: -40.166302, mean_eps: 0.100000\n",
      " 318480/500000: episode: 2262, duration: 0.504s, episode steps:  94, steps per second: 187, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.429971, mae: 28.106223, mean_q: -39.743933, mean_eps: 0.100000\n",
      " 318607/500000: episode: 2263, duration: 0.658s, episode steps: 127, steps per second: 193, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.156220, mae: 27.633343, mean_q: -39.040184, mean_eps: 0.100000\n",
      " 318735/500000: episode: 2264, duration: 0.643s, episode steps: 128, steps per second: 199, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.406 [0.000, 2.000],  loss: 0.251667, mae: 28.290005, mean_q: -39.842098, mean_eps: 0.100000\n",
      " 318845/500000: episode: 2265, duration: 0.561s, episode steps: 110, steps per second: 196, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.285305, mae: 28.303113, mean_q: -39.651594, mean_eps: 0.100000\n",
      " 318960/500000: episode: 2266, duration: 0.587s, episode steps: 115, steps per second: 196, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.151164, mae: 28.059363, mean_q: -39.491615, mean_eps: 0.100000\n",
      " 319137/500000: episode: 2267, duration: 0.821s, episode steps: 177, steps per second: 216, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.247796, mae: 27.260343, mean_q: -38.627855, mean_eps: 0.100000\n",
      " 319307/500000: episode: 2268, duration: 0.808s, episode steps: 170, steps per second: 210, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 0.119288, mae: 26.772370, mean_q: -37.904124, mean_eps: 0.100000\n",
      " 319455/500000: episode: 2269, duration: 0.687s, episode steps: 148, steps per second: 215, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.953 [0.000, 2.000],  loss: 0.143283, mae: 27.133609, mean_q: -38.442909, mean_eps: 0.100000\n",
      " 319655/500000: episode: 2270, duration: 0.927s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.134591, mae: 27.705474, mean_q: -39.547092, mean_eps: 0.100000\n",
      " 319789/500000: episode: 2271, duration: 0.644s, episode steps: 134, steps per second: 208, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.186668, mae: 27.827811, mean_q: -39.965630, mean_eps: 0.100000\n",
      " 319912/500000: episode: 2272, duration: 0.567s, episode steps: 123, steps per second: 217, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.211 [0.000, 2.000],  loss: 0.210532, mae: 27.534566, mean_q: -39.489931, mean_eps: 0.100000\n",
      " 320082/500000: episode: 2273, duration: 0.795s, episode steps: 170, steps per second: 214, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.053 [0.000, 2.000],  loss: 0.191000, mae: 27.755171, mean_q: -39.913526, mean_eps: 0.100000\n",
      " 320255/500000: episode: 2274, duration: 0.809s, episode steps: 173, steps per second: 214, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.183284, mae: 27.964499, mean_q: -40.259996, mean_eps: 0.100000\n",
      " 320413/500000: episode: 2275, duration: 0.831s, episode steps: 158, steps per second: 190, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.175138, mae: 28.310681, mean_q: -40.698121, mean_eps: 0.100000\n",
      " 320533/500000: episode: 2276, duration: 0.611s, episode steps: 120, steps per second: 196, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.179133, mae: 27.884431, mean_q: -40.033673, mean_eps: 0.100000\n",
      " 320658/500000: episode: 2277, duration: 0.586s, episode steps: 125, steps per second: 213, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.135002, mae: 27.225527, mean_q: -39.069298, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 320778/500000: episode: 2278, duration: 0.563s, episode steps: 120, steps per second: 213, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.099871, mae: 27.552519, mean_q: -39.524530, mean_eps: 0.100000\n",
      " 320968/500000: episode: 2279, duration: 0.900s, episode steps: 190, steps per second: 211, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.148374, mae: 28.085232, mean_q: -40.219812, mean_eps: 0.100000\n",
      " 321083/500000: episode: 2280, duration: 0.539s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.151029, mae: 27.588027, mean_q: -39.303859, mean_eps: 0.100000\n",
      " 321202/500000: episode: 2281, duration: 0.547s, episode steps: 119, steps per second: 217, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.140262, mae: 27.193175, mean_q: -38.700575, mean_eps: 0.100000\n",
      " 321290/500000: episode: 2282, duration: 0.462s, episode steps:  88, steps per second: 190, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.125526, mae: 26.543636, mean_q: -37.552817, mean_eps: 0.100000\n",
      " 321418/500000: episode: 2283, duration: 0.613s, episode steps: 128, steps per second: 209, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.102022, mae: 26.490317, mean_q: -37.566630, mean_eps: 0.100000\n",
      " 321561/500000: episode: 2284, duration: 0.666s, episode steps: 143, steps per second: 215, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.078190, mae: 26.592257, mean_q: -37.855163, mean_eps: 0.100000\n",
      " 321759/500000: episode: 2285, duration: 0.923s, episode steps: 198, steps per second: 214, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.115240, mae: 26.809090, mean_q: -38.179087, mean_eps: 0.100000\n",
      " 321878/500000: episode: 2286, duration: 0.562s, episode steps: 119, steps per second: 212, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.145798, mae: 27.183778, mean_q: -38.746776, mean_eps: 0.100000\n",
      " 321988/500000: episode: 2287, duration: 0.510s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.105363, mae: 26.593285, mean_q: -38.013595, mean_eps: 0.100000\n",
      " 322105/500000: episode: 2288, duration: 0.537s, episode steps: 117, steps per second: 218, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.077931, mae: 26.403978, mean_q: -37.774508, mean_eps: 0.100000\n",
      " 322220/500000: episode: 2289, duration: 0.591s, episode steps: 115, steps per second: 195, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.070453, mae: 26.212022, mean_q: -37.571420, mean_eps: 0.100000\n",
      " 322343/500000: episode: 2290, duration: 0.583s, episode steps: 123, steps per second: 211, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000],  loss: 0.072300, mae: 26.630250, mean_q: -38.360493, mean_eps: 0.100000\n",
      " 322509/500000: episode: 2291, duration: 0.767s, episode steps: 166, steps per second: 216, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000],  loss: 0.084830, mae: 26.644360, mean_q: -38.271052, mean_eps: 0.100000\n",
      " 322597/500000: episode: 2292, duration: 0.420s, episode steps:  88, steps per second: 210, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.116169, mae: 26.583119, mean_q: -38.079329, mean_eps: 0.100000\n",
      " 322797/500000: episode: 2293, duration: 0.941s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.148676, mae: 26.349570, mean_q: -37.724682, mean_eps: 0.100000\n",
      " 322976/500000: episode: 2294, duration: 0.835s, episode steps: 179, steps per second: 214, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.849 [0.000, 2.000],  loss: 0.127240, mae: 27.493767, mean_q: -39.453792, mean_eps: 0.100000\n",
      " 323132/500000: episode: 2295, duration: 0.743s, episode steps: 156, steps per second: 210, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.111158, mae: 28.297064, mean_q: -40.628566, mean_eps: 0.100000\n",
      " 323247/500000: episode: 2296, duration: 0.535s, episode steps: 115, steps per second: 215, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.123072, mae: 28.225039, mean_q: -40.639086, mean_eps: 0.100000\n",
      " 323367/500000: episode: 2297, duration: 0.560s, episode steps: 120, steps per second: 214, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.130720, mae: 28.421528, mean_q: -40.921429, mean_eps: 0.100000\n",
      " 323521/500000: episode: 2298, duration: 0.730s, episode steps: 154, steps per second: 211, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.089462, mae: 27.984949, mean_q: -40.407843, mean_eps: 0.100000\n",
      " 323715/500000: episode: 2299, duration: 0.892s, episode steps: 194, steps per second: 217, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000],  loss: 0.125947, mae: 28.817137, mean_q: -41.827955, mean_eps: 0.100000\n",
      " 323876/500000: episode: 2300, duration: 0.755s, episode steps: 161, steps per second: 213, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.098839, mae: 28.635009, mean_q: -41.598933, mean_eps: 0.100000\n",
      " 324046/500000: episode: 2301, duration: 0.804s, episode steps: 170, steps per second: 212, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.091543, mae: 28.116657, mean_q: -40.892563, mean_eps: 0.100000\n",
      " 324200/500000: episode: 2302, duration: 0.734s, episode steps: 154, steps per second: 210, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.067420, mae: 27.896633, mean_q: -40.545594, mean_eps: 0.100000\n",
      " 324352/500000: episode: 2303, duration: 0.725s, episode steps: 152, steps per second: 210, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.947 [0.000, 2.000],  loss: 0.094732, mae: 28.464896, mean_q: -41.304719, mean_eps: 0.100000\n",
      " 324463/500000: episode: 2304, duration: 0.523s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.068938, mae: 28.660355, mean_q: -41.662269, mean_eps: 0.100000\n",
      " 324618/500000: episode: 2305, duration: 0.720s, episode steps: 155, steps per second: 215, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.039 [0.000, 2.000],  loss: 0.102742, mae: 28.210228, mean_q: -40.757553, mean_eps: 0.100000\n",
      " 324727/500000: episode: 2306, duration: 0.523s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.075938, mae: 28.151816, mean_q: -40.770343, mean_eps: 0.100000\n",
      " 324840/500000: episode: 2307, duration: 0.538s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.070303, mae: 27.304478, mean_q: -39.590615, mean_eps: 0.100000\n",
      " 324948/500000: episode: 2308, duration: 0.499s, episode steps: 108, steps per second: 216, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.082241, mae: 27.753045, mean_q: -40.142620, mean_eps: 0.100000\n",
      " 325059/500000: episode: 2309, duration: 0.514s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.070718, mae: 27.723016, mean_q: -39.958212, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 325176/500000: episode: 2310, duration: 0.559s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.089259, mae: 27.517765, mean_q: -39.704633, mean_eps: 0.100000\n",
      " 325328/500000: episode: 2311, duration: 0.706s, episode steps: 152, steps per second: 215, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.901 [0.000, 2.000],  loss: 0.070643, mae: 26.930721, mean_q: -38.901128, mean_eps: 0.100000\n",
      " 325481/500000: episode: 2312, duration: 0.716s, episode steps: 153, steps per second: 214, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.100385, mae: 27.209829, mean_q: -39.294397, mean_eps: 0.100000\n",
      " 325681/500000: episode: 2313, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.217227, mae: 27.531802, mean_q: -39.653841, mean_eps: 0.100000\n",
      " 325781/500000: episode: 2314, duration: 0.459s, episode steps: 100, steps per second: 218, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.139746, mae: 28.325147, mean_q: -40.655936, mean_eps: 0.100000\n",
      " 325972/500000: episode: 2315, duration: 0.928s, episode steps: 191, steps per second: 206, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.180894, mae: 29.055957, mean_q: -41.489675, mean_eps: 0.100000\n",
      " 326077/500000: episode: 2316, duration: 0.586s, episode steps: 105, steps per second: 179, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.150167, mae: 28.804975, mean_q: -41.145689, mean_eps: 0.100000\n",
      " 326249/500000: episode: 2317, duration: 0.820s, episode steps: 172, steps per second: 210, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.191271, mae: 29.274788, mean_q: -41.397029, mean_eps: 0.100000\n",
      " 326341/500000: episode: 2318, duration: 0.440s, episode steps:  92, steps per second: 209, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.151559, mae: 29.091166, mean_q: -40.974070, mean_eps: 0.100000\n",
      " 326462/500000: episode: 2319, duration: 0.578s, episode steps: 121, steps per second: 209, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.157057, mae: 28.531247, mean_q: -40.032147, mean_eps: 0.100000\n",
      " 326650/500000: episode: 2320, duration: 0.880s, episode steps: 188, steps per second: 214, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.107938, mae: 28.913299, mean_q: -40.564192, mean_eps: 0.100000\n",
      " 326756/500000: episode: 2321, duration: 0.497s, episode steps: 106, steps per second: 213, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.065142, mae: 28.443116, mean_q: -39.848487, mean_eps: 0.100000\n",
      " 326944/500000: episode: 2322, duration: 0.900s, episode steps: 188, steps per second: 209, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.076883, mae: 28.051691, mean_q: -39.499974, mean_eps: 0.100000\n",
      " 327035/500000: episode: 2323, duration: 0.432s, episode steps:  91, steps per second: 211, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.073962, mae: 27.902591, mean_q: -39.230433, mean_eps: 0.100000\n",
      " 327217/500000: episode: 2324, duration: 0.852s, episode steps: 182, steps per second: 214, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.874 [0.000, 2.000],  loss: 0.057136, mae: 28.384603, mean_q: -40.177309, mean_eps: 0.100000\n",
      " 327318/500000: episode: 2325, duration: 0.492s, episode steps: 101, steps per second: 205, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.048401, mae: 28.704243, mean_q: -40.666613, mean_eps: 0.100000\n",
      " 327430/500000: episode: 2326, duration: 0.519s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.077997, mae: 28.421587, mean_q: -40.260960, mean_eps: 0.100000\n",
      " 327518/500000: episode: 2327, duration: 0.409s, episode steps:  88, steps per second: 215, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.070118, mae: 28.049775, mean_q: -39.860827, mean_eps: 0.100000\n",
      " 327638/500000: episode: 2328, duration: 0.562s, episode steps: 120, steps per second: 214, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.055198, mae: 26.947297, mean_q: -38.157969, mean_eps: 0.100000\n",
      " 327798/500000: episode: 2329, duration: 0.759s, episode steps: 160, steps per second: 211, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.075145, mae: 27.282804, mean_q: -38.558687, mean_eps: 0.100000\n",
      " 327930/500000: episode: 2330, duration: 0.606s, episode steps: 132, steps per second: 218, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.077308, mae: 27.079894, mean_q: -38.388140, mean_eps: 0.100000\n",
      " 328066/500000: episode: 2331, duration: 0.632s, episode steps: 136, steps per second: 215, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.068216, mae: 27.696367, mean_q: -39.237442, mean_eps: 0.100000\n",
      " 328192/500000: episode: 2332, duration: 0.598s, episode steps: 126, steps per second: 211, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.099008, mae: 26.457481, mean_q: -37.095937, mean_eps: 0.100000\n",
      " 328360/500000: episode: 2333, duration: 0.839s, episode steps: 168, steps per second: 200, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.230077, mae: 27.674323, mean_q: -38.850085, mean_eps: 0.100000\n",
      " 328475/500000: episode: 2334, duration: 0.550s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.169476, mae: 28.035692, mean_q: -39.521500, mean_eps: 0.100000\n",
      " 328669/500000: episode: 2335, duration: 0.915s, episode steps: 194, steps per second: 212, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.143556, mae: 28.744715, mean_q: -40.328573, mean_eps: 0.100000\n",
      " 328869/500000: episode: 2336, duration: 1.029s, episode steps: 200, steps per second: 194, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.155681, mae: 29.071453, mean_q: -40.678678, mean_eps: 0.100000\n",
      " 328989/500000: episode: 2337, duration: 0.574s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.159489, mae: 28.762046, mean_q: -40.318252, mean_eps: 0.100000\n",
      " 329163/500000: episode: 2338, duration: 0.806s, episode steps: 174, steps per second: 216, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000],  loss: 0.171447, mae: 29.774515, mean_q: -41.643819, mean_eps: 0.100000\n",
      " 329286/500000: episode: 2339, duration: 0.574s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.146054, mae: 28.974245, mean_q: -40.701782, mean_eps: 0.100000\n",
      " 329470/500000: episode: 2340, duration: 0.871s, episode steps: 184, steps per second: 211, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.206387, mae: 29.378686, mean_q: -41.214359, mean_eps: 0.100000\n",
      " 329577/500000: episode: 2341, duration: 0.496s, episode steps: 107, steps per second: 216, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.179689, mae: 28.797305, mean_q: -40.585571, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 329696/500000: episode: 2342, duration: 0.553s, episode steps: 119, steps per second: 215, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.140257, mae: 28.538545, mean_q: -40.414739, mean_eps: 0.100000\n",
      " 329818/500000: episode: 2343, duration: 0.611s, episode steps: 122, steps per second: 200, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.136082, mae: 27.932642, mean_q: -39.592977, mean_eps: 0.100000\n",
      " 329942/500000: episode: 2344, duration: 0.646s, episode steps: 124, steps per second: 192, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.177 [0.000, 2.000],  loss: 0.092899, mae: 27.489306, mean_q: -38.737571, mean_eps: 0.100000\n",
      " 330054/500000: episode: 2345, duration: 0.536s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.069501, mae: 26.808546, mean_q: -37.943638, mean_eps: 0.100000\n",
      " 330240/500000: episode: 2346, duration: 0.942s, episode steps: 186, steps per second: 197, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.059287, mae: 26.983453, mean_q: -38.377234, mean_eps: 0.100000\n",
      " 330348/500000: episode: 2347, duration: 0.566s, episode steps: 108, steps per second: 191, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.079312, mae: 27.276884, mean_q: -38.931640, mean_eps: 0.100000\n",
      " 330479/500000: episode: 2348, duration: 0.668s, episode steps: 131, steps per second: 196, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.049908, mae: 26.800172, mean_q: -38.355953, mean_eps: 0.100000\n",
      " 330593/500000: episode: 2349, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.049465, mae: 26.613126, mean_q: -38.067712, mean_eps: 0.100000\n",
      " 330692/500000: episode: 2350, duration: 0.494s, episode steps:  99, steps per second: 200, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.073888, mae: 26.527177, mean_q: -37.684627, mean_eps: 0.100000\n",
      " 330800/500000: episode: 2351, duration: 0.538s, episode steps: 108, steps per second: 201, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.062900, mae: 26.176294, mean_q: -37.056692, mean_eps: 0.100000\n",
      " 330995/500000: episode: 2352, duration: 0.939s, episode steps: 195, steps per second: 208, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000],  loss: 0.069175, mae: 26.343153, mean_q: -37.377062, mean_eps: 0.100000\n",
      " 331153/500000: episode: 2353, duration: 0.766s, episode steps: 158, steps per second: 206, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.074264, mae: 26.636823, mean_q: -37.912758, mean_eps: 0.100000\n",
      " 331310/500000: episode: 2354, duration: 0.773s, episode steps: 157, steps per second: 203, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.097938, mae: 26.222738, mean_q: -37.029796, mean_eps: 0.100000\n",
      " 331418/500000: episode: 2355, duration: 0.544s, episode steps: 108, steps per second: 199, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.087911, mae: 26.520690, mean_q: -37.381865, mean_eps: 0.100000\n",
      " 331522/500000: episode: 2356, duration: 0.522s, episode steps: 104, steps per second: 199, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.071831, mae: 26.802342, mean_q: -37.842966, mean_eps: 0.100000\n",
      " 331619/500000: episode: 2357, duration: 0.472s, episode steps:  97, steps per second: 206, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.081197, mae: 26.173699, mean_q: -36.740521, mean_eps: 0.100000\n",
      " 331756/500000: episode: 2358, duration: 0.686s, episode steps: 137, steps per second: 200, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.081231, mae: 26.478995, mean_q: -37.336041, mean_eps: 0.100000\n",
      " 331888/500000: episode: 2359, duration: 0.639s, episode steps: 132, steps per second: 206, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.081636, mae: 26.010270, mean_q: -36.701452, mean_eps: 0.100000\n",
      " 332069/500000: episode: 2360, duration: 0.894s, episode steps: 181, steps per second: 203, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.100425, mae: 25.457684, mean_q: -35.853040, mean_eps: 0.100000\n",
      " 332178/500000: episode: 2361, duration: 0.545s, episode steps: 109, steps per second: 200, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.100976, mae: 25.602678, mean_q: -36.051659, mean_eps: 0.100000\n",
      " 332299/500000: episode: 2362, duration: 0.602s, episode steps: 121, steps per second: 201, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.072781, mae: 24.978687, mean_q: -35.434438, mean_eps: 0.100000\n",
      " 332459/500000: episode: 2363, duration: 0.782s, episode steps: 160, steps per second: 205, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.097627, mae: 25.606752, mean_q: -36.395306, mean_eps: 0.100000\n",
      " 332624/500000: episode: 2364, duration: 0.811s, episode steps: 165, steps per second: 204, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.148338, mae: 26.262666, mean_q: -37.302433, mean_eps: 0.100000\n",
      " 332736/500000: episode: 2365, duration: 0.586s, episode steps: 112, steps per second: 191, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.133082, mae: 26.324357, mean_q: -37.485675, mean_eps: 0.100000\n",
      " 332898/500000: episode: 2366, duration: 0.757s, episode steps: 162, steps per second: 214, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.152847, mae: 26.841245, mean_q: -38.229349, mean_eps: 0.100000\n",
      " 333000/500000: episode: 2367, duration: 0.486s, episode steps: 102, steps per second: 210, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.174748, mae: 26.706848, mean_q: -37.997621, mean_eps: 0.100000\n",
      " 333091/500000: episode: 2368, duration: 0.429s, episode steps:  91, steps per second: 212, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.091219, mae: 26.775587, mean_q: -38.155714, mean_eps: 0.100000\n",
      " 333250/500000: episode: 2369, duration: 0.766s, episode steps: 159, steps per second: 208, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.139750, mae: 27.256632, mean_q: -38.541790, mean_eps: 0.100000\n",
      " 333407/500000: episode: 2370, duration: 0.738s, episode steps: 157, steps per second: 213, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000],  loss: 0.119690, mae: 26.695274, mean_q: -37.633741, mean_eps: 0.100000\n",
      " 333525/500000: episode: 2371, duration: 0.596s, episode steps: 118, steps per second: 198, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.119316, mae: 26.283057, mean_q: -36.946545, mean_eps: 0.100000\n",
      " 333641/500000: episode: 2372, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.082088, mae: 25.820627, mean_q: -36.542293, mean_eps: 0.100000\n",
      " 333799/500000: episode: 2373, duration: 0.775s, episode steps: 158, steps per second: 204, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.059759, mae: 26.411736, mean_q: -37.350745, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 333909/500000: episode: 2374, duration: 0.565s, episode steps: 110, steps per second: 195, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.080800, mae: 26.189086, mean_q: -37.125087, mean_eps: 0.100000\n",
      " 334073/500000: episode: 2375, duration: 0.767s, episode steps: 164, steps per second: 214, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.114897, mae: 26.876582, mean_q: -38.202833, mean_eps: 0.100000\n",
      " 334243/500000: episode: 2376, duration: 0.839s, episode steps: 170, steps per second: 203, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.105661, mae: 27.190381, mean_q: -38.962650, mean_eps: 0.100000\n",
      " 334403/500000: episode: 2377, duration: 0.788s, episode steps: 160, steps per second: 203, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.138376, mae: 27.428200, mean_q: -39.396745, mean_eps: 0.100000\n",
      " 334523/500000: episode: 2378, duration: 0.576s, episode steps: 120, steps per second: 208, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.089742, mae: 27.776498, mean_q: -39.950479, mean_eps: 0.100000\n",
      " 334686/500000: episode: 2379, duration: 0.801s, episode steps: 163, steps per second: 204, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.147339, mae: 28.229006, mean_q: -40.387095, mean_eps: 0.100000\n",
      " 334853/500000: episode: 2380, duration: 0.808s, episode steps: 167, steps per second: 207, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.135960, mae: 28.560071, mean_q: -40.845343, mean_eps: 0.100000\n",
      " 334966/500000: episode: 2381, duration: 0.577s, episode steps: 113, steps per second: 196, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.128987, mae: 28.542900, mean_q: -40.657806, mean_eps: 0.100000\n",
      " 335065/500000: episode: 2382, duration: 0.527s, episode steps:  99, steps per second: 188, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.129941, mae: 27.457638, mean_q: -39.102244, mean_eps: 0.100000\n",
      " 335226/500000: episode: 2383, duration: 0.776s, episode steps: 161, steps per second: 208, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.115357, mae: 28.495699, mean_q: -40.541448, mean_eps: 0.100000\n",
      " 335384/500000: episode: 2384, duration: 0.823s, episode steps: 158, steps per second: 192, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.124747, mae: 28.202574, mean_q: -40.049216, mean_eps: 0.100000\n",
      " 335568/500000: episode: 2385, duration: 0.909s, episode steps: 184, steps per second: 203, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.891 [0.000, 2.000],  loss: 0.076817, mae: 28.974254, mean_q: -41.398551, mean_eps: 0.100000\n",
      " 335731/500000: episode: 2386, duration: 0.804s, episode steps: 163, steps per second: 203, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.091514, mae: 28.833385, mean_q: -41.251341, mean_eps: 0.100000\n",
      " 335831/500000: episode: 2387, duration: 0.490s, episode steps: 100, steps per second: 204, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.084313, mae: 28.605891, mean_q: -40.712633, mean_eps: 0.100000\n",
      " 335950/500000: episode: 2388, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.097561, mae: 28.497425, mean_q: -40.441674, mean_eps: 0.100000\n",
      " 336139/500000: episode: 2389, duration: 0.872s, episode steps: 189, steps per second: 217, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 0.073701, mae: 29.324541, mean_q: -41.807991, mean_eps: 0.100000\n",
      " 336320/500000: episode: 2390, duration: 0.862s, episode steps: 181, steps per second: 210, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.064145, mae: 29.465489, mean_q: -42.202087, mean_eps: 0.100000\n",
      " 336484/500000: episode: 2391, duration: 0.755s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.055729, mae: 29.982009, mean_q: -43.239640, mean_eps: 0.100000\n",
      " 336648/500000: episode: 2392, duration: 0.811s, episode steps: 164, steps per second: 202, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.079725, mae: 29.707453, mean_q: -42.691300, mean_eps: 0.100000\n",
      " 336774/500000: episode: 2393, duration: 0.642s, episode steps: 126, steps per second: 196, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.066681, mae: 29.642314, mean_q: -42.738125, mean_eps: 0.100000\n",
      " 336910/500000: episode: 2394, duration: 0.707s, episode steps: 136, steps per second: 192, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.061339, mae: 29.394600, mean_q: -42.332571, mean_eps: 0.100000\n",
      " 337062/500000: episode: 2395, duration: 0.786s, episode steps: 152, steps per second: 193, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.921 [0.000, 2.000],  loss: 0.108278, mae: 29.989245, mean_q: -43.055229, mean_eps: 0.100000\n",
      " 337175/500000: episode: 2396, duration: 0.583s, episode steps: 113, steps per second: 194, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.079512, mae: 29.392741, mean_q: -42.399199, mean_eps: 0.100000\n",
      " 337337/500000: episode: 2397, duration: 0.757s, episode steps: 162, steps per second: 214, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.082348, mae: 28.640348, mean_q: -41.060578, mean_eps: 0.100000\n",
      " 337457/500000: episode: 2398, duration: 0.620s, episode steps: 120, steps per second: 194, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.110667, mae: 28.500962, mean_q: -40.783233, mean_eps: 0.100000\n",
      " 337628/500000: episode: 2399, duration: 0.859s, episode steps: 171, steps per second: 199, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.152605, mae: 28.804596, mean_q: -41.089086, mean_eps: 0.100000\n",
      " 337736/500000: episode: 2400, duration: 0.549s, episode steps: 108, steps per second: 197, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.100556, mae: 28.302081, mean_q: -40.223435, mean_eps: 0.100000\n",
      " 337914/500000: episode: 2401, duration: 0.869s, episode steps: 178, steps per second: 205, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.140778, mae: 28.474259, mean_q: -40.506276, mean_eps: 0.100000\n",
      " 338104/500000: episode: 2402, duration: 0.917s, episode steps: 190, steps per second: 207, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.192659, mae: 28.346461, mean_q: -40.337234, mean_eps: 0.100000\n",
      " 338221/500000: episode: 2403, duration: 0.543s, episode steps: 117, steps per second: 216, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.120836, mae: 28.619170, mean_q: -40.700110, mean_eps: 0.100000\n",
      " 338334/500000: episode: 2404, duration: 0.537s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.081514, mae: 27.839524, mean_q: -39.432474, mean_eps: 0.100000\n",
      " 338500/500000: episode: 2405, duration: 0.778s, episode steps: 166, steps per second: 213, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.192105, mae: 28.023617, mean_q: -39.387658, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 338700/500000: episode: 2406, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.107257, mae: 27.454381, mean_q: -38.719854, mean_eps: 0.100000\n",
      " 338817/500000: episode: 2407, duration: 0.559s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.453 [0.000, 2.000],  loss: 0.065507, mae: 28.038861, mean_q: -39.693986, mean_eps: 0.100000\n",
      " 338932/500000: episode: 2408, duration: 0.539s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.091484, mae: 28.259331, mean_q: -39.877531, mean_eps: 0.100000\n",
      " 339044/500000: episode: 2409, duration: 0.525s, episode steps: 112, steps per second: 213, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.097440, mae: 27.753443, mean_q: -38.873148, mean_eps: 0.100000\n",
      " 339244/500000: episode: 2410, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.092559, mae: 28.202925, mean_q: -39.458312, mean_eps: 0.100000\n",
      " 339370/500000: episode: 2411, duration: 0.584s, episode steps: 126, steps per second: 216, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.169131, mae: 28.206333, mean_q: -39.480958, mean_eps: 0.100000\n",
      " 339516/500000: episode: 2412, duration: 0.735s, episode steps: 146, steps per second: 199, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.161833, mae: 28.146595, mean_q: -39.338212, mean_eps: 0.100000\n",
      " 339715/500000: episode: 2413, duration: 0.962s, episode steps: 199, steps per second: 207, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.154243, mae: 28.142886, mean_q: -38.959422, mean_eps: 0.100000\n",
      " 339846/500000: episode: 2414, duration: 0.642s, episode steps: 131, steps per second: 204, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.179116, mae: 27.998490, mean_q: -38.821766, mean_eps: 0.100000\n",
      " 339957/500000: episode: 2415, duration: 0.531s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.279 [0.000, 2.000],  loss: 0.177425, mae: 27.655841, mean_q: -38.634596, mean_eps: 0.100000\n",
      " 340130/500000: episode: 2416, duration: 0.813s, episode steps: 173, steps per second: 213, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000],  loss: 0.188973, mae: 27.848325, mean_q: -38.598745, mean_eps: 0.100000\n",
      " 340323/500000: episode: 2417, duration: 0.889s, episode steps: 193, steps per second: 217, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.157607, mae: 27.889345, mean_q: -38.664720, mean_eps: 0.100000\n",
      " 340487/500000: episode: 2418, duration: 0.774s, episode steps: 164, steps per second: 212, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.196212, mae: 28.001282, mean_q: -38.699942, mean_eps: 0.100000\n",
      " 340593/500000: episode: 2419, duration: 0.492s, episode steps: 106, steps per second: 215, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.177635, mae: 27.418816, mean_q: -37.841761, mean_eps: 0.100000\n",
      " 340780/500000: episode: 2420, duration: 0.887s, episode steps: 187, steps per second: 211, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 0.126028, mae: 27.132675, mean_q: -37.493146, mean_eps: 0.100000\n",
      " 340870/500000: episode: 2421, duration: 0.439s, episode steps:  90, steps per second: 205, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.111665, mae: 26.865933, mean_q: -37.017037, mean_eps: 0.100000\n",
      " 340996/500000: episode: 2422, duration: 0.584s, episode steps: 126, steps per second: 216, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.142641, mae: 26.826573, mean_q: -36.997718, mean_eps: 0.100000\n",
      " 341164/500000: episode: 2423, duration: 0.778s, episode steps: 168, steps per second: 216, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.135212, mae: 26.508625, mean_q: -36.556374, mean_eps: 0.100000\n",
      " 341266/500000: episode: 2424, duration: 0.483s, episode steps: 102, steps per second: 211, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.109706, mae: 25.759566, mean_q: -35.393480, mean_eps: 0.100000\n",
      " 341431/500000: episode: 2425, duration: 0.782s, episode steps: 165, steps per second: 211, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.163823, mae: 26.551912, mean_q: -36.537376, mean_eps: 0.100000\n",
      " 341552/500000: episode: 2426, duration: 0.572s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.122543, mae: 26.946152, mean_q: -37.407790, mean_eps: 0.100000\n",
      " 341741/500000: episode: 2427, duration: 0.911s, episode steps: 189, steps per second: 207, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.110858, mae: 27.470795, mean_q: -38.159748, mean_eps: 0.100000\n",
      " 341854/500000: episode: 2428, duration: 0.571s, episode steps: 113, steps per second: 198, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.122454, mae: 28.170444, mean_q: -39.616002, mean_eps: 0.100000\n",
      " 341964/500000: episode: 2429, duration: 0.566s, episode steps: 110, steps per second: 194, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.099411, mae: 27.965229, mean_q: -39.340965, mean_eps: 0.100000\n",
      " 342079/500000: episode: 2430, duration: 0.611s, episode steps: 115, steps per second: 188, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.125759, mae: 28.558455, mean_q: -40.410895, mean_eps: 0.100000\n",
      " 342194/500000: episode: 2431, duration: 0.580s, episode steps: 115, steps per second: 198, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.066549, mae: 27.700557, mean_q: -39.447349, mean_eps: 0.100000\n",
      " 342394/500000: episode: 2432, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.073900, mae: 27.701723, mean_q: -39.685496, mean_eps: 0.100000\n",
      " 342507/500000: episode: 2433, duration: 0.540s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.212841, mae: 27.669189, mean_q: -39.584825, mean_eps: 0.100000\n",
      " 342598/500000: episode: 2434, duration: 0.430s, episode steps:  91, steps per second: 212, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.106703, mae: 27.216377, mean_q: -39.015723, mean_eps: 0.100000\n",
      " 342708/500000: episode: 2435, duration: 0.513s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.147644, mae: 26.625556, mean_q: -38.128561, mean_eps: 0.100000\n",
      " 342867/500000: episode: 2436, duration: 0.753s, episode steps: 159, steps per second: 211, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.181803, mae: 27.578679, mean_q: -39.502250, mean_eps: 0.100000\n",
      " 343050/500000: episode: 2437, duration: 0.861s, episode steps: 183, steps per second: 213, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.140895, mae: 27.188445, mean_q: -39.008033, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 343169/500000: episode: 2438, duration: 0.567s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.101535, mae: 27.556559, mean_q: -39.708772, mean_eps: 0.100000\n",
      " 343317/500000: episode: 2439, duration: 0.727s, episode steps: 148, steps per second: 204, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.872 [0.000, 2.000],  loss: 0.115127, mae: 27.778668, mean_q: -39.942684, mean_eps: 0.100000\n",
      " 343423/500000: episode: 2440, duration: 0.524s, episode steps: 106, steps per second: 202, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.054010, mae: 27.354215, mean_q: -39.567981, mean_eps: 0.100000\n",
      " 343533/500000: episode: 2441, duration: 0.528s, episode steps: 110, steps per second: 208, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.052524, mae: 26.935116, mean_q: -39.014865, mean_eps: 0.100000\n",
      " 343724/500000: episode: 2442, duration: 0.945s, episode steps: 191, steps per second: 202, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.078134, mae: 27.633171, mean_q: -40.046608, mean_eps: 0.100000\n",
      " 343879/500000: episode: 2443, duration: 0.738s, episode steps: 155, steps per second: 210, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.852 [0.000, 2.000],  loss: 0.072819, mae: 27.618329, mean_q: -40.000840, mean_eps: 0.100000\n",
      " 343976/500000: episode: 2444, duration: 0.460s, episode steps:  97, steps per second: 211, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.086713, mae: 27.222742, mean_q: -39.275910, mean_eps: 0.100000\n",
      " 344091/500000: episode: 2445, duration: 0.542s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.129777, mae: 27.440866, mean_q: -39.281305, mean_eps: 0.100000\n",
      " 344206/500000: episode: 2446, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.065450, mae: 27.364842, mean_q: -39.294900, mean_eps: 0.100000\n",
      " 344328/500000: episode: 2447, duration: 0.578s, episode steps: 122, steps per second: 211, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.059613, mae: 27.469331, mean_q: -39.364135, mean_eps: 0.100000\n",
      " 344439/500000: episode: 2448, duration: 0.538s, episode steps: 111, steps per second: 206, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.097208, mae: 27.451996, mean_q: -39.086103, mean_eps: 0.100000\n",
      " 344592/500000: episode: 2449, duration: 0.762s, episode steps: 153, steps per second: 201, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.079468, mae: 28.220691, mean_q: -40.192465, mean_eps: 0.100000\n",
      " 344704/500000: episode: 2450, duration: 0.545s, episode steps: 112, steps per second: 205, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000],  loss: 0.059442, mae: 27.551678, mean_q: -39.413588, mean_eps: 0.100000\n",
      " 344856/500000: episode: 2451, duration: 0.724s, episode steps: 152, steps per second: 210, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.068931, mae: 27.269248, mean_q: -38.877854, mean_eps: 0.100000\n",
      " 344970/500000: episode: 2452, duration: 0.560s, episode steps: 114, steps per second: 204, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.050091, mae: 27.015568, mean_q: -38.492356, mean_eps: 0.100000\n",
      " 345069/500000: episode: 2453, duration: 0.474s, episode steps:  99, steps per second: 209, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000],  loss: 0.038777, mae: 26.500431, mean_q: -37.924711, mean_eps: 0.100000\n",
      " 345159/500000: episode: 2454, duration: 0.430s, episode steps:  90, steps per second: 209, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.144 [0.000, 2.000],  loss: 0.045767, mae: 26.269356, mean_q: -37.416267, mean_eps: 0.100000\n",
      " 345280/500000: episode: 2455, duration: 0.565s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.047939, mae: 26.578480, mean_q: -37.830937, mean_eps: 0.100000\n",
      " 345408/500000: episode: 2456, duration: 0.686s, episode steps: 128, steps per second: 187, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.069707, mae: 26.119051, mean_q: -37.265928, mean_eps: 0.100000\n",
      " 345521/500000: episode: 2457, duration: 0.580s, episode steps: 113, steps per second: 195, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.089637, mae: 25.535085, mean_q: -36.331840, mean_eps: 0.100000\n",
      " 345637/500000: episode: 2458, duration: 0.558s, episode steps: 116, steps per second: 208, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.085786, mae: 25.746304, mean_q: -36.677049, mean_eps: 0.100000\n",
      " 345749/500000: episode: 2459, duration: 0.544s, episode steps: 112, steps per second: 206, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.042131, mae: 25.641929, mean_q: -36.472087, mean_eps: 0.100000\n",
      " 345863/500000: episode: 2460, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.053142, mae: 25.421402, mean_q: -36.370290, mean_eps: 0.100000\n",
      " 345977/500000: episode: 2461, duration: 0.530s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.050738, mae: 25.187247, mean_q: -35.870828, mean_eps: 0.100000\n",
      " 346163/500000: episode: 2462, duration: 0.935s, episode steps: 186, steps per second: 199, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000],  loss: 0.130008, mae: 25.872458, mean_q: -36.730189, mean_eps: 0.100000\n",
      " 346317/500000: episode: 2463, duration: 0.746s, episode steps: 154, steps per second: 206, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.133733, mae: 26.661405, mean_q: -37.742418, mean_eps: 0.100000\n",
      " 346417/500000: episode: 2464, duration: 0.470s, episode steps: 100, steps per second: 213, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.084028, mae: 26.234759, mean_q: -37.118275, mean_eps: 0.100000\n",
      " 346572/500000: episode: 2465, duration: 0.788s, episode steps: 155, steps per second: 197, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.156489, mae: 26.735765, mean_q: -37.702254, mean_eps: 0.100000\n",
      " 346772/500000: episode: 2466, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.138114, mae: 27.439540, mean_q: -38.282396, mean_eps: 0.100000\n",
      " 346884/500000: episode: 2467, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.180090, mae: 27.739495, mean_q: -38.500338, mean_eps: 0.100000\n",
      " 347037/500000: episode: 2468, duration: 0.732s, episode steps: 153, steps per second: 209, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.122038, mae: 27.835738, mean_q: -38.761265, mean_eps: 0.100000\n",
      " 347156/500000: episode: 2469, duration: 0.564s, episode steps: 119, steps per second: 211, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.118645, mae: 26.915569, mean_q: -37.619116, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 347277/500000: episode: 2470, duration: 0.562s, episode steps: 121, steps per second: 215, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.463 [0.000, 2.000],  loss: 0.133515, mae: 27.072839, mean_q: -37.569212, mean_eps: 0.100000\n",
      " 347460/500000: episode: 2471, duration: 0.862s, episode steps: 183, steps per second: 212, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.169 [0.000, 2.000],  loss: 0.144819, mae: 27.629570, mean_q: -38.239246, mean_eps: 0.100000\n",
      " 347578/500000: episode: 2472, duration: 0.550s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.115886, mae: 27.115892, mean_q: -37.863079, mean_eps: 0.100000\n",
      " 347695/500000: episode: 2473, duration: 0.546s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.086731, mae: 26.718111, mean_q: -37.574067, mean_eps: 0.100000\n",
      " 347805/500000: episode: 2474, duration: 0.516s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.077921, mae: 26.340984, mean_q: -37.076237, mean_eps: 0.100000\n",
      " 347931/500000: episode: 2475, duration: 0.594s, episode steps: 126, steps per second: 212, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.071088, mae: 26.515561, mean_q: -37.482585, mean_eps: 0.100000\n",
      " 348074/500000: episode: 2476, duration: 0.668s, episode steps: 143, steps per second: 214, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.058626, mae: 26.271747, mean_q: -37.424058, mean_eps: 0.100000\n",
      " 348198/500000: episode: 2477, duration: 0.573s, episode steps: 124, steps per second: 216, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.403 [0.000, 2.000],  loss: 0.047196, mae: 26.785157, mean_q: -38.429639, mean_eps: 0.100000\n",
      " 348398/500000: episode: 2478, duration: 0.953s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.060275, mae: 26.639236, mean_q: -38.373604, mean_eps: 0.100000\n",
      " 348573/500000: episode: 2479, duration: 0.838s, episode steps: 175, steps per second: 209, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.147572, mae: 26.555765, mean_q: -38.145559, mean_eps: 0.100000\n",
      " 348773/500000: episode: 2480, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.126774, mae: 26.670385, mean_q: -38.196498, mean_eps: 0.100000\n",
      " 348962/500000: episode: 2481, duration: 0.907s, episode steps: 189, steps per second: 208, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.152456, mae: 27.592099, mean_q: -39.367460, mean_eps: 0.100000\n",
      " 349137/500000: episode: 2482, duration: 0.837s, episode steps: 175, steps per second: 209, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.140530, mae: 27.790910, mean_q: -39.563452, mean_eps: 0.100000\n",
      " 349263/500000: episode: 2483, duration: 0.608s, episode steps: 126, steps per second: 207, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.492 [0.000, 2.000],  loss: 0.171293, mae: 27.793482, mean_q: -39.628006, mean_eps: 0.100000\n",
      " 349437/500000: episode: 2484, duration: 0.815s, episode steps: 174, steps per second: 213, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 0.131724, mae: 28.125074, mean_q: -40.122207, mean_eps: 0.100000\n",
      " 349636/500000: episode: 2485, duration: 0.947s, episode steps: 199, steps per second: 210, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.138392, mae: 28.606371, mean_q: -40.961479, mean_eps: 0.100000\n",
      " 349769/500000: episode: 2486, duration: 0.611s, episode steps: 133, steps per second: 218, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.186746, mae: 28.512918, mean_q: -40.869807, mean_eps: 0.100000\n",
      " 349881/500000: episode: 2487, duration: 0.523s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.411 [0.000, 2.000],  loss: 0.190131, mae: 28.077665, mean_q: -40.228038, mean_eps: 0.100000\n",
      " 350031/500000: episode: 2488, duration: 0.714s, episode steps: 150, steps per second: 210, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.122573, mae: 27.800189, mean_q: -39.898872, mean_eps: 0.100000\n",
      " 350205/500000: episode: 2489, duration: 0.847s, episode steps: 174, steps per second: 205, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.143297, mae: 27.838206, mean_q: -40.029837, mean_eps: 0.100000\n",
      " 350366/500000: episode: 2490, duration: 0.790s, episode steps: 161, steps per second: 204, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.160030, mae: 27.724804, mean_q: -39.952209, mean_eps: 0.100000\n",
      " 350561/500000: episode: 2491, duration: 0.925s, episode steps: 195, steps per second: 211, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.207223, mae: 27.600547, mean_q: -39.603458, mean_eps: 0.100000\n",
      " 350680/500000: episode: 2492, duration: 0.548s, episode steps: 119, steps per second: 217, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000],  loss: 0.146246, mae: 27.314059, mean_q: -39.109323, mean_eps: 0.100000\n",
      " 350880/500000: episode: 2493, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.093536, mae: 27.926923, mean_q: -40.088515, mean_eps: 0.100000\n",
      " 350997/500000: episode: 2494, duration: 0.545s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.376 [0.000, 2.000],  loss: 0.165114, mae: 27.955182, mean_q: -40.091932, mean_eps: 0.100000\n",
      " 351109/500000: episode: 2495, duration: 0.522s, episode steps: 112, steps per second: 215, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.100071, mae: 27.878176, mean_q: -39.948307, mean_eps: 0.100000\n",
      " 351278/500000: episode: 2496, duration: 0.802s, episode steps: 169, steps per second: 211, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.864 [0.000, 2.000],  loss: 0.161693, mae: 28.518959, mean_q: -40.770616, mean_eps: 0.100000\n",
      " 351369/500000: episode: 2497, duration: 0.431s, episode steps:  91, steps per second: 211, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.084133, mae: 28.581946, mean_q: -40.771751, mean_eps: 0.100000\n",
      " 351479/500000: episode: 2498, duration: 0.513s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.119684, mae: 27.813031, mean_q: -39.592219, mean_eps: 0.100000\n",
      " 351659/500000: episode: 2499, duration: 0.845s, episode steps: 180, steps per second: 213, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.111 [0.000, 2.000],  loss: 0.102700, mae: 28.189625, mean_q: -40.006133, mean_eps: 0.100000\n",
      " 351776/500000: episode: 2500, duration: 0.546s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.083907, mae: 27.931940, mean_q: -39.681140, mean_eps: 0.100000\n",
      " 351866/500000: episode: 2501, duration: 0.414s, episode steps:  90, steps per second: 218, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.073372, mae: 26.831383, mean_q: -38.078901, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 352044/500000: episode: 2502, duration: 0.827s, episode steps: 178, steps per second: 215, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.065121, mae: 27.740439, mean_q: -39.400682, mean_eps: 0.100000\n",
      " 352135/500000: episode: 2503, duration: 0.437s, episode steps:  91, steps per second: 208, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.058875, mae: 27.777898, mean_q: -39.533768, mean_eps: 0.100000\n",
      " 352275/500000: episode: 2504, duration: 0.654s, episode steps: 140, steps per second: 214, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.108627, mae: 26.912359, mean_q: -38.137982, mean_eps: 0.100000\n",
      " 352389/500000: episode: 2505, duration: 0.537s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.165964, mae: 28.000867, mean_q: -39.587044, mean_eps: 0.100000\n",
      " 352546/500000: episode: 2506, duration: 0.746s, episode steps: 157, steps per second: 210, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.140099, mae: 28.112095, mean_q: -39.532894, mean_eps: 0.100000\n",
      " 352664/500000: episode: 2507, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.116207, mae: 28.122687, mean_q: -39.744654, mean_eps: 0.100000\n",
      " 352824/500000: episode: 2508, duration: 0.743s, episode steps: 160, steps per second: 215, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.120749, mae: 28.364426, mean_q: -39.645611, mean_eps: 0.100000\n",
      " 353014/500000: episode: 2509, duration: 0.900s, episode steps: 190, steps per second: 211, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.229817, mae: 28.367200, mean_q: -39.561250, mean_eps: 0.100000\n",
      " 353125/500000: episode: 2510, duration: 0.518s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.125603, mae: 27.836093, mean_q: -38.894783, mean_eps: 0.100000\n",
      " 353242/500000: episode: 2511, duration: 0.552s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.359 [0.000, 2.000],  loss: 0.073218, mae: 27.982104, mean_q: -39.248333, mean_eps: 0.100000\n",
      " 353423/500000: episode: 2512, duration: 0.864s, episode steps: 181, steps per second: 209, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.119431, mae: 28.084853, mean_q: -39.490085, mean_eps: 0.100000\n",
      " 353551/500000: episode: 2513, duration: 0.595s, episode steps: 128, steps per second: 215, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.125879, mae: 27.559862, mean_q: -38.769595, mean_eps: 0.100000\n",
      " 353751/500000: episode: 2514, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.232477, mae: 27.945237, mean_q: -39.060954, mean_eps: 0.100000\n",
      " 353919/500000: episode: 2515, duration: 0.794s, episode steps: 168, steps per second: 212, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.179062, mae: 28.743839, mean_q: -40.387839, mean_eps: 0.100000\n",
      " 354034/500000: episode: 2516, duration: 0.567s, episode steps: 115, steps per second: 203, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.169317, mae: 28.625010, mean_q: -40.354230, mean_eps: 0.100000\n",
      " 354188/500000: episode: 2517, duration: 0.761s, episode steps: 154, steps per second: 202, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000],  loss: 0.163531, mae: 29.785695, mean_q: -41.739531, mean_eps: 0.100000\n",
      " 354281/500000: episode: 2518, duration: 0.500s, episode steps:  93, steps per second: 186, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.169079, mae: 29.632709, mean_q: -41.614643, mean_eps: 0.100000\n",
      " 354437/500000: episode: 2519, duration: 0.815s, episode steps: 156, steps per second: 191, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.808 [0.000, 2.000],  loss: 0.156741, mae: 29.801974, mean_q: -41.939117, mean_eps: 0.100000\n",
      " 354633/500000: episode: 2520, duration: 0.992s, episode steps: 196, steps per second: 198, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.206703, mae: 29.958191, mean_q: -42.371665, mean_eps: 0.100000\n",
      " 354763/500000: episode: 2521, duration: 0.620s, episode steps: 130, steps per second: 210, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.122102, mae: 29.620728, mean_q: -42.072558, mean_eps: 0.100000\n",
      " 354924/500000: episode: 2522, duration: 0.771s, episode steps: 161, steps per second: 209, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 0.111635, mae: 28.777992, mean_q: -40.435978, mean_eps: 0.100000\n",
      " 355066/500000: episode: 2523, duration: 0.693s, episode steps: 142, steps per second: 205, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.131700, mae: 28.484785, mean_q: -39.876192, mean_eps: 0.100000\n",
      " 355244/500000: episode: 2524, duration: 0.837s, episode steps: 178, steps per second: 213, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.162101, mae: 28.342808, mean_q: -39.571639, mean_eps: 0.100000\n",
      " 355357/500000: episode: 2525, duration: 0.547s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.178545, mae: 27.746915, mean_q: -39.033979, mean_eps: 0.100000\n",
      " 355509/500000: episode: 2526, duration: 0.755s, episode steps: 152, steps per second: 201, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.158261, mae: 27.490381, mean_q: -38.550160, mean_eps: 0.100000\n",
      " 355644/500000: episode: 2527, duration: 0.625s, episode steps: 135, steps per second: 216, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.156418, mae: 27.671516, mean_q: -38.960730, mean_eps: 0.100000\n",
      " 355736/500000: episode: 2528, duration: 0.427s, episode steps:  92, steps per second: 216, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.137630, mae: 27.225810, mean_q: -38.077756, mean_eps: 0.100000\n",
      " 355847/500000: episode: 2529, duration: 0.532s, episode steps: 111, steps per second: 209, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.137460, mae: 26.863591, mean_q: -37.810443, mean_eps: 0.100000\n",
      " 356002/500000: episode: 2530, duration: 0.718s, episode steps: 155, steps per second: 216, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000],  loss: 0.096907, mae: 27.364516, mean_q: -38.628274, mean_eps: 0.100000\n",
      " 356118/500000: episode: 2531, duration: 0.531s, episode steps: 116, steps per second: 219, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.069125, mae: 27.149029, mean_q: -38.680274, mean_eps: 0.100000\n",
      " 356284/500000: episode: 2532, duration: 0.794s, episode steps: 166, steps per second: 209, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.256206, mae: 27.214894, mean_q: -38.514781, mean_eps: 0.100000\n",
      " 356400/500000: episode: 2533, duration: 0.543s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.164886, mae: 27.137119, mean_q: -38.328975, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 356549/500000: episode: 2534, duration: 0.702s, episode steps: 149, steps per second: 212, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.151174, mae: 27.091111, mean_q: -38.214739, mean_eps: 0.100000\n",
      " 356637/500000: episode: 2535, duration: 0.423s, episode steps:  88, steps per second: 208, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.099493, mae: 27.085444, mean_q: -38.330415, mean_eps: 0.100000\n",
      " 356755/500000: episode: 2536, duration: 0.564s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 0.099321, mae: 27.394436, mean_q: -38.836770, mean_eps: 0.100000\n",
      " 356875/500000: episode: 2537, duration: 0.567s, episode steps: 120, steps per second: 212, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.069922, mae: 27.153294, mean_q: -38.528695, mean_eps: 0.100000\n",
      " 357023/500000: episode: 2538, duration: 0.703s, episode steps: 148, steps per second: 211, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.399 [0.000, 2.000],  loss: 0.083940, mae: 26.411886, mean_q: -37.430787, mean_eps: 0.100000\n",
      " 357131/500000: episode: 2539, duration: 0.525s, episode steps: 108, steps per second: 206, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.084141, mae: 26.715193, mean_q: -37.666709, mean_eps: 0.100000\n",
      " 357322/500000: episode: 2540, duration: 0.897s, episode steps: 191, steps per second: 213, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.095299, mae: 27.075725, mean_q: -38.273379, mean_eps: 0.100000\n",
      " 357500/500000: episode: 2541, duration: 0.843s, episode steps: 178, steps per second: 211, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.104840, mae: 27.168613, mean_q: -38.419839, mean_eps: 0.100000\n",
      " 357623/500000: episode: 2542, duration: 0.576s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 0.124682, mae: 26.874724, mean_q: -37.896243, mean_eps: 0.100000\n",
      " 357762/500000: episode: 2543, duration: 0.656s, episode steps: 139, steps per second: 212, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 0.117301, mae: 27.041211, mean_q: -38.203655, mean_eps: 0.100000\n",
      " 357880/500000: episode: 2544, duration: 0.550s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.100432, mae: 27.140402, mean_q: -38.147716, mean_eps: 0.100000\n",
      " 357995/500000: episode: 2545, duration: 0.548s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.104166, mae: 27.155977, mean_q: -38.209115, mean_eps: 0.100000\n",
      " 358159/500000: episode: 2546, duration: 0.756s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000],  loss: 0.067321, mae: 27.132674, mean_q: -38.323687, mean_eps: 0.100000\n",
      " 358274/500000: episode: 2547, duration: 0.560s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.128323, mae: 26.069866, mean_q: -36.933711, mean_eps: 0.100000\n",
      " 358392/500000: episode: 2548, duration: 0.571s, episode steps: 118, steps per second: 207, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.061322, mae: 25.946647, mean_q: -36.915796, mean_eps: 0.100000\n",
      " 358512/500000: episode: 2549, duration: 0.573s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.047706, mae: 26.244192, mean_q: -37.627171, mean_eps: 0.100000\n",
      " 358626/500000: episode: 2550, duration: 0.542s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.044143, mae: 25.762683, mean_q: -36.906114, mean_eps: 0.100000\n",
      " 358723/500000: episode: 2551, duration: 0.547s, episode steps:  97, steps per second: 177, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.038879, mae: 25.558038, mean_q: -36.612148, mean_eps: 0.100000\n",
      " 358846/500000: episode: 2552, duration: 0.652s, episode steps: 123, steps per second: 189, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.046814, mae: 25.373844, mean_q: -36.596115, mean_eps: 0.100000\n",
      " 359032/500000: episode: 2553, duration: 0.974s, episode steps: 186, steps per second: 191, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.064451, mae: 25.600710, mean_q: -36.857689, mean_eps: 0.100000\n",
      " 359188/500000: episode: 2554, duration: 0.774s, episode steps: 156, steps per second: 202, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.063878, mae: 25.840753, mean_q: -37.181770, mean_eps: 0.100000\n",
      " 359300/500000: episode: 2555, duration: 0.531s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.065825, mae: 26.094266, mean_q: -37.505944, mean_eps: 0.100000\n",
      " 359412/500000: episode: 2556, duration: 0.528s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.065547, mae: 26.150426, mean_q: -37.564027, mean_eps: 0.100000\n",
      " 359512/500000: episode: 2557, duration: 0.466s, episode steps: 100, steps per second: 215, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.066221, mae: 26.090152, mean_q: -37.312185, mean_eps: 0.100000\n",
      " 359624/500000: episode: 2558, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.063337, mae: 25.888676, mean_q: -36.984791, mean_eps: 0.100000\n",
      " 359743/500000: episode: 2559, duration: 0.595s, episode steps: 119, steps per second: 200, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.063191, mae: 25.807441, mean_q: -36.918909, mean_eps: 0.100000\n",
      " 359878/500000: episode: 2560, duration: 0.731s, episode steps: 135, steps per second: 185, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.076333, mae: 26.121254, mean_q: -37.350510, mean_eps: 0.100000\n",
      " 360071/500000: episode: 2561, duration: 1.034s, episode steps: 193, steps per second: 187, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.979 [0.000, 2.000],  loss: 0.097020, mae: 26.283964, mean_q: -37.458659, mean_eps: 0.100000\n",
      " 360191/500000: episode: 2562, duration: 0.626s, episode steps: 120, steps per second: 192, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.075505, mae: 25.676755, mean_q: -36.647292, mean_eps: 0.100000\n",
      " 360308/500000: episode: 2563, duration: 0.613s, episode steps: 117, steps per second: 191, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.053931, mae: 25.562161, mean_q: -36.463575, mean_eps: 0.100000\n",
      " 360416/500000: episode: 2564, duration: 0.577s, episode steps: 108, steps per second: 187, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.061644, mae: 25.626007, mean_q: -36.617276, mean_eps: 0.100000\n",
      " 360529/500000: episode: 2565, duration: 0.603s, episode steps: 113, steps per second: 187, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.035209, mae: 25.690541, mean_q: -36.877653, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 360688/500000: episode: 2566, duration: 0.904s, episode steps: 159, steps per second: 176, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.969 [0.000, 2.000],  loss: 0.132129, mae: 26.243066, mean_q: -37.514470, mean_eps: 0.100000\n",
      " 360841/500000: episode: 2567, duration: 0.859s, episode steps: 153, steps per second: 178, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.948 [0.000, 2.000],  loss: 0.148772, mae: 26.691946, mean_q: -38.153685, mean_eps: 0.100000\n",
      " 361008/500000: episode: 2568, duration: 0.831s, episode steps: 167, steps per second: 201, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.838 [0.000, 2.000],  loss: 0.117217, mae: 27.082038, mean_q: -38.961484, mean_eps: 0.100000\n",
      " 361120/500000: episode: 2569, duration: 0.550s, episode steps: 112, steps per second: 204, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.043110, mae: 27.189979, mean_q: -39.314604, mean_eps: 0.100000\n",
      " 361273/500000: episode: 2570, duration: 0.804s, episode steps: 153, steps per second: 190, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.048470, mae: 27.537384, mean_q: -39.607675, mean_eps: 0.100000\n",
      " 361379/500000: episode: 2571, duration: 0.492s, episode steps: 106, steps per second: 216, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.067920, mae: 27.773523, mean_q: -39.953815, mean_eps: 0.100000\n",
      " 361555/500000: episode: 2572, duration: 0.853s, episode steps: 176, steps per second: 206, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.086414, mae: 28.342223, mean_q: -40.614811, mean_eps: 0.100000\n",
      " 361677/500000: episode: 2573, duration: 0.581s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.072327, mae: 28.235845, mean_q: -40.532596, mean_eps: 0.100000\n",
      " 361789/500000: episode: 2574, duration: 0.551s, episode steps: 112, steps per second: 203, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.067982, mae: 28.013177, mean_q: -40.188165, mean_eps: 0.100000\n",
      " 361903/500000: episode: 2575, duration: 0.580s, episode steps: 114, steps per second: 197, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.068238, mae: 27.847232, mean_q: -40.014140, mean_eps: 0.100000\n",
      " 362003/500000: episode: 2576, duration: 0.523s, episode steps: 100, steps per second: 191, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.051941, mae: 26.499265, mean_q: -37.915592, mean_eps: 0.100000\n",
      " 362089/500000: episode: 2577, duration: 0.475s, episode steps:  86, steps per second: 181, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.059490, mae: 26.436018, mean_q: -37.685723, mean_eps: 0.100000\n",
      " 362184/500000: episode: 2578, duration: 0.525s, episode steps:  95, steps per second: 181, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.071985, mae: 26.318619, mean_q: -37.546359, mean_eps: 0.100000\n",
      " 362305/500000: episode: 2579, duration: 0.664s, episode steps: 121, steps per second: 182, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.053291, mae: 25.615122, mean_q: -36.477108, mean_eps: 0.100000\n",
      " 362415/500000: episode: 2580, duration: 0.558s, episode steps: 110, steps per second: 197, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.055810, mae: 25.877114, mean_q: -36.769265, mean_eps: 0.100000\n",
      " 362522/500000: episode: 2581, duration: 0.502s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.049012, mae: 25.094779, mean_q: -35.822165, mean_eps: 0.100000\n",
      " 362636/500000: episode: 2582, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.038603, mae: 25.045375, mean_q: -35.681875, mean_eps: 0.100000\n",
      " 362741/500000: episode: 2583, duration: 0.506s, episode steps: 105, steps per second: 208, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.056934, mae: 24.687062, mean_q: -35.271984, mean_eps: 0.100000\n",
      " 362859/500000: episode: 2584, duration: 0.543s, episode steps: 118, steps per second: 217, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.034275, mae: 25.042556, mean_q: -35.832068, mean_eps: 0.100000\n",
      " 362995/500000: episode: 2585, duration: 0.626s, episode steps: 136, steps per second: 217, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.338 [0.000, 2.000],  loss: 0.052164, mae: 25.402642, mean_q: -36.077392, mean_eps: 0.100000\n",
      " 363114/500000: episode: 2586, duration: 0.623s, episode steps: 119, steps per second: 191, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.080327, mae: 25.677774, mean_q: -36.585545, mean_eps: 0.100000\n",
      " 363227/500000: episode: 2587, duration: 0.534s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.062463, mae: 25.861124, mean_q: -37.030773, mean_eps: 0.100000\n",
      " 363371/500000: episode: 2588, duration: 0.689s, episode steps: 144, steps per second: 209, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.068611, mae: 26.256797, mean_q: -37.673425, mean_eps: 0.100000\n",
      " 363501/500000: episode: 2589, duration: 0.697s, episode steps: 130, steps per second: 186, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000],  loss: 0.118238, mae: 26.534457, mean_q: -37.569546, mean_eps: 0.100000\n",
      " 363664/500000: episode: 2590, duration: 0.823s, episode steps: 163, steps per second: 198, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.096152, mae: 26.666507, mean_q: -37.514078, mean_eps: 0.100000\n",
      " 363842/500000: episode: 2591, duration: 0.932s, episode steps: 178, steps per second: 191, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 0.135354, mae: 27.645685, mean_q: -38.982598, mean_eps: 0.100000\n",
      " 363977/500000: episode: 2592, duration: 0.702s, episode steps: 135, steps per second: 192, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000],  loss: 0.087610, mae: 27.652191, mean_q: -39.180162, mean_eps: 0.100000\n",
      " 364177/500000: episode: 2593, duration: 0.935s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.075352, mae: 27.864516, mean_q: -39.312768, mean_eps: 0.100000\n",
      " 364267/500000: episode: 2594, duration: 0.431s, episode steps:  90, steps per second: 209, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.089510, mae: 27.672569, mean_q: -38.778566, mean_eps: 0.100000\n",
      " 364445/500000: episode: 2595, duration: 0.891s, episode steps: 178, steps per second: 200, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.099276, mae: 27.765212, mean_q: -39.083979, mean_eps: 0.100000\n",
      " 364580/500000: episode: 2596, duration: 0.646s, episode steps: 135, steps per second: 209, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.452 [0.000, 2.000],  loss: 0.111926, mae: 27.745319, mean_q: -39.386789, mean_eps: 0.100000\n",
      " 364702/500000: episode: 2597, duration: 0.580s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.091045, mae: 27.149165, mean_q: -38.527011, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 364824/500000: episode: 2598, duration: 0.631s, episode steps: 122, steps per second: 193, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.093481, mae: 26.334314, mean_q: -37.238554, mean_eps: 0.100000\n",
      " 364917/500000: episode: 2599, duration: 0.499s, episode steps:  93, steps per second: 186, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.077232, mae: 25.996475, mean_q: -36.567830, mean_eps: 0.100000\n",
      " 365017/500000: episode: 2600, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.116238, mae: 26.009758, mean_q: -36.492094, mean_eps: 0.100000\n",
      " 365178/500000: episode: 2601, duration: 0.958s, episode steps: 161, steps per second: 168, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.075018, mae: 26.420751, mean_q: -37.249952, mean_eps: 0.100000\n",
      " 365334/500000: episode: 2602, duration: 0.784s, episode steps: 156, steps per second: 199, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.094197, mae: 26.851432, mean_q: -37.758729, mean_eps: 0.100000\n",
      " 365528/500000: episode: 2603, duration: 0.944s, episode steps: 194, steps per second: 205, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.076903, mae: 27.054803, mean_q: -38.047128, mean_eps: 0.100000\n",
      " 365646/500000: episode: 2604, duration: 0.584s, episode steps: 118, steps per second: 202, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.059892, mae: 27.099905, mean_q: -38.222244, mean_eps: 0.100000\n",
      " 365757/500000: episode: 2605, duration: 0.541s, episode steps: 111, steps per second: 205, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.067312, mae: 27.283717, mean_q: -38.613445, mean_eps: 0.100000\n",
      " 365910/500000: episode: 2606, duration: 0.739s, episode steps: 153, steps per second: 207, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.045348, mae: 27.616483, mean_q: -38.992356, mean_eps: 0.100000\n",
      " 366062/500000: episode: 2607, duration: 0.729s, episode steps: 152, steps per second: 209, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.096872, mae: 28.338753, mean_q: -40.263404, mean_eps: 0.100000\n",
      " 366155/500000: episode: 2608, duration: 0.482s, episode steps:  93, steps per second: 193, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.065530, mae: 28.113066, mean_q: -40.236904, mean_eps: 0.100000\n",
      " 366313/500000: episode: 2609, duration: 0.770s, episode steps: 158, steps per second: 205, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.094302, mae: 27.797527, mean_q: -39.733325, mean_eps: 0.100000\n",
      " 366489/500000: episode: 2610, duration: 0.850s, episode steps: 176, steps per second: 207, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.142246, mae: 27.716186, mean_q: -39.440638, mean_eps: 0.100000\n",
      " 366587/500000: episode: 2611, duration: 0.468s, episode steps:  98, steps per second: 210, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.084511, mae: 27.739040, mean_q: -39.487998, mean_eps: 0.100000\n",
      " 366765/500000: episode: 2612, duration: 0.844s, episode steps: 178, steps per second: 211, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.072256, mae: 27.810595, mean_q: -39.364093, mean_eps: 0.100000\n",
      " 366965/500000: episode: 2613, duration: 0.936s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.685 [0.000, 2.000],  loss: 0.112446, mae: 28.208957, mean_q: -40.127951, mean_eps: 0.100000\n",
      " 367082/500000: episode: 2614, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.509665, mae: 28.889824, mean_q: -41.077910, mean_eps: 0.100000\n",
      " 367225/500000: episode: 2615, duration: 0.668s, episode steps: 143, steps per second: 214, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.322873, mae: 29.471050, mean_q: -41.953077, mean_eps: 0.100000\n",
      " 367343/500000: episode: 2616, duration: 0.550s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000],  loss: 0.265766, mae: 29.591625, mean_q: -42.222539, mean_eps: 0.100000\n",
      " 367487/500000: episode: 2617, duration: 0.675s, episode steps: 144, steps per second: 213, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.395366, mae: 29.621299, mean_q: -42.182599, mean_eps: 0.100000\n",
      " 367643/500000: episode: 2618, duration: 0.724s, episode steps: 156, steps per second: 215, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.446420, mae: 29.388567, mean_q: -41.809838, mean_eps: 0.100000\n",
      " 367794/500000: episode: 2619, duration: 0.694s, episode steps: 151, steps per second: 218, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.365674, mae: 29.364014, mean_q: -41.892184, mean_eps: 0.100000\n",
      " 367887/500000: episode: 2620, duration: 0.437s, episode steps:  93, steps per second: 213, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.449367, mae: 28.992306, mean_q: -41.256548, mean_eps: 0.100000\n",
      " 368059/500000: episode: 2621, duration: 0.817s, episode steps: 172, steps per second: 211, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.138016, mae: 27.875483, mean_q: -39.672950, mean_eps: 0.100000\n",
      " 368165/500000: episode: 2622, duration: 0.490s, episode steps: 106, steps per second: 216, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.061200, mae: 27.845612, mean_q: -39.583337, mean_eps: 0.100000\n",
      " 368270/500000: episode: 2623, duration: 0.492s, episode steps: 105, steps per second: 214, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000],  loss: 0.057787, mae: 27.864639, mean_q: -39.777523, mean_eps: 0.100000\n",
      " 368436/500000: episode: 2624, duration: 0.797s, episode steps: 166, steps per second: 208, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.091586, mae: 28.002937, mean_q: -39.777174, mean_eps: 0.100000\n",
      " 368553/500000: episode: 2625, duration: 0.547s, episode steps: 117, steps per second: 214, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.069948, mae: 28.242309, mean_q: -40.329047, mean_eps: 0.100000\n",
      " 368651/500000: episode: 2626, duration: 0.465s, episode steps:  98, steps per second: 211, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 0.069866, mae: 28.589960, mean_q: -40.770189, mean_eps: 0.100000\n",
      " 368851/500000: episode: 2627, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.112520, mae: 27.875268, mean_q: -39.418593, mean_eps: 0.100000\n",
      " 368961/500000: episode: 2628, duration: 0.521s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.188160, mae: 27.758424, mean_q: -39.144969, mean_eps: 0.100000\n",
      " 369068/500000: episode: 2629, duration: 0.499s, episode steps: 107, steps per second: 214, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.085116, mae: 26.859272, mean_q: -37.990743, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 369179/500000: episode: 2630, duration: 0.523s, episode steps: 111, steps per second: 212, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.092566, mae: 27.131677, mean_q: -38.644305, mean_eps: 0.100000\n",
      " 369332/500000: episode: 2631, duration: 0.718s, episode steps: 153, steps per second: 213, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.079878, mae: 27.442685, mean_q: -39.130208, mean_eps: 0.100000\n",
      " 369440/500000: episode: 2632, duration: 0.498s, episode steps: 108, steps per second: 217, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000],  loss: 0.064387, mae: 27.280507, mean_q: -38.981689, mean_eps: 0.100000\n",
      " 369549/500000: episode: 2633, duration: 0.509s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.072980, mae: 26.489243, mean_q: -37.558787, mean_eps: 0.100000\n",
      " 369705/500000: episode: 2634, duration: 0.734s, episode steps: 156, steps per second: 213, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.088380, mae: 27.229586, mean_q: -38.658006, mean_eps: 0.100000\n",
      " 369813/500000: episode: 2635, duration: 0.497s, episode steps: 108, steps per second: 217, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.092596, mae: 27.241134, mean_q: -38.838135, mean_eps: 0.100000\n",
      " 369913/500000: episode: 2636, duration: 0.463s, episode steps: 100, steps per second: 216, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.051640, mae: 26.564160, mean_q: -37.920242, mean_eps: 0.100000\n",
      " 370068/500000: episode: 2637, duration: 0.732s, episode steps: 155, steps per second: 212, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.053025, mae: 27.115835, mean_q: -38.485623, mean_eps: 0.100000\n",
      " 370246/500000: episode: 2638, duration: 0.824s, episode steps: 178, steps per second: 216, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 0.091110, mae: 26.523189, mean_q: -37.428287, mean_eps: 0.100000\n",
      " 370339/500000: episode: 2639, duration: 0.436s, episode steps:  93, steps per second: 214, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.096423, mae: 26.152445, mean_q: -36.719847, mean_eps: 0.100000\n",
      " 370462/500000: episode: 2640, duration: 0.592s, episode steps: 123, steps per second: 208, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.075156, mae: 26.278246, mean_q: -36.971650, mean_eps: 0.100000\n",
      " 370578/500000: episode: 2641, duration: 0.574s, episode steps: 116, steps per second: 202, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.109648, mae: 26.364656, mean_q: -37.081683, mean_eps: 0.100000\n",
      " 370749/500000: episode: 2642, duration: 0.796s, episode steps: 171, steps per second: 215, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.187 [0.000, 2.000],  loss: 0.098343, mae: 26.760262, mean_q: -37.495410, mean_eps: 0.100000\n",
      " 370949/500000: episode: 2643, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.154898, mae: 27.426348, mean_q: -38.441496, mean_eps: 0.100000\n",
      " 371138/500000: episode: 2644, duration: 1.022s, episode steps: 189, steps per second: 185, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.298253, mae: 28.364073, mean_q: -39.684543, mean_eps: 0.100000\n",
      " 371292/500000: episode: 2645, duration: 0.830s, episode steps: 154, steps per second: 186, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.218482, mae: 28.866952, mean_q: -40.471239, mean_eps: 0.100000\n",
      " 371403/500000: episode: 2646, duration: 0.593s, episode steps: 111, steps per second: 187, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.214710, mae: 29.122997, mean_q: -40.935512, mean_eps: 0.100000\n",
      " 371553/500000: episode: 2647, duration: 0.791s, episode steps: 150, steps per second: 190, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.225028, mae: 28.852224, mean_q: -40.672828, mean_eps: 0.100000\n",
      " 371665/500000: episode: 2648, duration: 0.589s, episode steps: 112, steps per second: 190, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.175094, mae: 28.700587, mean_q: -40.946311, mean_eps: 0.100000\n",
      " 371783/500000: episode: 2649, duration: 0.612s, episode steps: 118, steps per second: 193, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.185668, mae: 28.169484, mean_q: -40.147999, mean_eps: 0.100000\n",
      " 371893/500000: episode: 2650, duration: 0.565s, episode steps: 110, steps per second: 195, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.134810, mae: 27.854963, mean_q: -39.714732, mean_eps: 0.100000\n",
      " 372024/500000: episode: 2651, duration: 0.681s, episode steps: 131, steps per second: 192, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.077931, mae: 28.165236, mean_q: -40.210783, mean_eps: 0.100000\n",
      " 372148/500000: episode: 2652, duration: 0.661s, episode steps: 124, steps per second: 188, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000],  loss: 0.055069, mae: 27.497016, mean_q: -39.341481, mean_eps: 0.100000\n",
      " 372305/500000: episode: 2653, duration: 0.819s, episode steps: 157, steps per second: 192, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.052917, mae: 27.591761, mean_q: -39.407261, mean_eps: 0.100000\n",
      " 372418/500000: episode: 2654, duration: 0.599s, episode steps: 113, steps per second: 189, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.049884, mae: 27.837078, mean_q: -39.874367, mean_eps: 0.100000\n",
      " 372532/500000: episode: 2655, duration: 0.609s, episode steps: 114, steps per second: 187, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.052336, mae: 27.511558, mean_q: -39.632648, mean_eps: 0.100000\n",
      " 372661/500000: episode: 2656, duration: 0.676s, episode steps: 129, steps per second: 191, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.055970, mae: 27.838968, mean_q: -40.126734, mean_eps: 0.100000\n",
      " 372779/500000: episode: 2657, duration: 0.649s, episode steps: 118, steps per second: 182, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.045335, mae: 27.697812, mean_q: -39.788677, mean_eps: 0.100000\n",
      " 372956/500000: episode: 2658, duration: 1.001s, episode steps: 177, steps per second: 177, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.040973, mae: 27.511056, mean_q: -39.612990, mean_eps: 0.100000\n",
      " 373112/500000: episode: 2659, duration: 0.850s, episode steps: 156, steps per second: 184, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.066177, mae: 27.294257, mean_q: -39.169062, mean_eps: 0.100000\n",
      " 373226/500000: episode: 2660, duration: 0.592s, episode steps: 114, steps per second: 193, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.073882, mae: 26.632112, mean_q: -38.219883, mean_eps: 0.100000\n",
      " 373405/500000: episode: 2661, duration: 1.020s, episode steps: 179, steps per second: 176, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.061114, mae: 26.527079, mean_q: -38.203788, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 373595/500000: episode: 2662, duration: 0.965s, episode steps: 190, steps per second: 197, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.111177, mae: 26.930463, mean_q: -38.667748, mean_eps: 0.100000\n",
      " 373795/500000: episode: 2663, duration: 0.969s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.137040, mae: 27.371262, mean_q: -39.415154, mean_eps: 0.100000\n",
      " 373892/500000: episode: 2664, duration: 0.528s, episode steps:  97, steps per second: 184, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.173531, mae: 27.269722, mean_q: -39.173738, mean_eps: 0.100000\n",
      " 374070/500000: episode: 2665, duration: 0.890s, episode steps: 178, steps per second: 200, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.200745, mae: 27.661583, mean_q: -39.644766, mean_eps: 0.100000\n",
      " 374196/500000: episode: 2666, duration: 0.645s, episode steps: 126, steps per second: 195, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.429 [0.000, 2.000],  loss: 0.139511, mae: 27.843934, mean_q: -39.994331, mean_eps: 0.100000\n",
      " 374314/500000: episode: 2667, duration: 0.598s, episode steps: 118, steps per second: 197, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000],  loss: 0.171040, mae: 28.029014, mean_q: -39.954422, mean_eps: 0.100000\n",
      " 374439/500000: episode: 2668, duration: 0.658s, episode steps: 125, steps per second: 190, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.130474, mae: 27.558866, mean_q: -39.306860, mean_eps: 0.100000\n",
      " 374617/500000: episode: 2669, duration: 0.953s, episode steps: 178, steps per second: 187, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.129810, mae: 27.346575, mean_q: -38.969249, mean_eps: 0.100000\n",
      " 374735/500000: episode: 2670, duration: 0.559s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.110253, mae: 27.108227, mean_q: -38.623355, mean_eps: 0.100000\n",
      " 374892/500000: episode: 2671, duration: 0.736s, episode steps: 157, steps per second: 213, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.082923, mae: 27.278194, mean_q: -38.815563, mean_eps: 0.100000\n",
      " 375077/500000: episode: 2672, duration: 0.916s, episode steps: 185, steps per second: 202, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.189650, mae: 27.454769, mean_q: -38.746829, mean_eps: 0.100000\n",
      " 375227/500000: episode: 2673, duration: 0.775s, episode steps: 150, steps per second: 194, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.130027, mae: 28.053984, mean_q: -39.900930, mean_eps: 0.100000\n",
      " 375380/500000: episode: 2674, duration: 0.763s, episode steps: 153, steps per second: 200, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.190711, mae: 28.696108, mean_q: -40.940389, mean_eps: 0.100000\n",
      " 375506/500000: episode: 2675, duration: 0.689s, episode steps: 126, steps per second: 183, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.141259, mae: 28.724393, mean_q: -40.999318, mean_eps: 0.100000\n",
      " 375669/500000: episode: 2676, duration: 0.784s, episode steps: 163, steps per second: 208, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.173761, mae: 29.442927, mean_q: -41.607244, mean_eps: 0.100000\n",
      " 375821/500000: episode: 2677, duration: 0.794s, episode steps: 152, steps per second: 192, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000],  loss: 0.154850, mae: 29.480022, mean_q: -41.701450, mean_eps: 0.100000\n",
      " 375994/500000: episode: 2678, duration: 0.893s, episode steps: 173, steps per second: 194, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.159278, mae: 29.786343, mean_q: -42.517988, mean_eps: 0.100000\n",
      " 376146/500000: episode: 2679, duration: 0.763s, episode steps: 152, steps per second: 199, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 0.121072, mae: 30.434741, mean_q: -43.521313, mean_eps: 0.100000\n",
      " 376318/500000: episode: 2680, duration: 0.883s, episode steps: 172, steps per second: 195, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.131636, mae: 29.912849, mean_q: -42.706286, mean_eps: 0.100000\n",
      " 376485/500000: episode: 2681, duration: 0.849s, episode steps: 167, steps per second: 197, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.086829, mae: 29.549116, mean_q: -42.088821, mean_eps: 0.100000\n",
      " 376680/500000: episode: 2682, duration: 0.962s, episode steps: 195, steps per second: 203, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.092882, mae: 29.854545, mean_q: -42.824126, mean_eps: 0.100000\n",
      " 376799/500000: episode: 2683, duration: 0.588s, episode steps: 119, steps per second: 202, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.378 [0.000, 2.000],  loss: 0.102661, mae: 29.415370, mean_q: -42.197556, mean_eps: 0.100000\n",
      " 376990/500000: episode: 2684, duration: 0.890s, episode steps: 191, steps per second: 215, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.159590, mae: 29.291551, mean_q: -41.822652, mean_eps: 0.100000\n",
      " 377146/500000: episode: 2685, duration: 0.735s, episode steps: 156, steps per second: 212, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.141638, mae: 28.931252, mean_q: -41.246291, mean_eps: 0.100000\n",
      " 377298/500000: episode: 2686, duration: 0.711s, episode steps: 152, steps per second: 214, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.212585, mae: 28.980118, mean_q: -41.291157, mean_eps: 0.100000\n",
      " 377428/500000: episode: 2687, duration: 0.594s, episode steps: 130, steps per second: 219, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.102882, mae: 28.675784, mean_q: -41.045186, mean_eps: 0.100000\n",
      " 377551/500000: episode: 2688, duration: 0.578s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 0.112696, mae: 28.423232, mean_q: -40.538890, mean_eps: 0.100000\n",
      " 377681/500000: episode: 2689, duration: 0.607s, episode steps: 130, steps per second: 214, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.095381, mae: 27.786501, mean_q: -39.561251, mean_eps: 0.100000\n",
      " 377798/500000: episode: 2690, duration: 0.538s, episode steps: 117, steps per second: 218, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.129237, mae: 27.670772, mean_q: -39.333439, mean_eps: 0.100000\n",
      " 377955/500000: episode: 2691, duration: 0.847s, episode steps: 157, steps per second: 185, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.106242, mae: 27.628310, mean_q: -39.365307, mean_eps: 0.100000\n",
      " 378150/500000: episode: 2692, duration: 1.030s, episode steps: 195, steps per second: 189, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.084783, mae: 27.397363, mean_q: -39.147983, mean_eps: 0.100000\n",
      " 378267/500000: episode: 2693, duration: 0.621s, episode steps: 117, steps per second: 188, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.087699, mae: 26.377532, mean_q: -37.683393, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 378402/500000: episode: 2694, duration: 0.719s, episode steps: 135, steps per second: 188, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 0.071103, mae: 26.378446, mean_q: -37.549429, mean_eps: 0.100000\n",
      " 378602/500000: episode: 2695, duration: 1.019s, episode steps: 200, steps per second: 196, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.114424, mae: 27.400348, mean_q: -39.020143, mean_eps: 0.100000\n",
      " 378713/500000: episode: 2696, duration: 0.539s, episode steps: 111, steps per second: 206, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.402182, mae: 27.620175, mean_q: -39.145453, mean_eps: 0.100000\n",
      " 378824/500000: episode: 2697, duration: 0.631s, episode steps: 111, steps per second: 176, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.284744, mae: 27.730744, mean_q: -39.376023, mean_eps: 0.100000\n",
      " 378944/500000: episode: 2698, duration: 0.560s, episode steps: 120, steps per second: 214, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.283408, mae: 27.244630, mean_q: -38.861339, mean_eps: 0.100000\n",
      " 379057/500000: episode: 2699, duration: 0.585s, episode steps: 113, steps per second: 193, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000],  loss: 0.257888, mae: 26.860863, mean_q: -38.257215, mean_eps: 0.100000\n",
      " 379170/500000: episode: 2700, duration: 0.557s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.127527, mae: 27.224460, mean_q: -39.029987, mean_eps: 0.100000\n",
      " 379264/500000: episode: 2701, duration: 0.473s, episode steps:  94, steps per second: 199, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.214784, mae: 26.840849, mean_q: -38.422646, mean_eps: 0.100000\n",
      " 379380/500000: episode: 2702, duration: 0.585s, episode steps: 116, steps per second: 198, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.148005, mae: 26.338711, mean_q: -37.676633, mean_eps: 0.100000\n",
      " 379580/500000: episode: 2703, duration: 0.956s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.155914, mae: 26.439502, mean_q: -37.970450, mean_eps: 0.100000\n",
      " 379685/500000: episode: 2704, duration: 0.490s, episode steps: 105, steps per second: 214, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 1.154356, mae: 26.907460, mean_q: -38.700363, mean_eps: 0.100000\n",
      " 379786/500000: episode: 2705, duration: 0.539s, episode steps: 101, steps per second: 187, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 1.042502, mae: 27.084877, mean_q: -38.626866, mean_eps: 0.100000\n",
      " 379903/500000: episode: 2706, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.737119, mae: 26.938843, mean_q: -38.476054, mean_eps: 0.100000\n",
      " 380066/500000: episode: 2707, duration: 0.854s, episode steps: 163, steps per second: 191, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.859 [0.000, 2.000],  loss: 0.439188, mae: 26.952988, mean_q: -38.531300, mean_eps: 0.100000\n",
      " 380181/500000: episode: 2708, duration: 0.535s, episode steps: 115, steps per second: 215, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.740648, mae: 27.159561, mean_q: -38.763117, mean_eps: 0.100000\n",
      " 380298/500000: episode: 2709, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.723120, mae: 27.114721, mean_q: -38.846772, mean_eps: 0.100000\n",
      " 380414/500000: episode: 2710, duration: 0.551s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.670052, mae: 27.421174, mean_q: -39.303754, mean_eps: 0.100000\n",
      " 380521/500000: episode: 2711, duration: 0.501s, episode steps: 107, steps per second: 214, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.390645, mae: 26.325262, mean_q: -37.740684, mean_eps: 0.100000\n",
      " 380615/500000: episode: 2712, duration: 0.506s, episode steps:  94, steps per second: 186, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.698494, mae: 25.370415, mean_q: -36.263230, mean_eps: 0.100000\n",
      " 380730/500000: episode: 2713, duration: 0.608s, episode steps: 115, steps per second: 189, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.135114, mae: 25.376049, mean_q: -36.692929, mean_eps: 0.100000\n",
      " 380860/500000: episode: 2714, duration: 0.628s, episode steps: 130, steps per second: 207, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000],  loss: 0.086433, mae: 25.288142, mean_q: -36.559090, mean_eps: 0.100000\n",
      " 380975/500000: episode: 2715, duration: 0.576s, episode steps: 115, steps per second: 200, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.112913, mae: 25.453077, mean_q: -36.598769, mean_eps: 0.100000\n",
      " 381103/500000: episode: 2716, duration: 0.661s, episode steps: 128, steps per second: 194, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.067045, mae: 24.980877, mean_q: -35.838296, mean_eps: 0.100000\n",
      " 381222/500000: episode: 2717, duration: 0.566s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.055381, mae: 24.805551, mean_q: -35.592749, mean_eps: 0.100000\n",
      " 381393/500000: episode: 2718, duration: 0.867s, episode steps: 171, steps per second: 197, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000],  loss: 0.072718, mae: 24.982911, mean_q: -35.694852, mean_eps: 0.100000\n",
      " 381512/500000: episode: 2719, duration: 0.572s, episode steps: 119, steps per second: 208, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.071139, mae: 25.047984, mean_q: -35.785113, mean_eps: 0.100000\n",
      " 381628/500000: episode: 2720, duration: 0.614s, episode steps: 116, steps per second: 189, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.073620, mae: 25.194646, mean_q: -36.086373, mean_eps: 0.100000\n",
      " 381828/500000: episode: 2721, duration: 0.987s, episode steps: 200, steps per second: 203, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.114175, mae: 25.389320, mean_q: -36.241311, mean_eps: 0.100000\n",
      " 381921/500000: episode: 2722, duration: 0.485s, episode steps:  93, steps per second: 192, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.273064, mae: 25.230132, mean_q: -36.072301, mean_eps: 0.100000\n",
      " 382040/500000: episode: 2723, duration: 0.581s, episode steps: 119, steps per second: 205, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.303660, mae: 24.982060, mean_q: -35.789272, mean_eps: 0.100000\n",
      " 382168/500000: episode: 2724, duration: 0.610s, episode steps: 128, steps per second: 210, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.273987, mae: 25.018899, mean_q: -36.007198, mean_eps: 0.100000\n",
      " 382263/500000: episode: 2725, duration: 0.549s, episode steps:  95, steps per second: 173, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.303402, mae: 24.819857, mean_q: -35.570116, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 382362/500000: episode: 2726, duration: 0.511s, episode steps:  99, steps per second: 194, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000],  loss: 0.164790, mae: 24.430222, mean_q: -35.053970, mean_eps: 0.100000\n",
      " 382544/500000: episode: 2727, duration: 0.878s, episode steps: 182, steps per second: 207, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.159235, mae: 25.026647, mean_q: -35.719449, mean_eps: 0.100000\n",
      " 382739/500000: episode: 2728, duration: 0.977s, episode steps: 195, steps per second: 200, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.229855, mae: 24.752045, mean_q: -35.070916, mean_eps: 0.100000\n",
      " 382833/500000: episode: 2729, duration: 0.459s, episode steps:  94, steps per second: 205, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.163570, mae: 24.329358, mean_q: -34.506539, mean_eps: 0.100000\n",
      " 382986/500000: episode: 2730, duration: 0.710s, episode steps: 153, steps per second: 215, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.902 [0.000, 2.000],  loss: 0.130602, mae: 25.063442, mean_q: -35.505792, mean_eps: 0.100000\n",
      " 383141/500000: episode: 2731, duration: 0.739s, episode steps: 155, steps per second: 210, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.116124, mae: 25.631419, mean_q: -36.255342, mean_eps: 0.100000\n",
      " 383259/500000: episode: 2732, duration: 0.603s, episode steps: 118, steps per second: 196, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.157485, mae: 25.741952, mean_q: -36.662607, mean_eps: 0.100000\n",
      " 383418/500000: episode: 2733, duration: 0.762s, episode steps: 159, steps per second: 209, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.127810, mae: 25.975744, mean_q: -37.204283, mean_eps: 0.100000\n",
      " 383538/500000: episode: 2734, duration: 0.568s, episode steps: 120, steps per second: 211, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.149353, mae: 25.602296, mean_q: -36.831755, mean_eps: 0.100000\n",
      " 383650/500000: episode: 2735, duration: 0.537s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.081737, mae: 25.502354, mean_q: -36.718324, mean_eps: 0.100000\n",
      " 383776/500000: episode: 2736, duration: 0.594s, episode steps: 126, steps per second: 212, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.068548, mae: 26.380632, mean_q: -37.993536, mean_eps: 0.100000\n",
      " 383898/500000: episode: 2737, duration: 0.561s, episode steps: 122, steps per second: 218, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.094233, mae: 26.626101, mean_q: -38.291727, mean_eps: 0.100000\n",
      " 384022/500000: episode: 2738, duration: 0.584s, episode steps: 124, steps per second: 212, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.056835, mae: 26.242045, mean_q: -37.853210, mean_eps: 0.100000\n",
      " 384160/500000: episode: 2739, duration: 0.635s, episode steps: 138, steps per second: 217, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.063777, mae: 25.724776, mean_q: -37.130637, mean_eps: 0.100000\n",
      " 384356/500000: episode: 2740, duration: 0.911s, episode steps: 196, steps per second: 215, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.145496, mae: 26.354096, mean_q: -37.824362, mean_eps: 0.100000\n",
      " 384516/500000: episode: 2741, duration: 0.757s, episode steps: 160, steps per second: 211, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000],  loss: 0.101920, mae: 26.246544, mean_q: -37.421154, mean_eps: 0.100000\n",
      " 384705/500000: episode: 2742, duration: 0.887s, episode steps: 189, steps per second: 213, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.132264, mae: 27.070476, mean_q: -38.595012, mean_eps: 0.100000\n",
      " 384817/500000: episode: 2743, duration: 0.555s, episode steps: 112, steps per second: 202, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.120345, mae: 26.693777, mean_q: -38.273499, mean_eps: 0.100000\n",
      " 384926/500000: episode: 2744, duration: 0.514s, episode steps: 109, steps per second: 212, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.105371, mae: 26.426346, mean_q: -37.829471, mean_eps: 0.100000\n",
      " 385039/500000: episode: 2745, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.098467, mae: 26.914354, mean_q: -38.645975, mean_eps: 0.100000\n",
      " 385153/500000: episode: 2746, duration: 0.541s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.077989, mae: 26.647944, mean_q: -38.404791, mean_eps: 0.100000\n",
      " 385242/500000: episode: 2747, duration: 0.433s, episode steps:  89, steps per second: 205, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.084759, mae: 26.374334, mean_q: -37.996473, mean_eps: 0.100000\n",
      " 385351/500000: episode: 2748, duration: 0.517s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.082156, mae: 25.975826, mean_q: -37.351704, mean_eps: 0.100000\n",
      " 385551/500000: episode: 2749, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.142465, mae: 26.451301, mean_q: -37.960177, mean_eps: 0.100000\n",
      " 385659/500000: episode: 2750, duration: 0.521s, episode steps: 108, steps per second: 207, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.090897, mae: 25.927851, mean_q: -37.185969, mean_eps: 0.100000\n",
      " 385777/500000: episode: 2751, duration: 0.558s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.061524, mae: 25.894786, mean_q: -37.081311, mean_eps: 0.100000\n",
      " 385937/500000: episode: 2752, duration: 0.739s, episode steps: 160, steps per second: 217, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.080648, mae: 26.576097, mean_q: -38.081382, mean_eps: 0.100000\n",
      " 386094/500000: episode: 2753, duration: 0.742s, episode steps: 157, steps per second: 212, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.097938, mae: 26.484343, mean_q: -37.883594, mean_eps: 0.100000\n",
      " 386275/500000: episode: 2754, duration: 0.879s, episode steps: 181, steps per second: 206, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.098200, mae: 27.716358, mean_q: -39.730033, mean_eps: 0.100000\n",
      " 386382/500000: episode: 2755, duration: 0.507s, episode steps: 107, steps per second: 211, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.064413, mae: 27.989576, mean_q: -40.476433, mean_eps: 0.100000\n",
      " 386500/500000: episode: 2756, duration: 0.583s, episode steps: 118, steps per second: 202, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.062181, mae: 27.473362, mean_q: -39.875547, mean_eps: 0.100000\n",
      " 386607/500000: episode: 2757, duration: 0.517s, episode steps: 107, steps per second: 207, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.054223, mae: 27.163939, mean_q: -39.509220, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 386716/500000: episode: 2758, duration: 0.520s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.051379, mae: 27.539490, mean_q: -40.188008, mean_eps: 0.100000\n",
      " 386869/500000: episode: 2759, duration: 0.749s, episode steps: 153, steps per second: 204, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.077539, mae: 27.363660, mean_q: -40.001156, mean_eps: 0.100000\n",
      " 387030/500000: episode: 2760, duration: 0.776s, episode steps: 161, steps per second: 208, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.894 [0.000, 2.000],  loss: 0.059866, mae: 27.266535, mean_q: -39.887691, mean_eps: 0.100000\n",
      " 387143/500000: episode: 2761, duration: 0.548s, episode steps: 113, steps per second: 206, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.038856, mae: 27.321679, mean_q: -40.071093, mean_eps: 0.100000\n",
      " 387299/500000: episode: 2762, duration: 0.752s, episode steps: 156, steps per second: 207, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.041338, mae: 26.908165, mean_q: -39.471463, mean_eps: 0.100000\n",
      " 387451/500000: episode: 2763, duration: 0.738s, episode steps: 152, steps per second: 206, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.035655, mae: 27.713571, mean_q: -40.649315, mean_eps: 0.100000\n",
      " 387574/500000: episode: 2764, duration: 0.579s, episode steps: 123, steps per second: 212, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.043681, mae: 27.788278, mean_q: -40.711459, mean_eps: 0.100000\n",
      " 387691/500000: episode: 2765, duration: 0.544s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.066214, mae: 28.004377, mean_q: -40.862214, mean_eps: 0.100000\n",
      " 387849/500000: episode: 2766, duration: 0.737s, episode steps: 158, steps per second: 214, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.047411, mae: 28.225104, mean_q: -41.216421, mean_eps: 0.100000\n",
      " 388003/500000: episode: 2767, duration: 0.727s, episode steps: 154, steps per second: 212, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.903 [0.000, 2.000],  loss: 0.067792, mae: 28.486182, mean_q: -41.582169, mean_eps: 0.100000\n",
      " 388112/500000: episode: 2768, duration: 0.522s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.061274, mae: 28.467664, mean_q: -41.470058, mean_eps: 0.100000\n",
      " 388228/500000: episode: 2769, duration: 0.554s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.054649, mae: 27.817169, mean_q: -40.397363, mean_eps: 0.100000\n",
      " 388390/500000: episode: 2770, duration: 0.770s, episode steps: 162, steps per second: 210, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.075488, mae: 27.864140, mean_q: -40.477632, mean_eps: 0.100000\n",
      " 388499/500000: episode: 2771, duration: 0.518s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.044584, mae: 27.974543, mean_q: -40.681824, mean_eps: 0.100000\n",
      " 388604/500000: episode: 2772, duration: 0.505s, episode steps: 105, steps per second: 208, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.049714, mae: 27.991117, mean_q: -40.879093, mean_eps: 0.100000\n",
      " 388717/500000: episode: 2773, duration: 0.527s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.048689, mae: 28.120449, mean_q: -41.099899, mean_eps: 0.100000\n",
      " 388833/500000: episode: 2774, duration: 0.556s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000],  loss: 0.042773, mae: 26.916084, mean_q: -39.280323, mean_eps: 0.100000\n",
      " 388966/500000: episode: 2775, duration: 0.743s, episode steps: 133, steps per second: 179, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.041072, mae: 27.107019, mean_q: -39.604212, mean_eps: 0.100000\n",
      " 389076/500000: episode: 2776, duration: 0.559s, episode steps: 110, steps per second: 197, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.039455, mae: 26.811440, mean_q: -39.189366, mean_eps: 0.100000\n",
      " 389186/500000: episode: 2777, duration: 0.586s, episode steps: 110, steps per second: 188, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.028778, mae: 27.085309, mean_q: -39.751945, mean_eps: 0.100000\n",
      " 389281/500000: episode: 2778, duration: 0.487s, episode steps:  95, steps per second: 195, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.030123, mae: 27.061950, mean_q: -39.639219, mean_eps: 0.100000\n",
      " 389396/500000: episode: 2779, duration: 0.553s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.024458, mae: 26.450884, mean_q: -38.653858, mean_eps: 0.100000\n",
      " 389514/500000: episode: 2780, duration: 0.551s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.021913, mae: 26.126439, mean_q: -38.054500, mean_eps: 0.100000\n",
      " 389646/500000: episode: 2781, duration: 0.601s, episode steps: 132, steps per second: 220, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.026889, mae: 26.715870, mean_q: -38.953226, mean_eps: 0.100000\n",
      " 389832/500000: episode: 2782, duration: 0.868s, episode steps: 186, steps per second: 214, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.199 [0.000, 2.000],  loss: 0.053772, mae: 26.647404, mean_q: -38.701888, mean_eps: 0.100000\n",
      " 390013/500000: episode: 2783, duration: 0.837s, episode steps: 181, steps per second: 216, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.053489, mae: 26.841777, mean_q: -38.765530, mean_eps: 0.100000\n",
      " 390139/500000: episode: 2784, duration: 0.668s, episode steps: 126, steps per second: 189, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.080981, mae: 26.789030, mean_q: -38.656950, mean_eps: 0.100000\n",
      " 390258/500000: episode: 2785, duration: 0.650s, episode steps: 119, steps per second: 183, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000],  loss: 0.082194, mae: 27.406737, mean_q: -39.575092, mean_eps: 0.100000\n",
      " 390451/500000: episode: 2786, duration: 0.986s, episode steps: 193, steps per second: 196, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.053709, mae: 27.442682, mean_q: -39.825238, mean_eps: 0.100000\n",
      " 390562/500000: episode: 2787, duration: 0.598s, episode steps: 111, steps per second: 186, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.041333, mae: 27.450365, mean_q: -40.035379, mean_eps: 0.100000\n",
      " 390675/500000: episode: 2788, duration: 0.601s, episode steps: 113, steps per second: 188, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.032819, mae: 27.570847, mean_q: -40.175707, mean_eps: 0.100000\n",
      " 390786/500000: episode: 2789, duration: 0.539s, episode steps: 111, steps per second: 206, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.036732, mae: 26.538681, mean_q: -38.755433, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 390898/500000: episode: 2790, duration: 0.557s, episode steps: 112, steps per second: 201, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.027854, mae: 26.876796, mean_q: -39.318291, mean_eps: 0.100000\n",
      " 391061/500000: episode: 2791, duration: 0.798s, episode steps: 163, steps per second: 204, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.049592, mae: 26.942690, mean_q: -39.523992, mean_eps: 0.100000\n",
      " 391192/500000: episode: 2792, duration: 0.624s, episode steps: 131, steps per second: 210, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.045275, mae: 27.245430, mean_q: -39.896995, mean_eps: 0.100000\n",
      " 391361/500000: episode: 2793, duration: 0.793s, episode steps: 169, steps per second: 213, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 0.093146, mae: 26.897614, mean_q: -39.170112, mean_eps: 0.100000\n",
      " 391532/500000: episode: 2794, duration: 0.856s, episode steps: 171, steps per second: 200, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.772 [0.000, 2.000],  loss: 0.070800, mae: 27.375641, mean_q: -39.872885, mean_eps: 0.100000\n",
      " 391723/500000: episode: 2795, duration: 0.942s, episode steps: 191, steps per second: 203, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.112610, mae: 28.116680, mean_q: -40.723034, mean_eps: 0.100000\n",
      " 391835/500000: episode: 2796, duration: 0.578s, episode steps: 112, steps per second: 194, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.074221, mae: 27.873801, mean_q: -40.407347, mean_eps: 0.100000\n",
      " 391940/500000: episode: 2797, duration: 0.494s, episode steps: 105, steps per second: 213, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.081955, mae: 28.482062, mean_q: -41.268541, mean_eps: 0.100000\n",
      " 392053/500000: episode: 2798, duration: 0.524s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.067877, mae: 27.487514, mean_q: -39.803761, mean_eps: 0.100000\n",
      " 392207/500000: episode: 2799, duration: 0.729s, episode steps: 154, steps per second: 211, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.066093, mae: 28.679960, mean_q: -41.512732, mean_eps: 0.100000\n",
      " 392378/500000: episode: 2800, duration: 0.798s, episode steps: 171, steps per second: 214, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.085614, mae: 28.628753, mean_q: -41.604902, mean_eps: 0.100000\n",
      " 392473/500000: episode: 2801, duration: 0.440s, episode steps:  95, steps per second: 216, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.059961, mae: 28.194210, mean_q: -40.897700, mean_eps: 0.100000\n",
      " 392632/500000: episode: 2802, duration: 0.739s, episode steps: 159, steps per second: 215, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.063661, mae: 27.917812, mean_q: -40.370585, mean_eps: 0.100000\n",
      " 392751/500000: episode: 2803, duration: 0.578s, episode steps: 119, steps per second: 206, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.059453, mae: 27.694102, mean_q: -40.051792, mean_eps: 0.100000\n",
      " 392869/500000: episode: 2804, duration: 0.546s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.052066, mae: 27.415856, mean_q: -39.640113, mean_eps: 0.100000\n",
      " 392986/500000: episode: 2805, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.359 [0.000, 2.000],  loss: 0.051225, mae: 27.961591, mean_q: -40.477484, mean_eps: 0.100000\n",
      " 393106/500000: episode: 2806, duration: 0.571s, episode steps: 120, steps per second: 210, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.050891, mae: 27.565236, mean_q: -39.950133, mean_eps: 0.100000\n",
      " 393216/500000: episode: 2807, duration: 0.519s, episode steps: 110, steps per second: 212, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.042267, mae: 26.944910, mean_q: -39.056274, mean_eps: 0.100000\n",
      " 393416/500000: episode: 2808, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.078102, mae: 26.280481, mean_q: -37.938209, mean_eps: 0.100000\n",
      " 393609/500000: episode: 2809, duration: 0.919s, episode steps: 193, steps per second: 210, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.088512, mae: 26.910171, mean_q: -38.906588, mean_eps: 0.100000\n",
      " 393711/500000: episode: 2810, duration: 0.508s, episode steps: 102, steps per second: 201, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.086587, mae: 26.737667, mean_q: -38.507451, mean_eps: 0.100000\n",
      " 393911/500000: episode: 2811, duration: 1.017s, episode steps: 200, steps per second: 197, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.163632, mae: 27.913704, mean_q: -40.018453, mean_eps: 0.100000\n",
      " 393998/500000: episode: 2812, duration: 0.418s, episode steps:  87, steps per second: 208, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.090175, mae: 27.668362, mean_q: -39.647153, mean_eps: 0.100000\n",
      " 394085/500000: episode: 2813, duration: 0.415s, episode steps:  87, steps per second: 210, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.082301, mae: 27.081750, mean_q: -38.743115, mean_eps: 0.100000\n",
      " 394207/500000: episode: 2814, duration: 0.581s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000],  loss: 0.085958, mae: 27.324296, mean_q: -39.046326, mean_eps: 0.100000\n",
      " 394395/500000: episode: 2815, duration: 0.905s, episode steps: 188, steps per second: 208, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.079818, mae: 27.945589, mean_q: -39.945740, mean_eps: 0.100000\n",
      " 394493/500000: episode: 2816, duration: 0.468s, episode steps:  98, steps per second: 210, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.079957, mae: 27.082575, mean_q: -38.561835, mean_eps: 0.100000\n",
      " 394662/500000: episode: 2817, duration: 0.801s, episode steps: 169, steps per second: 211, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.089 [0.000, 2.000],  loss: 0.091201, mae: 27.604526, mean_q: -39.362000, mean_eps: 0.100000\n",
      " 394818/500000: episode: 2818, duration: 0.748s, episode steps: 156, steps per second: 209, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.737 [0.000, 2.000],  loss: 0.096202, mae: 28.103604, mean_q: -40.210663, mean_eps: 0.100000\n",
      " 394978/500000: episode: 2819, duration: 0.781s, episode steps: 160, steps per second: 205, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.112790, mae: 27.821169, mean_q: -39.801926, mean_eps: 0.100000\n",
      " 395135/500000: episode: 2820, duration: 0.741s, episode steps: 157, steps per second: 212, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.064 [0.000, 2.000],  loss: 0.100321, mae: 28.941046, mean_q: -41.650703, mean_eps: 0.100000\n",
      " 395324/500000: episode: 2821, duration: 0.901s, episode steps: 189, steps per second: 210, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.099241, mae: 28.906021, mean_q: -41.618108, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 395491/500000: episode: 2822, duration: 0.806s, episode steps: 167, steps per second: 207, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.120309, mae: 28.599254, mean_q: -41.189129, mean_eps: 0.100000\n",
      " 395674/500000: episode: 2823, duration: 0.905s, episode steps: 183, steps per second: 202, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000],  loss: 0.099418, mae: 28.324999, mean_q: -40.796066, mean_eps: 0.100000\n",
      " 395793/500000: episode: 2824, duration: 0.576s, episode steps: 119, steps per second: 207, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.095832, mae: 27.501627, mean_q: -39.519641, mean_eps: 0.100000\n",
      " 395957/500000: episode: 2825, duration: 0.770s, episode steps: 164, steps per second: 213, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.095822, mae: 28.225627, mean_q: -40.306992, mean_eps: 0.100000\n",
      " 396119/500000: episode: 2826, duration: 0.770s, episode steps: 162, steps per second: 210, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.113429, mae: 28.203276, mean_q: -40.190511, mean_eps: 0.100000\n",
      " 396233/500000: episode: 2827, duration: 0.547s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.091846, mae: 28.184190, mean_q: -40.184587, mean_eps: 0.100000\n",
      " 396397/500000: episode: 2828, duration: 0.796s, episode steps: 164, steps per second: 206, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.117947, mae: 28.022269, mean_q: -39.939411, mean_eps: 0.100000\n",
      " 396558/500000: episode: 2829, duration: 0.775s, episode steps: 161, steps per second: 208, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.119598, mae: 28.420722, mean_q: -40.439378, mean_eps: 0.100000\n",
      " 396673/500000: episode: 2830, duration: 0.538s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.072308, mae: 28.025689, mean_q: -40.060201, mean_eps: 0.100000\n",
      " 396781/500000: episode: 2831, duration: 0.507s, episode steps: 108, steps per second: 213, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.068975, mae: 28.176181, mean_q: -40.263911, mean_eps: 0.100000\n",
      " 396908/500000: episode: 2832, duration: 0.607s, episode steps: 127, steps per second: 209, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.072827, mae: 27.661941, mean_q: -39.739518, mean_eps: 0.100000\n",
      " 397014/500000: episode: 2833, duration: 0.500s, episode steps: 106, steps per second: 212, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.100014, mae: 27.771889, mean_q: -39.683844, mean_eps: 0.100000\n",
      " 397128/500000: episode: 2834, duration: 0.532s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.103812, mae: 27.543234, mean_q: -39.459103, mean_eps: 0.100000\n",
      " 397235/500000: episode: 2835, duration: 0.507s, episode steps: 107, steps per second: 211, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 0.077747, mae: 27.388074, mean_q: -39.235751, mean_eps: 0.100000\n",
      " 397342/500000: episode: 2836, duration: 0.504s, episode steps: 107, steps per second: 212, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.062527, mae: 26.845647, mean_q: -38.656687, mean_eps: 0.100000\n",
      " 397456/500000: episode: 2837, duration: 0.525s, episode steps: 114, steps per second: 217, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.078738, mae: 26.943225, mean_q: -38.800623, mean_eps: 0.100000\n",
      " 397570/500000: episode: 2838, duration: 0.526s, episode steps: 114, steps per second: 217, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.056894, mae: 26.694774, mean_q: -38.359451, mean_eps: 0.100000\n",
      " 397683/500000: episode: 2839, duration: 0.537s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.049639, mae: 26.627406, mean_q: -38.015892, mean_eps: 0.100000\n",
      " 397790/500000: episode: 2840, duration: 0.508s, episode steps: 107, steps per second: 211, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.346 [0.000, 2.000],  loss: 0.063590, mae: 27.103929, mean_q: -38.649704, mean_eps: 0.100000\n",
      " 397954/500000: episode: 2841, duration: 0.761s, episode steps: 164, steps per second: 216, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000],  loss: 0.054713, mae: 27.343710, mean_q: -39.151442, mean_eps: 0.100000\n",
      " 398071/500000: episode: 2842, duration: 0.537s, episode steps: 117, steps per second: 218, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.043891, mae: 27.508681, mean_q: -39.500974, mean_eps: 0.100000\n",
      " 398178/500000: episode: 2843, duration: 0.502s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.037519, mae: 27.071797, mean_q: -38.798224, mean_eps: 0.100000\n",
      " 398291/500000: episode: 2844, duration: 0.532s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.041307, mae: 26.943145, mean_q: -38.583605, mean_eps: 0.100000\n",
      " 398407/500000: episode: 2845, duration: 0.551s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.091171, mae: 27.206679, mean_q: -38.819022, mean_eps: 0.100000\n",
      " 398518/500000: episode: 2846, duration: 0.540s, episode steps: 111, steps per second: 206, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.085085, mae: 27.561318, mean_q: -39.299185, mean_eps: 0.100000\n",
      " 398629/500000: episode: 2847, duration: 0.528s, episode steps: 111, steps per second: 210, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.099657, mae: 27.165276, mean_q: -38.595873, mean_eps: 0.100000\n",
      " 398741/500000: episode: 2848, duration: 0.550s, episode steps: 112, steps per second: 204, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.123671, mae: 27.281005, mean_q: -38.529687, mean_eps: 0.100000\n",
      " 398847/500000: episode: 2849, duration: 0.513s, episode steps: 106, steps per second: 206, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.082895, mae: 27.761587, mean_q: -39.293470, mean_eps: 0.100000\n",
      " 399024/500000: episode: 2850, duration: 0.844s, episode steps: 177, steps per second: 210, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.097992, mae: 26.964930, mean_q: -38.096835, mean_eps: 0.100000\n",
      " 399138/500000: episode: 2851, duration: 0.535s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000],  loss: 0.089627, mae: 27.182999, mean_q: -38.303475, mean_eps: 0.100000\n",
      " 399252/500000: episode: 2852, duration: 0.552s, episode steps: 114, steps per second: 207, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.076423, mae: 26.995586, mean_q: -38.086083, mean_eps: 0.100000\n",
      " 399360/500000: episode: 2853, duration: 0.528s, episode steps: 108, steps per second: 205, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.053754, mae: 27.102067, mean_q: -38.474690, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 399480/500000: episode: 2854, duration: 0.574s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.061559, mae: 26.427075, mean_q: -37.617740, mean_eps: 0.100000\n",
      " 399646/500000: episode: 2855, duration: 0.835s, episode steps: 166, steps per second: 199, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.056521, mae: 27.694621, mean_q: -39.782758, mean_eps: 0.100000\n",
      " 399839/500000: episode: 2856, duration: 0.931s, episode steps: 193, steps per second: 207, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.068776, mae: 27.827373, mean_q: -40.292186, mean_eps: 0.100000\n",
      " 399959/500000: episode: 2857, duration: 0.560s, episode steps: 120, steps per second: 214, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.059166, mae: 27.287670, mean_q: -39.565236, mean_eps: 0.100000\n",
      " 400069/500000: episode: 2858, duration: 0.524s, episode steps: 110, steps per second: 210, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.060503, mae: 27.514367, mean_q: -39.924615, mean_eps: 0.100000\n",
      " 400162/500000: episode: 2859, duration: 0.461s, episode steps:  93, steps per second: 202, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.062984, mae: 27.723793, mean_q: -40.126575, mean_eps: 0.100000\n",
      " 400285/500000: episode: 2860, duration: 0.614s, episode steps: 123, steps per second: 200, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.293 [0.000, 2.000],  loss: 0.062764, mae: 27.267155, mean_q: -39.369465, mean_eps: 0.100000\n",
      " 400399/500000: episode: 2861, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.089475, mae: 26.995586, mean_q: -38.775034, mean_eps: 0.100000\n",
      " 400533/500000: episode: 2862, duration: 0.668s, episode steps: 134, steps per second: 201, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.138485, mae: 27.169471, mean_q: -38.970383, mean_eps: 0.100000\n",
      " 400695/500000: episode: 2863, duration: 0.791s, episode steps: 162, steps per second: 205, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.152955, mae: 26.469665, mean_q: -37.536377, mean_eps: 0.100000\n",
      " 400834/500000: episode: 2864, duration: 0.768s, episode steps: 139, steps per second: 181, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.088812, mae: 26.619055, mean_q: -37.946995, mean_eps: 0.100000\n",
      " 400946/500000: episode: 2865, duration: 0.585s, episode steps: 112, steps per second: 191, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000],  loss: 0.078591, mae: 26.961682, mean_q: -38.610182, mean_eps: 0.100000\n",
      " 401066/500000: episode: 2866, duration: 0.623s, episode steps: 120, steps per second: 193, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.078232, mae: 26.673376, mean_q: -38.209481, mean_eps: 0.100000\n",
      " 401175/500000: episode: 2867, duration: 0.510s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.093526, mae: 26.945478, mean_q: -38.679219, mean_eps: 0.100000\n",
      " 401290/500000: episode: 2868, duration: 0.550s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.083440, mae: 26.890479, mean_q: -38.656650, mean_eps: 0.100000\n",
      " 401378/500000: episode: 2869, duration: 0.428s, episode steps:  88, steps per second: 205, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000],  loss: 0.098364, mae: 26.495787, mean_q: -38.050313, mean_eps: 0.100000\n",
      " 401488/500000: episode: 2870, duration: 0.530s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.072303, mae: 26.691216, mean_q: -38.340430, mean_eps: 0.100000\n",
      " 401655/500000: episode: 2871, duration: 0.827s, episode steps: 167, steps per second: 202, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.082461, mae: 26.817005, mean_q: -38.718095, mean_eps: 0.100000\n",
      " 401770/500000: episode: 2872, duration: 0.597s, episode steps: 115, steps per second: 193, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.056467, mae: 26.715374, mean_q: -38.619202, mean_eps: 0.100000\n",
      " 401970/500000: episode: 2873, duration: 0.948s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.085170, mae: 26.199707, mean_q: -37.736379, mean_eps: 0.100000\n",
      " 402078/500000: episode: 2874, duration: 0.498s, episode steps: 108, steps per second: 217, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.115949, mae: 26.628845, mean_q: -38.273734, mean_eps: 0.100000\n",
      " 402189/500000: episode: 2875, duration: 0.521s, episode steps: 111, steps per second: 213, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.097793, mae: 26.434959, mean_q: -37.824387, mean_eps: 0.100000\n",
      " 402344/500000: episode: 2876, duration: 0.720s, episode steps: 155, steps per second: 215, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.102217, mae: 27.051304, mean_q: -38.583767, mean_eps: 0.100000\n",
      " 402511/500000: episode: 2877, duration: 0.778s, episode steps: 167, steps per second: 215, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.204984, mae: 28.132633, mean_q: -40.091839, mean_eps: 0.100000\n",
      " 402625/500000: episode: 2878, duration: 0.527s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000],  loss: 0.163169, mae: 27.186092, mean_q: -38.843980, mean_eps: 0.100000\n",
      " 402736/500000: episode: 2879, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.129898, mae: 27.391104, mean_q: -39.107856, mean_eps: 0.100000\n",
      " 402853/500000: episode: 2880, duration: 0.545s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.094516, mae: 27.367090, mean_q: -39.004070, mean_eps: 0.100000\n",
      " 402971/500000: episode: 2881, duration: 0.547s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000],  loss: 0.066842, mae: 27.200453, mean_q: -38.978585, mean_eps: 0.100000\n",
      " 403094/500000: episode: 2882, duration: 0.572s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.103325, mae: 27.299233, mean_q: -39.101516, mean_eps: 0.100000\n",
      " 403211/500000: episode: 2883, duration: 0.557s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.062207, mae: 27.353500, mean_q: -39.389674, mean_eps: 0.100000\n",
      " 403323/500000: episode: 2884, duration: 0.528s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000],  loss: 0.061647, mae: 26.489094, mean_q: -38.236049, mean_eps: 0.100000\n",
      " 403440/500000: episode: 2885, duration: 0.558s, episode steps: 117, steps per second: 210, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.044565, mae: 26.397265, mean_q: -38.192540, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 403549/500000: episode: 2886, duration: 0.538s, episode steps: 109, steps per second: 203, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.035860, mae: 25.987375, mean_q: -37.583777, mean_eps: 0.100000\n",
      " 403641/500000: episode: 2887, duration: 0.454s, episode steps:  92, steps per second: 203, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.045077, mae: 25.656058, mean_q: -36.871032, mean_eps: 0.100000\n",
      " 403741/500000: episode: 2888, duration: 0.531s, episode steps: 100, steps per second: 188, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.048057, mae: 25.384803, mean_q: -36.387264, mean_eps: 0.100000\n",
      " 403855/500000: episode: 2889, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.041944, mae: 25.281575, mean_q: -36.136339, mean_eps: 0.100000\n",
      " 403967/500000: episode: 2890, duration: 0.522s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.051390, mae: 25.561360, mean_q: -36.578413, mean_eps: 0.100000\n",
      " 404079/500000: episode: 2891, duration: 0.527s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000],  loss: 0.046002, mae: 25.240364, mean_q: -36.084963, mean_eps: 0.100000\n",
      " 404193/500000: episode: 2892, duration: 0.554s, episode steps: 114, steps per second: 206, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.035434, mae: 24.944390, mean_q: -35.713399, mean_eps: 0.100000\n",
      " 404304/500000: episode: 2893, duration: 0.515s, episode steps: 111, steps per second: 215, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.051072, mae: 25.002287, mean_q: -35.869363, mean_eps: 0.100000\n",
      " 404442/500000: episode: 2894, duration: 0.645s, episode steps: 138, steps per second: 214, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000],  loss: 0.030874, mae: 25.372905, mean_q: -36.322166, mean_eps: 0.100000\n",
      " 404606/500000: episode: 2895, duration: 0.755s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.054318, mae: 25.992380, mean_q: -37.151245, mean_eps: 0.100000\n",
      " 404719/500000: episode: 2896, duration: 0.530s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.109760, mae: 26.385836, mean_q: -37.783478, mean_eps: 0.100000\n",
      " 404902/500000: episode: 2897, duration: 0.864s, episode steps: 183, steps per second: 212, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.099610, mae: 26.609622, mean_q: -38.097878, mean_eps: 0.100000\n",
      " 405070/500000: episode: 2898, duration: 0.781s, episode steps: 168, steps per second: 215, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.738 [0.000, 2.000],  loss: 0.234414, mae: 26.990156, mean_q: -38.499651, mean_eps: 0.100000\n",
      " 405185/500000: episode: 2899, duration: 0.535s, episode steps: 115, steps per second: 215, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.109434, mae: 28.012821, mean_q: -40.133630, mean_eps: 0.100000\n",
      " 405319/500000: episode: 2900, duration: 0.638s, episode steps: 134, steps per second: 210, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.158903, mae: 28.045415, mean_q: -40.061173, mean_eps: 0.100000\n",
      " 405436/500000: episode: 2901, duration: 0.549s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.077534, mae: 27.449502, mean_q: -39.242528, mean_eps: 0.100000\n",
      " 405550/500000: episode: 2902, duration: 0.577s, episode steps: 114, steps per second: 198, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.351 [0.000, 2.000],  loss: 0.086074, mae: 27.495743, mean_q: -39.476640, mean_eps: 0.100000\n",
      " 405750/500000: episode: 2903, duration: 0.931s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 0.079758, mae: 26.920462, mean_q: -38.416552, mean_eps: 0.100000\n",
      " 405868/500000: episode: 2904, duration: 0.544s, episode steps: 118, steps per second: 217, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.821264, mae: 27.154953, mean_q: -38.604576, mean_eps: 0.100000\n",
      " 406027/500000: episode: 2905, duration: 0.732s, episode steps: 159, steps per second: 217, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000],  loss: 0.575585, mae: 26.711396, mean_q: -37.929414, mean_eps: 0.100000\n",
      " 406153/500000: episode: 2906, duration: 0.595s, episode steps: 126, steps per second: 212, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.471259, mae: 25.921890, mean_q: -36.725592, mean_eps: 0.100000\n",
      " 406286/500000: episode: 2907, duration: 0.623s, episode steps: 133, steps per second: 213, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.335595, mae: 25.721226, mean_q: -36.356207, mean_eps: 0.100000\n",
      " 406475/500000: episode: 2908, duration: 0.886s, episode steps: 189, steps per second: 213, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.921 [0.000, 2.000],  loss: 0.216962, mae: 26.185169, mean_q: -36.962724, mean_eps: 0.100000\n",
      " 406655/500000: episode: 2909, duration: 0.838s, episode steps: 180, steps per second: 215, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.906 [0.000, 2.000],  loss: 0.227067, mae: 26.439228, mean_q: -37.323664, mean_eps: 0.100000\n",
      " 406855/500000: episode: 2910, duration: 0.964s, episode steps: 200, steps per second: 208, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.198290, mae: 26.597800, mean_q: -37.759120, mean_eps: 0.100000\n",
      " 407038/500000: episode: 2911, duration: 0.874s, episode steps: 183, steps per second: 209, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.913 [0.000, 2.000],  loss: 0.116786, mae: 26.547573, mean_q: -37.691923, mean_eps: 0.100000\n",
      " 407156/500000: episode: 2912, duration: 0.582s, episode steps: 118, steps per second: 203, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000],  loss: 0.097281, mae: 26.389353, mean_q: -37.630590, mean_eps: 0.100000\n",
      " 407356/500000: episode: 2913, duration: 1.071s, episode steps: 200, steps per second: 187, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.098343, mae: 27.578966, mean_q: -39.553093, mean_eps: 0.100000\n",
      " 407541/500000: episode: 2914, duration: 0.910s, episode steps: 185, steps per second: 203, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.090769, mae: 28.605234, mean_q: -41.269697, mean_eps: 0.100000\n",
      " 407661/500000: episode: 2915, duration: 0.580s, episode steps: 120, steps per second: 207, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 0.099911, mae: 27.830273, mean_q: -40.202443, mean_eps: 0.100000\n",
      " 407781/500000: episode: 2916, duration: 0.564s, episode steps: 120, steps per second: 213, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.094566, mae: 27.609964, mean_q: -39.889163, mean_eps: 0.100000\n",
      " 407898/500000: episode: 2917, duration: 0.551s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000],  loss: 0.069554, mae: 27.933741, mean_q: -40.425842, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 407992/500000: episode: 2918, duration: 0.438s, episode steps:  94, steps per second: 215, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.064011, mae: 26.900811, mean_q: -38.954108, mean_eps: 0.100000\n",
      " 408102/500000: episode: 2919, duration: 0.547s, episode steps: 110, steps per second: 201, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.051016, mae: 27.420090, mean_q: -39.686645, mean_eps: 0.100000\n",
      " 408195/500000: episode: 2920, duration: 0.451s, episode steps:  93, steps per second: 206, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.044532, mae: 27.027245, mean_q: -39.161291, mean_eps: 0.100000\n",
      " 408365/500000: episode: 2921, duration: 0.798s, episode steps: 170, steps per second: 213, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.077992, mae: 26.063854, mean_q: -37.524647, mean_eps: 0.100000\n",
      " 408475/500000: episode: 2922, duration: 0.513s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.057980, mae: 25.550816, mean_q: -36.877984, mean_eps: 0.100000\n",
      " 408614/500000: episode: 2923, duration: 0.651s, episode steps: 139, steps per second: 214, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.439 [0.000, 2.000],  loss: 0.044299, mae: 25.966957, mean_q: -37.538053, mean_eps: 0.100000\n",
      " 408724/500000: episode: 2924, duration: 0.531s, episode steps: 110, steps per second: 207, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.042078, mae: 26.134827, mean_q: -37.676057, mean_eps: 0.100000\n",
      " 408837/500000: episode: 2925, duration: 0.522s, episode steps: 113, steps per second: 216, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.055518, mae: 26.324305, mean_q: -37.950284, mean_eps: 0.100000\n",
      " 408957/500000: episode: 2926, duration: 0.562s, episode steps: 120, steps per second: 214, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.051821, mae: 26.554900, mean_q: -38.458291, mean_eps: 0.100000\n",
      " 409066/500000: episode: 2927, duration: 0.530s, episode steps: 109, steps per second: 205, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.070494, mae: 26.524822, mean_q: -38.481677, mean_eps: 0.100000\n",
      " 409266/500000: episode: 2928, duration: 0.943s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 0.074203, mae: 26.568599, mean_q: -38.634570, mean_eps: 0.100000\n",
      " 409365/500000: episode: 2929, duration: 0.486s, episode steps:  99, steps per second: 204, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.096159, mae: 25.588812, mean_q: -37.144293, mean_eps: 0.100000\n",
      " 409480/500000: episode: 2930, duration: 0.552s, episode steps: 115, steps per second: 208, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.080628, mae: 26.138554, mean_q: -37.965635, mean_eps: 0.100000\n",
      " 409579/500000: episode: 2931, duration: 0.468s, episode steps:  99, steps per second: 212, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.081793, mae: 25.630654, mean_q: -37.140068, mean_eps: 0.100000\n",
      " 409698/500000: episode: 2932, duration: 0.573s, episode steps: 119, steps per second: 208, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.064721, mae: 25.222755, mean_q: -36.669988, mean_eps: 0.100000\n",
      " 409877/500000: episode: 2933, duration: 0.872s, episode steps: 179, steps per second: 205, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000],  loss: 0.095886, mae: 25.840689, mean_q: -37.281276, mean_eps: 0.100000\n",
      " 409991/500000: episode: 2934, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.075106, mae: 25.653587, mean_q: -36.912838, mean_eps: 0.100000\n",
      " 410089/500000: episode: 2935, duration: 0.456s, episode steps:  98, steps per second: 215, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.067880, mae: 25.450347, mean_q: -36.605449, mean_eps: 0.100000\n",
      " 410181/500000: episode: 2936, duration: 0.433s, episode steps:  92, steps per second: 213, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.042765, mae: 25.344164, mean_q: -36.402206, mean_eps: 0.100000\n",
      " 410294/500000: episode: 2937, duration: 0.531s, episode steps: 113, steps per second: 213, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.039170, mae: 25.257562, mean_q: -36.197964, mean_eps: 0.100000\n",
      " 410406/500000: episode: 2938, duration: 0.536s, episode steps: 112, steps per second: 209, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.039844, mae: 25.724990, mean_q: -37.026605, mean_eps: 0.100000\n",
      " 410525/500000: episode: 2939, duration: 0.569s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000],  loss: 0.040408, mae: 25.850859, mean_q: -37.202596, mean_eps: 0.100000\n",
      " 410725/500000: episode: 2940, duration: 1.006s, episode steps: 200, steps per second: 199, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 0.037778, mae: 25.770277, mean_q: -37.054289, mean_eps: 0.100000\n",
      " 410925/500000: episode: 2941, duration: 0.983s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000],  loss: 0.299216, mae: 25.775980, mean_q: -37.083838, mean_eps: 0.100000\n",
      " 411094/500000: episode: 2942, duration: 0.807s, episode steps: 169, steps per second: 209, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.337 [0.000, 2.000],  loss: 0.570413, mae: 26.558511, mean_q: -37.974158, mean_eps: 0.100000\n",
      " 411258/500000: episode: 2943, duration: 0.804s, episode steps: 164, steps per second: 204, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.640657, mae: 27.662811, mean_q: -39.593878, mean_eps: 0.100000\n",
      " 411387/500000: episode: 2944, duration: 0.613s, episode steps: 129, steps per second: 210, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.594935, mae: 27.891764, mean_q: -39.906728, mean_eps: 0.100000\n",
      " 411503/500000: episode: 2945, duration: 0.544s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.289467, mae: 27.513793, mean_q: -39.331981, mean_eps: 0.100000\n",
      " 411618/500000: episode: 2946, duration: 0.537s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.591099, mae: 27.861871, mean_q: -39.935131, mean_eps: 0.100000\n",
      " 411792/500000: episode: 2947, duration: 0.815s, episode steps: 174, steps per second: 213, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.461319, mae: 28.382418, mean_q: -40.756818, mean_eps: 0.100000\n",
      " 411975/500000: episode: 2948, duration: 0.856s, episode steps: 183, steps per second: 214, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 0.130827, mae: 27.984498, mean_q: -40.233531, mean_eps: 0.100000\n",
      " 412152/500000: episode: 2949, duration: 0.830s, episode steps: 177, steps per second: 213, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.104181, mae: 27.744776, mean_q: -39.884662, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 412334/500000: episode: 2950, duration: 0.851s, episode steps: 182, steps per second: 214, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000],  loss: 0.081047, mae: 27.256677, mean_q: -39.085030, mean_eps: 0.100000\n",
      " 412471/500000: episode: 2951, duration: 0.652s, episode steps: 137, steps per second: 210, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.067668, mae: 27.592390, mean_q: -39.761852, mean_eps: 0.100000\n",
      " 412639/500000: episode: 2952, duration: 0.800s, episode steps: 168, steps per second: 210, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.069048, mae: 28.465171, mean_q: -41.161568, mean_eps: 0.100000\n",
      " 412755/500000: episode: 2953, duration: 0.555s, episode steps: 116, steps per second: 209, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000],  loss: 0.084681, mae: 28.143179, mean_q: -40.846957, mean_eps: 0.100000\n",
      " 412884/500000: episode: 2954, duration: 0.628s, episode steps: 129, steps per second: 206, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.052147, mae: 28.395235, mean_q: -41.349234, mean_eps: 0.100000\n",
      " 413000/500000: episode: 2955, duration: 0.543s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.047399, mae: 27.948509, mean_q: -40.678377, mean_eps: 0.100000\n",
      " 413181/500000: episode: 2956, duration: 0.841s, episode steps: 181, steps per second: 215, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000],  loss: 0.044913, mae: 27.779260, mean_q: -40.461235, mean_eps: 0.100000\n",
      " 413369/500000: episode: 2957, duration: 0.887s, episode steps: 188, steps per second: 212, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.463 [0.000, 2.000],  loss: 0.089135, mae: 27.625903, mean_q: -40.163172, mean_eps: 0.100000\n",
      " 413556/500000: episode: 2958, duration: 0.887s, episode steps: 187, steps per second: 211, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.081721, mae: 27.972148, mean_q: -40.547486, mean_eps: 0.100000\n",
      " 413732/500000: episode: 2959, duration: 0.876s, episode steps: 176, steps per second: 201, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.088142, mae: 27.279749, mean_q: -39.383903, mean_eps: 0.100000\n",
      " 413919/500000: episode: 2960, duration: 0.878s, episode steps: 187, steps per second: 213, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.100927, mae: 27.586477, mean_q: -39.752476, mean_eps: 0.100000\n",
      " 414011/500000: episode: 2961, duration: 0.446s, episode steps:  92, steps per second: 206, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.154074, mae: 27.647589, mean_q: -39.797958, mean_eps: 0.100000\n",
      " 414104/500000: episode: 2962, duration: 0.435s, episode steps:  93, steps per second: 214, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.089368, mae: 27.601668, mean_q: -39.657757, mean_eps: 0.100000\n",
      " 414221/500000: episode: 2963, duration: 0.561s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.102964, mae: 27.287313, mean_q: -39.068356, mean_eps: 0.100000\n",
      " 414381/500000: episode: 2964, duration: 0.763s, episode steps: 160, steps per second: 210, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000],  loss: 0.084412, mae: 27.174807, mean_q: -38.750918, mean_eps: 0.100000\n",
      " 414492/500000: episode: 2965, duration: 0.518s, episode steps: 111, steps per second: 214, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 0.100928, mae: 26.943604, mean_q: -38.339016, mean_eps: 0.100000\n",
      " 414660/500000: episode: 2966, duration: 0.783s, episode steps: 168, steps per second: 214, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.113269, mae: 27.377054, mean_q: -38.665884, mean_eps: 0.100000\n",
      " 414775/500000: episode: 2967, duration: 0.540s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.094990, mae: 27.126893, mean_q: -38.163482, mean_eps: 0.100000\n",
      " 414892/500000: episode: 2968, duration: 0.560s, episode steps: 117, steps per second: 209, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.114190, mae: 27.550839, mean_q: -38.658795, mean_eps: 0.100000\n",
      " 415051/500000: episode: 2969, duration: 0.739s, episode steps: 159, steps per second: 215, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.119912, mae: 27.751784, mean_q: -39.069940, mean_eps: 0.100000\n",
      " 415230/500000: episode: 2970, duration: 0.824s, episode steps: 179, steps per second: 217, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000],  loss: 0.178900, mae: 27.743250, mean_q: -39.122394, mean_eps: 0.100000\n",
      " 415403/500000: episode: 2971, duration: 0.805s, episode steps: 173, steps per second: 215, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.175614, mae: 28.017627, mean_q: -39.656571, mean_eps: 0.100000\n",
      " 415510/500000: episode: 2972, duration: 0.502s, episode steps: 107, steps per second: 213, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.128990, mae: 27.832230, mean_q: -39.600160, mean_eps: 0.100000\n",
      " 415622/500000: episode: 2973, duration: 0.529s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000],  loss: 0.107536, mae: 27.483294, mean_q: -39.345736, mean_eps: 0.100000\n",
      " 415796/500000: episode: 2974, duration: 0.849s, episode steps: 174, steps per second: 205, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.088862, mae: 27.831642, mean_q: -39.901864, mean_eps: 0.100000\n",
      " 415972/500000: episode: 2975, duration: 0.872s, episode steps: 176, steps per second: 202, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.051 [0.000, 2.000],  loss: 0.073291, mae: 28.123673, mean_q: -40.463204, mean_eps: 0.100000\n",
      " 416123/500000: episode: 2976, duration: 0.715s, episode steps: 151, steps per second: 211, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.065230, mae: 27.974601, mean_q: -40.164016, mean_eps: 0.100000\n",
      " 416318/500000: episode: 2977, duration: 0.922s, episode steps: 195, steps per second: 212, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000],  loss: 0.067058, mae: 28.436214, mean_q: -40.987115, mean_eps: 0.100000\n",
      " 416489/500000: episode: 2978, duration: 0.803s, episode steps: 171, steps per second: 213, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.084832, mae: 28.238279, mean_q: -40.588482, mean_eps: 0.100000\n",
      " 416644/500000: episode: 2979, duration: 0.725s, episode steps: 155, steps per second: 214, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.067200, mae: 28.761977, mean_q: -41.430360, mean_eps: 0.100000\n",
      " 416774/500000: episode: 2980, duration: 0.605s, episode steps: 130, steps per second: 215, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.076170, mae: 28.694790, mean_q: -41.333724, mean_eps: 0.100000\n",
      " 416886/500000: episode: 2981, duration: 0.518s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.103783, mae: 28.161711, mean_q: -40.303096, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 417054/500000: episode: 2982, duration: 0.774s, episode steps: 168, steps per second: 217, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.156053, mae: 28.480897, mean_q: -40.564562, mean_eps: 0.100000\n",
      " 417214/500000: episode: 2983, duration: 0.737s, episode steps: 160, steps per second: 217, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.125709, mae: 28.487731, mean_q: -40.521373, mean_eps: 0.100000\n",
      " 417347/500000: episode: 2984, duration: 0.610s, episode steps: 133, steps per second: 218, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.429 [0.000, 2.000],  loss: 0.102011, mae: 28.455137, mean_q: -40.617220, mean_eps: 0.100000\n",
      " 417471/500000: episode: 2985, duration: 0.584s, episode steps: 124, steps per second: 213, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.323 [0.000, 2.000],  loss: 0.128721, mae: 27.976180, mean_q: -39.980482, mean_eps: 0.100000\n",
      " 417580/500000: episode: 2986, duration: 0.505s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.110964, mae: 27.668855, mean_q: -39.586352, mean_eps: 0.100000\n",
      " 417707/500000: episode: 2987, duration: 0.581s, episode steps: 127, steps per second: 219, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.099774, mae: 27.295540, mean_q: -38.998928, mean_eps: 0.100000\n",
      " 417894/500000: episode: 2988, duration: 0.860s, episode steps: 187, steps per second: 217, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.123589, mae: 27.886072, mean_q: -39.834169, mean_eps: 0.100000\n",
      " 418055/500000: episode: 2989, duration: 0.743s, episode steps: 161, steps per second: 217, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.115220, mae: 27.664743, mean_q: -39.707836, mean_eps: 0.100000\n",
      " 418156/500000: episode: 2990, duration: 0.468s, episode steps: 101, steps per second: 216, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000],  loss: 0.088679, mae: 27.322140, mean_q: -39.249694, mean_eps: 0.100000\n",
      " 418249/500000: episode: 2991, duration: 0.434s, episode steps:  93, steps per second: 214, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000],  loss: 0.106895, mae: 27.696150, mean_q: -39.596823, mean_eps: 0.100000\n",
      " 418422/500000: episode: 2992, duration: 0.791s, episode steps: 173, steps per second: 219, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.971 [0.000, 2.000],  loss: 0.117126, mae: 27.612054, mean_q: -39.456561, mean_eps: 0.100000\n",
      " 418533/500000: episode: 2993, duration: 0.534s, episode steps: 111, steps per second: 208, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.093952, mae: 27.908276, mean_q: -40.003641, mean_eps: 0.100000\n",
      " 418646/500000: episode: 2994, duration: 0.535s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.103274, mae: 27.690195, mean_q: -39.619951, mean_eps: 0.100000\n",
      " 418834/500000: episode: 2995, duration: 0.875s, episode steps: 188, steps per second: 215, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.086108, mae: 28.074507, mean_q: -40.200903, mean_eps: 0.100000\n",
      " 418957/500000: episode: 2996, duration: 0.571s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000],  loss: 0.086280, mae: 27.554057, mean_q: -39.427404, mean_eps: 0.100000\n",
      " 419069/500000: episode: 2997, duration: 0.519s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.074503, mae: 27.471707, mean_q: -39.411618, mean_eps: 0.100000\n",
      " 419180/500000: episode: 2998, duration: 0.509s, episode steps: 111, steps per second: 218, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.064096, mae: 27.026127, mean_q: -38.894350, mean_eps: 0.100000\n",
      " 419273/500000: episode: 2999, duration: 0.432s, episode steps:  93, steps per second: 215, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.063080, mae: 27.526456, mean_q: -39.687841, mean_eps: 0.100000\n",
      " 419391/500000: episode: 3000, duration: 0.549s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000],  loss: 0.059062, mae: 26.608621, mean_q: -38.288752, mean_eps: 0.100000\n",
      " 419591/500000: episode: 3001, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.073020, mae: 26.790920, mean_q: -38.427931, mean_eps: 0.100000\n",
      " 419791/500000: episode: 3002, duration: 0.924s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.325654, mae: 26.874899, mean_q: -38.456865, mean_eps: 0.100000\n",
      " 419907/500000: episode: 3003, duration: 0.539s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.344295, mae: 26.900286, mean_q: -38.486731, mean_eps: 0.100000\n",
      " 420030/500000: episode: 3004, duration: 0.571s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.556792, mae: 26.756933, mean_q: -38.102685, mean_eps: 0.100000\n",
      " 420196/500000: episode: 3005, duration: 0.771s, episode steps: 166, steps per second: 215, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.298980, mae: 27.451933, mean_q: -39.116423, mean_eps: 0.100000\n",
      " 420281/500000: episode: 3006, duration: 0.398s, episode steps:  85, steps per second: 213, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.325449, mae: 27.469014, mean_q: -39.102768, mean_eps: 0.100000\n",
      " 420388/500000: episode: 3007, duration: 0.511s, episode steps: 107, steps per second: 209, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.401258, mae: 26.996578, mean_q: -38.635399, mean_eps: 0.100000\n",
      " 420495/500000: episode: 3008, duration: 0.513s, episode steps: 107, steps per second: 209, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.284800, mae: 26.645275, mean_q: -38.122591, mean_eps: 0.100000\n",
      " 420633/500000: episode: 3009, duration: 0.648s, episode steps: 138, steps per second: 213, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.129122, mae: 26.461250, mean_q: -38.045205, mean_eps: 0.100000\n",
      " 420741/500000: episode: 3010, duration: 0.518s, episode steps: 108, steps per second: 208, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.224594, mae: 26.359826, mean_q: -37.994768, mean_eps: 0.100000\n",
      " 420936/500000: episode: 3011, duration: 0.918s, episode steps: 195, steps per second: 212, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.091660, mae: 26.264071, mean_q: -37.862788, mean_eps: 0.100000\n",
      " 421044/500000: episode: 3012, duration: 0.515s, episode steps: 108, steps per second: 210, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000],  loss: 0.116369, mae: 26.663250, mean_q: -38.343564, mean_eps: 0.100000\n",
      " 421236/500000: episode: 3013, duration: 0.914s, episode steps: 192, steps per second: 210, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.080900, mae: 26.072920, mean_q: -37.555743, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 421337/500000: episode: 3014, duration: 0.481s, episode steps: 101, steps per second: 210, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.082244, mae: 26.700391, mean_q: -38.424288, mean_eps: 0.100000\n",
      " 421445/500000: episode: 3015, duration: 0.531s, episode steps: 108, steps per second: 203, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.080196, mae: 26.696166, mean_q: -38.299220, mean_eps: 0.100000\n",
      " 421598/500000: episode: 3016, duration: 0.722s, episode steps: 153, steps per second: 212, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.102203, mae: 26.980302, mean_q: -38.701660, mean_eps: 0.100000\n",
      " 421719/500000: episode: 3017, duration: 0.569s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.111077, mae: 26.845558, mean_q: -38.348478, mean_eps: 0.100000\n",
      " 421842/500000: episode: 3018, duration: 0.573s, episode steps: 123, steps per second: 215, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000],  loss: 0.104572, mae: 27.072028, mean_q: -38.674174, mean_eps: 0.100000\n",
      " 421964/500000: episode: 3019, duration: 0.569s, episode steps: 122, steps per second: 214, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.099162, mae: 27.100181, mean_q: -38.613802, mean_eps: 0.100000\n",
      " 422071/500000: episode: 3020, duration: 0.501s, episode steps: 107, steps per second: 214, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.100506, mae: 26.709450, mean_q: -37.943037, mean_eps: 0.100000\n",
      " 422180/500000: episode: 3021, duration: 0.610s, episode steps: 109, steps per second: 179, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.085610, mae: 26.169674, mean_q: -37.255867, mean_eps: 0.100000\n",
      " 422326/500000: episode: 3022, duration: 0.715s, episode steps: 146, steps per second: 204, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.094766, mae: 27.019271, mean_q: -38.614238, mean_eps: 0.100000\n",
      " 422455/500000: episode: 3023, duration: 0.637s, episode steps: 129, steps per second: 203, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.076424, mae: 26.664175, mean_q: -38.048730, mean_eps: 0.100000\n",
      " 422655/500000: episode: 3024, duration: 0.979s, episode steps: 200, steps per second: 204, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.086321, mae: 26.794420, mean_q: -38.067633, mean_eps: 0.100000\n",
      " 422850/500000: episode: 3025, duration: 0.929s, episode steps: 195, steps per second: 210, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.242895, mae: 27.088829, mean_q: -38.595167, mean_eps: 0.100000\n",
      " 422974/500000: episode: 3026, duration: 0.614s, episode steps: 124, steps per second: 202, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.408935, mae: 26.677164, mean_q: -38.161051, mean_eps: 0.100000\n",
      " 423141/500000: episode: 3027, duration: 0.779s, episode steps: 167, steps per second: 214, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.500106, mae: 27.400675, mean_q: -38.882213, mean_eps: 0.100000\n",
      " 423229/500000: episode: 3028, duration: 0.414s, episode steps:  88, steps per second: 212, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.304307, mae: 27.284824, mean_q: -38.632878, mean_eps: 0.100000\n",
      " 423423/500000: episode: 3029, duration: 0.912s, episode steps: 194, steps per second: 213, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000],  loss: 0.468323, mae: 26.674564, mean_q: -37.570509, mean_eps: 0.100000\n",
      " 423533/500000: episode: 3030, duration: 0.520s, episode steps: 110, steps per second: 211, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.449292, mae: 26.592126, mean_q: -37.543598, mean_eps: 0.100000\n",
      " 423648/500000: episode: 3031, duration: 0.537s, episode steps: 115, steps per second: 214, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.178981, mae: 26.570730, mean_q: -37.624945, mean_eps: 0.100000\n",
      " 423770/500000: episode: 3032, duration: 0.638s, episode steps: 122, steps per second: 191, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.101795, mae: 25.761837, mean_q: -36.471062, mean_eps: 0.100000\n",
      " 423868/500000: episode: 3033, duration: 0.471s, episode steps:  98, steps per second: 208, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.204 [0.000, 2.000],  loss: 0.155327, mae: 25.398397, mean_q: -35.852752, mean_eps: 0.100000\n",
      " 424035/500000: episode: 3034, duration: 0.854s, episode steps: 167, steps per second: 196, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.117247, mae: 25.847244, mean_q: -36.481160, mean_eps: 0.100000\n",
      " 424123/500000: episode: 3035, duration: 0.436s, episode steps:  88, steps per second: 202, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.125054, mae: 25.144476, mean_q: -35.715805, mean_eps: 0.100000\n",
      " 424302/500000: episode: 3036, duration: 0.851s, episode steps: 179, steps per second: 210, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.099585, mae: 26.003072, mean_q: -37.125844, mean_eps: 0.100000\n",
      " 424464/500000: episode: 3037, duration: 0.752s, episode steps: 162, steps per second: 215, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.112970, mae: 26.477194, mean_q: -37.975205, mean_eps: 0.100000\n",
      " 424573/500000: episode: 3038, duration: 0.506s, episode steps: 109, steps per second: 215, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.128 [0.000, 2.000],  loss: 0.076138, mae: 26.539420, mean_q: -38.044384, mean_eps: 0.100000\n",
      " 424737/500000: episode: 3039, duration: 0.761s, episode steps: 164, steps per second: 216, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.093407, mae: 27.319316, mean_q: -39.234649, mean_eps: 0.100000\n",
      " 424847/500000: episode: 3040, duration: 0.512s, episode steps: 110, steps per second: 215, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.086201, mae: 27.506574, mean_q: -39.719243, mean_eps: 0.100000\n",
      " 424961/500000: episode: 3041, duration: 0.530s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.098051, mae: 27.196021, mean_q: -39.378963, mean_eps: 0.100000\n",
      " 425130/500000: episode: 3042, duration: 0.784s, episode steps: 169, steps per second: 216, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.083 [0.000, 2.000],  loss: 0.099978, mae: 27.725251, mean_q: -40.055101, mean_eps: 0.100000\n",
      " 425244/500000: episode: 3043, duration: 0.590s, episode steps: 114, steps per second: 193, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.107640, mae: 26.944134, mean_q: -38.974742, mean_eps: 0.100000\n",
      " 425355/500000: episode: 3044, duration: 0.587s, episode steps: 111, steps per second: 189, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.116643, mae: 27.013851, mean_q: -38.966689, mean_eps: 0.100000\n",
      " 425465/500000: episode: 3045, duration: 0.576s, episode steps: 110, steps per second: 191, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.051337, mae: 26.577264, mean_q: -38.423303, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 425601/500000: episode: 3046, duration: 0.703s, episode steps: 136, steps per second: 194, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.072445, mae: 26.771754, mean_q: -38.597905, mean_eps: 0.100000\n",
      " 425760/500000: episode: 3047, duration: 0.812s, episode steps: 159, steps per second: 196, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.117751, mae: 26.515625, mean_q: -37.893401, mean_eps: 0.100000\n",
      " 425873/500000: episode: 3048, duration: 0.584s, episode steps: 113, steps per second: 194, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.101108, mae: 26.210275, mean_q: -37.475367, mean_eps: 0.100000\n",
      " 426033/500000: episode: 3049, duration: 0.803s, episode steps: 160, steps per second: 199, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.131947, mae: 26.690823, mean_q: -38.217700, mean_eps: 0.100000\n",
      " 426143/500000: episode: 3050, duration: 0.553s, episode steps: 110, steps per second: 199, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.134326, mae: 26.191062, mean_q: -37.439220, mean_eps: 0.100000\n",
      " 426251/500000: episode: 3051, duration: 0.549s, episode steps: 108, steps per second: 197, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.103237, mae: 26.118013, mean_q: -37.360688, mean_eps: 0.100000\n",
      " 426361/500000: episode: 3052, duration: 0.556s, episode steps: 110, steps per second: 198, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.079986, mae: 26.547963, mean_q: -38.079765, mean_eps: 0.100000\n",
      " 426494/500000: episode: 3053, duration: 0.664s, episode steps: 133, steps per second: 200, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.098677, mae: 26.829762, mean_q: -38.391454, mean_eps: 0.100000\n",
      " 426624/500000: episode: 3054, duration: 0.663s, episode steps: 130, steps per second: 196, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.098022, mae: 26.712291, mean_q: -38.162598, mean_eps: 0.100000\n",
      " 426735/500000: episode: 3055, duration: 0.557s, episode steps: 111, steps per second: 199, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.083297, mae: 25.967410, mean_q: -37.053388, mean_eps: 0.100000\n",
      " 426852/500000: episode: 3056, duration: 0.592s, episode steps: 117, steps per second: 198, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.111116, mae: 26.317735, mean_q: -37.542788, mean_eps: 0.100000\n",
      " 427035/500000: episode: 3057, duration: 0.870s, episode steps: 183, steps per second: 210, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.063659, mae: 26.887371, mean_q: -38.382437, mean_eps: 0.100000\n",
      " 427191/500000: episode: 3058, duration: 0.785s, episode steps: 156, steps per second: 199, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.074980, mae: 27.546547, mean_q: -39.354109, mean_eps: 0.100000\n",
      " 427288/500000: episode: 3059, duration: 0.499s, episode steps:  97, steps per second: 194, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.096984, mae: 27.360542, mean_q: -39.120510, mean_eps: 0.100000\n",
      " 427462/500000: episode: 3060, duration: 0.882s, episode steps: 174, steps per second: 197, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.113212, mae: 27.636614, mean_q: -39.395269, mean_eps: 0.100000\n",
      " 427569/500000: episode: 3061, duration: 0.538s, episode steps: 107, steps per second: 199, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.116053, mae: 27.301607, mean_q: -38.994695, mean_eps: 0.100000\n",
      " 427665/500000: episode: 3062, duration: 0.476s, episode steps:  96, steps per second: 202, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.095131, mae: 27.083368, mean_q: -38.745253, mean_eps: 0.100000\n",
      " 427772/500000: episode: 3063, duration: 0.539s, episode steps: 107, steps per second: 198, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.148724, mae: 27.465399, mean_q: -39.193768, mean_eps: 0.100000\n",
      " 427885/500000: episode: 3064, duration: 0.537s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.120326, mae: 27.299477, mean_q: -39.096622, mean_eps: 0.100000\n",
      " 428000/500000: episode: 3065, duration: 0.561s, episode steps: 115, steps per second: 205, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.097432, mae: 25.855045, mean_q: -37.006656, mean_eps: 0.100000\n",
      " 428168/500000: episode: 3066, duration: 0.816s, episode steps: 168, steps per second: 206, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.097536, mae: 26.098256, mean_q: -37.126312, mean_eps: 0.100000\n",
      " 428274/500000: episode: 3067, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.098140, mae: 26.522178, mean_q: -37.800295, mean_eps: 0.100000\n",
      " 428389/500000: episode: 3068, duration: 0.544s, episode steps: 115, steps per second: 211, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.087534, mae: 26.100198, mean_q: -37.279506, mean_eps: 0.100000\n",
      " 428507/500000: episode: 3069, duration: 0.543s, episode steps: 118, steps per second: 217, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.061770, mae: 25.994539, mean_q: -37.073209, mean_eps: 0.100000\n",
      " 428617/500000: episode: 3070, duration: 0.517s, episode steps: 110, steps per second: 213, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.082097, mae: 26.202038, mean_q: -37.202488, mean_eps: 0.100000\n",
      " 428737/500000: episode: 3071, duration: 0.555s, episode steps: 120, steps per second: 216, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000],  loss: 0.064696, mae: 26.293942, mean_q: -37.419357, mean_eps: 0.100000\n",
      " 428830/500000: episode: 3072, duration: 0.475s, episode steps:  93, steps per second: 196, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.058067, mae: 26.299259, mean_q: -37.271013, mean_eps: 0.100000\n",
      " 428945/500000: episode: 3073, duration: 0.605s, episode steps: 115, steps per second: 190, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.065993, mae: 26.118486, mean_q: -36.949105, mean_eps: 0.100000\n",
      " 429058/500000: episode: 3074, duration: 0.557s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000],  loss: 0.046836, mae: 25.980126, mean_q: -37.132161, mean_eps: 0.100000\n",
      " 429192/500000: episode: 3075, duration: 0.682s, episode steps: 134, steps per second: 196, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000],  loss: 0.047715, mae: 25.128151, mean_q: -35.907510, mean_eps: 0.100000\n",
      " 429311/500000: episode: 3076, duration: 0.616s, episode steps: 119, steps per second: 193, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.040565, mae: 25.260755, mean_q: -35.887254, mean_eps: 0.100000\n",
      " 429427/500000: episode: 3077, duration: 0.572s, episode steps: 116, steps per second: 203, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.267 [0.000, 2.000],  loss: 0.036081, mae: 25.241912, mean_q: -36.001120, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 429542/500000: episode: 3078, duration: 0.539s, episode steps: 115, steps per second: 213, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000],  loss: 0.032775, mae: 24.622008, mean_q: -35.192872, mean_eps: 0.100000\n",
      " 429656/500000: episode: 3079, duration: 0.586s, episode steps: 114, steps per second: 194, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.039637, mae: 25.172680, mean_q: -36.040502, mean_eps: 0.100000\n",
      " 429764/500000: episode: 3080, duration: 0.506s, episode steps: 108, steps per second: 213, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.343 [0.000, 2.000],  loss: 0.036484, mae: 25.056359, mean_q: -35.813075, mean_eps: 0.100000\n",
      " 429964/500000: episode: 3081, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 0.051596, mae: 25.908937, mean_q: -36.785451, mean_eps: 0.100000\n",
      " 430116/500000: episode: 3082, duration: 0.790s, episode steps: 152, steps per second: 192, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.102028, mae: 26.543153, mean_q: -37.643600, mean_eps: 0.100000\n",
      " 430230/500000: episode: 3083, duration: 0.528s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.237 [0.000, 2.000],  loss: 0.110568, mae: 26.782783, mean_q: -38.137421, mean_eps: 0.100000\n",
      " 430351/500000: episode: 3084, duration: 0.569s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.083707, mae: 27.059456, mean_q: -38.570172, mean_eps: 0.100000\n",
      " 430485/500000: episode: 3085, duration: 0.619s, episode steps: 134, steps per second: 216, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 0.127226, mae: 26.679834, mean_q: -37.957856, mean_eps: 0.100000\n",
      " 430574/500000: episode: 3086, duration: 0.429s, episode steps:  89, steps per second: 208, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.110949, mae: 26.646331, mean_q: -37.738093, mean_eps: 0.100000\n",
      " 430750/500000: episode: 3087, duration: 0.844s, episode steps: 176, steps per second: 209, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000],  loss: 0.107528, mae: 27.687213, mean_q: -39.379758, mean_eps: 0.100000\n",
      " 430864/500000: episode: 3088, duration: 0.539s, episode steps: 114, steps per second: 212, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.135781, mae: 27.837192, mean_q: -39.763667, mean_eps: 0.100000\n",
      " 431019/500000: episode: 3089, duration: 0.743s, episode steps: 155, steps per second: 209, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.097984, mae: 28.050165, mean_q: -40.234843, mean_eps: 0.100000\n",
      " 431125/500000: episode: 3090, duration: 0.510s, episode steps: 106, steps per second: 208, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.083258, mae: 27.607465, mean_q: -39.761634, mean_eps: 0.100000\n",
      " 431289/500000: episode: 3091, duration: 0.776s, episode steps: 164, steps per second: 211, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.083376, mae: 28.165821, mean_q: -40.643703, mean_eps: 0.100000\n",
      " 431397/500000: episode: 3092, duration: 0.517s, episode steps: 108, steps per second: 209, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.063719, mae: 28.928925, mean_q: -41.852356, mean_eps: 0.100000\n",
      " 431509/500000: episode: 3093, duration: 0.531s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.094479, mae: 28.212492, mean_q: -40.908197, mean_eps: 0.100000\n",
      " 431677/500000: episode: 3094, duration: 0.796s, episode steps: 168, steps per second: 211, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.131 [0.000, 2.000],  loss: 0.105780, mae: 28.301362, mean_q: -41.080840, mean_eps: 0.100000\n",
      " 431791/500000: episode: 3095, duration: 0.559s, episode steps: 114, steps per second: 204, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.097511, mae: 27.445385, mean_q: -39.683454, mean_eps: 0.100000\n",
      " 431880/500000: episode: 3096, duration: 0.429s, episode steps:  89, steps per second: 207, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.079701, mae: 27.093444, mean_q: -39.167405, mean_eps: 0.100000\n",
      " 431986/500000: episode: 3097, duration: 0.518s, episode steps: 106, steps per second: 205, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.082762, mae: 26.789674, mean_q: -38.535885, mean_eps: 0.100000\n",
      " 432075/500000: episode: 3098, duration: 0.422s, episode steps:  89, steps per second: 211, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.086317, mae: 26.393087, mean_q: -37.874037, mean_eps: 0.100000\n",
      " 432189/500000: episode: 3099, duration: 0.544s, episode steps: 114, steps per second: 210, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.064814, mae: 25.742149, mean_q: -37.053627, mean_eps: 0.100000\n",
      " 432285/500000: episode: 3100, duration: 0.450s, episode steps:  96, steps per second: 213, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.057999, mae: 25.303215, mean_q: -36.425751, mean_eps: 0.100000\n",
      " 432394/500000: episode: 3101, duration: 0.511s, episode steps: 109, steps per second: 213, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.059452, mae: 25.126515, mean_q: -35.968594, mean_eps: 0.100000\n",
      " 432557/500000: episode: 3102, duration: 0.762s, episode steps: 163, steps per second: 214, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.853 [0.000, 2.000],  loss: 0.086562, mae: 25.851456, mean_q: -36.864983, mean_eps: 0.100000\n",
      " 432688/500000: episode: 3103, duration: 0.621s, episode steps: 131, steps per second: 211, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.051577, mae: 25.459469, mean_q: -36.379382, mean_eps: 0.100000\n",
      " 432839/500000: episode: 3104, duration: 0.716s, episode steps: 151, steps per second: 211, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000],  loss: 0.067552, mae: 25.195528, mean_q: -35.926158, mean_eps: 0.100000\n",
      " 432951/500000: episode: 3105, duration: 0.530s, episode steps: 112, steps per second: 211, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.099028, mae: 25.723739, mean_q: -36.617462, mean_eps: 0.100000\n",
      " 433075/500000: episode: 3106, duration: 0.585s, episode steps: 124, steps per second: 212, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.183697, mae: 25.896919, mean_q: -36.726809, mean_eps: 0.100000\n",
      " 433193/500000: episode: 3107, duration: 0.566s, episode steps: 118, steps per second: 209, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.129309, mae: 25.784328, mean_q: -36.630362, mean_eps: 0.100000\n",
      " 433291/500000: episode: 3108, duration: 0.462s, episode steps:  98, steps per second: 212, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 0.082051, mae: 25.814824, mean_q: -36.580346, mean_eps: 0.100000\n",
      " 433423/500000: episode: 3109, duration: 0.612s, episode steps: 132, steps per second: 216, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000],  loss: 0.078148, mae: 25.813204, mean_q: -36.772226, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 433539/500000: episode: 3110, duration: 0.535s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000],  loss: 0.071853, mae: 25.153727, mean_q: -35.904021, mean_eps: 0.100000\n",
      " 433678/500000: episode: 3111, duration: 0.652s, episode steps: 139, steps per second: 213, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.396 [0.000, 2.000],  loss: 0.090253, mae: 25.559850, mean_q: -36.459187, mean_eps: 0.100000\n",
      " 433789/500000: episode: 3112, duration: 0.534s, episode steps: 111, steps per second: 208, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.066473, mae: 25.223642, mean_q: -36.095576, mean_eps: 0.100000\n",
      " 433912/500000: episode: 3113, duration: 0.605s, episode steps: 123, steps per second: 203, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.040492, mae: 24.806989, mean_q: -35.611566, mean_eps: 0.100000\n",
      " 434112/500000: episode: 3114, duration: 1.041s, episode steps: 200, steps per second: 192, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.039157, mae: 24.931211, mean_q: -35.969146, mean_eps: 0.100000\n",
      " 434232/500000: episode: 3115, duration: 0.639s, episode steps: 120, steps per second: 188, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000],  loss: 0.259530, mae: 25.141263, mean_q: -36.159999, mean_eps: 0.100000\n",
      " 434409/500000: episode: 3116, duration: 0.838s, episode steps: 177, steps per second: 211, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.160089, mae: 26.357463, mean_q: -37.961230, mean_eps: 0.100000\n",
      " 434522/500000: episode: 3117, duration: 0.541s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.137406, mae: 26.614175, mean_q: -38.383064, mean_eps: 0.100000\n",
      " 434695/500000: episode: 3118, duration: 0.939s, episode steps: 173, steps per second: 184, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.151455, mae: 27.444628, mean_q: -39.648840, mean_eps: 0.100000\n",
      " 434802/500000: episode: 3119, duration: 0.522s, episode steps: 107, steps per second: 205, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.087169, mae: 27.463326, mean_q: -39.666939, mean_eps: 0.100000\n",
      " 434913/500000: episode: 3120, duration: 0.527s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.090897, mae: 27.487769, mean_q: -39.647089, mean_eps: 0.100000\n",
      " 435113/500000: episode: 3121, duration: 0.970s, episode steps: 200, steps per second: 206, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.170290, mae: 27.901932, mean_q: -40.168241, mean_eps: 0.100000\n",
      " 435219/500000: episode: 3122, duration: 0.531s, episode steps: 106, steps per second: 200, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.619650, mae: 27.922420, mean_q: -39.939014, mean_eps: 0.100000\n",
      " 435393/500000: episode: 3123, duration: 0.852s, episode steps: 174, steps per second: 204, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.308590, mae: 27.356848, mean_q: -39.241989, mean_eps: 0.100000\n",
      " 435500/500000: episode: 3124, duration: 0.528s, episode steps: 107, steps per second: 203, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.318 [0.000, 2.000],  loss: 0.256357, mae: 27.578384, mean_q: -39.482346, mean_eps: 0.100000\n",
      " 435619/500000: episode: 3125, duration: 0.590s, episode steps: 119, steps per second: 202, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.327060, mae: 27.431418, mean_q: -39.313560, mean_eps: 0.100000\n",
      " 435728/500000: episode: 3126, duration: 0.510s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.291428, mae: 26.932100, mean_q: -38.372980, mean_eps: 0.100000\n",
      " 435841/500000: episode: 3127, duration: 0.540s, episode steps: 113, steps per second: 209, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.214181, mae: 27.349882, mean_q: -38.927167, mean_eps: 0.100000\n",
      " 435950/500000: episode: 3128, duration: 0.555s, episode steps: 109, steps per second: 196, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.245597, mae: 27.621196, mean_q: -39.308154, mean_eps: 0.100000\n",
      " 436059/500000: episode: 3129, duration: 0.529s, episode steps: 109, steps per second: 206, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000],  loss: 0.244239, mae: 27.085541, mean_q: -38.485008, mean_eps: 0.100000\n",
      " 436217/500000: episode: 3130, duration: 0.744s, episode steps: 158, steps per second: 212, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.943 [0.000, 2.000],  loss: 0.053408, mae: 27.443851, mean_q: -39.351856, mean_eps: 0.100000\n",
      " 436331/500000: episode: 3131, duration: 0.544s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.046995, mae: 26.924988, mean_q: -38.638771, mean_eps: 0.100000\n",
      " 436512/500000: episode: 3132, duration: 0.936s, episode steps: 181, steps per second: 193, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.199 [0.000, 2.000],  loss: 0.052968, mae: 27.259810, mean_q: -39.030162, mean_eps: 0.100000\n",
      " 436625/500000: episode: 3133, duration: 0.556s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.059503, mae: 27.107648, mean_q: -38.777501, mean_eps: 0.100000\n",
      " 436743/500000: episode: 3134, duration: 0.558s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.058308, mae: 26.483256, mean_q: -38.228147, mean_eps: 0.100000\n",
      " 436870/500000: episode: 3135, duration: 0.604s, episode steps: 127, steps per second: 210, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.142 [0.000, 2.000],  loss: 0.061253, mae: 26.575644, mean_q: -38.380284, mean_eps: 0.100000\n",
      " 436985/500000: episode: 3136, duration: 0.589s, episode steps: 115, steps per second: 195, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000],  loss: 0.059431, mae: 26.714708, mean_q: -38.796235, mean_eps: 0.100000\n",
      " 437104/500000: episode: 3137, duration: 0.593s, episode steps: 119, steps per second: 201, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.047069, mae: 26.251291, mean_q: -38.258808, mean_eps: 0.100000\n",
      " 437225/500000: episode: 3138, duration: 0.576s, episode steps: 121, steps per second: 210, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.049386, mae: 25.448757, mean_q: -37.077068, mean_eps: 0.100000\n",
      " 437334/500000: episode: 3139, duration: 0.534s, episode steps: 109, steps per second: 204, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.048935, mae: 25.636499, mean_q: -37.287958, mean_eps: 0.100000\n",
      " 437452/500000: episode: 3140, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.030179, mae: 25.036102, mean_q: -36.598866, mean_eps: 0.100000\n",
      " 437563/500000: episode: 3141, duration: 0.512s, episode steps: 111, steps per second: 217, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.013480, mae: 25.450932, mean_q: -37.301550, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 437736/500000: episode: 3142, duration: 0.828s, episode steps: 173, steps per second: 209, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000],  loss: 0.026025, mae: 25.882592, mean_q: -37.871216, mean_eps: 0.100000\n",
      " 437848/500000: episode: 3143, duration: 0.560s, episode steps: 112, steps per second: 200, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.029061, mae: 26.150245, mean_q: -38.255678, mean_eps: 0.100000\n",
      " 437961/500000: episode: 3144, duration: 0.562s, episode steps: 113, steps per second: 201, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000],  loss: 0.025415, mae: 26.002593, mean_q: -37.994896, mean_eps: 0.100000\n",
      " 438112/500000: episode: 3145, duration: 0.743s, episode steps: 151, steps per second: 203, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 0.090652, mae: 26.062191, mean_q: -37.881023, mean_eps: 0.100000\n",
      " 438287/500000: episode: 3146, duration: 0.824s, episode steps: 175, steps per second: 212, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.099880, mae: 26.972200, mean_q: -38.939141, mean_eps: 0.100000\n",
      " 438399/500000: episode: 3147, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.086603, mae: 26.497754, mean_q: -38.105164, mean_eps: 0.100000\n",
      " 438572/500000: episode: 3148, duration: 0.821s, episode steps: 173, steps per second: 211, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.125525, mae: 27.436166, mean_q: -39.531701, mean_eps: 0.100000\n",
      " 438666/500000: episode: 3149, duration: 0.488s, episode steps:  94, steps per second: 193, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.124611, mae: 26.763837, mean_q: -38.499230, mean_eps: 0.100000\n",
      " 438822/500000: episode: 3150, duration: 0.723s, episode steps: 156, steps per second: 216, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.142355, mae: 26.728658, mean_q: -38.202010, mean_eps: 0.100000\n",
      " 438970/500000: episode: 3151, duration: 0.698s, episode steps: 148, steps per second: 212, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.157267, mae: 27.151229, mean_q: -38.864183, mean_eps: 0.100000\n",
      " 439094/500000: episode: 3152, duration: 0.636s, episode steps: 124, steps per second: 195, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.108583, mae: 27.079311, mean_q: -38.939910, mean_eps: 0.100000\n",
      " 439208/500000: episode: 3153, duration: 0.536s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.128385, mae: 27.047830, mean_q: -38.933510, mean_eps: 0.100000\n",
      " 439327/500000: episode: 3154, duration: 0.604s, episode steps: 119, steps per second: 197, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.098851, mae: 26.986681, mean_q: -38.881150, mean_eps: 0.100000\n",
      " 439444/500000: episode: 3155, duration: 0.567s, episode steps: 117, steps per second: 206, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.075523, mae: 26.684145, mean_q: -38.269231, mean_eps: 0.100000\n",
      " 439560/500000: episode: 3156, duration: 0.554s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.083213, mae: 25.923340, mean_q: -37.200791, mean_eps: 0.100000\n",
      " 439681/500000: episode: 3157, duration: 0.571s, episode steps: 121, steps per second: 212, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.054050, mae: 26.510120, mean_q: -38.197618, mean_eps: 0.100000\n",
      " 439805/500000: episode: 3158, duration: 0.592s, episode steps: 124, steps per second: 209, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000],  loss: 0.056024, mae: 26.018068, mean_q: -37.586114, mean_eps: 0.100000\n",
      " 439986/500000: episode: 3159, duration: 0.835s, episode steps: 181, steps per second: 217, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.079885, mae: 26.514964, mean_q: -38.200766, mean_eps: 0.100000\n",
      " 440102/500000: episode: 3160, duration: 0.540s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.353 [0.000, 2.000],  loss: 0.101856, mae: 26.488626, mean_q: -38.160470, mean_eps: 0.100000\n",
      " 440246/500000: episode: 3161, duration: 0.672s, episode steps: 144, steps per second: 214, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.417 [0.000, 2.000],  loss: 0.073778, mae: 26.585082, mean_q: -38.287028, mean_eps: 0.100000\n",
      " 440370/500000: episode: 3162, duration: 0.575s, episode steps: 124, steps per second: 216, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.403 [0.000, 2.000],  loss: 0.102803, mae: 26.858670, mean_q: -38.644530, mean_eps: 0.100000\n",
      " 440533/500000: episode: 3163, duration: 0.751s, episode steps: 163, steps per second: 217, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.112480, mae: 27.127590, mean_q: -38.844828, mean_eps: 0.100000\n",
      " 440632/500000: episode: 3164, duration: 0.464s, episode steps:  99, steps per second: 213, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.138748, mae: 27.104079, mean_q: -38.725623, mean_eps: 0.100000\n",
      " 440761/500000: episode: 3165, duration: 0.604s, episode steps: 129, steps per second: 214, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.104786, mae: 26.738370, mean_q: -38.012037, mean_eps: 0.100000\n",
      " 440883/500000: episode: 3166, duration: 0.570s, episode steps: 122, steps per second: 214, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.100894, mae: 27.012313, mean_q: -38.577567, mean_eps: 0.100000\n",
      " 441011/500000: episode: 3167, duration: 0.604s, episode steps: 128, steps per second: 212, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.074916, mae: 26.242576, mean_q: -37.566316, mean_eps: 0.100000\n",
      " 441133/500000: episode: 3168, duration: 0.580s, episode steps: 122, steps per second: 210, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000],  loss: 0.077369, mae: 26.164447, mean_q: -37.439327, mean_eps: 0.100000\n",
      " 441252/500000: episode: 3169, duration: 0.570s, episode steps: 119, steps per second: 209, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.087773, mae: 25.547038, mean_q: -36.674637, mean_eps: 0.100000\n",
      " 441364/500000: episode: 3170, duration: 0.528s, episode steps: 112, steps per second: 212, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.069567, mae: 25.585462, mean_q: -36.868326, mean_eps: 0.100000\n",
      " 441494/500000: episode: 3171, duration: 0.620s, episode steps: 130, steps per second: 210, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.051118, mae: 25.880176, mean_q: -37.538265, mean_eps: 0.100000\n",
      " 441614/500000: episode: 3172, duration: 0.573s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.392 [0.000, 2.000],  loss: 0.056496, mae: 25.773829, mean_q: -37.276691, mean_eps: 0.100000\n",
      " 441730/500000: episode: 3173, duration: 0.549s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.397 [0.000, 2.000],  loss: 0.040858, mae: 25.911121, mean_q: -37.677865, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 441913/500000: episode: 3174, duration: 0.850s, episode steps: 183, steps per second: 215, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000],  loss: 0.041727, mae: 26.437152, mean_q: -38.480205, mean_eps: 0.100000\n",
      " 442034/500000: episode: 3175, duration: 0.557s, episode steps: 121, steps per second: 217, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000],  loss: 0.050071, mae: 26.823876, mean_q: -38.958558, mean_eps: 0.100000\n",
      " 442146/500000: episode: 3176, duration: 0.514s, episode steps: 112, steps per second: 218, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000],  loss: 0.049703, mae: 26.783881, mean_q: -38.866039, mean_eps: 0.100000\n",
      " 442271/500000: episode: 3177, duration: 0.576s, episode steps: 125, steps per second: 217, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.416 [0.000, 2.000],  loss: 0.046662, mae: 26.452079, mean_q: -38.420242, mean_eps: 0.100000\n",
      " 442384/500000: episode: 3178, duration: 0.525s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.050384, mae: 26.865274, mean_q: -38.936011, mean_eps: 0.100000\n",
      " 442504/500000: episode: 3179, duration: 0.554s, episode steps: 120, steps per second: 217, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.037394, mae: 26.144894, mean_q: -38.019180, mean_eps: 0.100000\n",
      " 442624/500000: episode: 3180, duration: 0.555s, episode steps: 120, steps per second: 216, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.056304, mae: 26.481916, mean_q: -38.482911, mean_eps: 0.100000\n",
      " 442738/500000: episode: 3181, duration: 0.534s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.072654, mae: 26.871138, mean_q: -38.917272, mean_eps: 0.100000\n",
      " 442865/500000: episode: 3182, duration: 0.597s, episode steps: 127, steps per second: 213, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.441 [0.000, 2.000],  loss: 0.067186, mae: 25.931377, mean_q: -37.628992, mean_eps: 0.100000\n",
      " 442980/500000: episode: 3183, duration: 0.550s, episode steps: 115, steps per second: 209, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.099698, mae: 25.500103, mean_q: -37.005274, mean_eps: 0.100000\n",
      " 443099/500000: episode: 3184, duration: 0.564s, episode steps: 119, steps per second: 211, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.049023, mae: 25.819094, mean_q: -37.577848, mean_eps: 0.100000\n",
      " 443212/500000: episode: 3185, duration: 0.537s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000],  loss: 0.041791, mae: 25.371032, mean_q: -36.810349, mean_eps: 0.100000\n",
      " 443319/500000: episode: 3186, duration: 0.510s, episode steps: 107, steps per second: 210, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.047717, mae: 25.628133, mean_q: -37.291063, mean_eps: 0.100000\n",
      " 443430/500000: episode: 3187, duration: 0.514s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.049353, mae: 25.759667, mean_q: -37.544980, mean_eps: 0.100000\n",
      " 443550/500000: episode: 3188, duration: 0.554s, episode steps: 120, steps per second: 217, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000],  loss: 0.034578, mae: 25.656042, mean_q: -37.430814, mean_eps: 0.100000\n",
      " 443677/500000: episode: 3189, duration: 0.588s, episode steps: 127, steps per second: 216, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.022853, mae: 25.993825, mean_q: -38.117178, mean_eps: 0.100000\n",
      " 443795/500000: episode: 3190, duration: 0.555s, episode steps: 118, steps per second: 213, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000],  loss: 0.026787, mae: 25.782925, mean_q: -37.664651, mean_eps: 0.100000\n",
      " 443911/500000: episode: 3191, duration: 0.541s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.031304, mae: 25.903920, mean_q: -37.815105, mean_eps: 0.100000\n",
      " 444074/500000: episode: 3192, duration: 0.753s, episode steps: 163, steps per second: 216, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.070587, mae: 25.892115, mean_q: -37.571588, mean_eps: 0.100000\n",
      " 444195/500000: episode: 3193, duration: 0.566s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000],  loss: 0.086011, mae: 26.245935, mean_q: -37.995541, mean_eps: 0.100000\n",
      " 444395/500000: episode: 3194, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 2.000],  loss: 0.089374, mae: 26.521723, mean_q: -38.300225, mean_eps: 0.100000\n",
      " 444504/500000: episode: 3195, duration: 0.519s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.358 [0.000, 2.000],  loss: 0.144475, mae: 27.087944, mean_q: -38.962391, mean_eps: 0.100000\n",
      " 444613/500000: episode: 3196, duration: 0.521s, episode steps: 109, steps per second: 209, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000],  loss: 0.125900, mae: 27.291749, mean_q: -39.273799, mean_eps: 0.100000\n",
      " 444765/500000: episode: 3197, duration: 0.720s, episode steps: 152, steps per second: 211, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.090398, mae: 27.258565, mean_q: -39.204838, mean_eps: 0.100000\n",
      " 444877/500000: episode: 3198, duration: 0.540s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.132099, mae: 27.182662, mean_q: -38.949678, mean_eps: 0.100000\n",
      " 444985/500000: episode: 3199, duration: 0.498s, episode steps: 108, steps per second: 217, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.163614, mae: 27.229178, mean_q: -38.883034, mean_eps: 0.100000\n",
      " 445097/500000: episode: 3200, duration: 0.519s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.088365, mae: 27.199740, mean_q: -38.926280, mean_eps: 0.100000\n",
      " 445212/500000: episode: 3201, duration: 0.531s, episode steps: 115, steps per second: 217, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.103941, mae: 27.483315, mean_q: -39.174772, mean_eps: 0.100000\n",
      " 445311/500000: episode: 3202, duration: 0.462s, episode steps:  99, steps per second: 214, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.081317, mae: 26.692615, mean_q: -37.943898, mean_eps: 0.100000\n",
      " 445413/500000: episode: 3203, duration: 0.474s, episode steps: 102, steps per second: 215, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.078600, mae: 26.622125, mean_q: -37.745728, mean_eps: 0.100000\n",
      " 445527/500000: episode: 3204, duration: 0.526s, episode steps: 114, steps per second: 217, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.055823, mae: 26.242828, mean_q: -37.208129, mean_eps: 0.100000\n",
      " 445727/500000: episode: 3205, duration: 0.925s, episode steps: 200, steps per second: 216, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 0.076076, mae: 26.338530, mean_q: -37.334290, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 445859/500000: episode: 3206, duration: 0.607s, episode steps: 132, steps per second: 218, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 0.094415, mae: 26.171643, mean_q: -37.157862, mean_eps: 0.100000\n",
      " 445973/500000: episode: 3207, duration: 0.532s, episode steps: 114, steps per second: 214, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.048667, mae: 26.097935, mean_q: -37.374679, mean_eps: 0.100000\n",
      " 446110/500000: episode: 3208, duration: 0.651s, episode steps: 137, steps per second: 210, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.053654, mae: 26.040081, mean_q: -37.210360, mean_eps: 0.100000\n",
      " 446224/500000: episode: 3209, duration: 0.540s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.046914, mae: 25.742780, mean_q: -36.849046, mean_eps: 0.100000\n",
      " 446384/500000: episode: 3210, duration: 0.751s, episode steps: 160, steps per second: 213, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.064701, mae: 26.800651, mean_q: -38.504314, mean_eps: 0.100000\n",
      " 446533/500000: episode: 3211, duration: 0.700s, episode steps: 149, steps per second: 213, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.070219, mae: 27.618875, mean_q: -39.762599, mean_eps: 0.100000\n",
      " 446698/500000: episode: 3212, duration: 0.771s, episode steps: 165, steps per second: 214, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000],  loss: 0.109236, mae: 26.898903, mean_q: -38.558138, mean_eps: 0.100000\n",
      " 446896/500000: episode: 3213, duration: 0.911s, episode steps: 198, steps per second: 217, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.152960, mae: 27.017234, mean_q: -38.711586, mean_eps: 0.100000\n",
      " 447005/500000: episode: 3214, duration: 0.520s, episode steps: 109, steps per second: 210, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.116804, mae: 27.066358, mean_q: -38.825673, mean_eps: 0.100000\n",
      " 447124/500000: episode: 3215, duration: 0.556s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.091770, mae: 26.901323, mean_q: -38.736541, mean_eps: 0.100000\n",
      " 447233/500000: episode: 3216, duration: 0.504s, episode steps: 109, steps per second: 216, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000],  loss: 0.081436, mae: 27.021618, mean_q: -38.841284, mean_eps: 0.100000\n",
      " 447328/500000: episode: 3217, duration: 0.448s, episode steps:  95, steps per second: 212, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.090655, mae: 26.485614, mean_q: -37.856254, mean_eps: 0.100000\n",
      " 447528/500000: episode: 3218, duration: 0.934s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.090906, mae: 26.351299, mean_q: -37.564821, mean_eps: 0.100000\n",
      " 447658/500000: episode: 3219, duration: 0.597s, episode steps: 130, steps per second: 218, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 0.107526, mae: 26.089770, mean_q: -37.106451, mean_eps: 0.100000\n",
      " 447774/500000: episode: 3220, duration: 0.539s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000],  loss: 0.195602, mae: 26.800688, mean_q: -37.947645, mean_eps: 0.100000\n",
      " 447926/500000: episode: 3221, duration: 0.705s, episode steps: 152, steps per second: 216, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.131211, mae: 27.050872, mean_q: -38.343572, mean_eps: 0.100000\n",
      " 448049/500000: episode: 3222, duration: 0.570s, episode steps: 123, steps per second: 216, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.123140, mae: 27.132032, mean_q: -38.358622, mean_eps: 0.100000\n",
      " 448167/500000: episode: 3223, duration: 0.546s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000],  loss: 0.110273, mae: 26.959087, mean_q: -38.243917, mean_eps: 0.100000\n",
      " 448283/500000: episode: 3224, duration: 0.541s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.153736, mae: 27.086899, mean_q: -38.461183, mean_eps: 0.100000\n",
      " 448399/500000: episode: 3225, duration: 0.537s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.164 [0.000, 2.000],  loss: 0.127310, mae: 27.132391, mean_q: -38.860400, mean_eps: 0.100000\n",
      " 448498/500000: episode: 3226, duration: 0.458s, episode steps:  99, steps per second: 216, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.113473, mae: 26.963985, mean_q: -38.613281, mean_eps: 0.100000\n",
      " 448646/500000: episode: 3227, duration: 0.696s, episode steps: 148, steps per second: 213, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.076619, mae: 27.049312, mean_q: -38.737230, mean_eps: 0.100000\n",
      " 448767/500000: episode: 3228, duration: 0.568s, episode steps: 121, steps per second: 213, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.076534, mae: 26.952819, mean_q: -38.723020, mean_eps: 0.100000\n",
      " 448890/500000: episode: 3229, duration: 0.587s, episode steps: 123, steps per second: 209, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.051039, mae: 26.276368, mean_q: -37.822445, mean_eps: 0.100000\n",
      " 449050/500000: episode: 3230, duration: 0.749s, episode steps: 160, steps per second: 214, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.806 [0.000, 2.000],  loss: 0.064300, mae: 26.866534, mean_q: -38.670300, mean_eps: 0.100000\n",
      " 449163/500000: episode: 3231, duration: 0.539s, episode steps: 113, steps per second: 210, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.067173, mae: 27.298924, mean_q: -39.495229, mean_eps: 0.100000\n",
      " 449286/500000: episode: 3232, duration: 0.579s, episode steps: 123, steps per second: 212, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000],  loss: 0.072903, mae: 26.941418, mean_q: -38.956842, mean_eps: 0.100000\n",
      " 449397/500000: episode: 3233, duration: 0.521s, episode steps: 111, steps per second: 213, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.087068, mae: 26.273683, mean_q: -37.889239, mean_eps: 0.100000\n",
      " 449507/500000: episode: 3234, duration: 0.511s, episode steps: 110, steps per second: 215, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.191 [0.000, 2.000],  loss: 0.065116, mae: 26.742782, mean_q: -38.675859, mean_eps: 0.100000\n",
      " 449676/500000: episode: 3235, duration: 0.786s, episode steps: 169, steps per second: 215, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.075594, mae: 26.480166, mean_q: -38.338765, mean_eps: 0.100000\n",
      " 449793/500000: episode: 3236, duration: 0.545s, episode steps: 117, steps per second: 215, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.073761, mae: 26.372271, mean_q: -38.117510, mean_eps: 0.100000\n",
      " 449904/500000: episode: 3237, duration: 0.517s, episode steps: 111, steps per second: 215, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.158341, mae: 26.047607, mean_q: -37.563286, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450023/500000: episode: 3238, duration: 0.555s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.155956, mae: 25.523918, mean_q: -36.500493, mean_eps: 0.100000\n",
      " 450128/500000: episode: 3239, duration: 0.489s, episode steps: 105, steps per second: 215, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000],  loss: 0.081760, mae: 25.412177, mean_q: -36.037081, mean_eps: 0.100000\n",
      " 450244/500000: episode: 3240, duration: 0.534s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.068404, mae: 25.489625, mean_q: -36.198549, mean_eps: 0.100000\n",
      " 450363/500000: episode: 3241, duration: 0.557s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.063932, mae: 25.695133, mean_q: -36.493151, mean_eps: 0.100000\n",
      " 450488/500000: episode: 3242, duration: 0.598s, episode steps: 125, steps per second: 209, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.075532, mae: 25.240891, mean_q: -35.861800, mean_eps: 0.100000\n",
      " 450603/500000: episode: 3243, duration: 0.547s, episode steps: 115, steps per second: 210, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000],  loss: 0.063884, mae: 25.409129, mean_q: -36.119207, mean_eps: 0.100000\n",
      " 450719/500000: episode: 3244, duration: 0.548s, episode steps: 116, steps per second: 212, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.058445, mae: 25.646342, mean_q: -36.585844, mean_eps: 0.100000\n",
      " 450854/500000: episode: 3245, duration: 0.624s, episode steps: 135, steps per second: 216, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000],  loss: 0.075507, mae: 25.335413, mean_q: -35.819461, mean_eps: 0.100000\n",
      " 450973/500000: episode: 3246, duration: 0.549s, episode steps: 119, steps per second: 217, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.042024, mae: 25.564230, mean_q: -36.268882, mean_eps: 0.100000\n",
      " 451097/500000: episode: 3247, duration: 0.572s, episode steps: 124, steps per second: 217, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.032881, mae: 25.715930, mean_q: -36.521638, mean_eps: 0.100000\n",
      " 451211/500000: episode: 3248, duration: 0.561s, episode steps: 114, steps per second: 203, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.184 [0.000, 2.000],  loss: 0.037624, mae: 25.513819, mean_q: -36.221441, mean_eps: 0.100000\n",
      " 451386/500000: episode: 3249, duration: 0.809s, episode steps: 175, steps per second: 216, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.017 [0.000, 2.000],  loss: 0.050783, mae: 25.855991, mean_q: -36.706390, mean_eps: 0.100000\n",
      " 451508/500000: episode: 3250, duration: 0.560s, episode steps: 122, steps per second: 218, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000],  loss: 0.057146, mae: 26.162845, mean_q: -37.051364, mean_eps: 0.100000\n",
      " 451667/500000: episode: 3251, duration: 0.734s, episode steps: 159, steps per second: 217, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.063 [0.000, 2.000],  loss: 0.088525, mae: 27.369990, mean_q: -38.820778, mean_eps: 0.100000\n",
      " 451777/500000: episode: 3252, duration: 0.523s, episode steps: 110, steps per second: 210, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.109348, mae: 27.062608, mean_q: -38.511806, mean_eps: 0.100000\n",
      " 451911/500000: episode: 3253, duration: 0.630s, episode steps: 134, steps per second: 213, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.077189, mae: 27.160374, mean_q: -38.800249, mean_eps: 0.100000\n",
      " 452045/500000: episode: 3254, duration: 0.630s, episode steps: 134, steps per second: 213, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.068548, mae: 27.601504, mean_q: -39.592269, mean_eps: 0.100000\n",
      " 452239/500000: episode: 3255, duration: 0.909s, episode steps: 194, steps per second: 213, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.201 [0.000, 2.000],  loss: 0.090858, mae: 27.571305, mean_q: -39.442192, mean_eps: 0.100000\n",
      " 452421/500000: episode: 3256, duration: 0.844s, episode steps: 182, steps per second: 216, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.086183, mae: 27.065533, mean_q: -38.654103, mean_eps: 0.100000\n",
      " 452535/500000: episode: 3257, duration: 0.535s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.146950, mae: 26.838760, mean_q: -38.431703, mean_eps: 0.100000\n",
      " 452706/500000: episode: 3258, duration: 0.798s, episode steps: 171, steps per second: 214, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.156479, mae: 25.797754, mean_q: -36.838941, mean_eps: 0.100000\n",
      " 452796/500000: episode: 3259, duration: 0.421s, episode steps:  90, steps per second: 214, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.106416, mae: 25.376787, mean_q: -36.182983, mean_eps: 0.100000\n",
      " 452958/500000: episode: 3260, duration: 0.760s, episode steps: 162, steps per second: 213, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.130788, mae: 25.648279, mean_q: -36.464599, mean_eps: 0.100000\n",
      " 453072/500000: episode: 3261, duration: 0.529s, episode steps: 114, steps per second: 216, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.307 [0.000, 2.000],  loss: 0.141415, mae: 25.673338, mean_q: -36.489083, mean_eps: 0.100000\n",
      " 453226/500000: episode: 3262, duration: 0.709s, episode steps: 154, steps per second: 217, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.131575, mae: 25.857859, mean_q: -36.643154, mean_eps: 0.100000\n",
      " 453352/500000: episode: 3263, duration: 0.591s, episode steps: 126, steps per second: 213, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.099930, mae: 26.053048, mean_q: -37.055263, mean_eps: 0.100000\n",
      " 453508/500000: episode: 3264, duration: 0.715s, episode steps: 156, steps per second: 218, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.094485, mae: 26.983903, mean_q: -38.376844, mean_eps: 0.100000\n",
      " 453624/500000: episode: 3265, duration: 0.539s, episode steps: 116, steps per second: 215, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.096596, mae: 27.118018, mean_q: -38.682243, mean_eps: 0.100000\n",
      " 453792/500000: episode: 3266, duration: 0.778s, episode steps: 168, steps per second: 216, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.083 [0.000, 2.000],  loss: 0.084605, mae: 27.298608, mean_q: -38.978485, mean_eps: 0.100000\n",
      " 453977/500000: episode: 3267, duration: 0.851s, episode steps: 185, steps per second: 217, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000],  loss: 0.099935, mae: 27.889950, mean_q: -39.855485, mean_eps: 0.100000\n",
      " 454105/500000: episode: 3268, duration: 0.596s, episode steps: 128, steps per second: 215, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.164 [0.000, 2.000],  loss: 0.070271, mae: 28.132560, mean_q: -40.597842, mean_eps: 0.100000\n",
      " 454219/500000: episode: 3269, duration: 0.559s, episode steps: 114, steps per second: 204, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.066772, mae: 27.769187, mean_q: -40.073433, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 454402/500000: episode: 3270, duration: 0.863s, episode steps: 183, steps per second: 212, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000],  loss: 0.085768, mae: 27.525111, mean_q: -39.627342, mean_eps: 0.100000\n",
      " 454560/500000: episode: 3271, duration: 0.744s, episode steps: 158, steps per second: 212, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000],  loss: 0.124121, mae: 27.159831, mean_q: -38.905944, mean_eps: 0.100000\n",
      " 454701/500000: episode: 3272, duration: 0.678s, episode steps: 141, steps per second: 208, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000],  loss: 0.088619, mae: 27.336442, mean_q: -39.234707, mean_eps: 0.100000\n",
      " 454861/500000: episode: 3273, duration: 0.742s, episode steps: 160, steps per second: 216, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.912 [0.000, 2.000],  loss: 0.106434, mae: 27.495770, mean_q: -39.485441, mean_eps: 0.100000\n",
      " 455017/500000: episode: 3274, duration: 0.720s, episode steps: 156, steps per second: 217, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.138526, mae: 27.046845, mean_q: -38.724130, mean_eps: 0.100000\n",
      " 455132/500000: episode: 3275, duration: 0.534s, episode steps: 115, steps per second: 215, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.093838, mae: 27.003139, mean_q: -38.633323, mean_eps: 0.100000\n",
      " 455255/500000: episode: 3276, duration: 0.566s, episode steps: 123, steps per second: 217, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.087015, mae: 27.174451, mean_q: -38.722700, mean_eps: 0.100000\n",
      " 455409/500000: episode: 3277, duration: 0.720s, episode steps: 154, steps per second: 214, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.084871, mae: 27.287072, mean_q: -39.015651, mean_eps: 0.100000\n",
      " 455550/500000: episode: 3278, duration: 0.652s, episode steps: 141, steps per second: 216, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.404 [0.000, 2.000],  loss: 0.088935, mae: 26.709106, mean_q: -38.064528, mean_eps: 0.100000\n",
      " 455674/500000: episode: 3279, duration: 0.594s, episode steps: 124, steps per second: 209, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 2.000],  loss: 0.126297, mae: 26.457411, mean_q: -37.450106, mean_eps: 0.100000\n",
      " 455815/500000: episode: 3280, duration: 0.663s, episode steps: 141, steps per second: 213, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000],  loss: 0.097415, mae: 26.625788, mean_q: -37.502654, mean_eps: 0.100000\n",
      " 455992/500000: episode: 3281, duration: 0.836s, episode steps: 177, steps per second: 212, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.105255, mae: 27.054852, mean_q: -37.841914, mean_eps: 0.100000\n",
      " 456148/500000: episode: 3282, duration: 0.721s, episode steps: 156, steps per second: 217, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.130392, mae: 27.318592, mean_q: -38.021019, mean_eps: 0.100000\n",
      " 456256/500000: episode: 3283, duration: 0.502s, episode steps: 108, steps per second: 215, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000],  loss: 0.132163, mae: 27.128579, mean_q: -37.931915, mean_eps: 0.100000\n",
      " 456415/500000: episode: 3284, duration: 0.748s, episode steps: 159, steps per second: 212, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000],  loss: 0.089479, mae: 27.680355, mean_q: -38.679400, mean_eps: 0.100000\n",
      " 456553/500000: episode: 3285, duration: 0.640s, episode steps: 138, steps per second: 216, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000],  loss: 0.093505, mae: 28.387013, mean_q: -39.922522, mean_eps: 0.100000\n",
      " 456655/500000: episode: 3286, duration: 0.475s, episode steps: 102, steps per second: 215, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.074806, mae: 28.426273, mean_q: -40.161612, mean_eps: 0.100000\n",
      " 456765/500000: episode: 3287, duration: 0.514s, episode steps: 110, steps per second: 214, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000],  loss: 0.112613, mae: 27.647247, mean_q: -38.926645, mean_eps: 0.100000\n",
      " 456965/500000: episode: 3288, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.072040, mae: 27.484396, mean_q: -38.838193, mean_eps: 0.100000\n",
      " 457083/500000: episode: 3289, duration: 0.569s, episode steps: 118, steps per second: 207, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.246 [0.000, 2.000],  loss: 0.130210, mae: 27.788780, mean_q: -39.350628, mean_eps: 0.100000\n",
      " 457238/500000: episode: 3290, duration: 0.741s, episode steps: 155, steps per second: 209, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.039 [0.000, 2.000],  loss: 0.149310, mae: 27.368508, mean_q: -38.540055, mean_eps: 0.100000\n",
      " 457406/500000: episode: 3291, duration: 0.791s, episode steps: 168, steps per second: 212, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000],  loss: 0.170429, mae: 27.525382, mean_q: -38.741170, mean_eps: 0.100000\n",
      " 457606/500000: episode: 3292, duration: 0.950s, episode steps: 200, steps per second: 210, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.176681, mae: 27.230139, mean_q: -37.877442, mean_eps: 0.100000\n",
      " 457801/500000: episode: 3293, duration: 0.915s, episode steps: 195, steps per second: 213, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.198370, mae: 28.240072, mean_q: -39.530756, mean_eps: 0.100000\n",
      " 457967/500000: episode: 3294, duration: 0.770s, episode steps: 166, steps per second: 215, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.072 [0.000, 2.000],  loss: 0.214892, mae: 28.915780, mean_q: -40.648240, mean_eps: 0.100000\n",
      " 458055/500000: episode: 3295, duration: 0.419s, episode steps:  88, steps per second: 210, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.208747, mae: 28.530995, mean_q: -39.991916, mean_eps: 0.100000\n",
      " 458155/500000: episode: 3296, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.208990, mae: 27.886878, mean_q: -38.917690, mean_eps: 0.100000\n",
      " 458285/500000: episode: 3297, duration: 0.604s, episode steps: 130, steps per second: 215, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 0.130178, mae: 28.072665, mean_q: -39.110036, mean_eps: 0.100000\n",
      " 458445/500000: episode: 3298, duration: 0.751s, episode steps: 160, steps per second: 213, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.168990, mae: 27.433127, mean_q: -38.328020, mean_eps: 0.100000\n",
      " 458614/500000: episode: 3299, duration: 0.797s, episode steps: 169, steps per second: 212, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.095771, mae: 27.780274, mean_q: -39.000675, mean_eps: 0.100000\n",
      " 458736/500000: episode: 3300, duration: 0.563s, episode steps: 122, steps per second: 217, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.077933, mae: 27.238103, mean_q: -38.368688, mean_eps: 0.100000\n",
      " 458861/500000: episode: 3301, duration: 0.603s, episode steps: 125, steps per second: 207, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000],  loss: 0.076002, mae: 26.855613, mean_q: -37.898508, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 459048/500000: episode: 3302, duration: 0.863s, episode steps: 187, steps per second: 217, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.065747, mae: 26.846395, mean_q: -38.016370, mean_eps: 0.100000\n",
      " 459172/500000: episode: 3303, duration: 0.572s, episode steps: 124, steps per second: 217, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.077681, mae: 27.417096, mean_q: -39.105529, mean_eps: 0.100000\n",
      " 459267/500000: episode: 3304, duration: 0.445s, episode steps:  95, steps per second: 214, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000],  loss: 0.061348, mae: 26.940578, mean_q: -38.647274, mean_eps: 0.100000\n",
      " 459436/500000: episode: 3305, duration: 0.797s, episode steps: 169, steps per second: 212, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 0.070883, mae: 27.348122, mean_q: -39.249336, mean_eps: 0.100000\n",
      " 459546/500000: episode: 3306, duration: 0.535s, episode steps: 110, steps per second: 206, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.111200, mae: 27.165420, mean_q: -39.051194, mean_eps: 0.100000\n",
      " 459654/500000: episode: 3307, duration: 0.524s, episode steps: 108, steps per second: 206, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.083079, mae: 26.564739, mean_q: -38.173314, mean_eps: 0.100000\n",
      " 459770/500000: episode: 3308, duration: 0.553s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000],  loss: 0.092390, mae: 26.794726, mean_q: -38.451789, mean_eps: 0.100000\n",
      " 459879/500000: episode: 3309, duration: 0.506s, episode steps: 109, steps per second: 215, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.084904, mae: 26.820146, mean_q: -38.393459, mean_eps: 0.100000\n",
      " 460038/500000: episode: 3310, duration: 0.729s, episode steps: 159, steps per second: 218, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.083016, mae: 26.046458, mean_q: -37.126801, mean_eps: 0.100000\n",
      " 460161/500000: episode: 3311, duration: 0.578s, episode steps: 123, steps per second: 213, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.113904, mae: 26.048340, mean_q: -36.980866, mean_eps: 0.100000\n",
      " 460276/500000: episode: 3312, duration: 0.531s, episode steps: 115, steps per second: 217, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000],  loss: 0.108584, mae: 27.088887, mean_q: -38.345887, mean_eps: 0.100000\n",
      " 460434/500000: episode: 3313, duration: 0.728s, episode steps: 158, steps per second: 217, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.094757, mae: 26.642399, mean_q: -37.950394, mean_eps: 0.100000\n",
      " 460586/500000: episode: 3314, duration: 0.708s, episode steps: 152, steps per second: 215, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.079 [0.000, 2.000],  loss: 0.099760, mae: 27.587973, mean_q: -39.355443, mean_eps: 0.100000\n",
      " 460766/500000: episode: 3315, duration: 0.827s, episode steps: 180, steps per second: 218, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000],  loss: 0.154155, mae: 27.769837, mean_q: -39.608556, mean_eps: 0.100000\n",
      " 460954/500000: episode: 3316, duration: 0.869s, episode steps: 188, steps per second: 216, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000],  loss: 0.134440, mae: 28.326105, mean_q: -40.431252, mean_eps: 0.100000\n",
      " 461061/500000: episode: 3317, duration: 0.505s, episode steps: 107, steps per second: 212, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.175098, mae: 28.230764, mean_q: -40.399694, mean_eps: 0.100000\n",
      " 461191/500000: episode: 3318, duration: 0.600s, episode steps: 130, steps per second: 217, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000],  loss: 0.124200, mae: 28.379693, mean_q: -40.699062, mean_eps: 0.100000\n",
      " 461311/500000: episode: 3319, duration: 0.574s, episode steps: 120, steps per second: 209, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.122539, mae: 27.993503, mean_q: -39.946056, mean_eps: 0.100000\n",
      " 461438/500000: episode: 3320, duration: 0.588s, episode steps: 127, steps per second: 216, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000],  loss: 0.167009, mae: 27.478128, mean_q: -39.256740, mean_eps: 0.100000\n",
      " 461558/500000: episode: 3321, duration: 0.550s, episode steps: 120, steps per second: 218, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000],  loss: 0.170732, mae: 26.664001, mean_q: -37.938860, mean_eps: 0.100000\n",
      " 461674/500000: episode: 3322, duration: 0.543s, episode steps: 116, steps per second: 214, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.276 [0.000, 2.000],  loss: 0.122018, mae: 26.914876, mean_q: -38.109631, mean_eps: 0.100000\n",
      " 461792/500000: episode: 3323, duration: 0.552s, episode steps: 118, steps per second: 214, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.373 [0.000, 2.000],  loss: 0.102329, mae: 26.151714, mean_q: -37.056392, mean_eps: 0.100000\n",
      " 461909/500000: episode: 3324, duration: 0.541s, episode steps: 117, steps per second: 216, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000],  loss: 0.066393, mae: 25.766978, mean_q: -36.530426, mean_eps: 0.100000\n",
      " 462073/500000: episode: 3325, duration: 0.755s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.063641, mae: 26.189803, mean_q: -37.205314, mean_eps: 0.100000\n",
      " 462266/500000: episode: 3326, duration: 0.924s, episode steps: 193, steps per second: 209, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.068768, mae: 26.686650, mean_q: -38.174778, mean_eps: 0.100000\n",
      " 462438/500000: episode: 3327, duration: 0.875s, episode steps: 172, steps per second: 196, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.097131, mae: 27.694338, mean_q: -39.688854, mean_eps: 0.100000\n",
      " 462627/500000: episode: 3328, duration: 0.968s, episode steps: 189, steps per second: 195, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.228 [0.000, 2.000],  loss: 0.121617, mae: 28.706304, mean_q: -41.237970, mean_eps: 0.100000\n",
      " 462744/500000: episode: 3329, duration: 0.555s, episode steps: 117, steps per second: 211, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.101884, mae: 28.683534, mean_q: -41.342414, mean_eps: 0.100000\n",
      " 462857/500000: episode: 3330, duration: 0.545s, episode steps: 113, steps per second: 207, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.086547, mae: 28.828158, mean_q: -41.596550, mean_eps: 0.100000\n",
      " 463039/500000: episode: 3331, duration: 0.842s, episode steps: 182, steps per second: 216, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.187 [0.000, 2.000],  loss: 0.122463, mae: 28.809817, mean_q: -41.484406, mean_eps: 0.100000\n",
      " 463222/500000: episode: 3332, duration: 0.844s, episode steps: 183, steps per second: 217, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.183149, mae: 28.669060, mean_q: -40.938232, mean_eps: 0.100000\n",
      " 463377/500000: episode: 3333, duration: 0.739s, episode steps: 155, steps per second: 210, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.132666, mae: 28.418441, mean_q: -40.519786, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 463489/500000: episode: 3334, duration: 0.547s, episode steps: 112, steps per second: 205, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000],  loss: 0.119883, mae: 28.085350, mean_q: -39.969501, mean_eps: 0.100000\n",
      " 463689/500000: episode: 3335, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 0.108286, mae: 27.474543, mean_q: -38.964496, mean_eps: 0.100000\n",
      " 463803/500000: episode: 3336, duration: 0.531s, episode steps: 114, steps per second: 215, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.108280, mae: 27.841567, mean_q: -39.559318, mean_eps: 0.100000\n",
      " 463952/500000: episode: 3337, duration: 0.730s, episode steps: 149, steps per second: 204, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.423 [0.000, 2.000],  loss: 0.121118, mae: 28.004786, mean_q: -39.528181, mean_eps: 0.100000\n",
      " 464057/500000: episode: 3338, duration: 0.505s, episode steps: 105, steps per second: 208, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.097104, mae: 28.089743, mean_q: -39.542475, mean_eps: 0.100000\n",
      " 464153/500000: episode: 3339, duration: 0.458s, episode steps:  96, steps per second: 210, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.072014, mae: 27.209488, mean_q: -38.152841, mean_eps: 0.100000\n",
      " 464266/500000: episode: 3340, duration: 0.534s, episode steps: 113, steps per second: 211, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.063199, mae: 27.197341, mean_q: -38.117746, mean_eps: 0.100000\n",
      " 464427/500000: episode: 3341, duration: 0.761s, episode steps: 161, steps per second: 212, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.051461, mae: 27.069167, mean_q: -38.096178, mean_eps: 0.100000\n",
      " 464609/500000: episode: 3342, duration: 0.837s, episode steps: 182, steps per second: 217, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000],  loss: 0.046109, mae: 27.573801, mean_q: -38.928992, mean_eps: 0.100000\n",
      " 464718/500000: episode: 3343, duration: 0.510s, episode steps: 109, steps per second: 214, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000],  loss: 0.054248, mae: 27.344025, mean_q: -38.490625, mean_eps: 0.100000\n",
      " 464876/500000: episode: 3344, duration: 0.727s, episode steps: 158, steps per second: 217, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.106401, mae: 27.117211, mean_q: -38.091105, mean_eps: 0.100000\n",
      " 464995/500000: episode: 3345, duration: 0.556s, episode steps: 119, steps per second: 214, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.091288, mae: 26.581489, mean_q: -37.690938, mean_eps: 0.100000\n",
      " 465112/500000: episode: 3346, duration: 0.549s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000],  loss: 0.079148, mae: 26.136420, mean_q: -37.198269, mean_eps: 0.100000\n",
      " 465295/500000: episode: 3347, duration: 0.847s, episode steps: 183, steps per second: 216, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.117721, mae: 26.896825, mean_q: -38.410387, mean_eps: 0.100000\n",
      " 465402/500000: episode: 3348, duration: 0.492s, episode steps: 107, steps per second: 217, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000],  loss: 0.092105, mae: 25.751576, mean_q: -36.786321, mean_eps: 0.100000\n",
      " 465602/500000: episode: 3349, duration: 0.918s, episode steps: 200, steps per second: 218, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.107452, mae: 25.928225, mean_q: -36.706420, mean_eps: 0.100000\n",
      " 465718/500000: episode: 3350, duration: 0.537s, episode steps: 116, steps per second: 216, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.170837, mae: 26.348494, mean_q: -37.334787, mean_eps: 0.100000\n",
      " 465846/500000: episode: 3351, duration: 0.598s, episode steps: 128, steps per second: 214, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.138473, mae: 26.292646, mean_q: -37.396856, mean_eps: 0.100000\n",
      " 466003/500000: episode: 3352, duration: 0.741s, episode steps: 157, steps per second: 212, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.140947, mae: 26.905785, mean_q: -37.995961, mean_eps: 0.100000\n",
      " 466163/500000: episode: 3353, duration: 0.750s, episode steps: 160, steps per second: 213, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 0.145944, mae: 27.622999, mean_q: -38.799135, mean_eps: 0.100000\n",
      " 466311/500000: episode: 3354, duration: 0.686s, episode steps: 148, steps per second: 216, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.227778, mae: 27.434082, mean_q: -38.492441, mean_eps: 0.100000\n",
      " 466435/500000: episode: 3355, duration: 0.573s, episode steps: 124, steps per second: 216, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000],  loss: 0.134690, mae: 27.671383, mean_q: -38.859046, mean_eps: 0.100000\n",
      " 466587/500000: episode: 3356, duration: 0.706s, episode steps: 152, steps per second: 215, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000],  loss: 0.188922, mae: 28.085595, mean_q: -39.340611, mean_eps: 0.100000\n",
      " 466767/500000: episode: 3357, duration: 0.833s, episode steps: 180, steps per second: 216, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000],  loss: 0.174998, mae: 28.990246, mean_q: -40.503405, mean_eps: 0.100000\n",
      " 466893/500000: episode: 3358, duration: 0.588s, episode steps: 126, steps per second: 214, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.127673, mae: 29.161002, mean_q: -41.048655, mean_eps: 0.100000\n",
      " 467016/500000: episode: 3359, duration: 0.586s, episode steps: 123, steps per second: 210, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.187 [0.000, 2.000],  loss: 0.175457, mae: 28.877003, mean_q: -40.536249, mean_eps: 0.100000\n",
      " 467141/500000: episode: 3360, duration: 0.588s, episode steps: 125, steps per second: 212, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.163031, mae: 28.706587, mean_q: -40.070571, mean_eps: 0.100000\n",
      " 467239/500000: episode: 3361, duration: 0.464s, episode steps:  98, steps per second: 211, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.177441, mae: 28.148958, mean_q: -39.315504, mean_eps: 0.100000\n",
      " 467357/500000: episode: 3362, duration: 0.556s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.121436, mae: 28.092028, mean_q: -39.064132, mean_eps: 0.100000\n",
      " 467475/500000: episode: 3363, duration: 0.559s, episode steps: 118, steps per second: 211, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.254 [0.000, 2.000],  loss: 0.111564, mae: 27.519747, mean_q: -38.332000, mean_eps: 0.100000\n",
      " 467638/500000: episode: 3364, duration: 0.779s, episode steps: 163, steps per second: 209, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.077275, mae: 27.168554, mean_q: -38.109942, mean_eps: 0.100000\n",
      " 467814/500000: episode: 3365, duration: 0.827s, episode steps: 176, steps per second: 213, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.075214, mae: 26.508918, mean_q: -37.340268, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 468014/500000: episode: 3366, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.102691, mae: 27.003870, mean_q: -38.005570, mean_eps: 0.100000\n",
      " 468123/500000: episode: 3367, duration: 0.501s, episode steps: 109, steps per second: 218, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.120683, mae: 27.237419, mean_q: -38.394691, mean_eps: 0.100000\n",
      " 468309/500000: episode: 3368, duration: 0.853s, episode steps: 186, steps per second: 218, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.124958, mae: 27.920568, mean_q: -39.369913, mean_eps: 0.100000\n",
      " 468469/500000: episode: 3369, duration: 0.736s, episode steps: 160, steps per second: 218, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000],  loss: 0.168474, mae: 28.511244, mean_q: -40.240025, mean_eps: 0.100000\n",
      " 468599/500000: episode: 3370, duration: 0.601s, episode steps: 130, steps per second: 216, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000],  loss: 0.120755, mae: 28.212949, mean_q: -39.929427, mean_eps: 0.100000\n",
      " 468749/500000: episode: 3371, duration: 0.692s, episode steps: 150, steps per second: 217, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000],  loss: 0.142656, mae: 28.505252, mean_q: -39.931438, mean_eps: 0.100000\n",
      " 468903/500000: episode: 3372, duration: 0.714s, episode steps: 154, steps per second: 216, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000],  loss: 0.092688, mae: 28.264026, mean_q: -39.629863, mean_eps: 0.100000\n",
      " 469068/500000: episode: 3373, duration: 0.761s, episode steps: 165, steps per second: 217, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.116034, mae: 28.358518, mean_q: -39.888217, mean_eps: 0.100000\n",
      " 469221/500000: episode: 3374, duration: 0.707s, episode steps: 153, steps per second: 216, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.098083, mae: 28.596134, mean_q: -40.367164, mean_eps: 0.100000\n",
      " 469318/500000: episode: 3375, duration: 0.453s, episode steps:  97, steps per second: 214, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000],  loss: 0.110565, mae: 27.946505, mean_q: -39.622119, mean_eps: 0.100000\n",
      " 469472/500000: episode: 3376, duration: 0.736s, episode steps: 154, steps per second: 209, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.961 [0.000, 2.000],  loss: 0.080310, mae: 28.233421, mean_q: -39.983630, mean_eps: 0.100000\n",
      " 469630/500000: episode: 3377, duration: 0.748s, episode steps: 158, steps per second: 211, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.076071, mae: 28.495102, mean_q: -40.448971, mean_eps: 0.100000\n",
      " 469755/500000: episode: 3378, duration: 0.589s, episode steps: 125, steps per second: 212, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.077451, mae: 28.276244, mean_q: -40.466576, mean_eps: 0.100000\n",
      " 469883/500000: episode: 3379, duration: 0.608s, episode steps: 128, steps per second: 210, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000],  loss: 0.071184, mae: 28.142673, mean_q: -40.327050, mean_eps: 0.100000\n",
      " 469996/500000: episode: 3380, duration: 0.528s, episode steps: 113, steps per second: 214, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.052775, mae: 27.419755, mean_q: -39.261171, mean_eps: 0.100000\n",
      " 470085/500000: episode: 3381, duration: 0.423s, episode steps:  89, steps per second: 210, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.043395, mae: 26.872502, mean_q: -38.399864, mean_eps: 0.100000\n",
      " 470249/500000: episode: 3382, duration: 0.757s, episode steps: 164, steps per second: 217, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.069129, mae: 26.445734, mean_q: -37.707655, mean_eps: 0.100000\n",
      " 470369/500000: episode: 3383, duration: 0.552s, episode steps: 120, steps per second: 217, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.097878, mae: 26.711536, mean_q: -38.175040, mean_eps: 0.100000\n",
      " 470496/500000: episode: 3384, duration: 0.593s, episode steps: 127, steps per second: 214, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.323 [0.000, 2.000],  loss: 0.092735, mae: 26.284123, mean_q: -37.427939, mean_eps: 0.100000\n",
      " 470659/500000: episode: 3385, duration: 0.752s, episode steps: 163, steps per second: 217, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.939 [0.000, 2.000],  loss: 0.081763, mae: 26.293360, mean_q: -37.135798, mean_eps: 0.100000\n",
      " 470804/500000: episode: 3386, duration: 0.670s, episode steps: 145, steps per second: 216, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.428 [0.000, 2.000],  loss: 0.104995, mae: 26.176464, mean_q: -36.665280, mean_eps: 0.100000\n",
      " 470975/500000: episode: 3387, duration: 0.815s, episode steps: 171, steps per second: 210, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.181463, mae: 26.474495, mean_q: -37.013342, mean_eps: 0.100000\n",
      " 471135/500000: episode: 3388, duration: 0.748s, episode steps: 160, steps per second: 214, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000],  loss: 0.136190, mae: 27.362603, mean_q: -38.358612, mean_eps: 0.100000\n",
      " 471291/500000: episode: 3389, duration: 0.729s, episode steps: 156, steps per second: 214, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.109708, mae: 27.706229, mean_q: -39.041015, mean_eps: 0.100000\n",
      " 471446/500000: episode: 3390, duration: 0.716s, episode steps: 155, steps per second: 216, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.096277, mae: 28.282279, mean_q: -39.869521, mean_eps: 0.100000\n",
      " 471604/500000: episode: 3391, duration: 0.740s, episode steps: 158, steps per second: 213, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.102793, mae: 28.616190, mean_q: -40.814824, mean_eps: 0.100000\n",
      " 471804/500000: episode: 3392, duration: 0.914s, episode steps: 200, steps per second: 219, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 0.161569, mae: 28.913065, mean_q: -41.347191, mean_eps: 0.100000\n",
      " 471990/500000: episode: 3393, duration: 0.858s, episode steps: 186, steps per second: 217, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 1.232227, mae: 29.439145, mean_q: -42.005368, mean_eps: 0.100000\n",
      " 472102/500000: episode: 3394, duration: 0.519s, episode steps: 112, steps per second: 216, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.476304, mae: 28.450081, mean_q: -40.716832, mean_eps: 0.100000\n",
      " 472218/500000: episode: 3395, duration: 0.533s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.598045, mae: 28.358799, mean_q: -40.276916, mean_eps: 0.100000\n",
      " 472329/500000: episode: 3396, duration: 0.514s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 0.535929, mae: 28.391870, mean_q: -40.121750, mean_eps: 0.100000\n",
      " 472427/500000: episode: 3397, duration: 0.472s, episode steps:  98, steps per second: 208, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000],  loss: 0.263780, mae: 27.083529, mean_q: -38.259390, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 472561/500000: episode: 3398, duration: 0.635s, episode steps: 134, steps per second: 211, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.636798, mae: 26.994654, mean_q: -37.780751, mean_eps: 0.100000\n",
      " 472680/500000: episode: 3399, duration: 0.568s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000],  loss: 0.500075, mae: 26.689889, mean_q: -37.060143, mean_eps: 0.100000\n",
      " 472851/500000: episode: 3400, duration: 0.802s, episode steps: 171, steps per second: 213, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.264153, mae: 26.865589, mean_q: -37.434598, mean_eps: 0.100000\n",
      " 472982/500000: episode: 3401, duration: 0.619s, episode steps: 131, steps per second: 212, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.091047, mae: 26.479760, mean_q: -37.088058, mean_eps: 0.100000\n",
      " 473100/500000: episode: 3402, duration: 0.562s, episode steps: 118, steps per second: 210, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.112107, mae: 26.701256, mean_q: -37.282176, mean_eps: 0.100000\n",
      " 473259/500000: episode: 3403, duration: 0.733s, episode steps: 159, steps per second: 217, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.131572, mae: 27.061192, mean_q: -37.962445, mean_eps: 0.100000\n",
      " 473386/500000: episode: 3404, duration: 0.598s, episode steps: 127, steps per second: 212, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.142895, mae: 27.356937, mean_q: -38.461481, mean_eps: 0.100000\n",
      " 473564/500000: episode: 3405, duration: 0.824s, episode steps: 178, steps per second: 216, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000],  loss: 0.169720, mae: 27.858321, mean_q: -39.235864, mean_eps: 0.100000\n",
      " 473687/500000: episode: 3406, duration: 0.575s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000],  loss: 0.169752, mae: 28.062955, mean_q: -39.526552, mean_eps: 0.100000\n",
      " 473824/500000: episode: 3407, duration: 0.632s, episode steps: 137, steps per second: 217, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.394 [0.000, 2.000],  loss: 0.115140, mae: 27.231511, mean_q: -38.398911, mean_eps: 0.100000\n",
      " 473950/500000: episode: 3408, duration: 0.584s, episode steps: 126, steps per second: 216, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000],  loss: 0.112565, mae: 27.750512, mean_q: -39.121427, mean_eps: 0.100000\n",
      " 474071/500000: episode: 3409, duration: 0.561s, episode steps: 121, steps per second: 216, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 0.088620, mae: 27.555043, mean_q: -39.063486, mean_eps: 0.100000\n",
      " 474237/500000: episode: 3410, duration: 0.767s, episode steps: 166, steps per second: 217, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.151 [0.000, 2.000],  loss: 0.072137, mae: 27.265628, mean_q: -38.847559, mean_eps: 0.100000\n",
      " 474351/500000: episode: 3411, duration: 0.548s, episode steps: 114, steps per second: 208, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.080271, mae: 27.447432, mean_q: -39.206510, mean_eps: 0.100000\n",
      " 474467/500000: episode: 3412, duration: 0.552s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.059848, mae: 27.455730, mean_q: -39.389176, mean_eps: 0.100000\n",
      " 474553/500000: episode: 3413, duration: 0.405s, episode steps:  86, steps per second: 212, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000],  loss: 0.046454, mae: 26.476426, mean_q: -37.932359, mean_eps: 0.100000\n",
      " 474746/500000: episode: 3414, duration: 0.883s, episode steps: 193, steps per second: 219, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.249 [0.000, 2.000],  loss: 0.064203, mae: 26.647891, mean_q: -38.159852, mean_eps: 0.100000\n",
      " 474929/500000: episode: 3415, duration: 0.849s, episode steps: 183, steps per second: 216, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000],  loss: 0.087668, mae: 27.305739, mean_q: -39.316936, mean_eps: 0.100000\n",
      " 475092/500000: episode: 3416, duration: 0.746s, episode steps: 163, steps per second: 218, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000],  loss: 0.126318, mae: 27.907986, mean_q: -40.179460, mean_eps: 0.100000\n",
      " 475276/500000: episode: 3417, duration: 0.849s, episode steps: 184, steps per second: 217, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.142048, mae: 27.290677, mean_q: -39.050573, mean_eps: 0.100000\n",
      " 475387/500000: episode: 3418, duration: 0.513s, episode steps: 111, steps per second: 216, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.121750, mae: 26.890419, mean_q: -38.573841, mean_eps: 0.100000\n",
      " 475574/500000: episode: 3419, duration: 0.874s, episode steps: 187, steps per second: 214, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.120287, mae: 27.445393, mean_q: -39.465884, mean_eps: 0.100000\n",
      " 475686/500000: episode: 3420, duration: 0.538s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.099133, mae: 27.502020, mean_q: -39.661912, mean_eps: 0.100000\n",
      " 475802/500000: episode: 3421, duration: 0.546s, episode steps: 116, steps per second: 213, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.092294, mae: 27.194736, mean_q: -39.199748, mean_eps: 0.100000\n",
      " 475916/500000: episode: 3422, duration: 0.540s, episode steps: 114, steps per second: 211, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.089537, mae: 26.626991, mean_q: -38.255297, mean_eps: 0.100000\n",
      " 476023/500000: episode: 3423, duration: 0.493s, episode steps: 107, steps per second: 217, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000],  loss: 0.049031, mae: 26.993035, mean_q: -38.919494, mean_eps: 0.100000\n",
      " 476149/500000: episode: 3424, duration: 0.581s, episode steps: 126, steps per second: 217, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000],  loss: 0.058886, mae: 26.529731, mean_q: -38.192057, mean_eps: 0.100000\n",
      " 476349/500000: episode: 3425, duration: 0.923s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 0.059740, mae: 27.042703, mean_q: -38.805947, mean_eps: 0.100000\n",
      " 476482/500000: episode: 3426, duration: 0.606s, episode steps: 133, steps per second: 219, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.429 [0.000, 2.000],  loss: 0.070888, mae: 27.011687, mean_q: -38.641155, mean_eps: 0.100000\n",
      " 476604/500000: episode: 3427, duration: 0.565s, episode steps: 122, steps per second: 216, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000],  loss: 0.066698, mae: 26.548307, mean_q: -37.871214, mean_eps: 0.100000\n",
      " 476723/500000: episode: 3428, duration: 0.561s, episode steps: 119, steps per second: 212, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.048679, mae: 26.955134, mean_q: -38.322703, mean_eps: 0.100000\n",
      " 476841/500000: episode: 3429, duration: 0.548s, episode steps: 118, steps per second: 215, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.032387, mae: 27.339967, mean_q: -38.920819, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 476972/500000: episode: 3430, duration: 0.603s, episode steps: 131, steps per second: 217, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 0.067717, mae: 26.977418, mean_q: -38.242534, mean_eps: 0.100000\n",
      " 477080/500000: episode: 3431, duration: 0.509s, episode steps: 108, steps per second: 212, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.064163, mae: 27.553644, mean_q: -38.898839, mean_eps: 0.100000\n",
      " 477166/500000: episode: 3432, duration: 0.416s, episode steps:  86, steps per second: 207, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000],  loss: 0.055294, mae: 27.366613, mean_q: -38.610751, mean_eps: 0.100000\n",
      " 477284/500000: episode: 3433, duration: 0.557s, episode steps: 118, steps per second: 212, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000],  loss: 0.065699, mae: 26.691680, mean_q: -37.735217, mean_eps: 0.100000\n",
      " 477396/500000: episode: 3434, duration: 0.523s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.042832, mae: 26.730106, mean_q: -37.854053, mean_eps: 0.100000\n",
      " 477579/500000: episode: 3435, duration: 0.838s, episode steps: 183, steps per second: 218, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000],  loss: 0.047754, mae: 26.696628, mean_q: -37.619868, mean_eps: 0.100000\n",
      " 477666/500000: episode: 3436, duration: 0.408s, episode steps:  87, steps per second: 213, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000],  loss: 0.083086, mae: 26.323297, mean_q: -37.107324, mean_eps: 0.100000\n",
      " 477841/500000: episode: 3437, duration: 0.803s, episode steps: 175, steps per second: 218, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.064612, mae: 27.018676, mean_q: -38.191252, mean_eps: 0.100000\n",
      " 478015/500000: episode: 3438, duration: 0.801s, episode steps: 174, steps per second: 217, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.106210, mae: 27.423760, mean_q: -39.067609, mean_eps: 0.100000\n",
      " 478194/500000: episode: 3439, duration: 0.825s, episode steps: 179, steps per second: 217, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.089 [0.000, 2.000],  loss: 0.122496, mae: 28.143450, mean_q: -40.161899, mean_eps: 0.100000\n",
      " 478312/500000: episode: 3440, duration: 0.547s, episode steps: 118, steps per second: 216, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 0.124327, mae: 27.899950, mean_q: -39.776205, mean_eps: 0.100000\n",
      " 478435/500000: episode: 3441, duration: 0.584s, episode steps: 123, steps per second: 211, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.124400, mae: 27.857059, mean_q: -39.814059, mean_eps: 0.100000\n",
      " 478635/500000: episode: 3442, duration: 0.945s, episode steps: 200, steps per second: 212, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.133840, mae: 27.863581, mean_q: -39.872588, mean_eps: 0.100000\n",
      " 478744/500000: episode: 3443, duration: 0.515s, episode steps: 109, steps per second: 212, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.117079, mae: 28.101900, mean_q: -40.254264, mean_eps: 0.100000\n",
      " 478917/500000: episode: 3444, duration: 0.809s, episode steps: 173, steps per second: 214, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.105173, mae: 27.547189, mean_q: -39.228039, mean_eps: 0.100000\n",
      " 479089/500000: episode: 3445, duration: 0.813s, episode steps: 172, steps per second: 212, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.134 [0.000, 2.000],  loss: 0.120610, mae: 27.195210, mean_q: -38.567227, mean_eps: 0.100000\n",
      " 479289/500000: episode: 3446, duration: 0.933s, episode steps: 200, steps per second: 214, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 0.134566, mae: 26.923511, mean_q: -37.980111, mean_eps: 0.100000\n",
      " 479390/500000: episode: 3447, duration: 0.479s, episode steps: 101, steps per second: 211, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000],  loss: 0.128744, mae: 27.137220, mean_q: -38.388243, mean_eps: 0.100000\n",
      " 479486/500000: episode: 3448, duration: 0.451s, episode steps:  96, steps per second: 213, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.139098, mae: 27.017416, mean_q: -37.928392, mean_eps: 0.100000\n",
      " 479667/500000: episode: 3449, duration: 0.830s, episode steps: 181, steps per second: 218, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.138 [0.000, 2.000],  loss: 0.341748, mae: 27.924921, mean_q: -39.058124, mean_eps: 0.100000\n",
      " 479769/500000: episode: 3450, duration: 0.480s, episode steps: 102, steps per second: 213, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000],  loss: 0.186337, mae: 28.338493, mean_q: -39.779705, mean_eps: 0.100000\n",
      " 479944/500000: episode: 3451, duration: 0.808s, episode steps: 175, steps per second: 217, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.149 [0.000, 2.000],  loss: 0.144110, mae: 28.956640, mean_q: -40.638030, mean_eps: 0.100000\n",
      " 480117/500000: episode: 3452, duration: 0.798s, episode steps: 173, steps per second: 217, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.160048, mae: 29.683657, mean_q: -41.885686, mean_eps: 0.100000\n",
      " 480275/500000: episode: 3453, duration: 0.731s, episode steps: 158, steps per second: 216, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.123369, mae: 29.385953, mean_q: -41.605287, mean_eps: 0.100000\n",
      " 480395/500000: episode: 3454, duration: 0.559s, episode steps: 120, steps per second: 215, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.088262, mae: 30.037935, mean_q: -42.684138, mean_eps: 0.100000\n",
      " 480486/500000: episode: 3455, duration: 0.440s, episode steps:  91, steps per second: 207, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.121 [0.000, 2.000],  loss: 0.080995, mae: 29.679708, mean_q: -42.361180, mean_eps: 0.100000\n",
      " 480654/500000: episode: 3456, duration: 0.795s, episode steps: 168, steps per second: 211, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.074319, mae: 28.854153, mean_q: -41.084998, mean_eps: 0.100000\n",
      " 480756/500000: episode: 3457, duration: 0.481s, episode steps: 102, steps per second: 212, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.087624, mae: 28.639129, mean_q: -40.766451, mean_eps: 0.100000\n",
      " 480937/500000: episode: 3458, duration: 0.837s, episode steps: 181, steps per second: 216, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.139809, mae: 28.265324, mean_q: -40.131534, mean_eps: 0.100000\n",
      " 481037/500000: episode: 3459, duration: 0.469s, episode steps: 100, steps per second: 213, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.113770, mae: 27.523856, mean_q: -38.820998, mean_eps: 0.100000\n",
      " 481206/500000: episode: 3460, duration: 0.794s, episode steps: 169, steps per second: 213, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.139882, mae: 27.229700, mean_q: -38.035771, mean_eps: 0.100000\n",
      " 481326/500000: episode: 3461, duration: 0.558s, episode steps: 120, steps per second: 215, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.124142, mae: 27.549353, mean_q: -38.600659, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 481501/500000: episode: 3462, duration: 0.818s, episode steps: 175, steps per second: 214, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000],  loss: 0.129475, mae: 28.449343, mean_q: -40.074280, mean_eps: 0.100000\n",
      " 481647/500000: episode: 3463, duration: 0.685s, episode steps: 146, steps per second: 213, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000],  loss: 0.137670, mae: 28.099044, mean_q: -39.546285, mean_eps: 0.100000\n",
      " 481766/500000: episode: 3464, duration: 0.547s, episode steps: 119, steps per second: 218, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.139159, mae: 27.853913, mean_q: -38.974526, mean_eps: 0.100000\n",
      " 481941/500000: episode: 3465, duration: 0.811s, episode steps: 175, steps per second: 216, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000],  loss: 0.126558, mae: 28.054293, mean_q: -39.311825, mean_eps: 0.100000\n",
      " 482109/500000: episode: 3466, duration: 0.774s, episode steps: 168, steps per second: 217, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.106415, mae: 28.995135, mean_q: -40.927106, mean_eps: 0.100000\n",
      " 482289/500000: episode: 3467, duration: 0.830s, episode steps: 180, steps per second: 217, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.911 [0.000, 2.000],  loss: 0.096264, mae: 29.952711, mean_q: -42.693153, mean_eps: 0.100000\n",
      " 482489/500000: episode: 3468, duration: 0.938s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.062759, mae: 30.197523, mean_q: -43.373912, mean_eps: 0.100000\n",
      " 482660/500000: episode: 3469, duration: 0.823s, episode steps: 171, steps per second: 208, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.225610, mae: 31.059899, mean_q: -44.618947, mean_eps: 0.100000\n",
      " 482828/500000: episode: 3470, duration: 0.786s, episode steps: 168, steps per second: 214, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 0.164379, mae: 31.732906, mean_q: -45.497381, mean_eps: 0.100000\n",
      " 482947/500000: episode: 3471, duration: 0.566s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.148770, mae: 30.737689, mean_q: -44.090354, mean_eps: 0.100000\n",
      " 483126/500000: episode: 3472, duration: 0.842s, episode steps: 179, steps per second: 213, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000],  loss: 0.169283, mae: 30.778165, mean_q: -43.833696, mean_eps: 0.100000\n",
      " 483298/500000: episode: 3473, duration: 0.795s, episode steps: 172, steps per second: 216, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.186746, mae: 30.428013, mean_q: -43.267713, mean_eps: 0.100000\n",
      " 483491/500000: episode: 3474, duration: 0.890s, episode steps: 193, steps per second: 217, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.165196, mae: 30.313919, mean_q: -43.063777, mean_eps: 0.100000\n",
      " 483590/500000: episode: 3475, duration: 0.463s, episode steps:  99, steps per second: 214, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000],  loss: 0.155289, mae: 29.613140, mean_q: -41.848178, mean_eps: 0.100000\n",
      " 483713/500000: episode: 3476, duration: 0.575s, episode steps: 123, steps per second: 214, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000],  loss: 0.115897, mae: 29.463843, mean_q: -41.517532, mean_eps: 0.100000\n",
      " 483811/500000: episode: 3477, duration: 0.471s, episode steps:  98, steps per second: 208, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.123479, mae: 28.764717, mean_q: -40.529090, mean_eps: 0.100000\n",
      " 483912/500000: episode: 3478, duration: 0.481s, episode steps: 101, steps per second: 210, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.119177, mae: 28.319251, mean_q: -39.769221, mean_eps: 0.100000\n",
      " 484085/500000: episode: 3479, duration: 0.814s, episode steps: 173, steps per second: 213, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.138563, mae: 27.882194, mean_q: -39.000853, mean_eps: 0.100000\n",
      " 484208/500000: episode: 3480, duration: 0.568s, episode steps: 123, steps per second: 216, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000],  loss: 0.118291, mae: 27.478264, mean_q: -38.435789, mean_eps: 0.100000\n",
      " 484344/500000: episode: 3481, duration: 0.630s, episode steps: 136, steps per second: 216, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.449 [0.000, 2.000],  loss: 0.118352, mae: 26.587420, mean_q: -37.124869, mean_eps: 0.100000\n",
      " 484491/500000: episode: 3482, duration: 0.683s, episode steps: 147, steps per second: 215, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.105092, mae: 26.266648, mean_q: -36.704840, mean_eps: 0.100000\n",
      " 484617/500000: episode: 3483, duration: 0.580s, episode steps: 126, steps per second: 217, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 0.112093, mae: 26.888142, mean_q: -37.876524, mean_eps: 0.100000\n",
      " 484817/500000: episode: 3484, duration: 0.929s, episode steps: 200, steps per second: 215, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 0.084942, mae: 26.754908, mean_q: -37.865537, mean_eps: 0.100000\n",
      " 484927/500000: episode: 3485, duration: 0.509s, episode steps: 110, steps per second: 216, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 0.172908, mae: 27.777473, mean_q: -39.594875, mean_eps: 0.100000\n",
      " 485046/500000: episode: 3486, duration: 0.547s, episode steps: 119, steps per second: 218, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000],  loss: 0.539484, mae: 27.647793, mean_q: -39.587701, mean_eps: 0.100000\n",
      " 485205/500000: episode: 3487, duration: 0.738s, episode steps: 159, steps per second: 216, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.318255, mae: 27.851710, mean_q: -39.933721, mean_eps: 0.100000\n",
      " 485320/500000: episode: 3488, duration: 0.541s, episode steps: 115, steps per second: 212, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.426 [0.000, 2.000],  loss: 0.430574, mae: 27.739235, mean_q: -39.634348, mean_eps: 0.100000\n",
      " 485441/500000: episode: 3489, duration: 0.566s, episode steps: 121, steps per second: 214, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000],  loss: 0.329877, mae: 27.370286, mean_q: -39.391709, mean_eps: 0.100000\n",
      " 485576/500000: episode: 3490, duration: 0.626s, episode steps: 135, steps per second: 216, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000],  loss: 0.252976, mae: 26.461600, mean_q: -38.091729, mean_eps: 0.100000\n",
      " 485691/500000: episode: 3491, duration: 0.532s, episode steps: 115, steps per second: 216, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.452 [0.000, 2.000],  loss: 0.234577, mae: 26.848788, mean_q: -38.468221, mean_eps: 0.100000\n",
      " 485825/500000: episode: 3492, duration: 0.623s, episode steps: 134, steps per second: 215, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000],  loss: 0.197657, mae: 26.657942, mean_q: -38.445709, mean_eps: 0.100000\n",
      " 486011/500000: episode: 3493, duration: 0.859s, episode steps: 186, steps per second: 217, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000],  loss: 0.059053, mae: 26.748315, mean_q: -38.608990, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 486104/500000: episode: 3494, duration: 0.435s, episode steps:  93, steps per second: 214, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.061194, mae: 26.277473, mean_q: -37.924178, mean_eps: 0.100000\n",
      " 486202/500000: episode: 3495, duration: 0.457s, episode steps:  98, steps per second: 214, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.049589, mae: 25.816160, mean_q: -37.150253, mean_eps: 0.100000\n",
      " 486383/500000: episode: 3496, duration: 0.836s, episode steps: 181, steps per second: 216, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000],  loss: 0.082954, mae: 25.860559, mean_q: -36.899339, mean_eps: 0.100000\n",
      " 486532/500000: episode: 3497, duration: 0.692s, episode steps: 149, steps per second: 215, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.409 [0.000, 2.000],  loss: 0.124914, mae: 25.978883, mean_q: -36.837286, mean_eps: 0.100000\n",
      " 486644/500000: episode: 3498, duration: 0.538s, episode steps: 112, steps per second: 208, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000],  loss: 0.077879, mae: 26.320837, mean_q: -37.472148, mean_eps: 0.100000\n",
      " 486756/500000: episode: 3499, duration: 0.544s, episode steps: 112, steps per second: 206, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000],  loss: 0.060041, mae: 26.052877, mean_q: -37.137112, mean_eps: 0.100000\n",
      " 486870/500000: episode: 3500, duration: 0.534s, episode steps: 114, steps per second: 213, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.281 [0.000, 2.000],  loss: 0.100647, mae: 25.835622, mean_q: -36.787109, mean_eps: 0.100000\n",
      " 486984/500000: episode: 3501, duration: 0.546s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.298 [0.000, 2.000],  loss: 0.071928, mae: 25.152831, mean_q: -35.906568, mean_eps: 0.100000\n",
      " 487100/500000: episode: 3502, duration: 0.550s, episode steps: 116, steps per second: 211, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.064094, mae: 26.039760, mean_q: -37.345850, mean_eps: 0.100000\n",
      " 487300/500000: episode: 3503, duration: 0.939s, episode steps: 200, steps per second: 213, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.270 [0.000, 2.000],  loss: 0.089707, mae: 26.615465, mean_q: -38.470161, mean_eps: 0.100000\n",
      " 487413/500000: episode: 3504, duration: 0.534s, episode steps: 113, steps per second: 212, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.049909, mae: 26.318836, mean_q: -38.282255, mean_eps: 0.100000\n",
      " 487574/500000: episode: 3505, duration: 0.763s, episode steps: 161, steps per second: 211, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000],  loss: 0.044625, mae: 26.561346, mean_q: -38.648733, mean_eps: 0.100000\n",
      " 487692/500000: episode: 3506, duration: 0.542s, episode steps: 118, steps per second: 218, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000],  loss: 0.056524, mae: 26.905322, mean_q: -39.161272, mean_eps: 0.100000\n",
      " 487804/500000: episode: 3507, duration: 0.523s, episode steps: 112, steps per second: 214, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.232 [0.000, 2.000],  loss: 0.051752, mae: 26.658001, mean_q: -38.786184, mean_eps: 0.100000\n",
      " 487921/500000: episode: 3508, duration: 0.550s, episode steps: 117, steps per second: 213, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.316 [0.000, 2.000],  loss: 0.048855, mae: 26.813695, mean_q: -39.031524, mean_eps: 0.100000\n",
      " 488088/500000: episode: 3509, duration: 0.781s, episode steps: 167, steps per second: 214, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.064970, mae: 27.294796, mean_q: -39.687709, mean_eps: 0.100000\n",
      " 488245/500000: episode: 3510, duration: 0.722s, episode steps: 157, steps per second: 217, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.100150, mae: 27.644812, mean_q: -40.118435, mean_eps: 0.100000\n",
      " 488394/500000: episode: 3511, duration: 0.707s, episode steps: 149, steps per second: 211, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000],  loss: 0.095272, mae: 27.770049, mean_q: -40.339113, mean_eps: 0.100000\n",
      " 488545/500000: episode: 3512, duration: 0.705s, episode steps: 151, steps per second: 214, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.947 [0.000, 2.000],  loss: 0.094443, mae: 27.952445, mean_q: -40.608129, mean_eps: 0.100000\n",
      " 488641/500000: episode: 3513, duration: 0.449s, episode steps:  96, steps per second: 214, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.177 [0.000, 2.000],  loss: 0.068241, mae: 27.761289, mean_q: -40.315606, mean_eps: 0.100000\n",
      " 488841/500000: episode: 3514, duration: 0.947s, episode steps: 200, steps per second: 211, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000],  loss: 0.099201, mae: 28.119178, mean_q: -40.686789, mean_eps: 0.100000\n",
      " 488958/500000: episode: 3515, duration: 0.553s, episode steps: 117, steps per second: 212, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.112546, mae: 28.739322, mean_q: -41.707072, mean_eps: 0.100000\n",
      " 489090/500000: episode: 3516, duration: 0.612s, episode steps: 132, steps per second: 216, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000],  loss: 0.065289, mae: 27.998949, mean_q: -40.664084, mean_eps: 0.100000\n",
      " 489209/500000: episode: 3517, duration: 0.565s, episode steps: 119, steps per second: 210, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.336 [0.000, 2.000],  loss: 0.139824, mae: 27.627690, mean_q: -39.825735, mean_eps: 0.100000\n",
      " 489316/500000: episode: 3518, duration: 0.508s, episode steps: 107, steps per second: 210, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000],  loss: 0.138216, mae: 27.631556, mean_q: -39.572276, mean_eps: 0.100000\n",
      " 489444/500000: episode: 3519, duration: 0.600s, episode steps: 128, steps per second: 213, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.414 [0.000, 2.000],  loss: 0.141951, mae: 26.873315, mean_q: -38.361731, mean_eps: 0.100000\n",
      " 489573/500000: episode: 3520, duration: 0.610s, episode steps: 129, steps per second: 211, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.426 [0.000, 2.000],  loss: 0.113651, mae: 27.009856, mean_q: -38.452515, mean_eps: 0.100000\n",
      " 489771/500000: episode: 3521, duration: 0.919s, episode steps: 198, steps per second: 215, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000],  loss: 0.079444, mae: 27.415561, mean_q: -39.009709, mean_eps: 0.100000\n",
      " 489971/500000: episode: 3522, duration: 0.920s, episode steps: 200, steps per second: 217, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000],  loss: 0.101596, mae: 27.353958, mean_q: -38.890118, mean_eps: 0.100000\n",
      " 490134/500000: episode: 3523, duration: 0.755s, episode steps: 163, steps per second: 216, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.166 [0.000, 2.000],  loss: 0.402060, mae: 28.574727, mean_q: -40.652417, mean_eps: 0.100000\n",
      " 490324/500000: episode: 3524, duration: 0.885s, episode steps: 190, steps per second: 215, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.389 [0.000, 2.000],  loss: 0.214151, mae: 28.965887, mean_q: -41.613523, mean_eps: 0.100000\n",
      " 490436/500000: episode: 3525, duration: 0.533s, episode steps: 112, steps per second: 210, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.411 [0.000, 2.000],  loss: 0.167810, mae: 29.016901, mean_q: -41.786606, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 490541/500000: episode: 3526, duration: 0.510s, episode steps: 105, steps per second: 206, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.182978, mae: 28.871867, mean_q: -41.365455, mean_eps: 0.100000\n",
      " 490697/500000: episode: 3527, duration: 0.736s, episode steps: 156, steps per second: 212, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.202780, mae: 28.741326, mean_q: -41.304741, mean_eps: 0.100000\n",
      " 490793/500000: episode: 3528, duration: 0.460s, episode steps:  96, steps per second: 209, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 0.125642, mae: 27.947685, mean_q: -40.079624, mean_eps: 0.100000\n",
      " 490976/500000: episode: 3529, duration: 0.865s, episode steps: 183, steps per second: 212, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 0.237411, mae: 28.251825, mean_q: -40.309867, mean_eps: 0.100000\n",
      " 491095/500000: episode: 3530, duration: 0.553s, episode steps: 119, steps per second: 215, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000],  loss: 0.129078, mae: 27.734417, mean_q: -39.390214, mean_eps: 0.100000\n",
      " 491266/500000: episode: 3531, duration: 0.794s, episode steps: 171, steps per second: 215, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.108322, mae: 28.010255, mean_q: -39.657568, mean_eps: 0.100000\n",
      " 491396/500000: episode: 3532, duration: 0.601s, episode steps: 130, steps per second: 216, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.323 [0.000, 2.000],  loss: 0.079404, mae: 28.453047, mean_q: -40.474710, mean_eps: 0.100000\n",
      " 491505/500000: episode: 3533, duration: 0.503s, episode steps: 109, steps per second: 217, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000],  loss: 0.077915, mae: 28.537208, mean_q: -40.578110, mean_eps: 0.100000\n",
      " 491669/500000: episode: 3534, duration: 0.760s, episode steps: 164, steps per second: 216, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000],  loss: 0.089199, mae: 28.878486, mean_q: -41.057686, mean_eps: 0.100000\n",
      " 491780/500000: episode: 3535, duration: 0.515s, episode steps: 111, steps per second: 215, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000],  loss: 0.083048, mae: 28.682228, mean_q: -40.883075, mean_eps: 0.100000\n",
      " 491884/500000: episode: 3536, duration: 0.486s, episode steps: 104, steps per second: 214, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.096785, mae: 28.394282, mean_q: -40.519937, mean_eps: 0.100000\n",
      " 492000/500000: episode: 3537, duration: 0.536s, episode steps: 116, steps per second: 217, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.058567, mae: 27.774117, mean_q: -39.549238, mean_eps: 0.100000\n",
      " 492109/500000: episode: 3538, duration: 0.516s, episode steps: 109, steps per second: 211, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 0.055986, mae: 27.793616, mean_q: -39.807412, mean_eps: 0.100000\n",
      " 492219/500000: episode: 3539, duration: 0.525s, episode steps: 110, steps per second: 210, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000],  loss: 0.052184, mae: 27.208132, mean_q: -39.057600, mean_eps: 0.100000\n",
      " 492414/500000: episode: 3540, duration: 0.895s, episode steps: 195, steps per second: 218, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000],  loss: 0.054490, mae: 27.598491, mean_q: -39.593280, mean_eps: 0.100000\n",
      " 492596/500000: episode: 3541, duration: 0.834s, episode steps: 182, steps per second: 218, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000],  loss: 0.112926, mae: 28.058895, mean_q: -40.273859, mean_eps: 0.100000\n",
      " 492760/500000: episode: 3542, duration: 0.761s, episode steps: 164, steps per second: 216, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000],  loss: 0.123067, mae: 27.778335, mean_q: -39.881321, mean_eps: 0.100000\n",
      " 492873/500000: episode: 3543, duration: 0.526s, episode steps: 113, steps per second: 215, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.133 [0.000, 2.000],  loss: 0.124094, mae: 28.125655, mean_q: -40.244210, mean_eps: 0.100000\n",
      " 492964/500000: episode: 3544, duration: 0.423s, episode steps:  91, steps per second: 215, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.149361, mae: 27.652211, mean_q: -39.639707, mean_eps: 0.100000\n",
      " 493081/500000: episode: 3545, duration: 0.542s, episode steps: 117, steps per second: 216, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000],  loss: 0.139658, mae: 28.103743, mean_q: -40.255936, mean_eps: 0.100000\n",
      " 493192/500000: episode: 3546, duration: 0.517s, episode steps: 111, steps per second: 215, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.234 [0.000, 2.000],  loss: 0.110594, mae: 27.839083, mean_q: -39.806368, mean_eps: 0.100000\n",
      " 493371/500000: episode: 3547, duration: 0.852s, episode steps: 179, steps per second: 210, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.117 [0.000, 2.000],  loss: 0.143647, mae: 27.486365, mean_q: -39.032534, mean_eps: 0.100000\n",
      " 493479/500000: episode: 3548, duration: 0.541s, episode steps: 108, steps per second: 200, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000],  loss: 0.142891, mae: 27.236829, mean_q: -38.490851, mean_eps: 0.100000\n",
      " 493634/500000: episode: 3549, duration: 0.747s, episode steps: 155, steps per second: 207, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.158765, mae: 27.323484, mean_q: -38.132028, mean_eps: 0.100000\n",
      " 493728/500000: episode: 3550, duration: 0.458s, episode steps:  94, steps per second: 205, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000],  loss: 0.119079, mae: 26.753880, mean_q: -37.384371, mean_eps: 0.100000\n",
      " 493838/500000: episode: 3551, duration: 0.535s, episode steps: 110, steps per second: 206, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000],  loss: 0.102868, mae: 26.888780, mean_q: -37.901053, mean_eps: 0.100000\n",
      " 493998/500000: episode: 3552, duration: 0.772s, episode steps: 160, steps per second: 207, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.078839, mae: 27.222064, mean_q: -38.344875, mean_eps: 0.100000\n",
      " 494129/500000: episode: 3553, duration: 0.627s, episode steps: 131, steps per second: 209, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.110263, mae: 27.531123, mean_q: -38.877673, mean_eps: 0.100000\n",
      " 494240/500000: episode: 3554, duration: 0.525s, episode steps: 111, steps per second: 211, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.324 [0.000, 2.000],  loss: 0.122368, mae: 26.724033, mean_q: -37.574503, mean_eps: 0.100000\n",
      " 494423/500000: episode: 3555, duration: 0.861s, episode steps: 183, steps per second: 212, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000],  loss: 0.093701, mae: 26.630319, mean_q: -37.539518, mean_eps: 0.100000\n",
      " 494594/500000: episode: 3556, duration: 0.800s, episode steps: 171, steps per second: 214, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.083529, mae: 26.727625, mean_q: -37.958531, mean_eps: 0.100000\n",
      " 494708/500000: episode: 3557, duration: 0.545s, episode steps: 114, steps per second: 209, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.092212, mae: 27.077256, mean_q: -38.380882, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 494811/500000: episode: 3558, duration: 0.486s, episode steps: 103, steps per second: 212, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 0.105921, mae: 26.691817, mean_q: -37.618191, mean_eps: 0.100000\n",
      " 494920/500000: episode: 3559, duration: 0.532s, episode steps: 109, steps per second: 205, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.085910, mae: 27.009289, mean_q: -38.058004, mean_eps: 0.100000\n",
      " 495045/500000: episode: 3560, duration: 0.595s, episode steps: 125, steps per second: 210, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000],  loss: 0.077517, mae: 26.515480, mean_q: -37.420562, mean_eps: 0.100000\n",
      " 495223/500000: episode: 3561, duration: 0.846s, episode steps: 178, steps per second: 210, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000],  loss: 0.104012, mae: 27.127847, mean_q: -38.241988, mean_eps: 0.100000\n",
      " 495418/500000: episode: 3562, duration: 0.925s, episode steps: 195, steps per second: 211, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000],  loss: 0.114153, mae: 27.264618, mean_q: -38.414105, mean_eps: 0.100000\n",
      " 495534/500000: episode: 3563, duration: 0.551s, episode steps: 116, steps per second: 210, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000],  loss: 0.230407, mae: 27.174680, mean_q: -38.064519, mean_eps: 0.100000\n",
      " 495734/500000: episode: 3564, duration: 0.955s, episode steps: 200, steps per second: 209, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000],  loss: 0.118345, mae: 26.774774, mean_q: -37.719884, mean_eps: 0.100000\n",
      " 495840/500000: episode: 3565, duration: 0.513s, episode steps: 106, steps per second: 206, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.121775, mae: 26.783694, mean_q: -38.133688, mean_eps: 0.100000\n",
      " 495935/500000: episode: 3566, duration: 0.464s, episode steps:  95, steps per second: 205, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000],  loss: 0.183314, mae: 26.826697, mean_q: -38.248016, mean_eps: 0.100000\n",
      " 496104/500000: episode: 3567, duration: 0.821s, episode steps: 169, steps per second: 206, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000],  loss: 0.139083, mae: 27.068306, mean_q: -38.454171, mean_eps: 0.100000\n",
      " 496294/500000: episode: 3568, duration: 0.912s, episode steps: 190, steps per second: 208, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000],  loss: 0.106168, mae: 27.191598, mean_q: -38.963213, mean_eps: 0.100000\n",
      " 496458/500000: episode: 3569, duration: 0.786s, episode steps: 164, steps per second: 209, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.103989, mae: 27.129593, mean_q: -38.869350, mean_eps: 0.100000\n",
      " 496587/500000: episode: 3570, duration: 0.620s, episode steps: 129, steps per second: 208, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 0.108275, mae: 27.369609, mean_q: -39.179883, mean_eps: 0.100000\n",
      " 496703/500000: episode: 3571, duration: 0.560s, episode steps: 116, steps per second: 207, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000],  loss: 0.091082, mae: 27.332514, mean_q: -39.166102, mean_eps: 0.100000\n",
      " 496816/500000: episode: 3572, duration: 0.554s, episode steps: 113, steps per second: 204, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000],  loss: 0.099849, mae: 27.176040, mean_q: -38.931838, mean_eps: 0.100000\n",
      " 497010/500000: episode: 3573, duration: 0.936s, episode steps: 194, steps per second: 207, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000],  loss: 0.107659, mae: 27.470247, mean_q: -39.407433, mean_eps: 0.100000\n",
      " 497099/500000: episode: 3574, duration: 0.434s, episode steps:  89, steps per second: 205, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000],  loss: 0.079634, mae: 27.013948, mean_q: -38.710818, mean_eps: 0.100000\n",
      " 497282/500000: episode: 3575, duration: 0.872s, episode steps: 183, steps per second: 210, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.065874, mae: 26.892437, mean_q: -38.497863, mean_eps: 0.100000\n",
      " 497385/500000: episode: 3576, duration: 0.499s, episode steps: 103, steps per second: 206, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000],  loss: 0.060593, mae: 26.573415, mean_q: -37.993343, mean_eps: 0.100000\n",
      " 497540/500000: episode: 3577, duration: 0.748s, episode steps: 155, steps per second: 207, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.013 [0.000, 2.000],  loss: 0.065829, mae: 26.623450, mean_q: -37.837689, mean_eps: 0.100000\n",
      " 497703/500000: episode: 3578, duration: 0.783s, episode steps: 163, steps per second: 208, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.088948, mae: 27.121353, mean_q: -38.525281, mean_eps: 0.100000\n",
      " 497818/500000: episode: 3579, duration: 0.554s, episode steps: 115, steps per second: 207, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 0.120052, mae: 27.368486, mean_q: -38.674373, mean_eps: 0.100000\n",
      " 497909/500000: episode: 3580, duration: 0.445s, episode steps:  91, steps per second: 204, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000],  loss: 0.205783, mae: 27.437186, mean_q: -38.450635, mean_eps: 0.100000\n",
      " 498091/500000: episode: 3581, duration: 0.886s, episode steps: 182, steps per second: 205, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.095789, mae: 27.784606, mean_q: -38.854350, mean_eps: 0.100000\n",
      " 498208/500000: episode: 3582, duration: 0.568s, episode steps: 117, steps per second: 206, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.453 [0.000, 2.000],  loss: 0.152466, mae: 28.163152, mean_q: -39.470746, mean_eps: 0.100000\n",
      " 498369/500000: episode: 3583, duration: 0.775s, episode steps: 161, steps per second: 208, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.173483, mae: 28.060291, mean_q: -39.428833, mean_eps: 0.100000\n",
      " 498536/500000: episode: 3584, duration: 0.801s, episode steps: 167, steps per second: 209, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.150986, mae: 28.334159, mean_q: -40.368428, mean_eps: 0.100000\n",
      " 498724/500000: episode: 3585, duration: 0.904s, episode steps: 188, steps per second: 208, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000],  loss: 0.246942, mae: 28.506291, mean_q: -40.436530, mean_eps: 0.100000\n",
      " 498819/500000: episode: 3586, duration: 0.463s, episode steps:  95, steps per second: 205, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 0.181211, mae: 28.192774, mean_q: -40.043388, mean_eps: 0.100000\n",
      " 498940/500000: episode: 3587, duration: 0.582s, episode steps: 121, steps per second: 208, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000],  loss: 0.167128, mae: 28.444065, mean_q: -40.556095, mean_eps: 0.100000\n",
      " 499113/500000: episode: 3588, duration: 0.827s, episode steps: 173, steps per second: 209, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000],  loss: 0.135569, mae: 28.200662, mean_q: -40.318633, mean_eps: 0.100000\n",
      " 499229/500000: episode: 3589, duration: 0.562s, episode steps: 116, steps per second: 206, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000],  loss: 0.120632, mae: 27.951896, mean_q: -39.886664, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 499336/500000: episode: 3590, duration: 0.545s, episode steps: 107, steps per second: 196, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000],  loss: 0.098896, mae: 27.295652, mean_q: -38.801526, mean_eps: 0.100000\n",
      " 499508/500000: episode: 3591, duration: 0.858s, episode steps: 172, steps per second: 201, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000],  loss: 0.129493, mae: 27.322656, mean_q: -38.492907, mean_eps: 0.100000\n",
      " 499637/500000: episode: 3592, duration: 0.637s, episode steps: 129, steps per second: 202, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000],  loss: 0.112017, mae: 27.326249, mean_q: -38.519105, mean_eps: 0.100000\n",
      " 499750/500000: episode: 3593, duration: 0.556s, episode steps: 113, steps per second: 203, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000],  loss: 0.100346, mae: 27.550859, mean_q: -38.787832, mean_eps: 0.100000\n",
      " 499855/500000: episode: 3594, duration: 0.524s, episode steps: 105, steps per second: 200, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000],  loss: 0.098386, mae: 27.649325, mean_q: -38.812982, mean_eps: 0.100000\n",
      " 499959/500000: episode: 3595, duration: 0.513s, episode steps: 104, steps per second: 203, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000],  loss: 0.128323, mae: 27.221329, mean_q: -38.170307, mean_eps: 0.100000\n",
      "done, took 2398.754 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set the path to save the log\n",
    "log_path = os.path.join(\"Training\", \"Logs\", \"MountainCar-v0_Keras-RL_SecondAgent\")\n",
    "\n",
    "# run the build_agent function to traing the agent\n",
    "agent_2 = build_agent(model, actions)                    # used build_agent to setup a dqn model\n",
    "\n",
    "agent_2.compile(Adam(learning_rate = 1e-3),              # use Adam optimisation with learning rate 0.0001\n",
    "            metrics = ['mae']                            # use mean absolute error to evaluate the metric\n",
    "           )      \n",
    "\n",
    "history = agent_2.fit(env, \n",
    "        nb_steps = 500000,                               # number of timesteps \n",
    "        visualize = False,                               # visualize during the training\n",
    "        verbose = 2                                      # how to show the training output\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent's Performance after 500k timesteps with 1e-3 learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -84.000, steps: 84\n",
      "Episode 2: reward: -106.000, steps: 106\n",
      "Episode 3: reward: -85.000, steps: 85\n",
      "Episode 4: reward: -87.000, steps: 87\n",
      "Episode 5: reward: -85.000, steps: 85\n",
      "-89.4\n"
     ]
    }
   ],
   "source": [
    "agent_2_scores = agent_2.test(env,                        # pass our environment into the .test agent\n",
    "                  nb_episodes = 5,                        # number of episodes\n",
    "                  visualize = True                        # set True if we want to visualize it\n",
    "                 )\n",
    "\n",
    "print(np.mean(agent_2_scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark:\n",
    "- The above 5 episode showing that the second Agent after training with ***learning rate 1e-3 for 500k training steps***, ***the car reaches the flag on the right with 89.4 timesteps spend on average***, which is ***20% timesteps saving than the simple policy*** without any learning and ***32% timesteps saving than the first agent's*** performance.\n",
    "- ***loss: 0.128*** at the end of the last episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***End of Page***"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "gym_interface.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
