{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ceb751f",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "I'm using the environment in the OpenAI gym - Classic Control to practice how to train an agent by reinforcement learning. In this notebook, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track and ***the goal is to prevent it from falling over.***\n",
    "\n",
    "I try differnet methods to complete this goal:\n",
    "\n",
    "***1. Create a simple policy (without any learning applied)*** (See Section 4)\n",
    "- Result: by using a simple policy ***the score in between 134 to 235 (average 181) prevent it from falling over*** \n",
    "- I tried choosing action by monitoring all four observations (Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity) in the simple policy, ***choosing action by monitoring the Pole Angular Velocity has the best score***.\n",
    "\n",
    "***2. Create a Deep Learning Neural Network with Keras*** (See Section 5)\n",
    "- Result: the agent received ***all perfect scores to prevent the Pole from falling over*** \n",
    "- the log on the above showing that we may ***reduce number of learning timesteps to 40000***, because the agent start received 500 episode reward start in the 36220 episode consistently.\n",
    "\n",
    "***3. Epsilon-Greedy Policy*** (See Section 6-7)\n",
    "- Result: The learning process with ***Epsilon-Greedy Policy reach perfect score at epoch #31*** (time spend: learning 6min).\n",
    "\n",
    "\n",
    "### ***Table of Content:***\n",
    "\n",
    "1. What is CartPole\n",
    "2. Import Dependencies\n",
    "3. Understanding The Environment\n",
    "4. Create a Simple Policy (without any Learning to Comlete the Goal)\n",
    "5. Create a Deep Learning Neural Network with Keras\n",
    "   - 5.1 Build Agent with Keras-RL\n",
    "   - 5.2 Train the Agents\n",
    "6. Epsilon-Greedy Policy\n",
    "7. Q-learning via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c4a71",
   "metadata": {},
   "source": [
    "# 1. What is CartPole?\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and ***the goal is to prevent it from falling over***. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "### Action Space\n",
    "- The agent take a 1-element vector for actions.\n",
    "- The action space is `(action)` in `[0, 1]`, where `action` is used to push the cart with a fixed amount of force:\n",
    "\n",
    "\n",
    " Num | Action                 |\n",
    "-----|------------------------|\n",
    " 0   | Push cart to the left  |\n",
    " 1   | Push cart to the right |\n",
    " \n",
    "### Observation Space\n",
    "The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n",
    "\n",
    " Num | Observation           | Min                   | Max                 |\n",
    "-----|-----------------------|-----------------------|---------------------|\n",
    " 0   | Cart Position         | -4.8*                 | 4.8*                |\n",
    " 1   | Cart Velocity         | -Inf                  | Inf                 |\n",
    " 2   | Pole Angle            | ~ -0.418 rad (-24°)** | ~ 0.418 rad (24°)** |\n",
    " 3   | Pole Angular Velocity | -Inf                  | Inf                 |\n",
    " \n",
    "### Rewards\n",
    "Reward is 1 for every step taken, including the termination step. The threshold is 475 for v1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74975576",
   "metadata": {},
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "states = env.observation_space.shape[0]      # 4 States under CartPole\n",
    "\n",
    "actions = env.action_space.n                 # 2 Actions: Left, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc8237",
   "metadata": {},
   "source": [
    "# 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3fd5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 13:26:45.868 python[1328:41163] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f994fe5d8c0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-19 13:26:45.869 python[1328:41163] Warning: Expected min height of view: (<NSButton: 0x7f994fe421e0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-19 13:26:45.872 python[1328:41163] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f994fe45100>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-19 13:26:45.874 python[1328:41163] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7f994fe4adc0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action space: Discrete(2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")                          # Using CartPole verion 1 in this notebook\n",
    "env.reset()                                            # reset the environment to initial default\n",
    "\n",
    "plt.imshow(env.render('rgb_array'))                    # display the environment\n",
    "print(\"Observation space:\", env.observation_space)     # ndarray with shape (4,)\n",
    "print(\"Action space:\", env.action_space)               # 2 actions could be taken in this environment: Push cart to the left, Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd45457f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<CartPoleEnv<CartPole-v1>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3a211",
   "metadata": {},
   "source": [
    "# 3. Understanding The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf951d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: [-0.01050036  0.03147893  0.03254453 -0.0046213 ]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(\"initial observation code:\", obs)\n",
    "\n",
    "# Note: in CartPole, observation is just 4 numbers: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7519ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 2 (right)\n",
      "new observation code: [-0.00987078  0.2261194   0.0324521  -0.286861  ]\n",
      "reward: 1.0\n",
      "is game over?: False\n"
     ]
    }
   ],
   "source": [
    "# If an action 2: acelerate to right is given into the environment\n",
    "new_obs, reward, is_done, _ = env.step(1)\n",
    "\n",
    "print(\"taking action 2 (right)\")\n",
    "\n",
    "# Then the environment will reply 3 things new_observation, reward and is_done to the agent\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "\n",
    "# Note: the car has moved to the right slightly (from -0.4775 to -0.4769 around 0.001 moved to right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef88fd",
   "metadata": {},
   "source": [
    "### Project Baseline Use Random Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175511a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:27.0 Timesteps Spend:27\n",
      "Episode:2 Score:41.0 Timesteps Spend:41\n",
      "Episode:3 Score:10.0 Timesteps Spend:10\n",
      "Episode:4 Score:17.0 Timesteps Spend:17\n",
      "Episode:5 Score:20.0 Timesteps Spend:20\n",
      "CPU times: user 52.5 ms, sys: 17.5 ms, total: 70.1 ms\n",
      "Wall time: 89.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    obs = env.reset()                                      # initial the set of observation\n",
    "    \n",
    "    done = False                                           # initial the game is over = False, until reach maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    timesteps_counter = 0                                  # count the number of timestep spend\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment\n",
    "        \n",
    "        action = env.action_space.sample()                 # random choose an action \n",
    "        \n",
    "        n_obs, reward, done, info = env.step(action)       # pass random actions into environment to get back\n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward is 1 for every step taken\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward                                    # accumulate each episodes' reward received into score\n",
    "        \n",
    "        timesteps_counter += 1                             # +1 in each timestep spend\n",
    "        \n",
    "    print('Episode:{} Score:{} Timesteps Spend:{}'.format(episode, score,timesteps_counter))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d22ca8",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, ***the score in between 10 to 41 (average 23) prevent it from falling over*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e20fe",
   "metadata": {},
   "source": [
    "# 4. Create a Simple Policy (without any Learning to Comlete the Goal)\n",
    "\n",
    "So, now let's create a simply policy (without any learning) to prevent it from falling over.\n",
    "\n",
    "Note: in CartPole, observation is just 4 numbers: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "596d407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3 actions as Dictionary\n",
    "actions = {'Push cart to the left': 0, 'Push cart to the right': 1}      \n",
    "\n",
    "# Define a policy\n",
    "def simple_policy(obs):\n",
    "    \n",
    "    position, cart_velocity, angle, pole_velocity = obs        # observe current postion and velocity\n",
    "\n",
    "    if pole_velocity < 0:                                      # if current velocity is positive\n",
    "        return actions['Push cart to the left']                # the policy return an actions 'Push cart to the left' \n",
    "\n",
    "    if pole_velocity > 0:                                      # if current velocity is negative\n",
    "        return actions['Push cart to the right']               # the policy choose an actions 'Push cart to the right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39c059",
   "metadata": {},
   "source": [
    "### Evaluate the Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0554e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:185.0 Timesteps Spend:185\n",
      "Episode:2 Score:235.0 Timesteps Spend:235\n",
      "Episode:3 Score:193.0 Timesteps Spend:193\n",
      "Episode:4 Score:134.0 Timesteps Spend:134\n",
      "Episode:5 Score:159.0 Timesteps Spend:159\n",
      "CPU times: user 305 ms, sys: 90.7 ms, total: 395 ms\n",
      "Wall time: 469 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    obs = env.reset()                                      # initial the set of observation\n",
    "    \n",
    "    done = False                                           # initial the game is over = False, until reach maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    timesteps_counter = 0                                  # count the number of timestep spend\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment\n",
    "        \n",
    "        action = simple_policy(obs)                        # random choose an action \n",
    "        \n",
    "        obs, reward, done, info = env.step(action)       # pass random actions into environment to get back\n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward (Positive value increment, negative value decrement)\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward                                    # accumulate each episodes' reward received into score\n",
    "        \n",
    "        timesteps_counter += 1                             # +1 in each timestep spend\n",
    "        \n",
    "    print('Episode:{} Score:{} Timesteps Spend:{}'.format(episode, score,timesteps_counter))    # print out score for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef0706",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, by using a simple policy ***the score in between 134 to 235 (average 181) prevent it from falling over*** \n",
    "- I tried choosing action by monitoring all four observations (Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity) in the simple policy, ***choosing action by monitoring the Pole Angular Velocity has the best score***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa507a",
   "metadata": {},
   "source": [
    "# 5. Create a Deep Learning Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e39dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build a Sequential Model\n",
    "def build_network(states, actions):\n",
    "    \n",
    "    network = Sequential()\n",
    "    \n",
    "    network.add(Flatten(input_shape = (1, states), name = 'States'))      # use Flatten layer as the input layer, input_shape = 1 x number of state \n",
    "    \n",
    "    network.add(Dense(512, activation = 'relu', name = 'Dense_1'))        # use Dense layer with 512 units of tensor with relu activation function\n",
    "    \n",
    "    network.add(Dense(256, activation = 'relu', name = 'Dense_2'))        # use another Dense layer with 256 units of tensor with relu activation function\n",
    "    \n",
    "    network.add(Dense(actions, activation = 'linear', name = 'Actions'))  # the output layer with shape = number of actions with linear activation function\n",
    "                                                                           # Output the probability for each actions \n",
    "    \n",
    "    return network                                                        # build_model return a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8036633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97ab043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "States (Flatten)             (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Actions (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 134,402\n",
      "Trainable params: 134,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the input_shape of the build_model function\n",
    "states = env.observation_space.shape[0]                         # use .shape[0] to return number of observation space\n",
    "\n",
    "# Set the output_shape of the build_model function\n",
    "actions = env.action_space.n                                    # use .n to return number of action\n",
    "\n",
    "# Run the build_model function to build the model\n",
    "network = build_network(states, actions)                            \n",
    "\n",
    "# See the model summary\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab60ca",
   "metadata": {},
   "source": [
    "# 5.1 Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7d380fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from rl.agents import DQNAgent                                                      # used DQNAgent here, should try other agents: SARSAAgent      \n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy      # used Policy base RL  \n",
    "from rl.memory import SequentialMemory\n",
    "import os\n",
    "\n",
    "# Build an Agent to learn from the model\n",
    "def build_agent(model, actions):            \n",
    "    \n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),    # use EpsGreedyQPolicy()\n",
    "                                  attr='eps',\n",
    "                                  value_max = 1,\n",
    "                                  value_min = .1,\n",
    "                                  value_test = 0.2,\n",
    "                                  nb_steps = 10000\n",
    "    )                    \n",
    "\n",
    "    memory = SequentialMemory(limit = 1000,              # buffer limit: number of episode\n",
    "                              window_length = 1          # store pass 1 window for 1000 episode \n",
    "                             )\n",
    "    \n",
    "    dqn = DQNAgent(model = model, \n",
    "                   memory = memory,\n",
    "                   policy = policy,\n",
    "                   enable_dueling_network = True,        # Dueling Networks split value and advantage, help the model learn when to take action and when not to bother\n",
    "                   dueling_type = 'avg',\n",
    "                   nb_actions = actions,                 # 3 actions to learn\n",
    "                   nb_steps_warmup = 1000,           \n",
    "                   target_model_update = 1e-2\n",
    "                  )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5c60",
   "metadata": {},
   "source": [
    "# 5.2 Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9932db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to save the log\n",
    "log_path = os.path.join(\"Training\", \"Logs\", \"CartPole-v1_Keras-RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15f69206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "     15/500000: episode: 1, duration: 0.070s, episode steps:  15, steps per second: 215, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     45/500000: episode: 2, duration: 0.014s, episode steps:  30, steps per second: 2086, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     78/500000: episode: 3, duration: 0.015s, episode steps:  33, steps per second: 2226, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "     90/500000: episode: 4, duration: 0.006s, episode steps:  12, steps per second: 2087, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    105/500000: episode: 5, duration: 0.007s, episode steps:  15, steps per second: 2071, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    135/500000: episode: 6, duration: 0.014s, episode steps:  30, steps per second: 2219, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    147/500000: episode: 7, duration: 0.006s, episode steps:  12, steps per second: 2126, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    181/500000: episode: 8, duration: 0.017s, episode steps:  34, steps per second: 2033, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    217/500000: episode: 9, duration: 0.016s, episode steps:  36, steps per second: 2247, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    225/500000: episode: 10, duration: 0.004s, episode steps:   8, steps per second: 2052, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    238/500000: episode: 11, duration: 0.006s, episode steps:  13, steps per second: 2237, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    274/500000: episode: 12, duration: 0.016s, episode steps:  36, steps per second: 2279, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    285/500000: episode: 13, duration: 0.005s, episode steps:  11, steps per second: 2084, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    304/500000: episode: 14, duration: 0.009s, episode steps:  19, steps per second: 2036, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    321/500000: episode: 15, duration: 0.009s, episode steps:  17, steps per second: 1903, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    332/500000: episode: 16, duration: 0.006s, episode steps:  11, steps per second: 1851, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    357/500000: episode: 17, duration: 0.011s, episode steps:  25, steps per second: 2207, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    373/500000: episode: 18, duration: 0.008s, episode steps:  16, steps per second: 2108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    398/500000: episode: 19, duration: 0.012s, episode steps:  25, steps per second: 2122, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    424/500000: episode: 20, duration: 0.012s, episode steps:  26, steps per second: 2259, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    438/500000: episode: 21, duration: 0.007s, episode steps:  14, steps per second: 2124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    458/500000: episode: 22, duration: 0.009s, episode steps:  20, steps per second: 2220, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    473/500000: episode: 23, duration: 0.007s, episode steps:  15, steps per second: 2245, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    510/500000: episode: 24, duration: 0.015s, episode steps:  37, steps per second: 2393, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.324 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    582/500000: episode: 25, duration: 0.029s, episode steps:  72, steps per second: 2450, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    609/500000: episode: 26, duration: 0.012s, episode steps:  27, steps per second: 2340, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    619/500000: episode: 27, duration: 0.005s, episode steps:  10, steps per second: 2173, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    628/500000: episode: 28, duration: 0.004s, episode steps:   9, steps per second: 2183, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    651/500000: episode: 29, duration: 0.010s, episode steps:  23, steps per second: 2281, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    663/500000: episode: 30, duration: 0.006s, episode steps:  12, steps per second: 2166, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    677/500000: episode: 31, duration: 0.006s, episode steps:  14, steps per second: 2161, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    694/500000: episode: 32, duration: 0.008s, episode steps:  17, steps per second: 2226, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    706/500000: episode: 33, duration: 0.005s, episode steps:  12, steps per second: 2199, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    721/500000: episode: 34, duration: 0.007s, episode steps:  15, steps per second: 2162, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    743/500000: episode: 35, duration: 0.010s, episode steps:  22, steps per second: 2157, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    761/500000: episode: 36, duration: 0.009s, episode steps:  18, steps per second: 1961, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    816/500000: episode: 37, duration: 0.024s, episode steps:  55, steps per second: 2275, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    832/500000: episode: 38, duration: 0.007s, episode steps:  16, steps per second: 2229, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    850/500000: episode: 39, duration: 0.008s, episode steps:  18, steps per second: 2122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    862/500000: episode: 40, duration: 0.006s, episode steps:  12, steps per second: 2171, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    877/500000: episode: 41, duration: 0.007s, episode steps:  15, steps per second: 2223, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    891/500000: episode: 42, duration: 0.006s, episode steps:  14, steps per second: 2192, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    902/500000: episode: 43, duration: 0.005s, episode steps:  11, steps per second: 2108, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    919/500000: episode: 44, duration: 0.008s, episode steps:  17, steps per second: 2235, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    931/500000: episode: 45, duration: 0.005s, episode steps:  12, steps per second: 2247, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    946/500000: episode: 46, duration: 0.007s, episode steps:  15, steps per second: 2086, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    965/500000: episode: 47, duration: 0.009s, episode steps:  19, steps per second: 2181, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    991/500000: episode: 48, duration: 0.013s, episode steps:  26, steps per second: 2068, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1017/500000: episode: 49, duration: 0.522s, episode steps:  26, steps per second:  50, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 16.887636, mae: 4.696716, mean_q: 8.960082, mean_eps: 0.909235\n",
      "   1032/500000: episode: 50, duration: 0.076s, episode steps:  15, steps per second: 197, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8.867466, mae: 4.665575, mean_q: 9.450425, mean_eps: 0.907840\n",
      "   1046/500000: episode: 51, duration: 0.066s, episode steps:  14, steps per second: 211, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 5.168766, mae: 5.049514, mean_q: 9.781283, mean_eps: 0.906535\n",
      "   1061/500000: episode: 52, duration: 0.074s, episode steps:  15, steps per second: 203, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 2.697559, mae: 5.071849, mean_q: 9.964766, mean_eps: 0.905230\n",
      "   1075/500000: episode: 53, duration: 0.070s, episode steps:  14, steps per second: 199, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.036870, mae: 5.176194, mean_q: 10.035054, mean_eps: 0.903925\n",
      "   1102/500000: episode: 54, duration: 0.125s, episode steps:  27, steps per second: 215, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 1.571145, mae: 5.236696, mean_q: 9.925457, mean_eps: 0.902080\n",
      "   1140/500000: episode: 55, duration: 0.183s, episode steps:  38, steps per second: 208, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.528410, mae: 5.326898, mean_q: 9.958125, mean_eps: 0.899155\n",
      "   1160/500000: episode: 56, duration: 0.101s, episode steps:  20, steps per second: 198, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.046956, mae: 5.328232, mean_q: 10.111547, mean_eps: 0.896545\n",
      "   1197/500000: episode: 57, duration: 0.177s, episode steps:  37, steps per second: 210, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.178925, mae: 5.426287, mean_q: 10.339112, mean_eps: 0.893980\n",
      "   1280/500000: episode: 58, duration: 0.398s, episode steps:  83, steps per second: 209, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 1.177247, mae: 5.516187, mean_q: 10.466323, mean_eps: 0.888580\n",
      "   1294/500000: episode: 59, duration: 0.070s, episode steps:  14, steps per second: 200, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.880583, mae: 5.620733, mean_q: 10.825864, mean_eps: 0.884215\n",
      "   1313/500000: episode: 60, duration: 0.095s, episode steps:  19, steps per second: 199, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.915767, mae: 5.663826, mean_q: 10.878106, mean_eps: 0.882730\n",
      "   1380/500000: episode: 61, duration: 0.339s, episode steps:  67, steps per second: 197, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.093391, mae: 5.742952, mean_q: 11.011401, mean_eps: 0.878860\n",
      "   1401/500000: episode: 62, duration: 0.103s, episode steps:  21, steps per second: 203, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.150509, mae: 5.827981, mean_q: 11.169497, mean_eps: 0.874900\n",
      "   1501/500000: episode: 63, duration: 0.478s, episode steps: 100, steps per second: 209, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.937577, mae: 5.991999, mean_q: 11.684216, mean_eps: 0.869455\n",
      "   1521/500000: episode: 64, duration: 0.097s, episode steps:  20, steps per second: 206, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.297201, mae: 6.183204, mean_q: 12.076945, mean_eps: 0.864055\n",
      "   1548/500000: episode: 65, duration: 0.127s, episode steps:  27, steps per second: 212, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.918858, mae: 6.263616, mean_q: 12.215168, mean_eps: 0.861940\n",
      "   1583/500000: episode: 66, duration: 0.179s, episode steps:  35, steps per second: 196, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.023314, mae: 6.346410, mean_q: 12.530695, mean_eps: 0.859150\n",
      "   1595/500000: episode: 67, duration: 0.061s, episode steps:  12, steps per second: 198, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.548182, mae: 6.416214, mean_q: 12.687695, mean_eps: 0.857035\n",
      "   1608/500000: episode: 68, duration: 0.064s, episode steps:  13, steps per second: 204, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.794899, mae: 6.553666, mean_q: 12.963838, mean_eps: 0.855910\n",
      "   1691/500000: episode: 69, duration: 0.405s, episode steps:  83, steps per second: 205, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.947638, mae: 6.624151, mean_q: 13.052556, mean_eps: 0.851590\n",
      "   1701/500000: episode: 70, duration: 0.049s, episode steps:  10, steps per second: 204, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.912374, mae: 6.778306, mean_q: 13.433519, mean_eps: 0.847405\n",
      "   1726/500000: episode: 71, duration: 0.117s, episode steps:  25, steps per second: 213, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.881242, mae: 6.850500, mean_q: 13.545796, mean_eps: 0.845830\n",
      "   1754/500000: episode: 72, duration: 0.134s, episode steps:  28, steps per second: 209, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.876559, mae: 6.949201, mean_q: 13.813674, mean_eps: 0.843445\n",
      "   1765/500000: episode: 73, duration: 0.059s, episode steps:  11, steps per second: 186, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.775083, mae: 7.047813, mean_q: 13.986150, mean_eps: 0.841690\n",
      "   1828/500000: episode: 74, duration: 0.295s, episode steps:  63, steps per second: 214, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.980498, mae: 7.159812, mean_q: 14.162586, mean_eps: 0.838360\n",
      "   1859/500000: episode: 75, duration: 0.141s, episode steps:  31, steps per second: 219, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.000006, mae: 7.314420, mean_q: 14.507539, mean_eps: 0.834130\n",
      "   1880/500000: episode: 76, duration: 0.108s, episode steps:  21, steps per second: 195, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 1.343775, mae: 7.412842, mean_q: 14.598011, mean_eps: 0.831790\n",
      "   1897/500000: episode: 77, duration: 0.085s, episode steps:  17, steps per second: 201, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 1.241254, mae: 7.460743, mean_q: 14.765107, mean_eps: 0.830080\n",
      "   1937/500000: episode: 78, duration: 0.199s, episode steps:  40, steps per second: 201, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.909030, mae: 7.616812, mean_q: 15.173162, mean_eps: 0.827515\n",
      "   1951/500000: episode: 79, duration: 0.071s, episode steps:  14, steps per second: 197, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.675481, mae: 7.818965, mean_q: 15.626391, mean_eps: 0.825085\n",
      "   2026/500000: episode: 80, duration: 0.372s, episode steps:  75, steps per second: 202, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.911721, mae: 7.886402, mean_q: 15.789751, mean_eps: 0.821080\n",
      "   2040/500000: episode: 81, duration: 0.082s, episode steps:  14, steps per second: 171, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.850603, mae: 8.068241, mean_q: 16.222460, mean_eps: 0.817075\n",
      "   2060/500000: episode: 82, duration: 0.096s, episode steps:  20, steps per second: 207, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.965370, mae: 8.144639, mean_q: 16.375363, mean_eps: 0.815545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2095/500000: episode: 83, duration: 0.167s, episode steps:  35, steps per second: 210, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.823913, mae: 8.260288, mean_q: 16.602191, mean_eps: 0.813070\n",
      "   2139/500000: episode: 84, duration: 0.209s, episode steps:  44, steps per second: 211, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.011767, mae: 8.403628, mean_q: 16.922491, mean_eps: 0.809515\n",
      "   2194/500000: episode: 85, duration: 0.259s, episode steps:  55, steps per second: 212, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.035371, mae: 8.638543, mean_q: 17.356946, mean_eps: 0.805060\n",
      "   2228/500000: episode: 86, duration: 0.159s, episode steps:  34, steps per second: 214, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 0.924543, mae: 8.774709, mean_q: 17.733921, mean_eps: 0.801055\n",
      "   2284/500000: episode: 87, duration: 0.260s, episode steps:  56, steps per second: 215, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.923631, mae: 8.959209, mean_q: 18.067101, mean_eps: 0.797005\n",
      "   2313/500000: episode: 88, duration: 0.143s, episode steps:  29, steps per second: 203, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.041863, mae: 9.131270, mean_q: 18.315794, mean_eps: 0.793180\n",
      "   2338/500000: episode: 89, duration: 0.130s, episode steps:  25, steps per second: 192, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.231543, mae: 9.133106, mean_q: 18.318507, mean_eps: 0.790750\n",
      "   2458/500000: episode: 90, duration: 0.577s, episode steps: 120, steps per second: 208, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.987115, mae: 9.520567, mean_q: 19.233557, mean_eps: 0.784225\n",
      "   2501/500000: episode: 91, duration: 0.224s, episode steps:  43, steps per second: 192, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.189975, mae: 9.838721, mean_q: 19.911986, mean_eps: 0.776890\n",
      "   2552/500000: episode: 92, duration: 0.256s, episode steps:  51, steps per second: 199, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.137680, mae: 10.024488, mean_q: 20.322970, mean_eps: 0.772660\n",
      "   2563/500000: episode: 93, duration: 0.055s, episode steps:  11, steps per second: 201, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.940070, mae: 10.299757, mean_q: 20.925843, mean_eps: 0.769870\n",
      "   2583/500000: episode: 94, duration: 0.095s, episode steps:  20, steps per second: 210, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.257106, mae: 10.201754, mean_q: 20.698419, mean_eps: 0.768475\n",
      "   2598/500000: episode: 95, duration: 0.085s, episode steps:  15, steps per second: 177, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.424757, mae: 10.313837, mean_q: 20.942416, mean_eps: 0.766900\n",
      "   2672/500000: episode: 96, duration: 0.373s, episode steps:  74, steps per second: 198, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.299089, mae: 10.496618, mean_q: 21.294225, mean_eps: 0.762895\n",
      "   2694/500000: episode: 97, duration: 0.106s, episode steps:  22, steps per second: 208, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.266216, mae: 10.719290, mean_q: 21.781412, mean_eps: 0.758575\n",
      "   2714/500000: episode: 98, duration: 0.102s, episode steps:  20, steps per second: 196, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 1.029396, mae: 10.809299, mean_q: 22.097803, mean_eps: 0.756685\n",
      "   2763/500000: episode: 99, duration: 0.233s, episode steps:  49, steps per second: 210, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.933743, mae: 10.903098, mean_q: 22.152471, mean_eps: 0.753580\n",
      "   2785/500000: episode: 100, duration: 0.104s, episode steps:  22, steps per second: 211, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.102932, mae: 11.135992, mean_q: 22.616130, mean_eps: 0.750385\n",
      "   2811/500000: episode: 101, duration: 0.122s, episode steps:  26, steps per second: 212, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 1.618138, mae: 11.139357, mean_q: 22.601787, mean_eps: 0.748225\n",
      "   2859/500000: episode: 102, duration: 0.249s, episode steps:  48, steps per second: 192, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 1.937772, mae: 11.444175, mean_q: 23.106081, mean_eps: 0.744895\n",
      "   2897/500000: episode: 103, duration: 0.181s, episode steps:  38, steps per second: 210, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 1.824651, mae: 11.539687, mean_q: 23.307319, mean_eps: 0.741025\n",
      "   2924/500000: episode: 104, duration: 0.129s, episode steps:  27, steps per second: 210, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.706826, mae: 11.560680, mean_q: 23.396726, mean_eps: 0.738100\n",
      "   2959/500000: episode: 105, duration: 0.161s, episode steps:  35, steps per second: 217, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.784575, mae: 11.745843, mean_q: 23.853668, mean_eps: 0.735310\n",
      "   3037/500000: episode: 106, duration: 0.360s, episode steps:  78, steps per second: 217, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.888265, mae: 11.982297, mean_q: 24.413439, mean_eps: 0.730225\n",
      "   3069/500000: episode: 107, duration: 0.151s, episode steps:  32, steps per second: 212, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.653190, mae: 12.321313, mean_q: 25.232844, mean_eps: 0.725275\n",
      "   3090/500000: episode: 108, duration: 0.103s, episode steps:  21, steps per second: 203, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.647882, mae: 12.373623, mean_q: 25.236737, mean_eps: 0.722890\n",
      "   3129/500000: episode: 109, duration: 0.176s, episode steps:  39, steps per second: 221, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 2.469156, mae: 12.448486, mean_q: 25.433301, mean_eps: 0.720190\n",
      "   3242/500000: episode: 110, duration: 0.511s, episode steps: 113, steps per second: 221, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.105532, mae: 12.879415, mean_q: 26.257118, mean_eps: 0.713350\n",
      "   3312/500000: episode: 111, duration: 0.318s, episode steps:  70, steps per second: 220, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.484050, mae: 13.243475, mean_q: 26.969169, mean_eps: 0.705115\n",
      "   3357/500000: episode: 112, duration: 0.216s, episode steps:  45, steps per second: 209, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.079594, mae: 13.581470, mean_q: 27.751819, mean_eps: 0.699940\n",
      "   3422/500000: episode: 113, duration: 0.302s, episode steps:  65, steps per second: 215, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.004319, mae: 13.697759, mean_q: 28.084427, mean_eps: 0.694990\n",
      "   3524/500000: episode: 114, duration: 0.464s, episode steps: 102, steps per second: 220, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 2.098582, mae: 14.119750, mean_q: 28.913726, mean_eps: 0.687475\n",
      "   3555/500000: episode: 115, duration: 0.146s, episode steps:  31, steps per second: 213, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.906299, mae: 14.487271, mean_q: 29.633371, mean_eps: 0.681490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3576/500000: episode: 116, duration: 0.102s, episode steps:  21, steps per second: 205, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.836663, mae: 14.576106, mean_q: 29.770343, mean_eps: 0.679150\n",
      "   3600/500000: episode: 117, duration: 0.117s, episode steps:  24, steps per second: 204, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.582568, mae: 14.727703, mean_q: 30.112892, mean_eps: 0.677125\n",
      "   3632/500000: episode: 118, duration: 0.153s, episode steps:  32, steps per second: 210, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.703619, mae: 14.882109, mean_q: 30.357943, mean_eps: 0.674605\n",
      "   3661/500000: episode: 119, duration: 0.132s, episode steps:  29, steps per second: 220, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 2.663419, mae: 15.108057, mean_q: 30.719630, mean_eps: 0.671860\n",
      "   3671/500000: episode: 120, duration: 0.048s, episode steps:  10, steps per second: 206, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.592498, mae: 15.118817, mean_q: 30.878460, mean_eps: 0.670105\n",
      "   3692/500000: episode: 121, duration: 0.104s, episode steps:  21, steps per second: 201, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.995129, mae: 15.370863, mean_q: 31.283064, mean_eps: 0.668710\n",
      "   3824/500000: episode: 122, duration: 0.616s, episode steps: 132, steps per second: 214, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.111639, mae: 15.494604, mean_q: 31.652383, mean_eps: 0.661825\n",
      "   3840/500000: episode: 123, duration: 0.077s, episode steps:  16, steps per second: 207, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.964906, mae: 15.571650, mean_q: 32.004598, mean_eps: 0.655165\n",
      "   3904/500000: episode: 124, duration: 0.295s, episode steps:  64, steps per second: 217, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.571990, mae: 16.120434, mean_q: 32.979197, mean_eps: 0.651565\n",
      "   3944/500000: episode: 125, duration: 0.185s, episode steps:  40, steps per second: 216, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.958113, mae: 16.306445, mean_q: 33.417032, mean_eps: 0.646885\n",
      "   4018/500000: episode: 126, duration: 0.343s, episode steps:  74, steps per second: 216, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.204628, mae: 16.538891, mean_q: 33.727345, mean_eps: 0.641755\n",
      "   4040/500000: episode: 127, duration: 0.109s, episode steps:  22, steps per second: 203, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.672541, mae: 16.610392, mean_q: 33.915622, mean_eps: 0.637435\n",
      "   4147/500000: episode: 128, duration: 0.514s, episode steps: 107, steps per second: 208, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.081064, mae: 17.087598, mean_q: 34.897828, mean_eps: 0.631630\n",
      "   4289/500000: episode: 129, duration: 0.704s, episode steps: 142, steps per second: 202, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 1.966606, mae: 17.680840, mean_q: 36.165104, mean_eps: 0.620425\n",
      "   4344/500000: episode: 130, duration: 0.275s, episode steps:  55, steps per second: 200, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.140841, mae: 17.873038, mean_q: 36.454991, mean_eps: 0.611560\n",
      "   4481/500000: episode: 131, duration: 0.690s, episode steps: 137, steps per second: 199, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.181613, mae: 18.420658, mean_q: 37.518339, mean_eps: 0.602920\n",
      "   4549/500000: episode: 132, duration: 0.345s, episode steps:  68, steps per second: 197, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.226718, mae: 18.890924, mean_q: 38.489324, mean_eps: 0.593695\n",
      "   4656/500000: episode: 133, duration: 0.517s, episode steps: 107, steps per second: 207, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.521166, mae: 19.172258, mean_q: 39.120381, mean_eps: 0.585820\n",
      "   4811/500000: episode: 134, duration: 0.753s, episode steps: 155, steps per second: 206, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.538232, mae: 19.913334, mean_q: 40.828983, mean_eps: 0.574030\n",
      "   4831/500000: episode: 135, duration: 0.094s, episode steps:  20, steps per second: 213, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.116731, mae: 20.117402, mean_q: 41.158498, mean_eps: 0.566155\n",
      "   4870/500000: episode: 136, duration: 0.189s, episode steps:  39, steps per second: 206, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 2.390819, mae: 20.245177, mean_q: 41.448219, mean_eps: 0.563500\n",
      "   5007/500000: episode: 137, duration: 0.705s, episode steps: 137, steps per second: 194, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2.020655, mae: 20.741935, mean_q: 42.442527, mean_eps: 0.555580\n",
      "   5054/500000: episode: 138, duration: 0.220s, episode steps:  47, steps per second: 214, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  loss: 1.943773, mae: 21.139236, mean_q: 43.318908, mean_eps: 0.547300\n",
      "   5181/500000: episode: 139, duration: 0.633s, episode steps: 127, steps per second: 201, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.302761, mae: 21.408296, mean_q: 43.851614, mean_eps: 0.539470\n",
      "   5338/500000: episode: 140, duration: 0.755s, episode steps: 157, steps per second: 208, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.042922, mae: 21.964393, mean_q: 44.937949, mean_eps: 0.526690\n",
      "   5559/500000: episode: 141, duration: 1.066s, episode steps: 221, steps per second: 207, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.621678, mae: 22.121762, mean_q: 45.243192, mean_eps: 0.509680\n",
      "   5616/500000: episode: 142, duration: 0.282s, episode steps:  57, steps per second: 202, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 2.782645, mae: 22.332855, mean_q: 45.505337, mean_eps: 0.497170\n",
      "   5780/500000: episode: 143, duration: 0.772s, episode steps: 164, steps per second: 212, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.021311, mae: 22.552120, mean_q: 46.040720, mean_eps: 0.487225\n",
      "   6037/500000: episode: 144, duration: 1.224s, episode steps: 257, steps per second: 210, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.756006, mae: 23.237215, mean_q: 47.313382, mean_eps: 0.468280\n",
      "   6200/500000: episode: 145, duration: 0.792s, episode steps: 163, steps per second: 206, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.342845, mae: 23.725015, mean_q: 48.253709, mean_eps: 0.449380\n",
      "   6334/500000: episode: 146, duration: 0.651s, episode steps: 134, steps per second: 206, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.944162, mae: 24.048768, mean_q: 48.798069, mean_eps: 0.436015\n",
      "   6497/500000: episode: 147, duration: 0.781s, episode steps: 163, steps per second: 209, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.253256, mae: 24.556927, mean_q: 49.894317, mean_eps: 0.422650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6713/500000: episode: 148, duration: 1.063s, episode steps: 216, steps per second: 203, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.464353, mae: 25.009956, mean_q: 50.726705, mean_eps: 0.405595\n",
      "   6836/500000: episode: 149, duration: 0.588s, episode steps: 123, steps per second: 209, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.106099, mae: 25.155809, mean_q: 51.020592, mean_eps: 0.390340\n",
      "   7036/500000: episode: 150, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.226385, mae: 25.107759, mean_q: 50.760701, mean_eps: 0.375805\n",
      "   7213/500000: episode: 151, duration: 0.808s, episode steps: 177, steps per second: 219, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.819000, mae: 24.572307, mean_q: 49.621997, mean_eps: 0.358840\n",
      "   7387/500000: episode: 152, duration: 0.808s, episode steps: 174, steps per second: 215, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.580759, mae: 24.711795, mean_q: 49.941201, mean_eps: 0.343045\n",
      "   7508/500000: episode: 153, duration: 0.566s, episode steps: 121, steps per second: 214, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.473672, mae: 24.811469, mean_q: 50.146396, mean_eps: 0.329770\n",
      "   7708/500000: episode: 154, duration: 0.914s, episode steps: 200, steps per second: 219, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.644602, mae: 25.660278, mean_q: 51.920908, mean_eps: 0.315325\n",
      "   7891/500000: episode: 155, duration: 0.824s, episode steps: 183, steps per second: 222, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.535415, mae: 26.036889, mean_q: 52.542620, mean_eps: 0.298090\n",
      "   8062/500000: episode: 156, duration: 0.768s, episode steps: 171, steps per second: 223, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.520803, mae: 25.761749, mean_q: 51.923518, mean_eps: 0.282160\n",
      "   8256/500000: episode: 157, duration: 0.868s, episode steps: 194, steps per second: 224, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.594187, mae: 26.103834, mean_q: 52.545343, mean_eps: 0.265735\n",
      "   8404/500000: episode: 158, duration: 0.666s, episode steps: 148, steps per second: 222, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 0.683287, mae: 26.380047, mean_q: 53.030806, mean_eps: 0.250345\n",
      "   8609/500000: episode: 159, duration: 0.940s, episode steps: 205, steps per second: 218, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.562098, mae: 26.736258, mean_q: 53.812606, mean_eps: 0.234460\n",
      "   8788/500000: episode: 160, duration: 0.823s, episode steps: 179, steps per second: 218, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.543740, mae: 26.549026, mean_q: 53.419331, mean_eps: 0.217180\n",
      "   8968/500000: episode: 161, duration: 0.807s, episode steps: 180, steps per second: 223, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.606779, mae: 26.898456, mean_q: 54.115697, mean_eps: 0.201025\n",
      "   9169/500000: episode: 162, duration: 0.917s, episode steps: 201, steps per second: 219, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.473061, mae: 27.184213, mean_q: 54.734374, mean_eps: 0.183880\n",
      "   9340/500000: episode: 163, duration: 0.815s, episode steps: 171, steps per second: 210, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.442070, mae: 27.276569, mean_q: 54.894976, mean_eps: 0.167140\n",
      "   9515/500000: episode: 164, duration: 0.817s, episode steps: 175, steps per second: 214, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.160681, mae: 26.991235, mean_q: 54.445411, mean_eps: 0.151570\n",
      "   9688/500000: episode: 165, duration: 0.788s, episode steps: 173, steps per second: 220, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.107238, mae: 26.893574, mean_q: 54.286892, mean_eps: 0.135910\n",
      "   9906/500000: episode: 166, duration: 0.984s, episode steps: 218, steps per second: 221, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.083224, mae: 27.099997, mean_q: 54.714466, mean_eps: 0.118315\n",
      "  10102/500000: episode: 167, duration: 0.973s, episode steps: 196, steps per second: 201, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.104650, mae: 27.576287, mean_q: 55.618224, mean_eps: 0.102050\n",
      "  10286/500000: episode: 168, duration: 0.890s, episode steps: 184, steps per second: 207, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.085955, mae: 27.786669, mean_q: 55.988963, mean_eps: 0.100000\n",
      "  10487/500000: episode: 169, duration: 1.016s, episode steps: 201, steps per second: 198, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.081836, mae: 28.044704, mean_q: 56.465919, mean_eps: 0.100000\n",
      "  10663/500000: episode: 170, duration: 0.832s, episode steps: 176, steps per second: 211, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.050650, mae: 27.928601, mean_q: 56.223335, mean_eps: 0.100000\n",
      "  10885/500000: episode: 171, duration: 1.105s, episode steps: 222, steps per second: 201, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.037526, mae: 28.314448, mean_q: 56.993594, mean_eps: 0.100000\n",
      "  11073/500000: episode: 172, duration: 0.879s, episode steps: 188, steps per second: 214, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.023371, mae: 28.084887, mean_q: 56.531447, mean_eps: 0.100000\n",
      "  11259/500000: episode: 173, duration: 0.875s, episode steps: 186, steps per second: 212, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.016749, mae: 28.268724, mean_q: 56.888027, mean_eps: 0.100000\n",
      "  11478/500000: episode: 174, duration: 1.077s, episode steps: 219, steps per second: 203, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.012941, mae: 28.524170, mean_q: 57.398470, mean_eps: 0.100000\n",
      "  11691/500000: episode: 175, duration: 1.005s, episode steps: 213, steps per second: 212, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.013053, mae: 28.796237, mean_q: 57.913257, mean_eps: 0.100000\n",
      "  11947/500000: episode: 176, duration: 1.249s, episode steps: 256, steps per second: 205, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.015956, mae: 29.197745, mean_q: 58.709270, mean_eps: 0.100000\n",
      "  12163/500000: episode: 177, duration: 1.018s, episode steps: 216, steps per second: 212, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.015238, mae: 29.616944, mean_q: 59.550920, mean_eps: 0.100000\n",
      "  12445/500000: episode: 178, duration: 1.357s, episode steps: 282, steps per second: 208, episode reward: 282.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.015489, mae: 30.514737, mean_q: 61.311171, mean_eps: 0.100000\n",
      "  12685/500000: episode: 179, duration: 1.117s, episode steps: 240, steps per second: 215, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.012855, mae: 30.929579, mean_q: 62.139183, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12907/500000: episode: 180, duration: 1.007s, episode steps: 222, steps per second: 221, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.011046, mae: 30.440739, mean_q: 61.141921, mean_eps: 0.100000\n",
      "  13139/500000: episode: 181, duration: 1.175s, episode steps: 232, steps per second: 197, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.009320, mae: 30.820974, mean_q: 61.874678, mean_eps: 0.100000\n",
      "  13393/500000: episode: 182, duration: 1.320s, episode steps: 254, steps per second: 192, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.009719, mae: 30.999265, mean_q: 62.196512, mean_eps: 0.100000\n",
      "  13628/500000: episode: 183, duration: 1.085s, episode steps: 235, steps per second: 217, episode reward: 235.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.007232, mae: 30.622329, mean_q: 61.438278, mean_eps: 0.100000\n",
      "  13902/500000: episode: 184, duration: 1.397s, episode steps: 274, steps per second: 196, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.006309, mae: 30.991201, mean_q: 62.169994, mean_eps: 0.100000\n",
      "  14184/500000: episode: 185, duration: 1.357s, episode steps: 282, steps per second: 208, episode reward: 282.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.011360, mae: 31.832106, mean_q: 63.838072, mean_eps: 0.100000\n",
      "  14449/500000: episode: 186, duration: 1.268s, episode steps: 265, steps per second: 209, episode reward: 265.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.013222, mae: 31.744437, mean_q: 63.631073, mean_eps: 0.100000\n",
      "  14698/500000: episode: 187, duration: 1.240s, episode steps: 249, steps per second: 201, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.008569, mae: 32.065598, mean_q: 64.268305, mean_eps: 0.100000\n",
      "  14966/500000: episode: 188, duration: 1.270s, episode steps: 268, steps per second: 211, episode reward: 268.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.012414, mae: 32.102720, mean_q: 64.318602, mean_eps: 0.100000\n",
      "  15236/500000: episode: 189, duration: 1.321s, episode steps: 270, steps per second: 204, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.008258, mae: 32.203350, mean_q: 64.525375, mean_eps: 0.100000\n",
      "  15496/500000: episode: 190, duration: 1.193s, episode steps: 260, steps per second: 218, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.005986, mae: 32.219095, mean_q: 64.559684, mean_eps: 0.100000\n",
      "  15788/500000: episode: 191, duration: 1.315s, episode steps: 292, steps per second: 222, episode reward: 292.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.034350, mae: 32.691442, mean_q: 65.460150, mean_eps: 0.100000\n",
      "  16199/500000: episode: 192, duration: 1.820s, episode steps: 411, steps per second: 226, episode reward: 411.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.016689, mae: 33.479742, mean_q: 67.064681, mean_eps: 0.100000\n",
      "  16519/500000: episode: 193, duration: 1.414s, episode steps: 320, steps per second: 226, episode reward: 320.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.015028, mae: 34.851538, mean_q: 69.817343, mean_eps: 0.100000\n",
      "  16825/500000: episode: 194, duration: 1.385s, episode steps: 306, steps per second: 221, episode reward: 306.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.053298, mae: 35.541827, mean_q: 71.146448, mean_eps: 0.100000\n",
      "  17325/500000: episode: 195, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.380134, mae: 38.904079, mean_q: 77.789126, mean_eps: 0.100000\n",
      "  17595/500000: episode: 196, duration: 1.213s, episode steps: 270, steps per second: 223, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.701235, mae: 41.748374, mean_q: 83.434876, mean_eps: 0.100000\n",
      "  17859/500000: episode: 197, duration: 1.272s, episode steps: 264, steps per second: 208, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 9.754187, mae: 43.461192, mean_q: 86.573532, mean_eps: 0.100000\n",
      "  18075/500000: episode: 198, duration: 0.995s, episode steps: 216, steps per second: 217, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 9.244721, mae: 43.500115, mean_q: 86.689127, mean_eps: 0.100000\n",
      "  18285/500000: episode: 199, duration: 0.943s, episode steps: 210, steps per second: 223, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 9.407959, mae: 43.308130, mean_q: 86.276299, mean_eps: 0.100000\n",
      "  18484/500000: episode: 200, duration: 0.898s, episode steps: 199, steps per second: 222, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 7.487781, mae: 43.887512, mean_q: 87.489562, mean_eps: 0.100000\n",
      "  18725/500000: episode: 201, duration: 1.110s, episode steps: 241, steps per second: 217, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 7.867874, mae: 43.585997, mean_q: 86.798779, mean_eps: 0.100000\n",
      "  18907/500000: episode: 202, duration: 0.875s, episode steps: 182, steps per second: 208, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.241226, mae: 43.384301, mean_q: 86.372899, mean_eps: 0.100000\n",
      "  19103/500000: episode: 203, duration: 0.945s, episode steps: 196, steps per second: 207, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.058122, mae: 43.014251, mean_q: 85.785189, mean_eps: 0.100000\n",
      "  19313/500000: episode: 204, duration: 0.957s, episode steps: 210, steps per second: 219, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.681293, mae: 42.895316, mean_q: 85.724003, mean_eps: 0.100000\n",
      "  19519/500000: episode: 205, duration: 0.955s, episode steps: 206, steps per second: 216, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.405131, mae: 42.476235, mean_q: 85.060323, mean_eps: 0.100000\n",
      "  19721/500000: episode: 206, duration: 0.963s, episode steps: 202, steps per second: 210, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.279717, mae: 42.029668, mean_q: 84.226986, mean_eps: 0.100000\n",
      "  19902/500000: episode: 207, duration: 0.831s, episode steps: 181, steps per second: 218, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.854655, mae: 41.928947, mean_q: 84.051515, mean_eps: 0.100000\n",
      "  20087/500000: episode: 208, duration: 0.836s, episode steps: 185, steps per second: 221, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.634264, mae: 41.815600, mean_q: 83.494867, mean_eps: 0.100000\n",
      "  20272/500000: episode: 209, duration: 0.830s, episode steps: 185, steps per second: 223, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.674945, mae: 41.341231, mean_q: 82.578624, mean_eps: 0.100000\n",
      "  20461/500000: episode: 210, duration: 0.848s, episode steps: 189, steps per second: 223, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3.558873, mae: 41.039053, mean_q: 82.092334, mean_eps: 0.100000\n",
      "  20662/500000: episode: 211, duration: 0.908s, episode steps: 201, steps per second: 221, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.939266, mae: 40.789587, mean_q: 81.678378, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20861/500000: episode: 212, duration: 0.975s, episode steps: 199, steps per second: 204, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.827188, mae: 40.468924, mean_q: 81.056425, mean_eps: 0.100000\n",
      "  21075/500000: episode: 213, duration: 1.116s, episode steps: 214, steps per second: 192, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 3.173036, mae: 40.337797, mean_q: 80.776400, mean_eps: 0.100000\n",
      "  21277/500000: episode: 214, duration: 0.978s, episode steps: 202, steps per second: 207, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.787800, mae: 40.408827, mean_q: 80.757304, mean_eps: 0.100000\n",
      "  21459/500000: episode: 215, duration: 0.988s, episode steps: 182, steps per second: 184, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.443058, mae: 40.379050, mean_q: 80.919369, mean_eps: 0.100000\n",
      "  21644/500000: episode: 216, duration: 0.909s, episode steps: 185, steps per second: 204, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.964615, mae: 40.058900, mean_q: 80.145405, mean_eps: 0.100000\n",
      "  21825/500000: episode: 217, duration: 0.923s, episode steps: 181, steps per second: 196, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.291891, mae: 39.785760, mean_q: 79.716846, mean_eps: 0.100000\n",
      "  22014/500000: episode: 218, duration: 0.915s, episode steps: 189, steps per second: 207, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.461569, mae: 39.615010, mean_q: 79.325116, mean_eps: 0.100000\n",
      "  22228/500000: episode: 219, duration: 1.018s, episode steps: 214, steps per second: 210, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.654132, mae: 39.857131, mean_q: 79.858106, mean_eps: 0.100000\n",
      "  22442/500000: episode: 220, duration: 1.042s, episode steps: 214, steps per second: 205, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.116282, mae: 39.880118, mean_q: 79.903069, mean_eps: 0.100000\n",
      "  22631/500000: episode: 221, duration: 0.865s, episode steps: 189, steps per second: 219, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.821908, mae: 39.792039, mean_q: 79.798367, mean_eps: 0.100000\n",
      "  22845/500000: episode: 222, duration: 1.031s, episode steps: 214, steps per second: 208, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.627603, mae: 39.738183, mean_q: 79.639800, mean_eps: 0.100000\n",
      "  23087/500000: episode: 223, duration: 1.152s, episode steps: 242, steps per second: 210, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.268883, mae: 39.841302, mean_q: 79.948880, mean_eps: 0.100000\n",
      "  23305/500000: episode: 224, duration: 0.999s, episode steps: 218, steps per second: 218, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.147254, mae: 39.857606, mean_q: 79.997485, mean_eps: 0.100000\n",
      "  23514/500000: episode: 225, duration: 0.974s, episode steps: 209, steps per second: 215, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.069441, mae: 39.723584, mean_q: 79.733596, mean_eps: 0.100000\n",
      "  23740/500000: episode: 226, duration: 1.100s, episode steps: 226, steps per second: 205, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.031060, mae: 39.581671, mean_q: 79.463533, mean_eps: 0.100000\n",
      "  24054/500000: episode: 227, duration: 1.476s, episode steps: 314, steps per second: 213, episode reward: 314.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.018940, mae: 40.037150, mean_q: 80.377499, mean_eps: 0.100000\n",
      "  24316/500000: episode: 228, duration: 1.229s, episode steps: 262, steps per second: 213, episode reward: 262.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.027109, mae: 40.304237, mean_q: 80.886904, mean_eps: 0.100000\n",
      "  24611/500000: episode: 229, duration: 1.397s, episode steps: 295, steps per second: 211, episode reward: 295.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.024317, mae: 40.643321, mean_q: 81.545772, mean_eps: 0.100000\n",
      "  24849/500000: episode: 230, duration: 1.071s, episode steps: 238, steps per second: 222, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.030794, mae: 40.670813, mean_q: 81.544838, mean_eps: 0.100000\n",
      "  25103/500000: episode: 231, duration: 1.176s, episode steps: 254, steps per second: 216, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.033045, mae: 39.878844, mean_q: 79.950053, mean_eps: 0.100000\n",
      "  25373/500000: episode: 232, duration: 1.272s, episode steps: 270, steps per second: 212, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.040780, mae: 39.721283, mean_q: 79.646542, mean_eps: 0.100000\n",
      "  25689/500000: episode: 233, duration: 1.465s, episode steps: 316, steps per second: 216, episode reward: 316.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.044479, mae: 39.301968, mean_q: 78.824252, mean_eps: 0.100000\n",
      "  26048/500000: episode: 234, duration: 1.726s, episode steps: 359, steps per second: 208, episode reward: 359.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.038841, mae: 39.840817, mean_q: 79.908286, mean_eps: 0.100000\n",
      "  26488/500000: episode: 235, duration: 2.077s, episode steps: 440, steps per second: 212, episode reward: 440.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.019312, mae: 41.304170, mean_q: 82.816245, mean_eps: 0.100000\n",
      "  26988/500000: episode: 236, duration: 2.352s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.008875, mae: 42.695472, mean_q: 85.580718, mean_eps: 0.100000\n",
      "  27356/500000: episode: 237, duration: 1.709s, episode steps: 368, steps per second: 215, episode reward: 368.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.304229, mae: 45.116104, mean_q: 90.231594, mean_eps: 0.100000\n",
      "  27620/500000: episode: 238, duration: 1.188s, episode steps: 264, steps per second: 222, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.886725, mae: 44.260069, mean_q: 88.610214, mean_eps: 0.100000\n",
      "  27806/500000: episode: 239, duration: 0.860s, episode steps: 186, steps per second: 216, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.147987, mae: 42.912495, mean_q: 85.956995, mean_eps: 0.100000\n",
      "  27982/500000: episode: 240, duration: 0.850s, episode steps: 176, steps per second: 207, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.891741, mae: 41.090841, mean_q: 82.328152, mean_eps: 0.100000\n",
      "  28311/500000: episode: 241, duration: 1.546s, episode steps: 329, steps per second: 213, episode reward: 329.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.071399, mae: 39.960436, mean_q: 80.156360, mean_eps: 0.100000\n",
      "  28675/500000: episode: 242, duration: 1.678s, episode steps: 364, steps per second: 217, episode reward: 364.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.059389, mae: 40.158105, mean_q: 80.569116, mean_eps: 0.100000\n",
      "  28977/500000: episode: 243, duration: 1.354s, episode steps: 302, steps per second: 223, episode reward: 302.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.058452, mae: 39.705652, mean_q: 79.596832, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29331/500000: episode: 244, duration: 1.577s, episode steps: 354, steps per second: 225, episode reward: 354.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.028059, mae: 39.074819, mean_q: 78.298890, mean_eps: 0.100000\n",
      "  29659/500000: episode: 245, duration: 1.507s, episode steps: 328, steps per second: 218, episode reward: 328.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.042834, mae: 38.549682, mean_q: 77.208909, mean_eps: 0.100000\n",
      "  29993/500000: episode: 246, duration: 1.627s, episode steps: 334, steps per second: 205, episode reward: 334.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.025065, mae: 39.117879, mean_q: 78.353836, mean_eps: 0.100000\n",
      "  30334/500000: episode: 247, duration: 1.683s, episode steps: 341, steps per second: 203, episode reward: 341.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.021773, mae: 38.599070, mean_q: 77.345875, mean_eps: 0.100000\n",
      "  30647/500000: episode: 248, duration: 1.485s, episode steps: 313, steps per second: 211, episode reward: 313.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.012202, mae: 38.512190, mean_q: 77.189446, mean_eps: 0.100000\n",
      "  30994/500000: episode: 249, duration: 1.612s, episode steps: 347, steps per second: 215, episode reward: 347.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.010821, mae: 38.186095, mean_q: 76.545424, mean_eps: 0.100000\n",
      "  31327/500000: episode: 250, duration: 1.599s, episode steps: 333, steps per second: 208, episode reward: 333.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.008259, mae: 38.030813, mean_q: 76.223303, mean_eps: 0.100000\n",
      "  31649/500000: episode: 251, duration: 1.668s, episode steps: 322, steps per second: 193, episode reward: 322.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.008538, mae: 37.763174, mean_q: 75.678621, mean_eps: 0.100000\n",
      "  32033/500000: episode: 252, duration: 1.908s, episode steps: 384, steps per second: 201, episode reward: 384.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.021597, mae: 37.621737, mean_q: 75.344817, mean_eps: 0.100000\n",
      "  32373/500000: episode: 253, duration: 1.677s, episode steps: 340, steps per second: 203, episode reward: 340.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.023224, mae: 37.538252, mean_q: 75.155887, mean_eps: 0.100000\n",
      "  32727/500000: episode: 254, duration: 1.676s, episode steps: 354, steps per second: 211, episode reward: 354.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.020838, mae: 37.376505, mean_q: 74.841735, mean_eps: 0.100000\n",
      "  33116/500000: episode: 255, duration: 1.791s, episode steps: 389, steps per second: 217, episode reward: 389.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.010363, mae: 37.390582, mean_q: 74.902821, mean_eps: 0.100000\n",
      "  33503/500000: episode: 256, duration: 1.879s, episode steps: 387, steps per second: 206, episode reward: 387.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.004824, mae: 37.566661, mean_q: 75.291340, mean_eps: 0.100000\n",
      "  33928/500000: episode: 257, duration: 1.975s, episode steps: 425, steps per second: 215, episode reward: 425.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.006599, mae: 38.129095, mean_q: 76.415284, mean_eps: 0.100000\n",
      "  34408/500000: episode: 258, duration: 2.296s, episode steps: 480, steps per second: 209, episode reward: 480.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.017517, mae: 38.683661, mean_q: 77.523802, mean_eps: 0.100000\n",
      "  34903/500000: episode: 259, duration: 2.285s, episode steps: 495, steps per second: 217, episode reward: 495.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.090628, mae: 39.224472, mean_q: 78.586714, mean_eps: 0.100000\n",
      "  35245/500000: episode: 260, duration: 1.576s, episode steps: 342, steps per second: 217, episode reward: 342.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.212902, mae: 38.885123, mean_q: 77.841396, mean_eps: 0.100000\n",
      "  35720/500000: episode: 261, duration: 2.189s, episode steps: 475, steps per second: 217, episode reward: 475.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.174042, mae: 40.398622, mean_q: 80.907749, mean_eps: 0.100000\n",
      "  36220/500000: episode: 262, duration: 2.309s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.062313, mae: 42.777115, mean_q: 85.712704, mean_eps: 0.100000\n",
      "  36671/500000: episode: 263, duration: 2.054s, episode steps: 451, steps per second: 220, episode reward: 451.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.877876, mae: 43.079156, mean_q: 86.127613, mean_eps: 0.100000\n",
      "  37171/500000: episode: 264, duration: 2.297s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.991226, mae: 43.310960, mean_q: 86.562720, mean_eps: 0.100000\n",
      "  37480/500000: episode: 265, duration: 1.384s, episode steps: 309, steps per second: 223, episode reward: 309.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 5.313086, mae: 45.725493, mean_q: 91.385293, mean_eps: 0.100000\n",
      "  37743/500000: episode: 266, duration: 1.179s, episode steps: 263, steps per second: 223, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.546142, mae: 44.393318, mean_q: 88.998932, mean_eps: 0.100000\n",
      "  38016/500000: episode: 267, duration: 1.221s, episode steps: 273, steps per second: 224, episode reward: 273.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.207794, mae: 42.197211, mean_q: 84.513180, mean_eps: 0.100000\n",
      "  38310/500000: episode: 268, duration: 1.336s, episode steps: 294, steps per second: 220, episode reward: 294.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.126489, mae: 38.942348, mean_q: 78.016502, mean_eps: 0.100000\n",
      "  38685/500000: episode: 269, duration: 1.731s, episode steps: 375, steps per second: 217, episode reward: 375.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.230968, mae: 38.167237, mean_q: 76.515321, mean_eps: 0.100000\n",
      "  39104/500000: episode: 270, duration: 1.963s, episode steps: 419, steps per second: 213, episode reward: 419.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.187935, mae: 39.546858, mean_q: 79.268803, mean_eps: 0.100000\n",
      "  39579/500000: episode: 271, duration: 2.312s, episode steps: 475, steps per second: 205, episode reward: 475.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.217386, mae: 40.361366, mean_q: 80.827570, mean_eps: 0.100000\n",
      "  40079/500000: episode: 272, duration: 2.399s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.264719, mae: 41.431846, mean_q: 82.937228, mean_eps: 0.100000\n",
      "  40456/500000: episode: 273, duration: 1.790s, episode steps: 377, steps per second: 211, episode reward: 377.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 1.745522, mae: 42.245317, mean_q: 84.460312, mean_eps: 0.100000\n",
      "  40864/500000: episode: 274, duration: 1.822s, episode steps: 408, steps per second: 224, episode reward: 408.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 1.087060, mae: 41.953323, mean_q: 83.969431, mean_eps: 0.100000\n",
      "  41337/500000: episode: 275, duration: 2.118s, episode steps: 473, steps per second: 223, episode reward: 473.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.505261, mae: 41.154459, mean_q: 82.438253, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41837/500000: episode: 276, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.027660, mae: 42.644245, mean_q: 85.451531, mean_eps: 0.100000\n",
      "  42337/500000: episode: 277, duration: 2.273s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.497746, mae: 46.663404, mean_q: 93.273722, mean_eps: 0.100000\n",
      "  42837/500000: episode: 278, duration: 2.323s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.426369, mae: 50.356322, mean_q: 100.514013, mean_eps: 0.100000\n",
      "  43337/500000: episode: 279, duration: 2.326s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.681182, mae: 51.136407, mean_q: 102.017513, mean_eps: 0.100000\n",
      "  43837/500000: episode: 280, duration: 2.394s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.201452, mae: 50.475701, mean_q: 100.829704, mean_eps: 0.100000\n",
      "  44337/500000: episode: 281, duration: 2.448s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 13.199374, mae: 50.086934, mean_q: 99.982896, mean_eps: 0.100000\n",
      "  44679/500000: episode: 282, duration: 1.671s, episode steps: 342, steps per second: 205, episode reward: 342.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 6.130997, mae: 46.279724, mean_q: 92.568060, mean_eps: 0.100000\n",
      "  45000/500000: episode: 283, duration: 1.657s, episode steps: 321, steps per second: 194, episode reward: 321.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.591728, mae: 40.998144, mean_q: 81.976986, mean_eps: 0.100000\n",
      "  45308/500000: episode: 284, duration: 1.520s, episode steps: 308, steps per second: 203, episode reward: 308.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.190563, mae: 36.351818, mean_q: 72.525859, mean_eps: 0.100000\n",
      "  45524/500000: episode: 285, duration: 1.026s, episode steps: 216, steps per second: 210, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.074998, mae: 35.742432, mean_q: 71.407214, mean_eps: 0.100000\n",
      "  45854/500000: episode: 286, duration: 1.490s, episode steps: 330, steps per second: 221, episode reward: 330.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.551298, mae: 38.151271, mean_q: 76.157898, mean_eps: 0.100000\n",
      "  46118/500000: episode: 287, duration: 1.231s, episode steps: 264, steps per second: 214, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.848901, mae: 38.076994, mean_q: 76.153299, mean_eps: 0.100000\n",
      "  46356/500000: episode: 288, duration: 1.143s, episode steps: 238, steps per second: 208, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 1.064833, mae: 37.252330, mean_q: 74.573467, mean_eps: 0.100000\n",
      "  46576/500000: episode: 289, duration: 1.011s, episode steps: 220, steps per second: 218, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.543454, mae: 36.265483, mean_q: 72.692539, mean_eps: 0.100000\n",
      "  46802/500000: episode: 290, duration: 1.030s, episode steps: 226, steps per second: 220, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.040428, mae: 33.345165, mean_q: 66.998946, mean_eps: 0.100000\n",
      "  47069/500000: episode: 291, duration: 1.288s, episode steps: 267, steps per second: 207, episode reward: 267.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.031536, mae: 32.976001, mean_q: 66.226264, mean_eps: 0.100000\n",
      "  47316/500000: episode: 292, duration: 1.208s, episode steps: 247, steps per second: 205, episode reward: 247.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.024619, mae: 32.997671, mean_q: 66.252929, mean_eps: 0.100000\n",
      "  47561/500000: episode: 293, duration: 1.195s, episode steps: 245, steps per second: 205, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.018461, mae: 32.893118, mean_q: 66.029604, mean_eps: 0.100000\n",
      "  47833/500000: episode: 294, duration: 1.306s, episode steps: 272, steps per second: 208, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.015315, mae: 33.337396, mean_q: 66.923077, mean_eps: 0.100000\n",
      "  48121/500000: episode: 295, duration: 1.334s, episode steps: 288, steps per second: 216, episode reward: 288.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.012967, mae: 32.771997, mean_q: 65.801919, mean_eps: 0.100000\n",
      "  48457/500000: episode: 296, duration: 1.584s, episode steps: 336, steps per second: 212, episode reward: 336.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.016424, mae: 33.342359, mean_q: 66.926094, mean_eps: 0.100000\n",
      "  48817/500000: episode: 297, duration: 1.644s, episode steps: 360, steps per second: 219, episode reward: 360.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.016693, mae: 34.105366, mean_q: 68.450065, mean_eps: 0.100000\n",
      "  49199/500000: episode: 298, duration: 1.737s, episode steps: 382, steps per second: 220, episode reward: 382.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.017239, mae: 34.993712, mean_q: 70.200180, mean_eps: 0.100000\n",
      "  49612/500000: episode: 299, duration: 1.949s, episode steps: 413, steps per second: 212, episode reward: 413.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.056341, mae: 35.408676, mean_q: 70.991633, mean_eps: 0.100000\n",
      "  50071/500000: episode: 300, duration: 2.294s, episode steps: 459, steps per second: 200, episode reward: 459.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.131290, mae: 36.261275, mean_q: 72.609791, mean_eps: 0.100000\n",
      "  50571/500000: episode: 301, duration: 2.403s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.133200, mae: 38.996075, mean_q: 78.099893, mean_eps: 0.100000\n",
      "  51071/500000: episode: 302, duration: 2.345s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.990054, mae: 45.917022, mean_q: 91.852124, mean_eps: 0.100000\n",
      "  51571/500000: episode: 303, duration: 2.398s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.347250, mae: 50.300009, mean_q: 100.504075, mean_eps: 0.100000\n",
      "  52071/500000: episode: 304, duration: 2.392s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.086445, mae: 50.050950, mean_q: 99.938652, mean_eps: 0.100000\n",
      "  52571/500000: episode: 305, duration: 2.444s, episode steps: 500, steps per second: 205, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.356613, mae: 49.654965, mean_q: 99.153595, mean_eps: 0.100000\n",
      "  53071/500000: episode: 306, duration: 2.465s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.168273, mae: 49.465167, mean_q: 98.783861, mean_eps: 0.100000\n",
      "  53571/500000: episode: 307, duration: 2.436s, episode steps: 500, steps per second: 205, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.041790, mae: 49.946235, mean_q: 99.778163, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  54071/500000: episode: 308, duration: 2.408s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.263963, mae: 49.613125, mean_q: 99.391540, mean_eps: 0.100000\n",
      "  54571/500000: episode: 309, duration: 2.400s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.685316, mae: 46.940982, mean_q: 93.961969, mean_eps: 0.100000\n",
      "  55071/500000: episode: 310, duration: 2.323s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.290096, mae: 45.275020, mean_q: 90.542647, mean_eps: 0.100000\n",
      "  55571/500000: episode: 311, duration: 2.487s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.912971, mae: 45.204767, mean_q: 90.327953, mean_eps: 0.100000\n",
      "  56071/500000: episode: 312, duration: 2.430s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.437119, mae: 45.247595, mean_q: 90.206511, mean_eps: 0.100000\n",
      "  56571/500000: episode: 313, duration: 2.547s, episode steps: 500, steps per second: 196, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.202352, mae: 45.174399, mean_q: 90.114375, mean_eps: 0.100000\n",
      "  57071/500000: episode: 314, duration: 2.321s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.344651, mae: 43.806793, mean_q: 87.398259, mean_eps: 0.100000\n",
      "  57571/500000: episode: 315, duration: 2.253s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.907664, mae: 43.097618, mean_q: 86.094033, mean_eps: 0.100000\n",
      "  58071/500000: episode: 316, duration: 2.252s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.059309, mae: 45.249862, mean_q: 90.454042, mean_eps: 0.100000\n",
      "  58571/500000: episode: 317, duration: 2.297s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.526346, mae: 47.732566, mean_q: 95.328170, mean_eps: 0.100000\n",
      "  59071/500000: episode: 318, duration: 2.468s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.627704, mae: 47.875983, mean_q: 95.568745, mean_eps: 0.100000\n",
      "  59571/500000: episode: 319, duration: 2.390s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.015174, mae: 46.755660, mean_q: 93.212848, mean_eps: 0.100000\n",
      "  60071/500000: episode: 320, duration: 2.432s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.444290, mae: 45.797989, mean_q: 91.629255, mean_eps: 0.100000\n",
      "  60571/500000: episode: 321, duration: 2.346s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.457370, mae: 45.786739, mean_q: 91.533203, mean_eps: 0.100000\n",
      "  61071/500000: episode: 322, duration: 2.369s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.541680, mae: 46.062585, mean_q: 92.077865, mean_eps: 0.100000\n",
      "  61571/500000: episode: 323, duration: 2.324s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.449701, mae: 43.758196, mean_q: 87.628970, mean_eps: 0.100000\n",
      "  62071/500000: episode: 324, duration: 2.273s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.815931, mae: 42.686628, mean_q: 85.615110, mean_eps: 0.100000\n",
      "  62571/500000: episode: 325, duration: 2.305s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.649114, mae: 47.271271, mean_q: 94.670294, mean_eps: 0.100000\n",
      "  62979/500000: episode: 326, duration: 1.952s, episode steps: 408, steps per second: 209, episode reward: 408.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 8.432614, mae: 50.545148, mean_q: 101.088243, mean_eps: 0.100000\n",
      "  63479/500000: episode: 327, duration: 2.450s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 4.637778, mae: 47.938894, mean_q: 95.968128, mean_eps: 0.100000\n",
      "  63872/500000: episode: 328, duration: 1.963s, episode steps: 393, steps per second: 200, episode reward: 393.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.923589, mae: 44.251829, mean_q: 88.659734, mean_eps: 0.100000\n",
      "  64355/500000: episode: 329, duration: 2.274s, episode steps: 483, steps per second: 212, episode reward: 483.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.319880, mae: 43.871301, mean_q: 87.877846, mean_eps: 0.100000\n",
      "  64855/500000: episode: 330, duration: 2.376s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.101955, mae: 43.935820, mean_q: 88.033303, mean_eps: 0.100000\n",
      "  65355/500000: episode: 331, duration: 2.414s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.446569, mae: 45.552216, mean_q: 91.191556, mean_eps: 0.100000\n",
      "  65803/500000: episode: 332, duration: 2.153s, episode steps: 448, steps per second: 208, episode reward: 448.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 5.344063, mae: 48.195313, mean_q: 96.314367, mean_eps: 0.100000\n",
      "  66303/500000: episode: 333, duration: 2.295s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.741190, mae: 47.341642, mean_q: 94.573639, mean_eps: 0.100000\n",
      "  66803/500000: episode: 334, duration: 2.264s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.704249, mae: 47.081544, mean_q: 94.153611, mean_eps: 0.100000\n",
      "  67303/500000: episode: 335, duration: 2.322s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.617059, mae: 47.410326, mean_q: 94.814363, mean_eps: 0.100000\n",
      "  67803/500000: episode: 336, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.151294, mae: 42.485637, mean_q: 84.833520, mean_eps: 0.100000\n",
      "  68303/500000: episode: 337, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.918393, mae: 40.877008, mean_q: 81.739792, mean_eps: 0.100000\n",
      "  68803/500000: episode: 338, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.428375, mae: 45.267509, mean_q: 90.542453, mean_eps: 0.100000\n",
      "  69102/500000: episode: 339, duration: 1.469s, episode steps: 299, steps per second: 204, episode reward: 299.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.877997, mae: 48.672213, mean_q: 97.462038, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  69372/500000: episode: 340, duration: 1.206s, episode steps: 270, steps per second: 224, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 8.391257, mae: 45.915764, mean_q: 91.805935, mean_eps: 0.100000\n",
      "  69749/500000: episode: 341, duration: 1.782s, episode steps: 377, steps per second: 212, episode reward: 377.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 5.471382, mae: 42.298916, mean_q: 84.736367, mean_eps: 0.100000\n",
      "  70249/500000: episode: 342, duration: 2.447s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.682816, mae: 40.223379, mean_q: 80.915358, mean_eps: 0.100000\n",
      "  70749/500000: episode: 343, duration: 2.465s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.744609, mae: 43.876250, mean_q: 87.990786, mean_eps: 0.100000\n",
      "  71249/500000: episode: 344, duration: 2.427s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.793841, mae: 48.704482, mean_q: 97.388979, mean_eps: 0.100000\n",
      "  71749/500000: episode: 345, duration: 2.488s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.500318, mae: 51.063706, mean_q: 102.014357, mean_eps: 0.100000\n",
      "  72249/500000: episode: 346, duration: 2.315s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.978792, mae: 48.158340, mean_q: 96.055536, mean_eps: 0.100000\n",
      "  72749/500000: episode: 347, duration: 2.302s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.157446, mae: 43.207866, mean_q: 86.178560, mean_eps: 0.100000\n",
      "  73249/500000: episode: 348, duration: 2.276s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.733317, mae: 39.265230, mean_q: 78.443848, mean_eps: 0.100000\n",
      "  73749/500000: episode: 349, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.641126, mae: 36.513201, mean_q: 73.095436, mean_eps: 0.100000\n",
      "  74249/500000: episode: 350, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 2.890990, mae: 31.927782, mean_q: 64.063479, mean_eps: 0.100000\n",
      "  74749/500000: episode: 351, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.884058, mae: 24.169531, mean_q: 48.003735, mean_eps: 0.100000\n",
      "  75249/500000: episode: 352, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.084006, mae: 17.787028, mean_q: 34.130621, mean_eps: 0.100000\n",
      "  75749/500000: episode: 353, duration: 2.366s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.027439, mae: 16.895366, mean_q: 33.612826, mean_eps: 0.100000\n",
      "  76249/500000: episode: 354, duration: 2.406s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.034689, mae: 20.201235, mean_q: 40.621835, mean_eps: 0.100000\n",
      "  76749/500000: episode: 355, duration: 2.347s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.113073, mae: 22.943476, mean_q: 46.039292, mean_eps: 0.100000\n",
      "  77249/500000: episode: 356, duration: 2.325s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.316389, mae: 25.131109, mean_q: 50.362951, mean_eps: 0.100000\n",
      "  77749/500000: episode: 357, duration: 2.354s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.703546, mae: 28.073660, mean_q: 56.214499, mean_eps: 0.100000\n",
      "  78249/500000: episode: 358, duration: 2.313s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.239780, mae: 31.737951, mean_q: 63.479986, mean_eps: 0.100000\n",
      "  78749/500000: episode: 359, duration: 2.285s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.789519, mae: 35.651048, mean_q: 71.274257, mean_eps: 0.100000\n",
      "  79249/500000: episode: 360, duration: 2.297s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.463725, mae: 39.781996, mean_q: 79.508351, mean_eps: 0.100000\n",
      "  79749/500000: episode: 361, duration: 2.344s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.530312, mae: 43.126788, mean_q: 86.196698, mean_eps: 0.100000\n",
      "  80249/500000: episode: 362, duration: 2.312s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.346039, mae: 45.852898, mean_q: 91.645324, mean_eps: 0.100000\n",
      "  80749/500000: episode: 363, duration: 2.297s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.860184, mae: 47.777148, mean_q: 95.401265, mean_eps: 0.100000\n",
      "  81249/500000: episode: 364, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 4.432396, mae: 45.057099, mean_q: 90.108749, mean_eps: 0.100000\n",
      "  81749/500000: episode: 365, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.666366, mae: 40.228342, mean_q: 80.409363, mean_eps: 0.100000\n",
      "  82249/500000: episode: 366, duration: 2.303s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.458479, mae: 37.702205, mean_q: 75.379983, mean_eps: 0.100000\n",
      "  82749/500000: episode: 367, duration: 2.298s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.721446, mae: 38.693453, mean_q: 77.334574, mean_eps: 0.100000\n",
      "  83249/500000: episode: 368, duration: 2.303s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.704043, mae: 43.850692, mean_q: 87.687997, mean_eps: 0.100000\n",
      "  83749/500000: episode: 369, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.064802, mae: 48.646588, mean_q: 97.370760, mean_eps: 0.100000\n",
      "  84249/500000: episode: 370, duration: 2.303s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.352625, mae: 51.347135, mean_q: 102.466618, mean_eps: 0.100000\n",
      "  84749/500000: episode: 371, duration: 2.355s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.879436, mae: 51.725494, mean_q: 103.317642, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85249/500000: episode: 372, duration: 2.461s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.187153, mae: 48.911267, mean_q: 97.684798, mean_eps: 0.100000\n",
      "  85749/500000: episode: 373, duration: 2.470s, episode steps: 500, steps per second: 202, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.588918, mae: 44.532834, mean_q: 88.982927, mean_eps: 0.100000\n",
      "  86249/500000: episode: 374, duration: 2.417s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.706586, mae: 43.946036, mean_q: 87.723079, mean_eps: 0.100000\n",
      "  86749/500000: episode: 375, duration: 2.445s, episode steps: 500, steps per second: 205, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.551957, mae: 46.166750, mean_q: 92.040500, mean_eps: 0.100000\n",
      "  87249/500000: episode: 376, duration: 2.393s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.534899, mae: 44.402211, mean_q: 88.786888, mean_eps: 0.100000\n",
      "  87749/500000: episode: 377, duration: 2.377s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.129168, mae: 38.853691, mean_q: 77.708022, mean_eps: 0.100000\n",
      "  88249/500000: episode: 378, duration: 2.439s, episode steps: 500, steps per second: 205, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.277658, mae: 39.212216, mean_q: 78.459784, mean_eps: 0.100000\n",
      "  88749/500000: episode: 379, duration: 2.373s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.920075, mae: 46.130621, mean_q: 92.226404, mean_eps: 0.100000\n",
      "  89249/500000: episode: 380, duration: 2.382s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.880105, mae: 51.653895, mean_q: 103.113435, mean_eps: 0.100000\n",
      "  89749/500000: episode: 381, duration: 2.409s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 8.062333, mae: 51.621796, mean_q: 103.213336, mean_eps: 0.100000\n",
      "  90249/500000: episode: 382, duration: 2.385s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.473397, mae: 45.398578, mean_q: 90.869822, mean_eps: 0.100000\n",
      "  90749/500000: episode: 383, duration: 2.418s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.086072, mae: 36.662773, mean_q: 73.471760, mean_eps: 0.100000\n",
      "  91249/500000: episode: 384, duration: 2.363s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.897765, mae: 31.777474, mean_q: 63.814049, mean_eps: 0.100000\n",
      "  91749/500000: episode: 385, duration: 2.304s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.005712, mae: 32.747673, mean_q: 65.784400, mean_eps: 0.100000\n",
      "  92249/500000: episode: 386, duration: 2.286s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.884085, mae: 36.716817, mean_q: 73.640375, mean_eps: 0.100000\n",
      "  92749/500000: episode: 387, duration: 2.360s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.670177, mae: 38.526456, mean_q: 77.152773, mean_eps: 0.100000\n",
      "  93249/500000: episode: 388, duration: 2.373s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.797635, mae: 39.565049, mean_q: 79.315610, mean_eps: 0.100000\n",
      "  93657/500000: episode: 389, duration: 1.917s, episode steps: 408, steps per second: 213, episode reward: 408.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.932573, mae: 42.706694, mean_q: 85.589461, mean_eps: 0.100000\n",
      "  94047/500000: episode: 390, duration: 1.812s, episode steps: 390, steps per second: 215, episode reward: 390.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 2.452843, mae: 43.225711, mean_q: 86.489302, mean_eps: 0.100000\n",
      "  94547/500000: episode: 391, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.852211, mae: 42.826571, mean_q: 85.732612, mean_eps: 0.100000\n",
      "  95047/500000: episode: 392, duration: 2.431s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.386954, mae: 46.614087, mean_q: 93.181959, mean_eps: 0.100000\n",
      "  95547/500000: episode: 393, duration: 2.372s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.260849, mae: 48.062164, mean_q: 96.143538, mean_eps: 0.100000\n",
      "  96047/500000: episode: 394, duration: 2.364s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.874592, mae: 45.626004, mean_q: 91.307807, mean_eps: 0.100000\n",
      "  96547/500000: episode: 395, duration: 2.323s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.176775, mae: 45.174510, mean_q: 90.142791, mean_eps: 0.100000\n",
      "  97047/500000: episode: 396, duration: 2.490s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.539400, mae: 47.136814, mean_q: 93.996401, mean_eps: 0.100000\n",
      "  97547/500000: episode: 397, duration: 2.588s, episode steps: 500, steps per second: 193, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.063263, mae: 49.317284, mean_q: 98.645079, mean_eps: 0.100000\n",
      "  98047/500000: episode: 398, duration: 2.413s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.586110, mae: 51.647286, mean_q: 103.156465, mean_eps: 0.100000\n",
      "  98547/500000: episode: 399, duration: 2.391s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.607457, mae: 52.353545, mean_q: 104.572096, mean_eps: 0.100000\n",
      "  99047/500000: episode: 400, duration: 2.480s, episode steps: 500, steps per second: 202, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.420757, mae: 51.561193, mean_q: 102.940899, mean_eps: 0.100000\n",
      "  99547/500000: episode: 401, duration: 2.358s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.495043, mae: 52.261192, mean_q: 104.482199, mean_eps: 0.100000\n",
      " 100047/500000: episode: 402, duration: 2.374s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.908315, mae: 52.637917, mean_q: 105.216375, mean_eps: 0.100000\n",
      " 100547/500000: episode: 403, duration: 2.415s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.268325, mae: 50.188598, mean_q: 100.519932, mean_eps: 0.100000\n",
      " 100557/500000: episode: 404, duration: 0.057s, episode steps:  10, steps per second: 176, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.063197, mae: 47.106726, mean_q: 94.953196, mean_eps: 0.100000\n",
      " 100566/500000: episode: 405, duration: 0.052s, episode steps:   9, steps per second: 173, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 15.064133, mae: 48.071184, mean_q: 96.448792, mean_eps: 0.100000\n",
      " 100576/500000: episode: 406, duration: 0.056s, episode steps:  10, steps per second: 179, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 28.442651, mae: 48.204616, mean_q: 95.981470, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100586/500000: episode: 407, duration: 0.059s, episode steps:  10, steps per second: 170, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 34.197634, mae: 47.741802, mean_q: 94.697960, mean_eps: 0.100000\n",
      " 101086/500000: episode: 408, duration: 2.468s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.175072, mae: 46.815515, mean_q: 93.384061, mean_eps: 0.100000\n",
      " 101524/500000: episode: 409, duration: 2.151s, episode steps: 438, steps per second: 204, episode reward: 438.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.886299, mae: 46.943006, mean_q: 94.012290, mean_eps: 0.100000\n",
      " 101789/500000: episode: 410, duration: 1.274s, episode steps: 265, steps per second: 208, episode reward: 265.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.671133, mae: 44.157732, mean_q: 88.696529, mean_eps: 0.100000\n",
      " 102176/500000: episode: 411, duration: 1.772s, episode steps: 387, steps per second: 218, episode reward: 387.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.501 [0.000, 1.000],  loss: 5.236814, mae: 39.473676, mean_q: 79.192761, mean_eps: 0.100000\n",
      " 102676/500000: episode: 412, duration: 2.321s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.139340, mae: 36.527706, mean_q: 73.578361, mean_eps: 0.100000\n",
      " 103176/500000: episode: 413, duration: 2.344s, episode steps: 500, steps per second: 213, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.055724, mae: 40.251684, mean_q: 81.238099, mean_eps: 0.100000\n",
      " 103676/500000: episode: 414, duration: 2.375s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.453422, mae: 47.765424, mean_q: 96.082307, mean_eps: 0.100000\n",
      " 104176/500000: episode: 415, duration: 2.364s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.573296, mae: 49.885011, mean_q: 100.284662, mean_eps: 0.100000\n",
      " 104676/500000: episode: 416, duration: 2.405s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.356191, mae: 38.477158, mean_q: 77.586276, mean_eps: 0.100000\n",
      " 105176/500000: episode: 417, duration: 2.391s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.615920, mae: 27.520993, mean_q: 55.632759, mean_eps: 0.100000\n",
      " 105676/500000: episode: 418, duration: 2.367s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.476497, mae: 25.535114, mean_q: 51.517518, mean_eps: 0.100000\n",
      " 106176/500000: episode: 419, duration: 2.421s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.678070, mae: 25.498990, mean_q: 51.384618, mean_eps: 0.100000\n",
      " 106676/500000: episode: 420, duration: 2.364s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.525513, mae: 25.140246, mean_q: 50.647531, mean_eps: 0.100000\n",
      " 107176/500000: episode: 421, duration: 2.296s, episode steps: 500, steps per second: 218, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.397489, mae: 26.008007, mean_q: 52.370818, mean_eps: 0.100000\n",
      " 107676/500000: episode: 422, duration: 2.488s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.511088, mae: 26.798350, mean_q: 53.897455, mean_eps: 0.100000\n",
      " 108176/500000: episode: 423, duration: 2.443s, episode steps: 500, steps per second: 205, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.507998, mae: 27.080733, mean_q: 54.432306, mean_eps: 0.100000\n",
      " 108676/500000: episode: 424, duration: 2.378s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.766526, mae: 29.278491, mean_q: 58.770998, mean_eps: 0.100000\n",
      " 109176/500000: episode: 425, duration: 2.512s, episode steps: 500, steps per second: 199, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.982939, mae: 31.850707, mean_q: 63.784780, mean_eps: 0.100000\n",
      " 109676/500000: episode: 426, duration: 2.376s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.125181, mae: 34.598566, mean_q: 69.345531, mean_eps: 0.100000\n",
      " 110176/500000: episode: 427, duration: 2.393s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.064890, mae: 37.954824, mean_q: 75.998685, mean_eps: 0.100000\n",
      " 110676/500000: episode: 428, duration: 2.388s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.927350, mae: 40.331072, mean_q: 80.694380, mean_eps: 0.100000\n",
      " 111176/500000: episode: 429, duration: 2.262s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.673373, mae: 40.907340, mean_q: 81.744971, mean_eps: 0.100000\n",
      " 111676/500000: episode: 430, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.978386, mae: 41.206750, mean_q: 82.386457, mean_eps: 0.100000\n",
      " 112176/500000: episode: 431, duration: 2.550s, episode steps: 500, steps per second: 196, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.424671, mae: 41.184610, mean_q: 82.274179, mean_eps: 0.100000\n",
      " 112676/500000: episode: 432, duration: 2.490s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.939904, mae: 41.590390, mean_q: 83.131321, mean_eps: 0.100000\n",
      " 113176/500000: episode: 433, duration: 2.372s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.426387, mae: 42.598630, mean_q: 85.092268, mean_eps: 0.100000\n",
      " 113676/500000: episode: 434, duration: 2.452s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.145739, mae: 41.684189, mean_q: 83.272856, mean_eps: 0.100000\n",
      " 114176/500000: episode: 435, duration: 2.413s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.439713, mae: 39.039073, mean_q: 78.067531, mean_eps: 0.100000\n",
      " 114676/500000: episode: 436, duration: 2.412s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.594271, mae: 39.940631, mean_q: 79.800040, mean_eps: 0.100000\n",
      " 115176/500000: episode: 437, duration: 2.389s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.768200, mae: 44.154013, mean_q: 88.215854, mean_eps: 0.100000\n",
      " 115676/500000: episode: 438, duration: 2.393s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.692493, mae: 46.790192, mean_q: 93.487590, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 115983/500000: episode: 439, duration: 1.393s, episode steps: 307, steps per second: 220, episode reward: 307.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4.239835, mae: 42.439800, mean_q: 84.874608, mean_eps: 0.100000\n",
      " 116471/500000: episode: 440, duration: 2.388s, episode steps: 488, steps per second: 204, episode reward: 488.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.116216, mae: 36.876160, mean_q: 73.982522, mean_eps: 0.100000\n",
      " 116971/500000: episode: 441, duration: 2.372s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.397423, mae: 35.619331, mean_q: 71.474394, mean_eps: 0.100000\n",
      " 117471/500000: episode: 442, duration: 2.284s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.895264, mae: 39.653960, mean_q: 79.360053, mean_eps: 0.100000\n",
      " 117971/500000: episode: 443, duration: 2.425s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.280856, mae: 45.797572, mean_q: 91.475155, mean_eps: 0.100000\n",
      " 118471/500000: episode: 444, duration: 2.400s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.414931, mae: 47.378338, mean_q: 94.721669, mean_eps: 0.100000\n",
      " 118971/500000: episode: 445, duration: 2.360s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.684039, mae: 45.981752, mean_q: 91.924792, mean_eps: 0.100000\n",
      " 119471/500000: episode: 446, duration: 2.302s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.796954, mae: 41.187411, mean_q: 82.272775, mean_eps: 0.100000\n",
      " 119971/500000: episode: 447, duration: 2.299s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.819396, mae: 37.894245, mean_q: 75.610148, mean_eps: 0.100000\n",
      " 120471/500000: episode: 448, duration: 2.311s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.499598, mae: 39.439296, mean_q: 78.736176, mean_eps: 0.100000\n",
      " 120971/500000: episode: 449, duration: 2.324s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.382730, mae: 41.593255, mean_q: 83.050058, mean_eps: 0.100000\n",
      " 121471/500000: episode: 450, duration: 2.417s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.724001, mae: 42.062243, mean_q: 84.047734, mean_eps: 0.100000\n",
      " 121971/500000: episode: 451, duration: 2.387s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.372533, mae: 45.007250, mean_q: 89.892107, mean_eps: 0.100000\n",
      " 122471/500000: episode: 452, duration: 2.360s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.634151, mae: 48.160998, mean_q: 96.119758, mean_eps: 0.100000\n",
      " 122971/500000: episode: 453, duration: 2.393s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.194607, mae: 44.959463, mean_q: 89.783227, mean_eps: 0.100000\n",
      " 123471/500000: episode: 454, duration: 2.389s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.803969, mae: 41.401464, mean_q: 82.799601, mean_eps: 0.100000\n",
      " 123971/500000: episode: 455, duration: 2.447s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.047099, mae: 43.461940, mean_q: 86.739895, mean_eps: 0.100000\n",
      " 124471/500000: episode: 456, duration: 2.339s, episode steps: 500, steps per second: 214, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.869074, mae: 45.262608, mean_q: 90.426997, mean_eps: 0.100000\n",
      " 124971/500000: episode: 457, duration: 2.280s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.250270, mae: 46.129900, mean_q: 92.171469, mean_eps: 0.100000\n",
      " 125471/500000: episode: 458, duration: 2.415s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.852722, mae: 45.662750, mean_q: 91.222430, mean_eps: 0.100000\n",
      " 125971/500000: episode: 459, duration: 2.468s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.235421, mae: 43.733207, mean_q: 87.506623, mean_eps: 0.100000\n",
      " 126471/500000: episode: 460, duration: 2.366s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 3.764935, mae: 41.292199, mean_q: 82.758755, mean_eps: 0.100000\n",
      " 126828/500000: episode: 461, duration: 1.723s, episode steps: 357, steps per second: 207, episode reward: 357.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.499 [0.000, 1.000],  loss: 4.708912, mae: 37.410401, mean_q: 75.319692, mean_eps: 0.100000\n",
      " 126855/500000: episode: 462, duration: 0.139s, episode steps:  27, steps per second: 194, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.271050, mae: 34.255752, mean_q: 68.861642, mean_eps: 0.100000\n",
      " 127061/500000: episode: 463, duration: 1.046s, episode steps: 206, steps per second: 197, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.726398, mae: 35.465366, mean_q: 71.335155, mean_eps: 0.100000\n",
      " 127079/500000: episode: 464, duration: 0.100s, episode steps:  18, steps per second: 180, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.117153, mae: 33.299956, mean_q: 67.033528, mean_eps: 0.100000\n",
      " 127098/500000: episode: 465, duration: 0.098s, episode steps:  19, steps per second: 194, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 11.259902, mae: 34.740349, mean_q: 69.728659, mean_eps: 0.100000\n",
      " 127112/500000: episode: 466, duration: 0.066s, episode steps:  14, steps per second: 211, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.197911, mae: 34.159592, mean_q: 68.465116, mean_eps: 0.100000\n",
      " 127328/500000: episode: 467, duration: 1.061s, episode steps: 216, steps per second: 204, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 18.440895, mae: 35.747832, mean_q: 71.065583, mean_eps: 0.100000\n",
      " 127816/500000: episode: 468, duration: 2.362s, episode steps: 488, steps per second: 207, episode reward: 488.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.576988, mae: 37.337672, mean_q: 74.437010, mean_eps: 0.100000\n",
      " 128061/500000: episode: 469, duration: 1.208s, episode steps: 245, steps per second: 203, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.662786, mae: 36.590460, mean_q: 73.244405, mean_eps: 0.100000\n",
      " 128321/500000: episode: 470, duration: 1.259s, episode steps: 260, steps per second: 207, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 1.377973, mae: 33.552775, mean_q: 67.559340, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 128645/500000: episode: 471, duration: 1.477s, episode steps: 324, steps per second: 219, episode reward: 324.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.461827, mae: 29.876611, mean_q: 60.094851, mean_eps: 0.100000\n",
      " 128989/500000: episode: 472, duration: 1.571s, episode steps: 344, steps per second: 219, episode reward: 344.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.285504, mae: 29.901567, mean_q: 60.180267, mean_eps: 0.100000\n",
      " 129366/500000: episode: 473, duration: 1.799s, episode steps: 377, steps per second: 210, episode reward: 377.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.050552, mae: 33.754607, mean_q: 67.856554, mean_eps: 0.100000\n",
      " 129734/500000: episode: 474, duration: 1.665s, episode steps: 368, steps per second: 221, episode reward: 368.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.028319, mae: 34.749935, mean_q: 69.796861, mean_eps: 0.100000\n",
      " 130113/500000: episode: 475, duration: 1.700s, episode steps: 379, steps per second: 223, episode reward: 379.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.018932, mae: 34.428512, mean_q: 69.140345, mean_eps: 0.100000\n",
      " 130544/500000: episode: 476, duration: 1.972s, episode steps: 431, steps per second: 219, episode reward: 431.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.016933, mae: 34.906810, mean_q: 70.085676, mean_eps: 0.100000\n",
      " 131030/500000: episode: 477, duration: 2.249s, episode steps: 486, steps per second: 216, episode reward: 486.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.050136, mae: 36.465243, mean_q: 73.188605, mean_eps: 0.100000\n",
      " 131530/500000: episode: 478, duration: 2.375s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.132042, mae: 38.818550, mean_q: 77.865727, mean_eps: 0.100000\n",
      " 132030/500000: episode: 479, duration: 2.464s, episode steps: 500, steps per second: 203, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.482349, mae: 44.033119, mean_q: 88.043441, mean_eps: 0.100000\n",
      " 132351/500000: episode: 480, duration: 1.562s, episode steps: 321, steps per second: 205, episode reward: 321.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 4.286956, mae: 47.869252, mean_q: 95.914715, mean_eps: 0.100000\n",
      " 132706/500000: episode: 481, duration: 1.747s, episode steps: 355, steps per second: 203, episode reward: 355.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 3.282151, mae: 46.562799, mean_q: 93.461497, mean_eps: 0.100000\n",
      " 133042/500000: episode: 482, duration: 1.601s, episode steps: 336, steps per second: 210, episode reward: 336.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.241778, mae: 43.108837, mean_q: 86.629054, mean_eps: 0.100000\n",
      " 133542/500000: episode: 483, duration: 2.395s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.094098, mae: 42.372170, mean_q: 85.327985, mean_eps: 0.100000\n",
      " 134042/500000: episode: 484, duration: 2.366s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.227645, mae: 43.550625, mean_q: 87.665912, mean_eps: 0.100000\n",
      " 134542/500000: episode: 485, duration: 2.355s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.372815, mae: 43.800278, mean_q: 87.995600, mean_eps: 0.100000\n",
      " 135042/500000: episode: 486, duration: 2.340s, episode steps: 500, steps per second: 214, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.936198, mae: 43.825804, mean_q: 87.820677, mean_eps: 0.100000\n",
      " 135542/500000: episode: 487, duration: 2.675s, episode steps: 500, steps per second: 187, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.063264, mae: 45.090771, mean_q: 90.254722, mean_eps: 0.100000\n",
      " 136042/500000: episode: 488, duration: 2.524s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.485422, mae: 45.661953, mean_q: 91.283617, mean_eps: 0.100000\n",
      " 136542/500000: episode: 489, duration: 2.379s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.351489, mae: 45.967736, mean_q: 91.912762, mean_eps: 0.100000\n",
      " 137042/500000: episode: 490, duration: 2.400s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.921805, mae: 45.734792, mean_q: 91.519875, mean_eps: 0.100000\n",
      " 137542/500000: episode: 491, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.647183, mae: 45.135515, mean_q: 90.222648, mean_eps: 0.100000\n",
      " 138042/500000: episode: 492, duration: 2.322s, episode steps: 500, steps per second: 215, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.061371, mae: 45.979748, mean_q: 91.897443, mean_eps: 0.100000\n",
      " 138542/500000: episode: 493, duration: 2.311s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.530878, mae: 46.556294, mean_q: 92.960898, mean_eps: 0.100000\n",
      " 139042/500000: episode: 494, duration: 2.450s, episode steps: 500, steps per second: 204, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.374289, mae: 46.672616, mean_q: 93.428884, mean_eps: 0.100000\n",
      " 139542/500000: episode: 495, duration: 2.490s, episode steps: 500, steps per second: 201, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.896026, mae: 48.340602, mean_q: 96.760405, mean_eps: 0.100000\n",
      " 140042/500000: episode: 496, duration: 2.390s, episode steps: 500, steps per second: 209, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.070612, mae: 50.606162, mean_q: 101.176594, mean_eps: 0.100000\n",
      " 140542/500000: episode: 497, duration: 2.413s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.549703, mae: 51.674425, mean_q: 103.311666, mean_eps: 0.100000\n",
      " 141042/500000: episode: 498, duration: 2.362s, episode steps: 500, steps per second: 212, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.558711, mae: 52.614329, mean_q: 105.081204, mean_eps: 0.100000\n",
      " 141542/500000: episode: 499, duration: 2.424s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.443379, mae: 50.788938, mean_q: 101.514207, mean_eps: 0.100000\n",
      " 142042/500000: episode: 500, duration: 2.423s, episode steps: 500, steps per second: 206, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.633998, mae: 43.537307, mean_q: 86.934753, mean_eps: 0.100000\n",
      " 142542/500000: episode: 501, duration: 2.399s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.670611, mae: 39.349806, mean_q: 78.572227, mean_eps: 0.100000\n",
      " 143042/500000: episode: 502, duration: 2.285s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.292547, mae: 39.379652, mean_q: 78.642840, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143542/500000: episode: 503, duration: 2.408s, episode steps: 500, steps per second: 208, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.335771, mae: 38.846480, mean_q: 77.578196, mean_eps: 0.100000\n",
      " 144042/500000: episode: 504, duration: 2.382s, episode steps: 500, steps per second: 210, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 1.165569, mae: 39.767248, mean_q: 79.483754, mean_eps: 0.100000\n",
      " 144542/500000: episode: 505, duration: 2.364s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.205193, mae: 43.141159, mean_q: 86.222088, mean_eps: 0.100000\n",
      " 145042/500000: episode: 506, duration: 2.521s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.340801, mae: 45.398265, mean_q: 90.685480, mean_eps: 0.100000\n",
      " 145542/500000: episode: 507, duration: 2.548s, episode steps: 500, steps per second: 196, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.878584, mae: 46.156579, mean_q: 92.298004, mean_eps: 0.100000\n",
      " 146042/500000: episode: 508, duration: 2.640s, episode steps: 500, steps per second: 189, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.815019, mae: 49.124651, mean_q: 98.201958, mean_eps: 0.100000\n",
      " 146542/500000: episode: 509, duration: 2.511s, episode steps: 500, steps per second: 199, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.878665, mae: 50.894068, mean_q: 101.544551, mean_eps: 0.100000\n",
      " 147042/500000: episode: 510, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.642671, mae: 50.675570, mean_q: 101.010799, mean_eps: 0.100000\n",
      " 147542/500000: episode: 511, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.712485, mae: 50.860463, mean_q: 101.408300, mean_eps: 0.100000\n",
      " 148042/500000: episode: 512, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.782639, mae: 52.109220, mean_q: 103.951303, mean_eps: 0.100000\n",
      " 148542/500000: episode: 513, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.134426, mae: 52.313098, mean_q: 104.464980, mean_eps: 0.100000\n",
      " 149042/500000: episode: 514, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.960114, mae: 51.048390, mean_q: 101.948279, mean_eps: 0.100000\n",
      " 149542/500000: episode: 515, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.584899, mae: 47.842256, mean_q: 95.610794, mean_eps: 0.100000\n",
      " 150042/500000: episode: 516, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.143670, mae: 44.207199, mean_q: 88.344452, mean_eps: 0.100000\n",
      " 150542/500000: episode: 517, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.468500, mae: 43.024700, mean_q: 85.986247, mean_eps: 0.100000\n",
      " 151042/500000: episode: 518, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.609993, mae: 43.972499, mean_q: 87.946000, mean_eps: 0.100000\n",
      " 151542/500000: episode: 519, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.064619, mae: 45.913228, mean_q: 91.751273, mean_eps: 0.100000\n",
      " 152042/500000: episode: 520, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.176205, mae: 48.054887, mean_q: 95.901495, mean_eps: 0.100000\n",
      " 152542/500000: episode: 521, duration: 2.247s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.891391, mae: 48.715292, mean_q: 97.270796, mean_eps: 0.100000\n",
      " 153042/500000: episode: 522, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.954665, mae: 46.438974, mean_q: 92.830475, mean_eps: 0.100000\n",
      " 153542/500000: episode: 523, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.539736, mae: 41.974951, mean_q: 83.822403, mean_eps: 0.100000\n",
      " 154042/500000: episode: 524, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.166208, mae: 40.035084, mean_q: 79.976473, mean_eps: 0.100000\n",
      " 154542/500000: episode: 525, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.326423, mae: 42.088584, mean_q: 84.149443, mean_eps: 0.100000\n",
      " 155042/500000: episode: 526, duration: 2.217s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.987613, mae: 43.936993, mean_q: 87.790544, mean_eps: 0.100000\n",
      " 155542/500000: episode: 527, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.212290, mae: 45.391776, mean_q: 90.906288, mean_eps: 0.100000\n",
      " 156042/500000: episode: 528, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.065023, mae: 45.308527, mean_q: 90.803787, mean_eps: 0.100000\n",
      " 156542/500000: episode: 529, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.129099, mae: 41.653466, mean_q: 83.483589, mean_eps: 0.100000\n",
      " 157042/500000: episode: 530, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.222114, mae: 37.501952, mean_q: 75.213847, mean_eps: 0.100000\n",
      " 157542/500000: episode: 531, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.350043, mae: 31.930299, mean_q: 64.343472, mean_eps: 0.100000\n",
      " 158042/500000: episode: 532, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.756933, mae: 25.411147, mean_q: 51.102385, mean_eps: 0.100000\n",
      " 158542/500000: episode: 533, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.317009, mae: 21.733454, mean_q: 43.753219, mean_eps: 0.100000\n",
      " 159042/500000: episode: 534, duration: 2.265s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.161701, mae: 19.898316, mean_q: 40.046184, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 159542/500000: episode: 535, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.117352, mae: 19.562317, mean_q: 39.314885, mean_eps: 0.100000\n",
      " 159933/500000: episode: 536, duration: 1.736s, episode steps: 391, steps per second: 225, episode reward: 391.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.074067, mae: 19.367812, mean_q: 38.934537, mean_eps: 0.100000\n",
      " 160261/500000: episode: 537, duration: 1.464s, episode steps: 328, steps per second: 224, episode reward: 328.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.176183, mae: 21.922183, mean_q: 43.972728, mean_eps: 0.100000\n",
      " 160428/500000: episode: 538, duration: 0.749s, episode steps: 167, steps per second: 223, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.230824, mae: 22.900833, mean_q: 45.926929, mean_eps: 0.100000\n",
      " 160928/500000: episode: 539, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.460156, mae: 27.812295, mean_q: 55.938711, mean_eps: 0.100000\n",
      " 161428/500000: episode: 540, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.249997, mae: 29.527231, mean_q: 59.485667, mean_eps: 0.100000\n",
      " 161928/500000: episode: 541, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.757082, mae: 32.640356, mean_q: 65.543390, mean_eps: 0.100000\n",
      " 162428/500000: episode: 542, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.638738, mae: 39.652331, mean_q: 79.329355, mean_eps: 0.100000\n",
      " 162928/500000: episode: 543, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.475592, mae: 43.884550, mean_q: 87.748377, mean_eps: 0.100000\n",
      " 163100/500000: episode: 544, duration: 0.773s, episode steps: 172, steps per second: 222, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.082422, mae: 46.444607, mean_q: 92.900049, mean_eps: 0.100000\n",
      " 163283/500000: episode: 545, duration: 0.817s, episode steps: 183, steps per second: 224, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 5.121775, mae: 44.363517, mean_q: 88.822852, mean_eps: 0.100000\n",
      " 163503/500000: episode: 546, duration: 0.990s, episode steps: 220, steps per second: 222, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 6.262484, mae: 43.947089, mean_q: 88.014835, mean_eps: 0.100000\n",
      " 163764/500000: episode: 547, duration: 1.166s, episode steps: 261, steps per second: 224, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.025660, mae: 40.554838, mean_q: 81.449084, mean_eps: 0.100000\n",
      " 164120/500000: episode: 548, duration: 1.593s, episode steps: 356, steps per second: 223, episode reward: 356.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.164312, mae: 36.962348, mean_q: 74.311018, mean_eps: 0.100000\n",
      " 164620/500000: episode: 549, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.258022, mae: 39.388962, mean_q: 79.088437, mean_eps: 0.100000\n",
      " 165120/500000: episode: 550, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.658486, mae: 46.873470, mean_q: 93.815098, mean_eps: 0.100000\n",
      " 165620/500000: episode: 551, duration: 2.262s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.003588, mae: 51.866303, mean_q: 103.583903, mean_eps: 0.100000\n",
      " 166120/500000: episode: 552, duration: 2.270s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.887848, mae: 51.763674, mean_q: 103.388308, mean_eps: 0.100000\n",
      " 166455/500000: episode: 553, duration: 1.498s, episode steps: 335, steps per second: 224, episode reward: 335.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.499 [0.000, 1.000],  loss: 8.757692, mae: 48.580832, mean_q: 97.099016, mean_eps: 0.100000\n",
      " 166794/500000: episode: 554, duration: 1.514s, episode steps: 339, steps per second: 224, episode reward: 339.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.811718, mae: 43.009887, mean_q: 86.168876, mean_eps: 0.100000\n",
      " 167183/500000: episode: 555, duration: 1.746s, episode steps: 389, steps per second: 223, episode reward: 389.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.627911, mae: 38.059504, mean_q: 76.280259, mean_eps: 0.100000\n",
      " 167683/500000: episode: 556, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.164579, mae: 38.406407, mean_q: 77.145751, mean_eps: 0.100000\n",
      " 168183/500000: episode: 557, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.064599, mae: 46.846267, mean_q: 93.744075, mean_eps: 0.100000\n",
      " 168683/500000: episode: 558, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.769462, mae: 52.989841, mean_q: 105.916795, mean_eps: 0.100000\n",
      " 169036/500000: episode: 559, duration: 1.575s, episode steps: 353, steps per second: 224, episode reward: 353.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 12.671112, mae: 51.311537, mean_q: 102.445124, mean_eps: 0.100000\n",
      " 169307/500000: episode: 560, duration: 1.210s, episode steps: 271, steps per second: 224, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 6.702734, mae: 44.145956, mean_q: 88.356614, mean_eps: 0.100000\n",
      " 169807/500000: episode: 561, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.817699, mae: 38.895674, mean_q: 78.033470, mean_eps: 0.100000\n",
      " 170307/500000: episode: 562, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.838824, mae: 43.866304, mean_q: 87.874735, mean_eps: 0.100000\n",
      " 170807/500000: episode: 563, duration: 2.216s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.006914, mae: 51.687496, mean_q: 103.556110, mean_eps: 0.100000\n",
      " 171307/500000: episode: 564, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.617604, mae: 52.511880, mean_q: 105.155016, mean_eps: 0.100000\n",
      " 171807/500000: episode: 565, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.765968, mae: 52.112590, mean_q: 104.563153, mean_eps: 0.100000\n",
      " 172307/500000: episode: 566, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.181154, mae: 53.131912, mean_q: 106.321769, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 172807/500000: episode: 567, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.068269, mae: 52.753228, mean_q: 105.577295, mean_eps: 0.100000\n",
      " 173307/500000: episode: 568, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.481054, mae: 51.876953, mean_q: 103.756328, mean_eps: 0.100000\n",
      " 173807/500000: episode: 569, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.398598, mae: 50.967459, mean_q: 101.966300, mean_eps: 0.100000\n",
      " 174307/500000: episode: 570, duration: 2.218s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.963231, mae: 50.036498, mean_q: 100.032493, mean_eps: 0.100000\n",
      " 174807/500000: episode: 571, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.956164, mae: 49.453241, mean_q: 98.873875, mean_eps: 0.100000\n",
      " 175307/500000: episode: 572, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.362229, mae: 50.251316, mean_q: 100.644879, mean_eps: 0.100000\n",
      " 175807/500000: episode: 573, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.404750, mae: 51.707867, mean_q: 103.391437, mean_eps: 0.100000\n",
      " 176307/500000: episode: 574, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.676146, mae: 52.015349, mean_q: 103.950100, mean_eps: 0.100000\n",
      " 176807/500000: episode: 575, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.168979, mae: 51.467837, mean_q: 102.854601, mean_eps: 0.100000\n",
      " 177307/500000: episode: 576, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.717790, mae: 50.446157, mean_q: 100.699876, mean_eps: 0.100000\n",
      " 177807/500000: episode: 577, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.320287, mae: 50.027498, mean_q: 100.012181, mean_eps: 0.100000\n",
      " 178307/500000: episode: 578, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.069756, mae: 50.807171, mean_q: 101.647027, mean_eps: 0.100000\n",
      " 178807/500000: episode: 579, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.680820, mae: 50.745242, mean_q: 101.382530, mean_eps: 0.100000\n",
      " 179123/500000: episode: 580, duration: 1.414s, episode steps: 316, steps per second: 224, episode reward: 316.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 9.665888, mae: 49.644507, mean_q: 99.244986, mean_eps: 0.100000\n",
      " 179623/500000: episode: 581, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.187650, mae: 44.779245, mean_q: 89.601886, mean_eps: 0.100000\n",
      " 180123/500000: episode: 582, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.803140, mae: 38.472142, mean_q: 77.143889, mean_eps: 0.100000\n",
      " 180623/500000: episode: 583, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.944930, mae: 39.971516, mean_q: 79.954137, mean_eps: 0.100000\n",
      " 181123/500000: episode: 584, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.297855, mae: 44.804303, mean_q: 89.519276, mean_eps: 0.100000\n",
      " 181623/500000: episode: 585, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.960078, mae: 46.663367, mean_q: 93.198915, mean_eps: 0.100000\n",
      " 182123/500000: episode: 586, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.080889, mae: 45.818635, mean_q: 91.701986, mean_eps: 0.100000\n",
      " 182623/500000: episode: 587, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.895456, mae: 46.068158, mean_q: 92.377374, mean_eps: 0.100000\n",
      " 183123/500000: episode: 588, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.896089, mae: 49.297076, mean_q: 98.758990, mean_eps: 0.100000\n",
      " 183623/500000: episode: 589, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.904540, mae: 50.456958, mean_q: 101.269957, mean_eps: 0.100000\n",
      " 184123/500000: episode: 590, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.058318, mae: 49.150635, mean_q: 98.683047, mean_eps: 0.100000\n",
      " 184623/500000: episode: 591, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.339085, mae: 46.074320, mean_q: 92.252453, mean_eps: 0.100000\n",
      " 184645/500000: episode: 592, duration: 0.104s, episode steps:  22, steps per second: 211, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 10.718270, mae: 44.439203, mean_q: 88.727101, mean_eps: 0.100000\n",
      " 185145/500000: episode: 593, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.505718, mae: 43.801262, mean_q: 87.503403, mean_eps: 0.100000\n",
      " 185645/500000: episode: 594, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.347266, mae: 42.869450, mean_q: 85.834175, mean_eps: 0.100000\n",
      " 186145/500000: episode: 595, duration: 2.263s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.579411, mae: 45.844412, mean_q: 92.013268, mean_eps: 0.100000\n",
      " 186645/500000: episode: 596, duration: 2.270s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.191879, mae: 50.071313, mean_q: 100.188890, mean_eps: 0.100000\n",
      " 187145/500000: episode: 597, duration: 2.417s, episode steps: 500, steps per second: 207, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.190772, mae: 51.243418, mean_q: 102.401816, mean_eps: 0.100000\n",
      " 187645/500000: episode: 598, duration: 2.284s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.636580, mae: 49.773213, mean_q: 99.541546, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 188145/500000: episode: 599, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.817883, mae: 45.338183, mean_q: 90.786131, mean_eps: 0.100000\n",
      " 188645/500000: episode: 600, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.928494, mae: 44.939236, mean_q: 89.982289, mean_eps: 0.100000\n",
      " 189145/500000: episode: 601, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.207838, mae: 48.558087, mean_q: 97.082038, mean_eps: 0.100000\n",
      " 189645/500000: episode: 602, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.971918, mae: 49.446796, mean_q: 98.831605, mean_eps: 0.100000\n",
      " 190145/500000: episode: 603, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.210154, mae: 49.484914, mean_q: 98.730094, mean_eps: 0.100000\n",
      " 190645/500000: episode: 604, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.123722, mae: 49.580406, mean_q: 99.015394, mean_eps: 0.100000\n",
      " 191145/500000: episode: 605, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.393282, mae: 48.886849, mean_q: 97.583399, mean_eps: 0.100000\n",
      " 191645/500000: episode: 606, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.469123, mae: 48.770021, mean_q: 97.619642, mean_eps: 0.100000\n",
      " 192145/500000: episode: 607, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.034194, mae: 49.772560, mean_q: 99.783200, mean_eps: 0.100000\n",
      " 192645/500000: episode: 608, duration: 2.271s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.303620, mae: 48.191051, mean_q: 96.357429, mean_eps: 0.100000\n",
      " 193145/500000: episode: 609, duration: 2.262s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.289153, mae: 44.189075, mean_q: 88.586461, mean_eps: 0.100000\n",
      " 193645/500000: episode: 610, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.349441, mae: 39.753482, mean_q: 79.584161, mean_eps: 0.100000\n",
      " 194145/500000: episode: 611, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.602874, mae: 36.430584, mean_q: 72.889349, mean_eps: 0.100000\n",
      " 194645/500000: episode: 612, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.664125, mae: 38.158153, mean_q: 76.358211, mean_eps: 0.100000\n",
      " 195145/500000: episode: 613, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.126722, mae: 42.158590, mean_q: 84.359210, mean_eps: 0.100000\n",
      " 195645/500000: episode: 614, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.670711, mae: 45.720093, mean_q: 91.577464, mean_eps: 0.100000\n",
      " 196145/500000: episode: 615, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.910159, mae: 49.656834, mean_q: 99.557427, mean_eps: 0.100000\n",
      " 196645/500000: episode: 616, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.574788, mae: 51.618266, mean_q: 103.371173, mean_eps: 0.100000\n",
      " 197145/500000: episode: 617, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.204278, mae: 49.693175, mean_q: 99.439681, mean_eps: 0.100000\n",
      " 197159/500000: episode: 618, duration: 0.066s, episode steps:  14, steps per second: 211, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 16.758643, mae: 47.757462, mean_q: 95.540633, mean_eps: 0.100000\n",
      " 197659/500000: episode: 619, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.300579, mae: 46.088466, mean_q: 92.099771, mean_eps: 0.100000\n",
      " 198159/500000: episode: 620, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.960672, mae: 43.736932, mean_q: 87.717538, mean_eps: 0.100000\n",
      " 198659/500000: episode: 621, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.524518, mae: 43.813172, mean_q: 87.886221, mean_eps: 0.100000\n",
      " 199159/500000: episode: 622, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.589132, mae: 43.458137, mean_q: 87.112230, mean_eps: 0.100000\n",
      " 199659/500000: episode: 623, duration: 2.251s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.064675, mae: 43.220748, mean_q: 86.644949, mean_eps: 0.100000\n",
      " 200159/500000: episode: 624, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.547529, mae: 46.170372, mean_q: 92.429864, mean_eps: 0.100000\n",
      " 200659/500000: episode: 625, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.176958, mae: 50.182352, mean_q: 100.484234, mean_eps: 0.100000\n",
      " 201159/500000: episode: 626, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.655863, mae: 52.181726, mean_q: 104.563011, mean_eps: 0.100000\n",
      " 201659/500000: episode: 627, duration: 2.270s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.282173, mae: 52.175005, mean_q: 104.365907, mean_eps: 0.100000\n",
      " 202159/500000: episode: 628, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.623238, mae: 51.239687, mean_q: 102.600169, mean_eps: 0.100000\n",
      " 202659/500000: episode: 629, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.530420, mae: 47.695613, mean_q: 95.595886, mean_eps: 0.100000\n",
      " 203159/500000: episode: 630, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.649133, mae: 41.760979, mean_q: 83.812558, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 203659/500000: episode: 631, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.996783, mae: 42.050360, mean_q: 84.479577, mean_eps: 0.100000\n",
      " 204159/500000: episode: 632, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.696278, mae: 47.917503, mean_q: 95.871565, mean_eps: 0.100000\n",
      " 204659/500000: episode: 633, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.362879, mae: 50.794831, mean_q: 101.562877, mean_eps: 0.100000\n",
      " 205159/500000: episode: 634, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.151442, mae: 50.970995, mean_q: 101.895292, mean_eps: 0.100000\n",
      " 205659/500000: episode: 635, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.371310, mae: 49.857001, mean_q: 99.561688, mean_eps: 0.100000\n",
      " 206159/500000: episode: 636, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.050995, mae: 49.345177, mean_q: 98.605670, mean_eps: 0.100000\n",
      " 206659/500000: episode: 637, duration: 2.313s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.688518, mae: 50.389223, mean_q: 100.752055, mean_eps: 0.100000\n",
      " 207159/500000: episode: 638, duration: 2.520s, episode steps: 500, steps per second: 198, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.884616, mae: 51.202304, mean_q: 102.249333, mean_eps: 0.100000\n",
      " 207659/500000: episode: 639, duration: 2.341s, episode steps: 500, steps per second: 214, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.881005, mae: 50.849764, mean_q: 101.474518, mean_eps: 0.100000\n",
      " 208159/500000: episode: 640, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.804336, mae: 49.823751, mean_q: 99.468065, mean_eps: 0.100000\n",
      " 208659/500000: episode: 641, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 12.271143, mae: 49.706056, mean_q: 99.148759, mean_eps: 0.100000\n",
      " 209159/500000: episode: 642, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.026506, mae: 49.290989, mean_q: 98.532620, mean_eps: 0.100000\n",
      " 209659/500000: episode: 643, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.547218, mae: 48.984684, mean_q: 98.119339, mean_eps: 0.100000\n",
      " 210159/500000: episode: 644, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.326153, mae: 50.533466, mean_q: 101.214768, mean_eps: 0.100000\n",
      " 210659/500000: episode: 645, duration: 2.218s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.650587, mae: 50.951095, mean_q: 102.016199, mean_eps: 0.100000\n",
      " 211159/500000: episode: 646, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.341892, mae: 48.941225, mean_q: 98.012675, mean_eps: 0.100000\n",
      " 211659/500000: episode: 647, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.515519, mae: 48.294164, mean_q: 96.663524, mean_eps: 0.100000\n",
      " 212159/500000: episode: 648, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.551532, mae: 50.328132, mean_q: 100.628689, mean_eps: 0.100000\n",
      " 212659/500000: episode: 649, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.154092, mae: 51.264587, mean_q: 102.451048, mean_eps: 0.100000\n",
      " 213159/500000: episode: 650, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.304632, mae: 50.983440, mean_q: 101.856413, mean_eps: 0.100000\n",
      " 213659/500000: episode: 651, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.433817, mae: 50.588495, mean_q: 101.025733, mean_eps: 0.100000\n",
      " 214159/500000: episode: 652, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.881578, mae: 50.186611, mean_q: 100.422002, mean_eps: 0.100000\n",
      " 214659/500000: episode: 653, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.536875, mae: 49.839786, mean_q: 99.699119, mean_eps: 0.100000\n",
      " 215159/500000: episode: 654, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.767183, mae: 49.572919, mean_q: 99.106861, mean_eps: 0.100000\n",
      " 215659/500000: episode: 655, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.339407, mae: 49.746657, mean_q: 99.277598, mean_eps: 0.100000\n",
      " 216159/500000: episode: 656, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.745672, mae: 49.674154, mean_q: 99.262154, mean_eps: 0.100000\n",
      " 216659/500000: episode: 657, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.671854, mae: 49.576336, mean_q: 99.075651, mean_eps: 0.100000\n",
      " 217159/500000: episode: 658, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.538414, mae: 49.443125, mean_q: 98.608254, mean_eps: 0.100000\n",
      " 217659/500000: episode: 659, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.509759, mae: 48.657823, mean_q: 97.115597, mean_eps: 0.100000\n",
      " 218159/500000: episode: 660, duration: 2.257s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.244112, mae: 48.635174, mean_q: 97.203549, mean_eps: 0.100000\n",
      " 218659/500000: episode: 661, duration: 2.253s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.061553, mae: 49.510203, mean_q: 98.972435, mean_eps: 0.100000\n",
      " 219159/500000: episode: 662, duration: 2.284s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.267211, mae: 49.674801, mean_q: 99.112068, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 219659/500000: episode: 663, duration: 2.269s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.855110, mae: 49.444239, mean_q: 98.717996, mean_eps: 0.100000\n",
      " 220159/500000: episode: 664, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.380193, mae: 49.263625, mean_q: 98.318904, mean_eps: 0.100000\n",
      " 220659/500000: episode: 665, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.638563, mae: 48.894172, mean_q: 97.900781, mean_eps: 0.100000\n",
      " 221159/500000: episode: 666, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.243375, mae: 46.499532, mean_q: 93.077034, mean_eps: 0.100000\n",
      " 221659/500000: episode: 667, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.221837, mae: 42.179175, mean_q: 84.288277, mean_eps: 0.100000\n",
      " 222159/500000: episode: 668, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.124936, mae: 39.797505, mean_q: 79.422965, mean_eps: 0.100000\n",
      " 222659/500000: episode: 669, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.873955, mae: 41.566427, mean_q: 83.223605, mean_eps: 0.100000\n",
      " 223159/500000: episode: 670, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.308430, mae: 46.320375, mean_q: 92.820484, mean_eps: 0.100000\n",
      " 223659/500000: episode: 671, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.018997, mae: 50.147809, mean_q: 100.183719, mean_eps: 0.100000\n",
      " 224159/500000: episode: 672, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.283671, mae: 50.316167, mean_q: 100.722803, mean_eps: 0.100000\n",
      " 224659/500000: episode: 673, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.918461, mae: 47.951869, mean_q: 95.837542, mean_eps: 0.100000\n",
      " 225159/500000: episode: 674, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.898919, mae: 46.356381, mean_q: 92.530257, mean_eps: 0.100000\n",
      " 225659/500000: episode: 675, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.035664, mae: 46.389760, mean_q: 92.697492, mean_eps: 0.100000\n",
      " 226159/500000: episode: 676, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.382666, mae: 47.979643, mean_q: 96.053126, mean_eps: 0.100000\n",
      " 226659/500000: episode: 677, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.513398, mae: 50.893510, mean_q: 101.709703, mean_eps: 0.100000\n",
      " 227159/500000: episode: 678, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.952206, mae: 51.816109, mean_q: 103.418688, mean_eps: 0.100000\n",
      " 227659/500000: episode: 679, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.103379, mae: 51.469167, mean_q: 102.923339, mean_eps: 0.100000\n",
      " 228159/500000: episode: 680, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.710350, mae: 50.305240, mean_q: 100.824130, mean_eps: 0.100000\n",
      " 228659/500000: episode: 681, duration: 2.266s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.758561, mae: 47.800544, mean_q: 95.705736, mean_eps: 0.100000\n",
      " 229159/500000: episode: 682, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.109485, mae: 45.064421, mean_q: 90.153666, mean_eps: 0.100000\n",
      " 229659/500000: episode: 683, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.213751, mae: 42.652548, mean_q: 85.265456, mean_eps: 0.100000\n",
      " 230159/500000: episode: 684, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.315346, mae: 41.887628, mean_q: 83.674396, mean_eps: 0.100000\n",
      " 230659/500000: episode: 685, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.046073, mae: 43.555440, mean_q: 86.996393, mean_eps: 0.100000\n",
      " 231159/500000: episode: 686, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.428165, mae: 44.187657, mean_q: 88.294064, mean_eps: 0.100000\n",
      " 231659/500000: episode: 687, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.608717, mae: 43.555360, mean_q: 87.028840, mean_eps: 0.100000\n",
      " 232159/500000: episode: 688, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.429199, mae: 43.766655, mean_q: 87.419737, mean_eps: 0.100000\n",
      " 232659/500000: episode: 689, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.119383, mae: 43.854584, mean_q: 87.712529, mean_eps: 0.100000\n",
      " 233159/500000: episode: 690, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.859416, mae: 46.038275, mean_q: 92.366879, mean_eps: 0.100000\n",
      " 233659/500000: episode: 691, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.468540, mae: 50.363953, mean_q: 100.927608, mean_eps: 0.100000\n",
      " 234159/500000: episode: 692, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.394175, mae: 51.696669, mean_q: 103.429078, mean_eps: 0.100000\n",
      " 234171/500000: episode: 693, duration: 0.058s, episode steps:  12, steps per second: 206, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 27.811987, mae: 51.467658, mean_q: 102.566115, mean_eps: 0.100000\n",
      " 234671/500000: episode: 694, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.667735, mae: 50.768753, mean_q: 101.276961, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 235171/500000: episode: 695, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.829032, mae: 50.649532, mean_q: 101.159700, mean_eps: 0.100000\n",
      " 235671/500000: episode: 696, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.224789, mae: 51.659830, mean_q: 103.070361, mean_eps: 0.100000\n",
      " 236171/500000: episode: 697, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.266616, mae: 51.448517, mean_q: 102.817041, mean_eps: 0.100000\n",
      " 236671/500000: episode: 698, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.733341, mae: 51.395109, mean_q: 102.723595, mean_eps: 0.100000\n",
      " 237171/500000: episode: 699, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.750633, mae: 50.995247, mean_q: 102.048335, mean_eps: 0.100000\n",
      " 237671/500000: episode: 700, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.532260, mae: 49.144868, mean_q: 98.204604, mean_eps: 0.100000\n",
      " 238171/500000: episode: 701, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.817293, mae: 48.175494, mean_q: 96.159231, mean_eps: 0.100000\n",
      " 238671/500000: episode: 702, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.426154, mae: 48.116635, mean_q: 96.224649, mean_eps: 0.100000\n",
      " 238731/500000: episode: 703, duration: 0.278s, episode steps:  60, steps per second: 216, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 7.777275, mae: 47.278455, mean_q: 94.705728, mean_eps: 0.100000\n",
      " 239231/500000: episode: 704, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.211210, mae: 45.120561, mean_q: 90.204421, mean_eps: 0.100000\n",
      " 239731/500000: episode: 705, duration: 2.252s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.232054, mae: 40.001539, mean_q: 80.149484, mean_eps: 0.100000\n",
      " 240231/500000: episode: 706, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.055759, mae: 36.267005, mean_q: 72.792449, mean_eps: 0.100000\n",
      " 240731/500000: episode: 707, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.035547, mae: 36.525814, mean_q: 73.205532, mean_eps: 0.100000\n",
      " 241231/500000: episode: 708, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.152628, mae: 40.102521, mean_q: 80.148302, mean_eps: 0.100000\n",
      " 241731/500000: episode: 709, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.679449, mae: 44.215282, mean_q: 88.446885, mean_eps: 0.100000\n",
      " 242231/500000: episode: 710, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.769426, mae: 47.868251, mean_q: 95.792656, mean_eps: 0.100000\n",
      " 242731/500000: episode: 711, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.579818, mae: 50.803095, mean_q: 101.501771, mean_eps: 0.100000\n",
      " 243231/500000: episode: 712, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.663783, mae: 51.607229, mean_q: 103.054196, mean_eps: 0.100000\n",
      " 243731/500000: episode: 713, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.449691, mae: 51.420715, mean_q: 102.727461, mean_eps: 0.100000\n",
      " 244231/500000: episode: 714, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.001294, mae: 50.543215, mean_q: 101.085428, mean_eps: 0.100000\n",
      " 244731/500000: episode: 715, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.992542, mae: 48.293345, mean_q: 96.485650, mean_eps: 0.100000\n",
      " 245231/500000: episode: 716, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.955185, mae: 46.945942, mean_q: 93.936649, mean_eps: 0.100000\n",
      " 245731/500000: episode: 717, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.321824, mae: 48.658327, mean_q: 97.948328, mean_eps: 0.100000\n",
      " 246231/500000: episode: 718, duration: 2.270s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.648796, mae: 46.526959, mean_q: 93.522283, mean_eps: 0.100000\n",
      " 246731/500000: episode: 719, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.873433, mae: 40.440517, mean_q: 80.878132, mean_eps: 0.100000\n",
      " 247231/500000: episode: 720, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.765661, mae: 39.262044, mean_q: 78.393099, mean_eps: 0.100000\n",
      " 247731/500000: episode: 721, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.256631, mae: 41.131151, mean_q: 82.064767, mean_eps: 0.100000\n",
      " 248231/500000: episode: 722, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.640912, mae: 44.269064, mean_q: 88.263883, mean_eps: 0.100000\n",
      " 248731/500000: episode: 723, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.112750, mae: 46.686545, mean_q: 93.112436, mean_eps: 0.100000\n",
      " 249231/500000: episode: 724, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.034255, mae: 46.907023, mean_q: 93.626212, mean_eps: 0.100000\n",
      " 249731/500000: episode: 725, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.155502, mae: 46.111338, mean_q: 92.042263, mean_eps: 0.100000\n",
      " 250231/500000: episode: 726, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.105508, mae: 44.585957, mean_q: 89.018107, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 250731/500000: episode: 727, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.507854, mae: 43.599164, mean_q: 87.170171, mean_eps: 0.100000\n",
      " 251231/500000: episode: 728, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.620464, mae: 47.892787, mean_q: 96.096200, mean_eps: 0.100000\n",
      " 251731/500000: episode: 729, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.025456, mae: 51.211432, mean_q: 102.551775, mean_eps: 0.100000\n",
      " 252231/500000: episode: 730, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.741118, mae: 52.176511, mean_q: 104.382097, mean_eps: 0.100000\n",
      " 252731/500000: episode: 731, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.258868, mae: 52.869987, mean_q: 105.606736, mean_eps: 0.100000\n",
      " 253231/500000: episode: 732, duration: 2.256s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.847205, mae: 52.235953, mean_q: 104.234569, mean_eps: 0.100000\n",
      " 253731/500000: episode: 733, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.612536, mae: 51.724617, mean_q: 103.176364, mean_eps: 0.100000\n",
      " 254231/500000: episode: 734, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.746647, mae: 49.923174, mean_q: 99.506639, mean_eps: 0.100000\n",
      " 254731/500000: episode: 735, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.753045, mae: 48.193439, mean_q: 96.302132, mean_eps: 0.100000\n",
      " 255231/500000: episode: 736, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.974181, mae: 47.799920, mean_q: 95.506203, mean_eps: 0.100000\n",
      " 255731/500000: episode: 737, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.846853, mae: 48.034595, mean_q: 95.919007, mean_eps: 0.100000\n",
      " 256231/500000: episode: 738, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.280338, mae: 49.638955, mean_q: 99.191353, mean_eps: 0.100000\n",
      " 256731/500000: episode: 739, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.033502, mae: 50.532336, mean_q: 100.860778, mean_eps: 0.100000\n",
      " 257231/500000: episode: 740, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.575777, mae: 49.774062, mean_q: 99.586613, mean_eps: 0.100000\n",
      " 257483/500000: episode: 741, duration: 1.130s, episode steps: 252, steps per second: 223, episode reward: 252.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 10.901235, mae: 48.181662, mean_q: 96.191717, mean_eps: 0.100000\n",
      " 257983/500000: episode: 742, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.870868, mae: 45.944776, mean_q: 91.940024, mean_eps: 0.100000\n",
      " 257997/500000: episode: 743, duration: 0.067s, episode steps:  14, steps per second: 208, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.265635, mae: 42.954310, mean_q: 86.410392, mean_eps: 0.100000\n",
      " 258497/500000: episode: 744, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.908532, mae: 39.657574, mean_q: 79.187277, mean_eps: 0.100000\n",
      " 258997/500000: episode: 745, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.530693, mae: 32.670265, mean_q: 65.354625, mean_eps: 0.100000\n",
      " 259497/500000: episode: 746, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.228574, mae: 35.025528, mean_q: 70.023651, mean_eps: 0.100000\n",
      " 259997/500000: episode: 747, duration: 2.256s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.513198, mae: 40.830963, mean_q: 81.561543, mean_eps: 0.100000\n",
      " 260497/500000: episode: 748, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.790863, mae: 44.444771, mean_q: 88.801668, mean_eps: 0.100000\n",
      " 260997/500000: episode: 749, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.660235, mae: 47.130952, mean_q: 94.112034, mean_eps: 0.100000\n",
      " 261497/500000: episode: 750, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.383368, mae: 48.853773, mean_q: 97.506903, mean_eps: 0.100000\n",
      " 261997/500000: episode: 751, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.495979, mae: 47.174178, mean_q: 94.266445, mean_eps: 0.100000\n",
      " 262497/500000: episode: 752, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 4.580182, mae: 42.663723, mean_q: 85.227307, mean_eps: 0.100000\n",
      " 262997/500000: episode: 753, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.511413, mae: 38.599543, mean_q: 77.250532, mean_eps: 0.100000\n",
      " 263335/500000: episode: 754, duration: 1.522s, episode steps: 338, steps per second: 222, episode reward: 338.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 1.847387, mae: 35.743288, mean_q: 71.711087, mean_eps: 0.100000\n",
      " 263752/500000: episode: 755, duration: 1.866s, episode steps: 417, steps per second: 223, episode reward: 417.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 2.097824, mae: 34.075038, mean_q: 68.414643, mean_eps: 0.100000\n",
      " 264232/500000: episode: 756, duration: 2.142s, episode steps: 480, steps per second: 224, episode reward: 480.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.348991, mae: 32.635794, mean_q: 65.568317, mean_eps: 0.100000\n",
      " 264732/500000: episode: 757, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 0.677392, mae: 34.788140, mean_q: 69.788172, mean_eps: 0.100000\n",
      " 265232/500000: episode: 758, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.614393, mae: 41.369618, mean_q: 82.892614, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 265732/500000: episode: 759, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.749319, mae: 49.167233, mean_q: 98.517053, mean_eps: 0.100000\n",
      " 266232/500000: episode: 760, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.124222, mae: 52.458696, mean_q: 104.748318, mean_eps: 0.100000\n",
      " 266732/500000: episode: 761, duration: 2.256s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.813203, mae: 51.985294, mean_q: 103.899256, mean_eps: 0.100000\n",
      " 267232/500000: episode: 762, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.636675, mae: 51.489984, mean_q: 102.812878, mean_eps: 0.100000\n",
      " 267732/500000: episode: 763, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.054265, mae: 50.547282, mean_q: 100.973066, mean_eps: 0.100000\n",
      " 268232/500000: episode: 764, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.803321, mae: 50.679498, mean_q: 101.431379, mean_eps: 0.100000\n",
      " 268732/500000: episode: 765, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.509263, mae: 51.405453, mean_q: 102.781090, mean_eps: 0.100000\n",
      " 269232/500000: episode: 766, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.810534, mae: 51.195880, mean_q: 102.387581, mean_eps: 0.100000\n",
      " 269732/500000: episode: 767, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.895585, mae: 50.794595, mean_q: 101.414884, mean_eps: 0.100000\n",
      " 270232/500000: episode: 768, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.828734, mae: 50.309249, mean_q: 100.564120, mean_eps: 0.100000\n",
      " 270732/500000: episode: 769, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.325979, mae: 49.014545, mean_q: 98.041117, mean_eps: 0.100000\n",
      " 271232/500000: episode: 770, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.432953, mae: 48.354109, mean_q: 96.764723, mean_eps: 0.100000\n",
      " 271732/500000: episode: 771, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.603034, mae: 50.109858, mean_q: 100.321484, mean_eps: 0.100000\n",
      " 272232/500000: episode: 772, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.255144, mae: 51.042518, mean_q: 102.113045, mean_eps: 0.100000\n",
      " 272732/500000: episode: 773, duration: 2.263s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.308556, mae: 50.826963, mean_q: 101.768876, mean_eps: 0.100000\n",
      " 273232/500000: episode: 774, duration: 2.286s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.879780, mae: 50.181959, mean_q: 100.365660, mean_eps: 0.100000\n",
      " 273732/500000: episode: 775, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.254191, mae: 49.642422, mean_q: 99.233073, mean_eps: 0.100000\n",
      " 273949/500000: episode: 776, duration: 0.975s, episode steps: 217, steps per second: 223, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 14.430959, mae: 49.006968, mean_q: 97.930043, mean_eps: 0.100000\n",
      " 274072/500000: episode: 777, duration: 0.554s, episode steps: 123, steps per second: 222, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 17.301998, mae: 48.443266, mean_q: 96.866131, mean_eps: 0.100000\n",
      " 274145/500000: episode: 778, duration: 0.334s, episode steps:  73, steps per second: 218, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 16.202532, mae: 48.012578, mean_q: 95.907675, mean_eps: 0.100000\n",
      " 274645/500000: episode: 779, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.514106, mae: 46.511378, mean_q: 92.842440, mean_eps: 0.100000\n",
      " 275145/500000: episode: 780, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.937559, mae: 45.197322, mean_q: 90.274642, mean_eps: 0.100000\n",
      " 275645/500000: episode: 781, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.212604, mae: 47.318942, mean_q: 94.908371, mean_eps: 0.100000\n",
      " 276145/500000: episode: 782, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.469855, mae: 49.098672, mean_q: 98.381225, mean_eps: 0.100000\n",
      " 276645/500000: episode: 783, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.949844, mae: 48.874930, mean_q: 97.936374, mean_eps: 0.100000\n",
      " 277145/500000: episode: 784, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.748780, mae: 47.837825, mean_q: 95.630685, mean_eps: 0.100000\n",
      " 277645/500000: episode: 785, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.486387, mae: 47.043678, mean_q: 94.186152, mean_eps: 0.100000\n",
      " 278145/500000: episode: 786, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.014542, mae: 46.444742, mean_q: 92.768457, mean_eps: 0.100000\n",
      " 278645/500000: episode: 787, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.899231, mae: 44.757372, mean_q: 89.444518, mean_eps: 0.100000\n",
      " 279145/500000: episode: 788, duration: 2.269s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.418283, mae: 42.626417, mean_q: 85.273918, mean_eps: 0.100000\n",
      " 279645/500000: episode: 789, duration: 2.276s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.297792, mae: 36.733735, mean_q: 73.515925, mean_eps: 0.100000\n",
      " 280145/500000: episode: 790, duration: 2.258s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.267514, mae: 28.056701, mean_q: 56.097973, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 280645/500000: episode: 791, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.506838, mae: 25.329898, mean_q: 50.773510, mean_eps: 0.100000\n",
      " 281145/500000: episode: 792, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.012993, mae: 30.748457, mean_q: 61.683679, mean_eps: 0.100000\n",
      " 281645/500000: episode: 793, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.083645, mae: 37.495605, mean_q: 75.094180, mean_eps: 0.100000\n",
      " 282145/500000: episode: 794, duration: 2.217s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.998425, mae: 40.717012, mean_q: 81.399964, mean_eps: 0.100000\n",
      " 282645/500000: episode: 795, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.013131, mae: 43.293009, mean_q: 86.978500, mean_eps: 0.100000\n",
      " 283145/500000: episode: 796, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.908519, mae: 48.126919, mean_q: 96.864987, mean_eps: 0.100000\n",
      " 283645/500000: episode: 797, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.294027, mae: 49.992359, mean_q: 100.300187, mean_eps: 0.100000\n",
      " 284145/500000: episode: 798, duration: 2.218s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.168387, mae: 48.490231, mean_q: 97.123409, mean_eps: 0.100000\n",
      " 284645/500000: episode: 799, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.981954, mae: 46.221222, mean_q: 92.595471, mean_eps: 0.100000\n",
      " 285145/500000: episode: 800, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.051584, mae: 47.222271, mean_q: 94.488746, mean_eps: 0.100000\n",
      " 285645/500000: episode: 801, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.152021, mae: 50.245439, mean_q: 100.549191, mean_eps: 0.100000\n",
      " 286145/500000: episode: 802, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.768841, mae: 51.042622, mean_q: 102.069046, mean_eps: 0.100000\n",
      " 286645/500000: episode: 803, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.469763, mae: 50.597077, mean_q: 101.127025, mean_eps: 0.100000\n",
      " 287145/500000: episode: 804, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.856314, mae: 50.014983, mean_q: 100.083466, mean_eps: 0.100000\n",
      " 287645/500000: episode: 805, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.128907, mae: 49.229597, mean_q: 98.445274, mean_eps: 0.100000\n",
      " 288145/500000: episode: 806, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.405983, mae: 48.723931, mean_q: 97.300327, mean_eps: 0.100000\n",
      " 288645/500000: episode: 807, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.897585, mae: 49.301527, mean_q: 98.475433, mean_eps: 0.100000\n",
      " 289145/500000: episode: 808, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.137553, mae: 49.830146, mean_q: 99.406619, mean_eps: 0.100000\n",
      " 289645/500000: episode: 809, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.736920, mae: 49.777035, mean_q: 99.394438, mean_eps: 0.100000\n",
      " 290145/500000: episode: 810, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.608205, mae: 49.166351, mean_q: 98.514976, mean_eps: 0.100000\n",
      " 290645/500000: episode: 811, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.465980, mae: 48.247019, mean_q: 96.539811, mean_eps: 0.100000\n",
      " 291145/500000: episode: 812, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.736081, mae: 47.392037, mean_q: 94.674806, mean_eps: 0.100000\n",
      " 291645/500000: episode: 813, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.699950, mae: 47.437285, mean_q: 94.717629, mean_eps: 0.100000\n",
      " 292145/500000: episode: 814, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.535694, mae: 48.200416, mean_q: 96.298360, mean_eps: 0.100000\n",
      " 292645/500000: episode: 815, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.823070, mae: 48.729092, mean_q: 97.457493, mean_eps: 0.100000\n",
      " 293145/500000: episode: 816, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.275207, mae: 47.931619, mean_q: 96.257508, mean_eps: 0.100000\n",
      " 293645/500000: episode: 817, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.276863, mae: 44.991359, mean_q: 90.200893, mean_eps: 0.100000\n",
      " 294145/500000: episode: 818, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.198469, mae: 42.658549, mean_q: 85.364642, mean_eps: 0.100000\n",
      " 294645/500000: episode: 819, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.553282, mae: 42.885219, mean_q: 85.831735, mean_eps: 0.100000\n",
      " 295145/500000: episode: 820, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.794400, mae: 43.221074, mean_q: 86.408095, mean_eps: 0.100000\n",
      " 295160/500000: episode: 821, duration: 0.072s, episode steps:  15, steps per second: 209, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7.968915, mae: 43.247300, mean_q: 86.558680, mean_eps: 0.100000\n",
      " 295660/500000: episode: 822, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.691884, mae: 43.815290, mean_q: 87.445090, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 296160/500000: episode: 823, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.453088, mae: 46.332095, mean_q: 92.800626, mean_eps: 0.100000\n",
      " 296660/500000: episode: 824, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.816582, mae: 49.892920, mean_q: 99.890307, mean_eps: 0.100000\n",
      " 297160/500000: episode: 825, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.624110, mae: 50.754508, mean_q: 101.613377, mean_eps: 0.100000\n",
      " 297660/500000: episode: 826, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.337996, mae: 50.462821, mean_q: 100.880764, mean_eps: 0.100000\n",
      " 298160/500000: episode: 827, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.382762, mae: 50.239191, mean_q: 100.459422, mean_eps: 0.100000\n",
      " 298660/500000: episode: 828, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.831574, mae: 49.874848, mean_q: 99.928675, mean_eps: 0.100000\n",
      " 299098/500000: episode: 829, duration: 1.948s, episode steps: 438, steps per second: 225, episode reward: 438.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.680570, mae: 47.120682, mean_q: 94.383557, mean_eps: 0.100000\n",
      " 299598/500000: episode: 830, duration: 2.280s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.652204, mae: 38.837428, mean_q: 77.940671, mean_eps: 0.100000\n",
      " 300098/500000: episode: 831, duration: 2.318s, episode steps: 500, steps per second: 216, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.040458, mae: 32.878437, mean_q: 65.923546, mean_eps: 0.100000\n",
      " 300598/500000: episode: 832, duration: 2.263s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 1.347609, mae: 32.839098, mean_q: 65.651411, mean_eps: 0.100000\n",
      " 301030/500000: episode: 833, duration: 1.944s, episode steps: 432, steps per second: 222, episode reward: 432.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.927433, mae: 32.776086, mean_q: 65.717051, mean_eps: 0.100000\n",
      " 301530/500000: episode: 834, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.002390, mae: 33.963654, mean_q: 68.170309, mean_eps: 0.100000\n",
      " 302030/500000: episode: 835, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.779550, mae: 35.794275, mean_q: 71.772316, mean_eps: 0.100000\n",
      " 302530/500000: episode: 836, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.524036, mae: 37.659376, mean_q: 75.520585, mean_eps: 0.100000\n",
      " 303030/500000: episode: 837, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.048247, mae: 42.249588, mean_q: 84.583101, mean_eps: 0.100000\n",
      " 303530/500000: episode: 838, duration: 2.619s, episode steps: 500, steps per second: 191, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.401195, mae: 47.736830, mean_q: 95.429809, mean_eps: 0.100000\n",
      " 304030/500000: episode: 839, duration: 2.366s, episode steps: 500, steps per second: 211, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.595521, mae: 49.849998, mean_q: 99.370759, mean_eps: 0.100000\n",
      " 304530/500000: episode: 840, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.606411, mae: 49.174927, mean_q: 98.226904, mean_eps: 0.100000\n",
      " 304881/500000: episode: 841, duration: 1.571s, episode steps: 351, steps per second: 223, episode reward: 351.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.578489, mae: 46.191493, mean_q: 92.454571, mean_eps: 0.100000\n",
      " 305264/500000: episode: 842, duration: 1.715s, episode steps: 383, steps per second: 223, episode reward: 383.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 6.012375, mae: 41.565279, mean_q: 83.139345, mean_eps: 0.100000\n",
      " 305560/500000: episode: 843, duration: 1.329s, episode steps: 296, steps per second: 223, episode reward: 296.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.591216, mae: 37.371307, mean_q: 74.823444, mean_eps: 0.100000\n",
      " 305886/500000: episode: 844, duration: 1.463s, episode steps: 326, steps per second: 223, episode reward: 326.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.667595, mae: 35.717738, mean_q: 71.513787, mean_eps: 0.100000\n",
      " 306165/500000: episode: 845, duration: 1.248s, episode steps: 279, steps per second: 224, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.828972, mae: 35.078556, mean_q: 70.271868, mean_eps: 0.100000\n",
      " 306484/500000: episode: 846, duration: 1.434s, episode steps: 319, steps per second: 222, episode reward: 319.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.808624, mae: 36.043708, mean_q: 72.210354, mean_eps: 0.100000\n",
      " 306862/500000: episode: 847, duration: 1.697s, episode steps: 378, steps per second: 223, episode reward: 378.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.719998, mae: 36.923148, mean_q: 74.091463, mean_eps: 0.100000\n",
      " 307301/500000: episode: 848, duration: 1.957s, episode steps: 439, steps per second: 224, episode reward: 439.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.559225, mae: 38.314807, mean_q: 76.902352, mean_eps: 0.100000\n",
      " 307783/500000: episode: 849, duration: 2.149s, episode steps: 482, steps per second: 224, episode reward: 482.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.538542, mae: 38.936470, mean_q: 78.039352, mean_eps: 0.100000\n",
      " 308244/500000: episode: 850, duration: 2.061s, episode steps: 461, steps per second: 224, episode reward: 461.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.449546, mae: 39.506118, mean_q: 79.154698, mean_eps: 0.100000\n",
      " 308744/500000: episode: 851, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.542992, mae: 40.362223, mean_q: 80.851766, mean_eps: 0.100000\n",
      " 309223/500000: episode: 852, duration: 2.142s, episode steps: 479, steps per second: 224, episode reward: 479.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.512630, mae: 40.767952, mean_q: 81.590140, mean_eps: 0.100000\n",
      " 309691/500000: episode: 853, duration: 2.091s, episode steps: 468, steps per second: 224, episode reward: 468.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.380010, mae: 40.169855, mean_q: 80.380008, mean_eps: 0.100000\n",
      " 310127/500000: episode: 854, duration: 1.939s, episode steps: 436, steps per second: 225, episode reward: 436.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.314485, mae: 40.617012, mean_q: 81.293985, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 310591/500000: episode: 855, duration: 2.065s, episode steps: 464, steps per second: 225, episode reward: 464.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.456873, mae: 41.352330, mean_q: 82.718567, mean_eps: 0.100000\n",
      " 311091/500000: episode: 856, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.372977, mae: 42.377696, mean_q: 84.792829, mean_eps: 0.100000\n",
      " 311591/500000: episode: 857, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.303030, mae: 44.876301, mean_q: 89.736205, mean_eps: 0.100000\n",
      " 312091/500000: episode: 858, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.670644, mae: 47.847282, mean_q: 95.566054, mean_eps: 0.100000\n",
      " 312591/500000: episode: 859, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.313708, mae: 49.804605, mean_q: 99.375183, mean_eps: 0.100000\n",
      " 313091/500000: episode: 860, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.635468, mae: 50.608918, mean_q: 100.922725, mean_eps: 0.100000\n",
      " 313591/500000: episode: 861, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 11.532253, mae: 50.270193, mean_q: 100.280070, mean_eps: 0.100000\n",
      " 314091/500000: episode: 862, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.288677, mae: 48.910859, mean_q: 97.802257, mean_eps: 0.100000\n",
      " 314591/500000: episode: 863, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.014784, mae: 49.213993, mean_q: 98.601973, mean_eps: 0.100000\n",
      " 315091/500000: episode: 864, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.162684, mae: 50.305193, mean_q: 100.509513, mean_eps: 0.100000\n",
      " 315591/500000: episode: 865, duration: 2.218s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.383963, mae: 50.031807, mean_q: 100.130583, mean_eps: 0.100000\n",
      " 316091/500000: episode: 866, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.758632, mae: 48.426776, mean_q: 97.029364, mean_eps: 0.100000\n",
      " 316591/500000: episode: 867, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.209187, mae: 45.620737, mean_q: 91.203843, mean_eps: 0.100000\n",
      " 317091/500000: episode: 868, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.000267, mae: 45.466613, mean_q: 90.797992, mean_eps: 0.100000\n",
      " 317591/500000: episode: 869, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.287852, mae: 46.893201, mean_q: 93.619123, mean_eps: 0.100000\n",
      " 318091/500000: episode: 870, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.323463, mae: 47.302367, mean_q: 94.658643, mean_eps: 0.100000\n",
      " 318591/500000: episode: 871, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.611138, mae: 49.335248, mean_q: 98.754470, mean_eps: 0.100000\n",
      " 319091/500000: episode: 872, duration: 2.216s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.242892, mae: 50.596307, mean_q: 101.211635, mean_eps: 0.100000\n",
      " 319591/500000: episode: 873, duration: 2.216s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.859370, mae: 50.037543, mean_q: 100.059852, mean_eps: 0.100000\n",
      " 320091/500000: episode: 874, duration: 2.257s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.375284, mae: 50.201446, mean_q: 100.525221, mean_eps: 0.100000\n",
      " 320591/500000: episode: 875, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.963096, mae: 50.718461, mean_q: 101.562474, mean_eps: 0.100000\n",
      " 321091/500000: episode: 876, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.269040, mae: 50.930260, mean_q: 101.814856, mean_eps: 0.100000\n",
      " 321591/500000: episode: 877, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.833519, mae: 51.465752, mean_q: 102.949261, mean_eps: 0.100000\n",
      " 322091/500000: episode: 878, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.272898, mae: 51.073441, mean_q: 102.102463, mean_eps: 0.100000\n",
      " 322591/500000: episode: 879, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.357645, mae: 49.832078, mean_q: 99.633347, mean_eps: 0.100000\n",
      " 323091/500000: episode: 880, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.445589, mae: 48.939572, mean_q: 97.673582, mean_eps: 0.100000\n",
      " 323591/500000: episode: 881, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.919273, mae: 49.231611, mean_q: 98.317461, mean_eps: 0.100000\n",
      " 324091/500000: episode: 882, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.374348, mae: 50.322432, mean_q: 100.628239, mean_eps: 0.100000\n",
      " 324591/500000: episode: 883, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.102747, mae: 50.739386, mean_q: 101.371365, mean_eps: 0.100000\n",
      " 325091/500000: episode: 884, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.464392, mae: 50.154657, mean_q: 100.131236, mean_eps: 0.100000\n",
      " 325591/500000: episode: 885, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.354298, mae: 49.511291, mean_q: 98.789083, mean_eps: 0.100000\n",
      " 326091/500000: episode: 886, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.025364, mae: 48.887393, mean_q: 97.448652, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 326591/500000: episode: 887, duration: 2.252s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.078638, mae: 48.672823, mean_q: 97.257522, mean_eps: 0.100000\n",
      " 327091/500000: episode: 888, duration: 2.253s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.962233, mae: 48.979580, mean_q: 97.884951, mean_eps: 0.100000\n",
      " 327591/500000: episode: 889, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.819514, mae: 48.702759, mean_q: 97.287246, mean_eps: 0.100000\n",
      " 328091/500000: episode: 890, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.098751, mae: 49.003293, mean_q: 98.074761, mean_eps: 0.100000\n",
      " 328591/500000: episode: 891, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.933703, mae: 50.054862, mean_q: 100.191246, mean_eps: 0.100000\n",
      " 329091/500000: episode: 892, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.042133, mae: 50.181880, mean_q: 100.237395, mean_eps: 0.100000\n",
      " 329591/500000: episode: 893, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.374786, mae: 49.923929, mean_q: 99.634076, mean_eps: 0.100000\n",
      " 330091/500000: episode: 894, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.754034, mae: 49.672688, mean_q: 99.101544, mean_eps: 0.100000\n",
      " 330591/500000: episode: 895, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.030097, mae: 49.376946, mean_q: 98.422945, mean_eps: 0.100000\n",
      " 331091/500000: episode: 896, duration: 2.217s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.635095, mae: 49.005346, mean_q: 98.063912, mean_eps: 0.100000\n",
      " 331591/500000: episode: 897, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.749819, mae: 47.986278, mean_q: 96.487505, mean_eps: 0.100000\n",
      " 331606/500000: episode: 898, duration: 0.073s, episode steps:  15, steps per second: 206, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.112368, mae: 46.439349, mean_q: 93.580023, mean_eps: 0.100000\n",
      " 331620/500000: episode: 899, duration: 0.067s, episode steps:  14, steps per second: 210, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.130872, mae: 46.314562, mean_q: 93.233343, mean_eps: 0.100000\n",
      " 331673/500000: episode: 900, duration: 0.243s, episode steps:  53, steps per second: 218, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 23.948752, mae: 46.815190, mean_q: 93.028412, mean_eps: 0.100000\n",
      " 331686/500000: episode: 901, duration: 0.062s, episode steps:  13, steps per second: 210, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.622298, mae: 46.414724, mean_q: 92.930075, mean_eps: 0.100000\n",
      " 332186/500000: episode: 902, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 13.648385, mae: 45.602995, mean_q: 91.039751, mean_eps: 0.100000\n",
      " 332686/500000: episode: 903, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.086863, mae: 46.786588, mean_q: 93.794775, mean_eps: 0.100000\n",
      " 333186/500000: episode: 904, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.603781, mae: 49.758009, mean_q: 100.036764, mean_eps: 0.100000\n",
      " 333686/500000: episode: 905, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.115665, mae: 50.710270, mean_q: 101.943808, mean_eps: 0.100000\n",
      " 334186/500000: episode: 906, duration: 2.217s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.850214, mae: 50.485633, mean_q: 101.440839, mean_eps: 0.100000\n",
      " 334686/500000: episode: 907, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.318378, mae: 49.517117, mean_q: 99.243528, mean_eps: 0.100000\n",
      " 335186/500000: episode: 908, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.968643, mae: 48.038853, mean_q: 96.396424, mean_eps: 0.100000\n",
      " 335686/500000: episode: 909, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.234881, mae: 46.737927, mean_q: 93.860208, mean_eps: 0.100000\n",
      " 336186/500000: episode: 910, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.284181, mae: 42.921239, mean_q: 86.445126, mean_eps: 0.100000\n",
      " 336686/500000: episode: 911, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.725370, mae: 34.381643, mean_q: 69.381096, mean_eps: 0.100000\n",
      " 337186/500000: episode: 912, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.307975, mae: 27.066899, mean_q: 54.445607, mean_eps: 0.100000\n",
      " 337686/500000: episode: 913, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.993721, mae: 25.753001, mean_q: 51.738624, mean_eps: 0.100000\n",
      " 338186/500000: episode: 914, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.102909, mae: 29.120428, mean_q: 58.437443, mean_eps: 0.100000\n",
      " 338686/500000: episode: 915, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.866632, mae: 33.022217, mean_q: 66.262718, mean_eps: 0.100000\n",
      " 339186/500000: episode: 916, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.715766, mae: 38.632594, mean_q: 77.496696, mean_eps: 0.100000\n",
      " 339686/500000: episode: 917, duration: 2.271s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.222579, mae: 46.475946, mean_q: 93.268629, mean_eps: 0.100000\n",
      " 340186/500000: episode: 918, duration: 2.263s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.051613, mae: 51.052366, mean_q: 102.322740, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 340686/500000: episode: 919, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.765935, mae: 51.106732, mean_q: 102.413076, mean_eps: 0.100000\n",
      " 341186/500000: episode: 920, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.169395, mae: 50.955926, mean_q: 101.978059, mean_eps: 0.100000\n",
      " 341686/500000: episode: 921, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.956041, mae: 50.441245, mean_q: 100.889528, mean_eps: 0.100000\n",
      " 342186/500000: episode: 922, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.694181, mae: 49.249048, mean_q: 98.626695, mean_eps: 0.100000\n",
      " 342686/500000: episode: 923, duration: 2.214s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.816457, mae: 49.750018, mean_q: 99.588179, mean_eps: 0.100000\n",
      " 343186/500000: episode: 924, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.736759, mae: 50.457823, mean_q: 100.975374, mean_eps: 0.100000\n",
      " 343686/500000: episode: 925, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.476230, mae: 50.104578, mean_q: 100.083903, mean_eps: 0.100000\n",
      " 344186/500000: episode: 926, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.907343, mae: 49.704039, mean_q: 99.411325, mean_eps: 0.100000\n",
      " 344686/500000: episode: 927, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.214687, mae: 49.248140, mean_q: 98.593586, mean_eps: 0.100000\n",
      " 345186/500000: episode: 928, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.931476, mae: 47.195984, mean_q: 94.589181, mean_eps: 0.100000\n",
      " 345686/500000: episode: 929, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.758270, mae: 41.066900, mean_q: 82.083652, mean_eps: 0.100000\n",
      " 346186/500000: episode: 930, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.779574, mae: 33.953766, mean_q: 68.004184, mean_eps: 0.100000\n",
      " 346686/500000: episode: 931, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.478217, mae: 35.653536, mean_q: 71.352790, mean_eps: 0.100000\n",
      " 347186/500000: episode: 932, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.342059, mae: 41.622487, mean_q: 83.135391, mean_eps: 0.100000\n",
      " 347686/500000: episode: 933, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.804396, mae: 44.119774, mean_q: 87.979596, mean_eps: 0.100000\n",
      " 348186/500000: episode: 934, duration: 2.251s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.058689, mae: 43.618997, mean_q: 87.000388, mean_eps: 0.100000\n",
      " 348686/500000: episode: 935, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.665650, mae: 40.896881, mean_q: 81.719221, mean_eps: 0.100000\n",
      " 349186/500000: episode: 936, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.825992, mae: 40.510833, mean_q: 81.140571, mean_eps: 0.100000\n",
      " 349686/500000: episode: 937, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.406143, mae: 45.644576, mean_q: 91.134543, mean_eps: 0.100000\n",
      " 350186/500000: episode: 938, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.010552, mae: 48.372587, mean_q: 96.542865, mean_eps: 0.100000\n",
      " 350686/500000: episode: 939, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.532716, mae: 47.415270, mean_q: 94.694564, mean_eps: 0.100000\n",
      " 351186/500000: episode: 940, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.915089, mae: 47.480008, mean_q: 94.815309, mean_eps: 0.100000\n",
      " 351686/500000: episode: 941, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.983738, mae: 49.427956, mean_q: 98.742726, mean_eps: 0.100000\n",
      " 352186/500000: episode: 942, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.679218, mae: 50.976216, mean_q: 101.833542, mean_eps: 0.100000\n",
      " 352686/500000: episode: 943, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.274514, mae: 49.699782, mean_q: 99.204435, mean_eps: 0.100000\n",
      " 353186/500000: episode: 944, duration: 2.304s, episode steps: 500, steps per second: 217, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.544495, mae: 48.895254, mean_q: 97.615479, mean_eps: 0.100000\n",
      " 353686/500000: episode: 945, duration: 2.280s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.376744, mae: 48.141445, mean_q: 95.913635, mean_eps: 0.100000\n",
      " 354186/500000: episode: 946, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.972865, mae: 45.867879, mean_q: 91.514760, mean_eps: 0.100000\n",
      " 354686/500000: episode: 947, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.331993, mae: 42.721825, mean_q: 85.406041, mean_eps: 0.100000\n",
      " 355186/500000: episode: 948, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.801139, mae: 40.070017, mean_q: 79.973623, mean_eps: 0.100000\n",
      " 355686/500000: episode: 949, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.303566, mae: 38.777016, mean_q: 77.426334, mean_eps: 0.100000\n",
      " 356186/500000: episode: 950, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.687369, mae: 40.691359, mean_q: 81.314698, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 356686/500000: episode: 951, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.675347, mae: 44.327709, mean_q: 88.502294, mean_eps: 0.100000\n",
      " 357186/500000: episode: 952, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.396918, mae: 45.366913, mean_q: 90.539536, mean_eps: 0.100000\n",
      " 357686/500000: episode: 953, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.126177, mae: 46.141335, mean_q: 92.260909, mean_eps: 0.100000\n",
      " 358186/500000: episode: 954, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.694888, mae: 48.997990, mean_q: 97.818387, mean_eps: 0.100000\n",
      " 358686/500000: episode: 955, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.692057, mae: 50.848368, mean_q: 101.390325, mean_eps: 0.100000\n",
      " 359186/500000: episode: 956, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.600589, mae: 51.424544, mean_q: 102.614049, mean_eps: 0.100000\n",
      " 359686/500000: episode: 957, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 11.687266, mae: 52.057226, mean_q: 103.892578, mean_eps: 0.100000\n",
      " 360186/500000: episode: 958, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.909531, mae: 52.319675, mean_q: 104.638121, mean_eps: 0.100000\n",
      " 360686/500000: episode: 959, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 12.542907, mae: 51.995578, mean_q: 103.787084, mean_eps: 0.100000\n",
      " 361186/500000: episode: 960, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.830782, mae: 50.383531, mean_q: 100.756255, mean_eps: 0.100000\n",
      " 361255/500000: episode: 961, duration: 0.317s, episode steps:  69, steps per second: 218, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 11.530962, mae: 48.561216, mean_q: 97.095408, mean_eps: 0.100000\n",
      " 361755/500000: episode: 962, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.319676, mae: 46.457121, mean_q: 92.774252, mean_eps: 0.100000\n",
      " 362255/500000: episode: 963, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.507871, mae: 46.147564, mean_q: 92.248880, mean_eps: 0.100000\n",
      " 362755/500000: episode: 964, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 8.300993, mae: 49.892426, mean_q: 99.924993, mean_eps: 0.100000\n",
      " 363255/500000: episode: 965, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.694274, mae: 51.092547, mean_q: 102.122448, mean_eps: 0.100000\n",
      " 363755/500000: episode: 966, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.697643, mae: 50.915599, mean_q: 101.815869, mean_eps: 0.100000\n",
      " 364255/500000: episode: 967, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.820810, mae: 51.093949, mean_q: 102.132038, mean_eps: 0.100000\n",
      " 364755/500000: episode: 968, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.514927, mae: 50.705418, mean_q: 101.467672, mean_eps: 0.100000\n",
      " 365255/500000: episode: 969, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.171962, mae: 49.369296, mean_q: 98.813636, mean_eps: 0.100000\n",
      " 365755/500000: episode: 970, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.537402, mae: 48.897477, mean_q: 97.604829, mean_eps: 0.100000\n",
      " 366255/500000: episode: 971, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.842050, mae: 49.975889, mean_q: 100.093948, mean_eps: 0.100000\n",
      " 366755/500000: episode: 972, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.290426, mae: 50.449316, mean_q: 101.072629, mean_eps: 0.100000\n",
      " 367255/500000: episode: 973, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.296822, mae: 48.517781, mean_q: 97.001510, mean_eps: 0.100000\n",
      " 367755/500000: episode: 974, duration: 2.254s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.449959, mae: 45.732792, mean_q: 91.322629, mean_eps: 0.100000\n",
      " 368255/500000: episode: 975, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.541131, mae: 44.228584, mean_q: 88.489696, mean_eps: 0.100000\n",
      " 368755/500000: episode: 976, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.725458, mae: 45.314810, mean_q: 90.547522, mean_eps: 0.100000\n",
      " 369255/500000: episode: 977, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.921609, mae: 49.178174, mean_q: 98.244742, mean_eps: 0.100000\n",
      " 369755/500000: episode: 978, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.481580, mae: 51.161451, mean_q: 102.219858, mean_eps: 0.100000\n",
      " 370255/500000: episode: 979, duration: 2.249s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.730023, mae: 50.280811, mean_q: 100.420419, mean_eps: 0.100000\n",
      " 370755/500000: episode: 980, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.587196, mae: 48.482088, mean_q: 96.768004, mean_eps: 0.100000\n",
      " 371255/500000: episode: 981, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.519202, mae: 47.152375, mean_q: 94.023043, mean_eps: 0.100000\n",
      " 371755/500000: episode: 982, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.109282, mae: 47.383074, mean_q: 94.469057, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 372255/500000: episode: 983, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.882605, mae: 46.632342, mean_q: 93.282351, mean_eps: 0.100000\n",
      " 372755/500000: episode: 984, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.446914, mae: 42.989252, mean_q: 86.205328, mean_eps: 0.100000\n",
      " 373255/500000: episode: 985, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.095949, mae: 43.982946, mean_q: 88.169361, mean_eps: 0.100000\n",
      " 373755/500000: episode: 986, duration: 2.249s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.515057, mae: 49.216676, mean_q: 98.618884, mean_eps: 0.100000\n",
      " 374255/500000: episode: 987, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.457105, mae: 51.876909, mean_q: 103.755717, mean_eps: 0.100000\n",
      " 374755/500000: episode: 988, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.752863, mae: 51.679905, mean_q: 103.208148, mean_eps: 0.100000\n",
      " 375255/500000: episode: 989, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.389790, mae: 51.286228, mean_q: 102.586327, mean_eps: 0.100000\n",
      " 375755/500000: episode: 990, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.079127, mae: 51.445419, mean_q: 102.761514, mean_eps: 0.100000\n",
      " 376255/500000: episode: 991, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.021363, mae: 50.936083, mean_q: 101.917269, mean_eps: 0.100000\n",
      " 376755/500000: episode: 992, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.675557, mae: 47.764484, mean_q: 95.494608, mean_eps: 0.100000\n",
      " 377255/500000: episode: 993, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.910201, mae: 44.971103, mean_q: 89.837427, mean_eps: 0.100000\n",
      " 377755/500000: episode: 994, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.161620, mae: 46.007807, mean_q: 91.905644, mean_eps: 0.100000\n",
      " 378255/500000: episode: 995, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.656025, mae: 46.059914, mean_q: 91.906447, mean_eps: 0.100000\n",
      " 378755/500000: episode: 996, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.153265, mae: 44.645961, mean_q: 89.018987, mean_eps: 0.100000\n",
      " 379255/500000: episode: 997, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.985288, mae: 42.648139, mean_q: 85.168896, mean_eps: 0.100000\n",
      " 379755/500000: episode: 998, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.054929, mae: 43.655157, mean_q: 87.223618, mean_eps: 0.100000\n",
      " 380255/500000: episode: 999, duration: 2.277s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.598797, mae: 48.359286, mean_q: 96.879510, mean_eps: 0.100000\n",
      " 380755/500000: episode: 1000, duration: 2.268s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.201839, mae: 51.318739, mean_q: 102.465971, mean_eps: 0.100000\n",
      " 381255/500000: episode: 1001, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.313471, mae: 51.663288, mean_q: 103.157345, mean_eps: 0.100000\n",
      " 381755/500000: episode: 1002, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.746942, mae: 50.955001, mean_q: 101.910354, mean_eps: 0.100000\n",
      " 382255/500000: episode: 1003, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.151331, mae: 48.011346, mean_q: 95.979490, mean_eps: 0.100000\n",
      " 382755/500000: episode: 1004, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.328845, mae: 45.498254, mean_q: 90.902742, mean_eps: 0.100000\n",
      " 383255/500000: episode: 1005, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.596045, mae: 46.283562, mean_q: 92.347401, mean_eps: 0.100000\n",
      " 383755/500000: episode: 1006, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.520051, mae: 46.551904, mean_q: 92.856107, mean_eps: 0.100000\n",
      " 384255/500000: episode: 1007, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.735960, mae: 45.767492, mean_q: 91.235938, mean_eps: 0.100000\n",
      " 384755/500000: episode: 1008, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.178103, mae: 44.497874, mean_q: 88.951290, mean_eps: 0.100000\n",
      " 385255/500000: episode: 1009, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.997873, mae: 40.484021, mean_q: 80.884304, mean_eps: 0.100000\n",
      " 385755/500000: episode: 1010, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.836362, mae: 38.945207, mean_q: 77.903191, mean_eps: 0.100000\n",
      " 386255/500000: episode: 1011, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.954283, mae: 43.397534, mean_q: 86.817519, mean_eps: 0.100000\n",
      " 386755/500000: episode: 1012, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.285194, mae: 46.523482, mean_q: 92.846375, mean_eps: 0.100000\n",
      " 387255/500000: episode: 1013, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.832945, mae: 45.119518, mean_q: 90.005033, mean_eps: 0.100000\n",
      " 387755/500000: episode: 1014, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.601301, mae: 43.379349, mean_q: 86.628412, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 388255/500000: episode: 1015, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.788525, mae: 45.412080, mean_q: 90.704898, mean_eps: 0.100000\n",
      " 388755/500000: episode: 1016, duration: 2.336s, episode steps: 500, steps per second: 214, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.107422, mae: 49.430812, mean_q: 98.837197, mean_eps: 0.100000\n",
      " 389255/500000: episode: 1017, duration: 2.258s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.202832, mae: 51.914199, mean_q: 103.578893, mean_eps: 0.100000\n",
      " 389755/500000: episode: 1018, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.675271, mae: 52.444797, mean_q: 104.852266, mean_eps: 0.100000\n",
      " 390255/500000: episode: 1019, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.726205, mae: 51.190952, mean_q: 102.454738, mean_eps: 0.100000\n",
      " 390755/500000: episode: 1020, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.507124, mae: 47.303110, mean_q: 94.625293, mean_eps: 0.100000\n",
      " 391255/500000: episode: 1021, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.698842, mae: 43.897532, mean_q: 87.754462, mean_eps: 0.100000\n",
      " 391755/500000: episode: 1022, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.710933, mae: 43.626408, mean_q: 87.133622, mean_eps: 0.100000\n",
      " 392255/500000: episode: 1023, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.466471, mae: 44.918085, mean_q: 89.728688, mean_eps: 0.100000\n",
      " 392755/500000: episode: 1024, duration: 2.217s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.492030, mae: 46.121939, mean_q: 92.019003, mean_eps: 0.100000\n",
      " 393255/500000: episode: 1025, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.678986, mae: 46.675414, mean_q: 93.127709, mean_eps: 0.100000\n",
      " 393755/500000: episode: 1026, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.443574, mae: 47.149317, mean_q: 94.195529, mean_eps: 0.100000\n",
      " 394255/500000: episode: 1027, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.491091, mae: 47.807633, mean_q: 95.446697, mean_eps: 0.100000\n",
      " 394755/500000: episode: 1028, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 11.063413, mae: 46.792240, mean_q: 93.465371, mean_eps: 0.100000\n",
      " 395255/500000: episode: 1029, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.803000, mae: 43.641802, mean_q: 87.290798, mean_eps: 0.100000\n",
      " 395755/500000: episode: 1030, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.777347, mae: 40.588380, mean_q: 81.134707, mean_eps: 0.100000\n",
      " 396255/500000: episode: 1031, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 3.621718, mae: 40.481029, mean_q: 81.122489, mean_eps: 0.100000\n",
      " 396755/500000: episode: 1032, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.782644, mae: 46.119726, mean_q: 92.644958, mean_eps: 0.100000\n",
      " 397255/500000: episode: 1033, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.779661, mae: 51.422223, mean_q: 102.741412, mean_eps: 0.100000\n",
      " 397755/500000: episode: 1034, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.524045, mae: 52.864840, mean_q: 105.727826, mean_eps: 0.100000\n",
      " 398255/500000: episode: 1035, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.953757, mae: 52.835710, mean_q: 105.484466, mean_eps: 0.100000\n",
      " 398755/500000: episode: 1036, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.497467, mae: 51.979696, mean_q: 103.735180, mean_eps: 0.100000\n",
      " 399255/500000: episode: 1037, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.717799, mae: 51.812551, mean_q: 103.460162, mean_eps: 0.100000\n",
      " 399755/500000: episode: 1038, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.389487, mae: 52.236167, mean_q: 104.381977, mean_eps: 0.100000\n",
      " 400255/500000: episode: 1039, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.587657, mae: 52.492080, mean_q: 104.898424, mean_eps: 0.100000\n",
      " 400755/500000: episode: 1040, duration: 2.251s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 12.178514, mae: 52.242329, mean_q: 104.298690, mean_eps: 0.100000\n",
      " 401255/500000: episode: 1041, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 14.880303, mae: 52.026330, mean_q: 103.812335, mean_eps: 0.100000\n",
      " 401755/500000: episode: 1042, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.805877, mae: 51.123368, mean_q: 102.369317, mean_eps: 0.100000\n",
      " 402255/500000: episode: 1043, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.885645, mae: 48.995059, mean_q: 98.010624, mean_eps: 0.100000\n",
      " 402755/500000: episode: 1044, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.050979, mae: 49.358247, mean_q: 98.751889, mean_eps: 0.100000\n",
      " 403255/500000: episode: 1045, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.593089, mae: 50.558211, mean_q: 101.001081, mean_eps: 0.100000\n",
      " 403755/500000: episode: 1046, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.562629, mae: 49.756335, mean_q: 99.637172, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 404255/500000: episode: 1047, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.746124, mae: 48.074126, mean_q: 96.228464, mean_eps: 0.100000\n",
      " 404755/500000: episode: 1048, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.602452, mae: 46.835268, mean_q: 93.551092, mean_eps: 0.100000\n",
      " 405255/500000: episode: 1049, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.249337, mae: 47.115476, mean_q: 94.154044, mean_eps: 0.100000\n",
      " 405755/500000: episode: 1050, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.796155, mae: 47.959803, mean_q: 95.730038, mean_eps: 0.100000\n",
      " 406255/500000: episode: 1051, duration: 2.220s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.038591, mae: 49.341868, mean_q: 98.732523, mean_eps: 0.100000\n",
      " 406755/500000: episode: 1052, duration: 2.264s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.198188, mae: 50.294161, mean_q: 100.432911, mean_eps: 0.100000\n",
      " 407255/500000: episode: 1053, duration: 2.284s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.899292, mae: 50.399223, mean_q: 100.494609, mean_eps: 0.100000\n",
      " 407755/500000: episode: 1054, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.238392, mae: 50.895509, mean_q: 101.945453, mean_eps: 0.100000\n",
      " 408255/500000: episode: 1055, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.491458, mae: 51.551590, mean_q: 103.243360, mean_eps: 0.100000\n",
      " 408268/500000: episode: 1056, duration: 0.061s, episode steps:  13, steps per second: 212, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.521164, mae: 51.295744, mean_q: 102.591191, mean_eps: 0.100000\n",
      " 408768/500000: episode: 1057, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.882954, mae: 50.698607, mean_q: 101.116486, mean_eps: 0.100000\n",
      " 409268/500000: episode: 1058, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.443673, mae: 50.769241, mean_q: 101.398167, mean_eps: 0.100000\n",
      " 409768/500000: episode: 1059, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.533357, mae: 51.636294, mean_q: 103.200407, mean_eps: 0.100000\n",
      " 410268/500000: episode: 1060, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.795969, mae: 51.618361, mean_q: 103.057564, mean_eps: 0.100000\n",
      " 410768/500000: episode: 1061, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 13.090906, mae: 51.211548, mean_q: 102.137918, mean_eps: 0.100000\n",
      " 411268/500000: episode: 1062, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.046358, mae: 50.549545, mean_q: 100.925996, mean_eps: 0.100000\n",
      " 411768/500000: episode: 1063, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.686820, mae: 50.063011, mean_q: 99.940583, mean_eps: 0.100000\n",
      " 412268/500000: episode: 1064, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.777768, mae: 49.921632, mean_q: 99.665393, mean_eps: 0.100000\n",
      " 412768/500000: episode: 1065, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.392751, mae: 49.826731, mean_q: 99.607114, mean_eps: 0.100000\n",
      " 413268/500000: episode: 1066, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.100997, mae: 49.598680, mean_q: 98.997868, mean_eps: 0.100000\n",
      " 413768/500000: episode: 1067, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.098409, mae: 49.347421, mean_q: 98.804615, mean_eps: 0.100000\n",
      " 414268/500000: episode: 1068, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.024588, mae: 48.445489, mean_q: 97.188287, mean_eps: 0.100000\n",
      " 414768/500000: episode: 1069, duration: 2.281s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.381233, mae: 45.235203, mean_q: 90.641496, mean_eps: 0.100000\n",
      " 415268/500000: episode: 1070, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.203730, mae: 40.316780, mean_q: 80.809978, mean_eps: 0.100000\n",
      " 415768/500000: episode: 1071, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.289625, mae: 37.799981, mean_q: 75.679074, mean_eps: 0.100000\n",
      " 416268/500000: episode: 1072, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.120627, mae: 38.335191, mean_q: 76.684500, mean_eps: 0.100000\n",
      " 416768/500000: episode: 1073, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.573794, mae: 36.391208, mean_q: 72.895461, mean_eps: 0.100000\n",
      " 417268/500000: episode: 1074, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.638666, mae: 33.391379, mean_q: 66.679930, mean_eps: 0.100000\n",
      " 417768/500000: episode: 1075, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 2.563055, mae: 32.926491, mean_q: 65.737626, mean_eps: 0.100000\n",
      " 418268/500000: episode: 1076, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.338978, mae: 35.014863, mean_q: 70.059175, mean_eps: 0.100000\n",
      " 418768/500000: episode: 1077, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.055073, mae: 38.188263, mean_q: 76.489264, mean_eps: 0.100000\n",
      " 419268/500000: episode: 1078, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.109882, mae: 40.572636, mean_q: 81.047405, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 419768/500000: episode: 1079, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.716638, mae: 42.701650, mean_q: 85.386726, mean_eps: 0.100000\n",
      " 420268/500000: episode: 1080, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.623290, mae: 45.260030, mean_q: 90.416493, mean_eps: 0.100000\n",
      " 420768/500000: episode: 1081, duration: 2.256s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.967806, mae: 46.248441, mean_q: 92.375793, mean_eps: 0.100000\n",
      " 421268/500000: episode: 1082, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.846483, mae: 47.139126, mean_q: 94.183542, mean_eps: 0.100000\n",
      " 421768/500000: episode: 1083, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.712014, mae: 46.457412, mean_q: 92.840811, mean_eps: 0.100000\n",
      " 422268/500000: episode: 1084, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.802449, mae: 43.649888, mean_q: 87.161554, mean_eps: 0.100000\n",
      " 422768/500000: episode: 1085, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.259526, mae: 43.257988, mean_q: 86.227366, mean_eps: 0.100000\n",
      " 423268/500000: episode: 1086, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.514114, mae: 44.640358, mean_q: 89.217551, mean_eps: 0.100000\n",
      " 423768/500000: episode: 1087, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.241201, mae: 48.678777, mean_q: 97.213256, mean_eps: 0.100000\n",
      " 424268/500000: episode: 1088, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.648083, mae: 51.458319, mean_q: 102.733045, mean_eps: 0.100000\n",
      " 424768/500000: episode: 1089, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.400505, mae: 51.320807, mean_q: 102.370450, mean_eps: 0.100000\n",
      " 425268/500000: episode: 1090, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 13.020414, mae: 51.072041, mean_q: 101.799413, mean_eps: 0.100000\n",
      " 425768/500000: episode: 1091, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.685243, mae: 49.792568, mean_q: 99.579107, mean_eps: 0.100000\n",
      " 426268/500000: episode: 1092, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.467529, mae: 46.310463, mean_q: 92.412490, mean_eps: 0.100000\n",
      " 426768/500000: episode: 1093, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.502095, mae: 43.042920, mean_q: 85.810585, mean_eps: 0.100000\n",
      " 427268/500000: episode: 1094, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.729353, mae: 42.267725, mean_q: 84.420839, mean_eps: 0.100000\n",
      " 427467/500000: episode: 1095, duration: 0.895s, episode steps: 199, steps per second: 222, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.414077, mae: 40.927956, mean_q: 81.906549, mean_eps: 0.100000\n",
      " 427721/500000: episode: 1096, duration: 1.139s, episode steps: 254, steps per second: 223, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 6.478306, mae: 39.686867, mean_q: 79.338922, mean_eps: 0.100000\n",
      " 428070/500000: episode: 1097, duration: 1.558s, episode steps: 349, steps per second: 224, episode reward: 349.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.501 [0.000, 1.000],  loss: 3.462124, mae: 35.645284, mean_q: 71.280750, mean_eps: 0.100000\n",
      " 428419/500000: episode: 1098, duration: 1.566s, episode steps: 349, steps per second: 223, episode reward: 349.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 1.606729, mae: 30.300232, mean_q: 60.547057, mean_eps: 0.100000\n",
      " 428793/500000: episode: 1099, duration: 1.679s, episode steps: 374, steps per second: 223, episode reward: 374.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.341424, mae: 25.512706, mean_q: 51.091873, mean_eps: 0.100000\n",
      " 429196/500000: episode: 1100, duration: 1.791s, episode steps: 403, steps per second: 225, episode reward: 403.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.056587, mae: 24.982718, mean_q: 50.074785, mean_eps: 0.100000\n",
      " 429582/500000: episode: 1101, duration: 1.727s, episode steps: 386, steps per second: 224, episode reward: 386.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.059998, mae: 26.323240, mean_q: 52.754754, mean_eps: 0.100000\n",
      " 430014/500000: episode: 1102, duration: 1.924s, episode steps: 432, steps per second: 225, episode reward: 432.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.076358, mae: 28.462179, mean_q: 57.077459, mean_eps: 0.100000\n",
      " 430440/500000: episode: 1103, duration: 1.897s, episode steps: 426, steps per second: 225, episode reward: 426.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.122879, mae: 29.930188, mean_q: 60.011409, mean_eps: 0.100000\n",
      " 430917/500000: episode: 1104, duration: 2.132s, episode steps: 477, steps per second: 224, episode reward: 477.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.156180, mae: 30.960210, mean_q: 62.042370, mean_eps: 0.100000\n",
      " 431417/500000: episode: 1105, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.194900, mae: 33.153383, mean_q: 66.429107, mean_eps: 0.100000\n",
      " 431917/500000: episode: 1106, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.622670, mae: 36.844030, mean_q: 73.708732, mean_eps: 0.100000\n",
      " 432284/500000: episode: 1107, duration: 1.650s, episode steps: 367, steps per second: 222, episode reward: 367.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.499 [0.000, 1.000],  loss: 0.704670, mae: 39.571880, mean_q: 79.183852, mean_eps: 0.100000\n",
      " 432784/500000: episode: 1108, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.731494, mae: 43.320042, mean_q: 86.662166, mean_eps: 0.100000\n",
      " 433284/500000: episode: 1109, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.330513, mae: 44.641677, mean_q: 89.342108, mean_eps: 0.100000\n",
      " 433784/500000: episode: 1110, duration: 2.272s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.719586, mae: 47.266535, mean_q: 94.545976, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 434284/500000: episode: 1111, duration: 2.278s, episode steps: 500, steps per second: 219, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.084630, mae: 50.295571, mean_q: 100.397332, mean_eps: 0.100000\n",
      " 434784/500000: episode: 1112, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.774870, mae: 49.759651, mean_q: 99.445260, mean_eps: 0.100000\n",
      " 435284/500000: episode: 1113, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.053076, mae: 50.635946, mean_q: 101.325856, mean_eps: 0.100000\n",
      " 435784/500000: episode: 1114, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.639907, mae: 51.543622, mean_q: 102.932804, mean_eps: 0.100000\n",
      " 436284/500000: episode: 1115, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.044372, mae: 51.532788, mean_q: 102.912559, mean_eps: 0.100000\n",
      " 436784/500000: episode: 1116, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.630265, mae: 51.317082, mean_q: 102.396785, mean_eps: 0.100000\n",
      " 437284/500000: episode: 1117, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.094432, mae: 50.994119, mean_q: 101.909559, mean_eps: 0.100000\n",
      " 437784/500000: episode: 1118, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.178663, mae: 50.717403, mean_q: 101.282257, mean_eps: 0.100000\n",
      " 438284/500000: episode: 1119, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.384356, mae: 50.339720, mean_q: 100.439758, mean_eps: 0.100000\n",
      " 438784/500000: episode: 1120, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.145576, mae: 49.913091, mean_q: 99.818666, mean_eps: 0.100000\n",
      " 439284/500000: episode: 1121, duration: 2.219s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.372268, mae: 49.387499, mean_q: 98.768938, mean_eps: 0.100000\n",
      " 439784/500000: episode: 1122, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.387352, mae: 49.499345, mean_q: 99.074886, mean_eps: 0.100000\n",
      " 440284/500000: episode: 1123, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.735329, mae: 49.502590, mean_q: 99.149576, mean_eps: 0.100000\n",
      " 440784/500000: episode: 1124, duration: 2.257s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.584320, mae: 48.937987, mean_q: 97.721419, mean_eps: 0.100000\n",
      " 441284/500000: episode: 1125, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.720138, mae: 48.589759, mean_q: 96.963415, mean_eps: 0.100000\n",
      " 441784/500000: episode: 1126, duration: 2.227s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.976094, mae: 47.472808, mean_q: 95.029492, mean_eps: 0.100000\n",
      " 442284/500000: episode: 1127, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.199434, mae: 46.452604, mean_q: 92.698109, mean_eps: 0.100000\n",
      " 442784/500000: episode: 1128, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.158832, mae: 45.578742, mean_q: 90.964037, mean_eps: 0.100000\n",
      " 443284/500000: episode: 1129, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.256047, mae: 40.835914, mean_q: 81.612040, mean_eps: 0.100000\n",
      " 443784/500000: episode: 1130, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.623618, mae: 36.704999, mean_q: 73.454733, mean_eps: 0.100000\n",
      " 444284/500000: episode: 1131, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.949538, mae: 41.193331, mean_q: 82.315046, mean_eps: 0.100000\n",
      " 444784/500000: episode: 1132, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.718017, mae: 46.683422, mean_q: 93.399461, mean_eps: 0.100000\n",
      " 445284/500000: episode: 1133, duration: 2.231s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.290098, mae: 48.609997, mean_q: 97.034811, mean_eps: 0.100000\n",
      " 445784/500000: episode: 1134, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.394999, mae: 49.479722, mean_q: 98.972597, mean_eps: 0.100000\n",
      " 446284/500000: episode: 1135, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.621487, mae: 49.711304, mean_q: 99.337497, mean_eps: 0.100000\n",
      " 446784/500000: episode: 1136, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.179380, mae: 49.715507, mean_q: 99.207793, mean_eps: 0.100000\n",
      " 447146/500000: episode: 1137, duration: 1.627s, episode steps: 362, steps per second: 222, episode reward: 362.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.266286, mae: 49.512052, mean_q: 98.884325, mean_eps: 0.100000\n",
      " 447646/500000: episode: 1138, duration: 2.259s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.159003, mae: 49.230381, mean_q: 98.440447, mean_eps: 0.100000\n",
      " 448146/500000: episode: 1139, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.987183, mae: 48.817280, mean_q: 97.748517, mean_eps: 0.100000\n",
      " 448646/500000: episode: 1140, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.821674, mae: 48.755185, mean_q: 97.654587, mean_eps: 0.100000\n",
      " 449146/500000: episode: 1141, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.873688, mae: 48.698582, mean_q: 97.448478, mean_eps: 0.100000\n",
      " 449646/500000: episode: 1142, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.642255, mae: 48.517371, mean_q: 97.115647, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 450146/500000: episode: 1143, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.939032, mae: 48.394340, mean_q: 96.866052, mean_eps: 0.100000\n",
      " 450646/500000: episode: 1144, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 8.634648, mae: 48.028706, mean_q: 96.218756, mean_eps: 0.100000\n",
      " 451146/500000: episode: 1145, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.595152, mae: 47.644177, mean_q: 95.333142, mean_eps: 0.100000\n",
      " 451646/500000: episode: 1146, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.172788, mae: 47.469779, mean_q: 95.006750, mean_eps: 0.100000\n",
      " 452146/500000: episode: 1147, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 9.566788, mae: 47.355653, mean_q: 94.749084, mean_eps: 0.100000\n",
      " 452646/500000: episode: 1148, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.384997, mae: 47.096551, mean_q: 94.350871, mean_eps: 0.100000\n",
      " 453146/500000: episode: 1149, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.970278, mae: 46.942400, mean_q: 93.941929, mean_eps: 0.100000\n",
      " 453646/500000: episode: 1150, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 9.870229, mae: 46.563229, mean_q: 93.173459, mean_eps: 0.100000\n",
      " 454146/500000: episode: 1151, duration: 2.253s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 8.005187, mae: 46.178238, mean_q: 92.403761, mean_eps: 0.100000\n",
      " 454646/500000: episode: 1152, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.171135, mae: 46.362650, mean_q: 92.829121, mean_eps: 0.100000\n",
      " 454827/500000: episode: 1153, duration: 0.811s, episode steps: 181, steps per second: 223, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.362867, mae: 46.607853, mean_q: 93.349014, mean_eps: 0.100000\n",
      " 455056/500000: episode: 1154, duration: 1.029s, episode steps: 229, steps per second: 222, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 11.897191, mae: 46.888244, mean_q: 93.682367, mean_eps: 0.100000\n",
      " 455291/500000: episode: 1155, duration: 1.051s, episode steps: 235, steps per second: 224, episode reward: 235.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 11.116895, mae: 46.963015, mean_q: 93.712836, mean_eps: 0.100000\n",
      " 455343/500000: episode: 1156, duration: 0.238s, episode steps:  52, steps per second: 218, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8.589795, mae: 46.920847, mean_q: 93.598079, mean_eps: 0.100000\n",
      " 455390/500000: episode: 1157, duration: 0.220s, episode steps:  47, steps per second: 214, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 8.743371, mae: 46.957593, mean_q: 93.720353, mean_eps: 0.100000\n",
      " 455890/500000: episode: 1158, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.165334, mae: 46.634716, mean_q: 93.375831, mean_eps: 0.100000\n",
      " 456390/500000: episode: 1159, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.504985, mae: 45.140910, mean_q: 90.736555, mean_eps: 0.100000\n",
      " 456890/500000: episode: 1160, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.206590, mae: 43.155092, mean_q: 86.584158, mean_eps: 0.100000\n",
      " 457390/500000: episode: 1161, duration: 2.222s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.626258, mae: 42.920293, mean_q: 85.938975, mean_eps: 0.100000\n",
      " 457890/500000: episode: 1162, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.610127, mae: 43.535434, mean_q: 87.118829, mean_eps: 0.100000\n",
      " 458390/500000: episode: 1163, duration: 2.237s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.689262, mae: 42.804122, mean_q: 85.774619, mean_eps: 0.100000\n",
      " 458890/500000: episode: 1164, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.255770, mae: 40.870050, mean_q: 81.768869, mean_eps: 0.100000\n",
      " 458904/500000: episode: 1165, duration: 0.068s, episode steps:  14, steps per second: 206, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.096475, mae: 39.590158, mean_q: 79.376007, mean_eps: 0.100000\n",
      " 459404/500000: episode: 1166, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.205964, mae: 40.001313, mean_q: 80.237988, mean_eps: 0.100000\n",
      " 459904/500000: episode: 1167, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.441357, mae: 42.336018, mean_q: 85.037749, mean_eps: 0.100000\n",
      " 460404/500000: episode: 1168, duration: 2.263s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.889514, mae: 45.336178, mean_q: 90.908193, mean_eps: 0.100000\n",
      " 460904/500000: episode: 1169, duration: 2.275s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.756720, mae: 46.684678, mean_q: 93.240533, mean_eps: 0.100000\n",
      " 461404/500000: episode: 1170, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.273581, mae: 47.711325, mean_q: 95.315916, mean_eps: 0.100000\n",
      " 461904/500000: episode: 1171, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.268276, mae: 48.734269, mean_q: 97.293857, mean_eps: 0.100000\n",
      " 462404/500000: episode: 1172, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.679789, mae: 47.778521, mean_q: 95.484339, mean_eps: 0.100000\n",
      " 462904/500000: episode: 1173, duration: 2.218s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.686709, mae: 45.412558, mean_q: 90.860314, mean_eps: 0.100000\n",
      " 463404/500000: episode: 1174, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.642984, mae: 43.952635, mean_q: 88.214573, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 463904/500000: episode: 1175, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.551735, mae: 44.107095, mean_q: 88.479065, mean_eps: 0.100000\n",
      " 464404/500000: episode: 1176, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.639314, mae: 44.766125, mean_q: 89.603743, mean_eps: 0.100000\n",
      " 464904/500000: episode: 1177, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.611023, mae: 45.045244, mean_q: 89.999962, mean_eps: 0.100000\n",
      " 465404/500000: episode: 1178, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.657985, mae: 44.283388, mean_q: 88.572637, mean_eps: 0.100000\n",
      " 465904/500000: episode: 1179, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.571796, mae: 42.518319, mean_q: 85.163149, mean_eps: 0.100000\n",
      " 466404/500000: episode: 1180, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.795107, mae: 41.402189, mean_q: 83.055275, mean_eps: 0.100000\n",
      " 466904/500000: episode: 1181, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.158488, mae: 40.647380, mean_q: 81.635095, mean_eps: 0.100000\n",
      " 467404/500000: episode: 1182, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.476833, mae: 40.321937, mean_q: 80.959055, mean_eps: 0.100000\n",
      " 467904/500000: episode: 1183, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.125525, mae: 41.516126, mean_q: 83.293819, mean_eps: 0.100000\n",
      " 468404/500000: episode: 1184, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.305738, mae: 43.968820, mean_q: 87.982605, mean_eps: 0.100000\n",
      " 468904/500000: episode: 1185, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.722890, mae: 45.464196, mean_q: 90.972934, mean_eps: 0.100000\n",
      " 469404/500000: episode: 1186, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 10.085926, mae: 46.495751, mean_q: 92.880952, mean_eps: 0.100000\n",
      " 469904/500000: episode: 1187, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.723428, mae: 46.519940, mean_q: 93.116807, mean_eps: 0.100000\n",
      " 470404/500000: episode: 1188, duration: 2.252s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.156695, mae: 45.472567, mean_q: 90.943128, mean_eps: 0.100000\n",
      " 470904/500000: episode: 1189, duration: 2.241s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.863662, mae: 45.985737, mean_q: 91.945280, mean_eps: 0.100000\n",
      " 471404/500000: episode: 1190, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.975356, mae: 47.519808, mean_q: 94.910996, mean_eps: 0.100000\n",
      " 471904/500000: episode: 1191, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.393630, mae: 47.585244, mean_q: 95.066553, mean_eps: 0.100000\n",
      " 472404/500000: episode: 1192, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.712295, mae: 46.231604, mean_q: 92.411538, mean_eps: 0.100000\n",
      " 472904/500000: episode: 1193, duration: 2.248s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 7.017133, mae: 44.639432, mean_q: 89.240577, mean_eps: 0.100000\n",
      " 473404/500000: episode: 1194, duration: 2.243s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 6.398472, mae: 43.844111, mean_q: 87.708085, mean_eps: 0.100000\n",
      " 473904/500000: episode: 1195, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.618776, mae: 43.806040, mean_q: 87.669768, mean_eps: 0.100000\n",
      " 474404/500000: episode: 1196, duration: 2.247s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.671857, mae: 44.064390, mean_q: 88.133826, mean_eps: 0.100000\n",
      " 474904/500000: episode: 1197, duration: 2.230s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 7.207245, mae: 43.339973, mean_q: 86.758197, mean_eps: 0.100000\n",
      " 475404/500000: episode: 1198, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.329706, mae: 40.874108, mean_q: 82.051630, mean_eps: 0.100000\n",
      " 475904/500000: episode: 1199, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.710124, mae: 37.912479, mean_q: 75.935954, mean_eps: 0.100000\n",
      " 476404/500000: episode: 1200, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.267642, mae: 36.567710, mean_q: 73.131880, mean_eps: 0.100000\n",
      " 476904/500000: episode: 1201, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 5.201891, mae: 36.729807, mean_q: 73.389810, mean_eps: 0.100000\n",
      " 477404/500000: episode: 1202, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.189203, mae: 39.337722, mean_q: 79.003211, mean_eps: 0.100000\n",
      " 477770/500000: episode: 1203, duration: 1.640s, episode steps: 366, steps per second: 223, episode reward: 366.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.795726, mae: 44.518687, mean_q: 89.517966, mean_eps: 0.100000\n",
      " 478270/500000: episode: 1204, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.067692, mae: 49.472266, mean_q: 99.101112, mean_eps: 0.100000\n",
      " 478770/500000: episode: 1205, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.344413, mae: 50.303182, mean_q: 100.894855, mean_eps: 0.100000\n",
      " 479270/500000: episode: 1206, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.359976, mae: 49.674038, mean_q: 99.337398, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 479770/500000: episode: 1207, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.997472, mae: 51.814181, mean_q: 103.595769, mean_eps: 0.100000\n",
      " 480270/500000: episode: 1208, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.721603, mae: 52.676944, mean_q: 105.194773, mean_eps: 0.100000\n",
      " 480770/500000: episode: 1209, duration: 2.226s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.108432, mae: 50.349002, mean_q: 100.450816, mean_eps: 0.100000\n",
      " 481270/500000: episode: 1210, duration: 2.250s, episode steps: 500, steps per second: 222, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.916877, mae: 47.820193, mean_q: 95.670641, mean_eps: 0.100000\n",
      " 481770/500000: episode: 1211, duration: 2.225s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.164100, mae: 49.210637, mean_q: 98.908673, mean_eps: 0.100000\n",
      " 482270/500000: episode: 1212, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.825368, mae: 53.662378, mean_q: 107.787027, mean_eps: 0.100000\n",
      " 482770/500000: episode: 1213, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.434498, mae: 54.911971, mean_q: 109.743174, mean_eps: 0.100000\n",
      " 483270/500000: episode: 1214, duration: 2.232s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 9.803451, mae: 55.155434, mean_q: 110.400635, mean_eps: 0.100000\n",
      " 483770/500000: episode: 1215, duration: 2.240s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 11.386484, mae: 55.529822, mean_q: 111.089090, mean_eps: 0.100000\n",
      " 484270/500000: episode: 1216, duration: 2.236s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.614980, mae: 54.759103, mean_q: 109.815842, mean_eps: 0.100000\n",
      " 484770/500000: episode: 1217, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.287845, mae: 52.495590, mean_q: 105.178123, mean_eps: 0.100000\n",
      " 485270/500000: episode: 1218, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.830444, mae: 49.060606, mean_q: 98.295209, mean_eps: 0.100000\n",
      " 485770/500000: episode: 1219, duration: 2.214s, episode steps: 500, steps per second: 226, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.923159, mae: 48.218930, mean_q: 96.494859, mean_eps: 0.100000\n",
      " 486270/500000: episode: 1220, duration: 2.237s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.687345, mae: 51.299045, mean_q: 102.808159, mean_eps: 0.100000\n",
      " 486770/500000: episode: 1221, duration: 2.261s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.128359, mae: 53.100291, mean_q: 106.345682, mean_eps: 0.100000\n",
      " 487270/500000: episode: 1222, duration: 2.261s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.412095, mae: 52.090115, mean_q: 104.084287, mean_eps: 0.100000\n",
      " 487770/500000: episode: 1223, duration: 2.276s, episode steps: 500, steps per second: 220, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.243343, mae: 52.464230, mean_q: 104.889309, mean_eps: 0.100000\n",
      " 488270/500000: episode: 1224, duration: 2.246s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.119567, mae: 52.919716, mean_q: 105.762322, mean_eps: 0.100000\n",
      " 488699/500000: episode: 1225, duration: 1.920s, episode steps: 429, steps per second: 223, episode reward: 429.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 13.080273, mae: 52.348917, mean_q: 104.437478, mean_eps: 0.100000\n",
      " 489199/500000: episode: 1226, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.656162, mae: 52.168503, mean_q: 104.336008, mean_eps: 0.100000\n",
      " 489699/500000: episode: 1227, duration: 2.239s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.804508, mae: 53.232927, mean_q: 106.803756, mean_eps: 0.100000\n",
      " 490199/500000: episode: 1228, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.558251, mae: 50.648036, mean_q: 101.329349, mean_eps: 0.100000\n",
      " 490699/500000: episode: 1229, duration: 2.242s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.632803, mae: 47.523991, mean_q: 94.890320, mean_eps: 0.100000\n",
      " 491199/500000: episode: 1230, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 6.182922, mae: 47.753753, mean_q: 95.339631, mean_eps: 0.100000\n",
      " 491699/500000: episode: 1231, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.142730, mae: 46.818216, mean_q: 93.550408, mean_eps: 0.100000\n",
      " 492199/500000: episode: 1232, duration: 2.224s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.681225, mae: 44.362400, mean_q: 88.735077, mean_eps: 0.100000\n",
      " 492699/500000: episode: 1233, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.695682, mae: 45.010410, mean_q: 90.073855, mean_eps: 0.100000\n",
      " 493199/500000: episode: 1234, duration: 2.244s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.369506, mae: 49.163912, mean_q: 98.263246, mean_eps: 0.100000\n",
      " 493699/500000: episode: 1235, duration: 2.234s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.092954, mae: 52.260852, mean_q: 104.306749, mean_eps: 0.100000\n",
      " 494199/500000: episode: 1236, duration: 2.227s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.727496, mae: 53.808187, mean_q: 107.298576, mean_eps: 0.100000\n",
      " 494699/500000: episode: 1237, duration: 2.238s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.802408, mae: 54.007898, mean_q: 107.764085, mean_eps: 0.100000\n",
      " 495199/500000: episode: 1238, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.040207, mae: 53.908824, mean_q: 107.871349, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 495699/500000: episode: 1239, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.039128, mae: 53.681506, mean_q: 107.262478, mean_eps: 0.100000\n",
      " 496199/500000: episode: 1240, duration: 2.223s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.641024, mae: 53.242662, mean_q: 106.506500, mean_eps: 0.100000\n",
      " 496699/500000: episode: 1241, duration: 2.221s, episode steps: 500, steps per second: 225, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.036201, mae: 53.355629, mean_q: 106.510265, mean_eps: 0.100000\n",
      " 497199/500000: episode: 1242, duration: 2.235s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.157477, mae: 53.154292, mean_q: 106.056610, mean_eps: 0.100000\n",
      " 497699/500000: episode: 1243, duration: 2.228s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 10.918704, mae: 52.741900, mean_q: 105.363435, mean_eps: 0.100000\n",
      " 498199/500000: episode: 1244, duration: 2.229s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.817107, mae: 52.556281, mean_q: 104.940410, mean_eps: 0.100000\n",
      " 498699/500000: episode: 1245, duration: 2.265s, episode steps: 500, steps per second: 221, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.422162, mae: 52.347566, mean_q: 104.596308, mean_eps: 0.100000\n",
      " 499199/500000: episode: 1246, duration: 2.233s, episode steps: 500, steps per second: 224, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.432104, mae: 51.969172, mean_q: 103.945634, mean_eps: 0.100000\n",
      " 499699/500000: episode: 1247, duration: 2.245s, episode steps: 500, steps per second: 223, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.691878, mae: 51.967469, mean_q: 103.831956, mean_eps: 0.100000\n",
      "done, took 2274.056 seconds\n",
      "CPU times: user 1h 2min 39s, sys: 11min 38s, total: 1h 14min 17s\n",
      "Wall time: 37min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run the build_agent function to traing the agent\n",
    "agent = build_agent(network, actions)                      # used build_agent to setup a dqn model\n",
    "\n",
    "agent.compile(Adam(learning_rate = 1e-4),                # use Adam optimisation with learning rate 0.0001\n",
    "            metrics = ['mae']                            # use mean absolute error to evaluate the metric\n",
    "           )      \n",
    "\n",
    "history = agent.fit(env, \n",
    "        nb_steps = 500000,                               # number of timesteps \n",
    "        visualize = False,                               # visualize during the training\n",
    "        verbose = 2                                      # how to show the training output\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21673339",
   "metadata": {},
   "source": [
    "### Evaluate the Agent's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e415206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "scores = agent.test(env,                             # pass our environment into the DQNagent.test agent\n",
    "                  nb_episodes = 5,                    # number of episodes\n",
    "                  visualize = True                     # set True if we want to visualize it\n",
    "                 )\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e1dde46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4d84b",
   "metadata": {},
   "source": [
    "Remark: \n",
    "- The above 5 episode showing that, the agent received ***all perfect scores to prevent the Pole from falling over*** \n",
    "- the log on the above showing that we may ***reduce number of learning timesteps to 40000***, because the agent start received 500 episode reward start in the 36220 episode consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929fa13",
   "metadata": {},
   "source": [
    "# 6. Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "353afc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/session.py:1766: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")                          # Using MountainCar verion 0 in this notebook\n",
    "env.reset()                                            # reset the environment to initial default\n",
    "\n",
    "# Set the input_shape of the build_model function\n",
    "num_states = env.observation_space.shape               # use .shape[0] to return number of observation space\n",
    "\n",
    "# Set the output_shape of the build_model function\n",
    "num_actions = env.action_space.n                       # use .n to return number of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "109103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network_EpsilonGreedyPolicy(states, actions):\n",
    "    \n",
    "    network_EpsilonGreedyPolicy = Sequential()\n",
    "    \n",
    "    network_EpsilonGreedyPolicy.add(Flatten(input_shape = (num_states), name = 'States'))        # use Flatten layer as the input layer, input_shape = number of state \n",
    "    \n",
    "    network_EpsilonGreedyPolicy.add(Dense(128, activation = 'relu', name = 'Dense_1'))           # use Dense layer with 128 units of tensor with relu activation function\n",
    "    \n",
    "    network_EpsilonGreedyPolicy.add(Dense(64, activation = 'relu', name = 'Dense_2'))            # use another Dense layer with 64 units of tensor with relu activation function\n",
    "    \n",
    "    network_EpsilonGreedyPolicy.add(Dense(num_actions, activation = 'linear', name = 'Actions')) # the output layer with shape = number of actions with linear activation function\n",
    "                                                                                                # Output the probability for each actions\n",
    "    \n",
    "    return network_EpsilonGreedyPolicy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29804b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    \n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network_EpsilonGreedyPolicy.predict(state[None])[0]              # q_values: the probability for each action, shape=[None]: size not fixed\n",
    "\n",
    "    action = np.random.choice(num_actions)                  # randomly pick an action\n",
    "    \n",
    "    # Choose the max of q values if a random number id greater than epsilon else action\n",
    "    if random.random() > epsilon:                           # if a random number > epsilon\n",
    "    \n",
    "        chosen_action = np.argmax(q_values)                 # output action with highest q_values\n",
    "    \n",
    "    else:                                                   # if a random number < epsilon\n",
    "    \n",
    "        chosen_action = action                              # output a random action\n",
    "        \n",
    "    return chosen_action                                    # output an action by epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e178680",
   "metadata": {},
   "source": [
    "# 7. Q-learning via Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cac05058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "States (Flatten)             (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Actions (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 9,026\n",
      "Trainable params: 9,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + env.observation_space.shape)\n",
    "\n",
    "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "\n",
    "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "\n",
    "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + env.observation_space.shape)\n",
    "\n",
    "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])\n",
    "\n",
    "# run the build_network function\n",
    "network_EpsilonGreedyPolicy = build_network_EpsilonGreedyPolicy(states, actions) \n",
    "\n",
    "network_EpsilonGreedyPolicy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf56c4",
   "metadata": {},
   "source": [
    "### Compute Q-Values in Current State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19635949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get q-values for all actions in current states\n",
    "predicted_qvalues = network_EpsilonGreedyPolicy(states_ph)                    # Output the probability for each actions from network\n",
    "\n",
    "# select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, env.action_space.n), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef3f15",
   "metadata": {},
   "source": [
    "### Predict Q-Values for Next State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31ef483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set reward discount factor gamma \n",
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "predicted_next_qvalues = network_EpsilonGreedyPolicy(next_states_ph)                 \n",
    "\n",
    "# compute V*(next_states) using predicted next q-values\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues,1)         \n",
    "\n",
    "# compute \"target q-values\" for loss. Also call: TD Target\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values \n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)    # is_done_ph: to check is the last state or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d744b",
   "metadata": {},
   "source": [
    "### Compute Loss and Training Function\n",
    "\n",
    "when doing gradient descent, __we won't propagate gradients through it__ to make training more stable.\n",
    "\n",
    "To do so, we shall use `tf.stop_gradient` function which basically says ***\"consider this thing constant when doingbackprop\"***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e00ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error loss to minimize (predicted_qvalue - target_qvalue)\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "\n",
    "loss = tf.reduce_mean(loss)                                       # reduce_mean = sum of all loss / number of loss\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "637b1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define session\n",
    "def generate_session(env, episode_max=600, epsilon=0, train=False):  # Max episode in CartPole = 500 \n",
    "    \n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0                                               # initial reward = 0\n",
    "    \n",
    "    state = env.reset()                                            # initialize the environment\n",
    "    \n",
    "    for episode in range(episode_max):\n",
    "        \n",
    "        action = epsilon_greedy_policy(state, epsilon=epsilon)     # chose an action by epsilon_greedy_policy\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)             # the environment's feedback in terms of next_state, reward, is epsilon done\n",
    "        \n",
    "        if train:                                                  \n",
    "            sess.run(train_step,                                   # agent.update(state, action, reward, next_state) from tabular agent\n",
    "                     { \n",
    "                        states_ph: [state], \n",
    "                        actions_ph: [action], \n",
    "                        rewards_ph: [reward], \n",
    "                        next_states_ph: [next_state], \n",
    "                        is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += reward                                     # update total_reward\n",
    "        \n",
    "        state = next_state                                         # update state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76b7fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\tmean reward = 12.000\tepsilon = 0.500\n",
      "epoch #1\tmean reward = 15.000\tepsilon = 0.495\n",
      "epoch #2\tmean reward = 9.000\tepsilon = 0.490\n",
      "epoch #3\tmean reward = 8.000\tepsilon = 0.485\n",
      "epoch #4\tmean reward = 16.000\tepsilon = 0.480\n",
      "epoch #5\tmean reward = 14.000\tepsilon = 0.475\n",
      "epoch #6\tmean reward = 12.000\tepsilon = 0.471\n",
      "epoch #7\tmean reward = 22.000\tepsilon = 0.466\n",
      "epoch #8\tmean reward = 124.000\tepsilon = 0.461\n",
      "epoch #9\tmean reward = 14.000\tepsilon = 0.457\n",
      "epoch #10\tmean reward = 19.000\tepsilon = 0.452\n",
      "epoch #11\tmean reward = 18.000\tepsilon = 0.448\n",
      "epoch #12\tmean reward = 49.000\tepsilon = 0.443\n",
      "epoch #13\tmean reward = 46.000\tepsilon = 0.439\n",
      "epoch #14\tmean reward = 109.000\tepsilon = 0.434\n",
      "epoch #15\tmean reward = 19.000\tepsilon = 0.430\n",
      "epoch #16\tmean reward = 17.000\tepsilon = 0.426\n",
      "epoch #17\tmean reward = 33.000\tepsilon = 0.421\n",
      "epoch #18\tmean reward = 100.000\tepsilon = 0.417\n",
      "epoch #19\tmean reward = 161.000\tepsilon = 0.413\n",
      "epoch #20\tmean reward = 184.000\tepsilon = 0.409\n",
      "epoch #21\tmean reward = 182.000\tepsilon = 0.405\n",
      "epoch #22\tmean reward = 81.000\tepsilon = 0.401\n",
      "epoch #23\tmean reward = 31.000\tepsilon = 0.397\n",
      "epoch #24\tmean reward = 262.000\tepsilon = 0.393\n",
      "epoch #25\tmean reward = 35.000\tepsilon = 0.389\n",
      "epoch #26\tmean reward = 203.000\tepsilon = 0.385\n",
      "epoch #27\tmean reward = 121.000\tepsilon = 0.381\n",
      "epoch #28\tmean reward = 129.000\tepsilon = 0.377\n",
      "epoch #29\tmean reward = 125.000\tepsilon = 0.374\n",
      "epoch #30\tmean reward = 147.000\tepsilon = 0.370\n",
      "epoch #31\tmean reward = 500.000\tepsilon = 0.366\n",
      "Learning Completed!\n",
      "CPU times: user 8min 48s, sys: 2min 52s, total: 11min 41s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set epsilon: the probability for the Agent choice to take an random action when less than epsilon\n",
    "epsilon = 0.5                  # initialze epsilon\n",
    "\n",
    "for i in range(500):                                                                 # number of epoch to run\n",
    "    \n",
    "    for _ in range(100):                                                             # number of session to run\n",
    "        \n",
    "        session_rewards = generate_session(env, epsilon=epsilon, train=True)         # Train the agent\n",
    "        \n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99                                                                  # decrease epsilon per epoch mean less explortation over time\n",
    "    \n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"    # epsilon should be always nonzero \n",
    "    \n",
    "    if np.mean(session_rewards) > 499:                                              # session terminate if session_rewards > 400\n",
    "        \n",
    "        print(\"Learning Completed!\")\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e05ed5",
   "metadata": {},
   "source": [
    "Remark: \n",
    "The learning process with ***Epsilon-Greedy Policy reach perfect score at epoch #31*** (time spend: learning 6min)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd00f11",
   "metadata": {},
   "source": [
    "***End of Page***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
