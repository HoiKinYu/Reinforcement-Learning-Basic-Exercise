{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b625610d",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "I'm using the environment in the OpenAI gym to practice how to train an agent to play Atari Game by reinforcement learning. In this notebook, I tried to ***build a deep learning model with Tensorflow for reinforcement learning*** and ***training a live reinforcement learning model using Keras-RL***. By comparing the agent's performance different between training for 100k timesteps and 1 million timesteps. The result show that ***the Agent's performance after training for 1 million timesteps perfrom 42.7% better in terms of average scores received***, and ***time spend more than 21 hours to train for 34,852,326 trainable params in 100k timesteps by CPU***. \n",
    " \n",
    " \n",
    "### Here is my key findings during the process:\n",
    "\n",
    "Agent's Performance ***Before Training***\n",
    "- Before training the agent to play the Atari Game - SpaceInvaders-v0, the score in the first 5 episodes are between 105-240 (score 179 on average). (See detail in Section 2. Test Random Environment with OpanAI Gym)\n",
    "\n",
    "Agent's Performance ***After Training 100k Timesteps***\n",
    "- After training 100k timesteps with the key parameters and models structure below, the agent's performance improved to score 45-625 in 10 episodes (score 195.5 on average) only. (See detail in Section 7. Evaluate the Agent's Performance after Training for 100k Timesteps )\n",
    "\n",
    "    Key Parameters:\n",
    "    - Squential image base model with 3 convolution2D layers, 1 Flatten layer and 5 Dense layers (Trainable params: 34,852,326)\n",
    "    - Optimizers: Adam\n",
    "    - Keras-RL Agent: DQNAgent\n",
    "    - Learning rate: 1e-4\n",
    "    - Number of training timesteps: 100k\n",
    "    \n",
    "Agent's Performance ***After Training 1 Million Timesteps***\n",
    "- After training 1 million timesteps with the same parameters on the above except for reduced last 2 Dense layers (Trainable params: 34,812,326), the agent's performance improved to score 120-530 in 10 episodes and average score increased to 279. (See detail in Section 9. Evaluate the Agent's Performance after Training for 1M Timesteps)\n",
    "\n",
    "### ***Table of Content:***\n",
    "\n",
    "1. What is SpaceInvaders\n",
    "2. Test Random Environment with OpanAI Gym\n",
    "3. Create a Deep Learning Model with Keras\n",
    "4. Build Agent with Keras-RL\n",
    "5. Train the Agent\n",
    "6. Save the Model\n",
    "7. Evaluate the Agent's Performance after Training for 100k Timesteps \n",
    "8. Further Improvement\n",
    "9. Evaluate the Agent's Performance after Training for 1M Timesteps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95392e",
   "metadata": {},
   "source": [
    "# 1. What is SpaceInvaders\n",
    "\n",
    "<img src='http://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/SpaceInvaders-v0/poster.jpg' width='250px'/>\n",
    "\n",
    "In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3) \n",
    "Each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from {2,3,4}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96131132",
   "metadata": {},
   "source": [
    "# 2. Test Random Environment with OpanAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b239f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "from multiprocessing import Process\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47405b04",
   "metadata": {},
   "source": [
    "***SpaceInvaders-v0***\n",
    "\n",
    "- The v0 environment of Space Invaders returns an image as part of the state.\n",
    "- We extract the shape of the image to pass to structure of neural netork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5cf5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'SpaceInvaders-v0'\n",
    "\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "height, width, channels = env.observation_space.shape\n",
    "\n",
    "actions = env.action_space.n                             # 6 actions we can take ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47324656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce873d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 19:22:10.791 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3e3877fe0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-13 19:22:10.792 python[43361:1411145] Warning: Expected min height of view: (<NSButton: 0x7fa3e38deb60>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-13 19:22:10.794 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3e38e1df0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-13 19:22:10.796 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3e38eb7a0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:220.0\n",
      "Episode:2 Score:210.0\n",
      "Episode:3 Score:105.0\n",
      "Episode:4 Score:240.0\n",
      "Episode:5 Score:120.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(1, episodes + 1):                     # looping from 1 to 5\n",
    "    \n",
    "    state = env.reset()                                    # initial set of observation (not just the pole)\n",
    "    \n",
    "    done = False                                           # maximum number of steps in this particular environment\n",
    "    \n",
    "    score = 0                                              # running score counter\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()                                       # view the graphical representation that environment(outside colab only)\n",
    "        \n",
    "        action = random.choice([0,1,2,3,4,5])               # \n",
    "        \n",
    "        n_state, reward, done, info = env.step(action)      # pass random actions to environment to get back \n",
    "                                                            # 1. next set of observation (4 observation in this case)\n",
    "                                                            # 2. reward (Positive value increment, negative value decrement)\n",
    "                                                            # 3. done (episode is done = True)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "    print('Episode:{} Score:{}'.format(episode, score))    # print out score for each episode\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81189e2b",
   "metadata": {},
   "source": [
    "During the above 5 games, the score between 105-240 (average score 179) before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee782133",
   "metadata": {},
   "source": [
    "# 3. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91adc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D   \n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e7f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(32,                                         # For image base model, we start with convolution2D layer with 32 convolution2D filters \n",
    "                            (8,8),                                      # Filter size 8 x 8\n",
    "                            strides = (4,4),                            # 4 x 4 means move 4 pixels to the right, and then 4 pixels to downward\n",
    "                            activation = 'relu',                       # use 'relu' function\n",
    "                            input_shape = (3, height, width, channels)  # pass through the image's heights, width, and channels as input into the model\n",
    "                           ))\n",
    "    \n",
    "    model.add(Convolution2D(64,                                         # 64 convolution2D filters in the second layer\n",
    "                            (4,4),                                      # Filter size 4 x 4\n",
    "                            strides = (2,2),                            # 2 x 2 means move 2 pixels to the right, and then 2 pixels to downward\n",
    "                            activation = 'relu'\n",
    "                           ))\n",
    "    \n",
    "    model.add(Convolution2D(64,                                         # 64 convolution2D filters in the second layer\n",
    "                            (3,3),                                      # Filter size 3 x 3\n",
    "                            strides = (1,1),                            # 1 x 1 means move 1 pixels to the right, and then 1 pixels to downward\n",
    "                            activation = 'relu'         \n",
    "                           ))\n",
    "    \n",
    "    model.add(Flatten())    # Flatten down all the above layers into a single layer, so that we can than pass it through to next Dense layer\n",
    "    \n",
    "    model.add(Dense(512, activation = 'relu'))                          # compress all the above input image into 512 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(256, activation = 'relu'))                          # compress down to 256 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(128, activation = 'relu'))                          # compress down to 128 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(64, activation = 'relu'))                           # compress down to 64 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(actions, activation = 'linear'))                    # compress down to number of actions as output of the model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143d4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51683e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 34,852,326\n",
      "Trainable params: 34,852,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7358f",
   "metadata": {},
   "source": [
    "# 4. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e859499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent                                  \n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad68cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):                       \n",
    "\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),   # Using LinearAnnealedPolicy pass EpsGreedyQPolicy in it\n",
    "                                attr = 'eps',\n",
    "                                value_min = .1,\n",
    "                                value_max = 1., \n",
    "                                value_test = .2,\n",
    "                                nb_steps = 10000       # number of steps\n",
    "                               )\n",
    "    \n",
    "    memory = SequentialMemory(limit = 1000,\n",
    "                              window_length = 3        # If we want to increase the windowed period, \n",
    "                                                        # we need to change the input_shape in DL model to match it\n",
    "                             )\n",
    "    \n",
    "    dqn = DQNAgent(model = model,\n",
    "                   memory = memory,\n",
    "                   policy = policy,\n",
    "                   enable_dueling_network = True,     # Duealing Networks split value and advantage, it help the model learn when to take action and when not to bother,\n",
    "                                                       # it is not so much competing but a modified network\n",
    "                   dueling_type = 'avg',\n",
    "                   nb_actions = actions,              # agent to learn 6 actions ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "                   nb_steps_warmup = 10000            # warmup help the agent get a bit information before kick of the training \n",
    "                  )\n",
    "    \n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd287fb7",
   "metadata": {},
   "source": [
    "# 5. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea39a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   639/100000: episode: 1, duration: 14.332s, episode steps: 639, steps per second:  45, episode reward: 75.000, mean reward:  0.117 [ 0.000, 25.000], mean action: 2.521 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  1227/100000: episode: 2, duration: 12.870s, episode steps: 588, steps per second:  46, episode reward: 135.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.408 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  2048/100000: episode: 3, duration: 17.886s, episode steps: 821, steps per second:  46, episode reward: 225.000, mean reward:  0.274 [ 0.000, 30.000], mean action: 2.317 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  2690/100000: episode: 4, duration: 13.928s, episode steps: 642, steps per second:  46, episode reward: 125.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.313 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  3322/100000: episode: 5, duration: 14.005s, episode steps: 632, steps per second:  45, episode reward: 50.000, mean reward:  0.079 [ 0.000, 15.000], mean action: 2.310 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  3806/100000: episode: 6, duration: 10.629s, episode steps: 484, steps per second:  46, episode reward: 30.000, mean reward:  0.062 [ 0.000, 10.000], mean action: 2.564 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  4499/100000: episode: 7, duration: 15.119s, episode steps: 693, steps per second:  46, episode reward: 135.000, mean reward:  0.195 [ 0.000, 30.000], mean action: 2.325 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  5184/100000: episode: 8, duration: 14.861s, episode steps: 685, steps per second:  46, episode reward: 245.000, mean reward:  0.358 [ 0.000, 30.000], mean action: 2.003 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  6140/100000: episode: 9, duration: 20.784s, episode steps: 956, steps per second:  46, episode reward: 250.000, mean reward:  0.262 [ 0.000, 30.000], mean action: 2.282 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  6793/100000: episode: 10, duration: 14.229s, episode steps: 653, steps per second:  46, episode reward: 180.000, mean reward:  0.276 [ 0.000, 30.000], mean action: 2.404 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  7459/100000: episode: 11, duration: 14.484s, episode steps: 666, steps per second:  46, episode reward: 100.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 1.685 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  8088/100000: episode: 12, duration: 13.708s, episode steps: 629, steps per second:  46, episode reward: 85.000, mean reward:  0.135 [ 0.000, 30.000], mean action: 2.207 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  8480/100000: episode: 13, duration: 8.546s, episode steps: 392, steps per second:  46, episode reward: 75.000, mean reward:  0.191 [ 0.000, 20.000], mean action: 1.921 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  9157/100000: episode: 14, duration: 14.723s, episode steps: 677, steps per second:  46, episode reward: 65.000, mean reward:  0.096 [ 0.000, 15.000], mean action: 1.504 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  9811/100000: episode: 15, duration: 14.226s, episode steps: 654, steps per second:  46, episode reward: 100.000, mean reward:  0.153 [ 0.000, 20.000], mean action: 1.729 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10663/100000: episode: 16, duration: 569.514s, episode steps: 852, steps per second:   1, episode reward: 65.000, mean reward:  0.076 [ 0.000, 25.000], mean action: 2.390 [0.000, 5.000],  loss: 1.378905, mean_q: 5.869412, mean_eps: 0.100000\n",
      " 11147/100000: episode: 17, duration: 403.949s, episode steps: 484, steps per second:   1, episode reward: 75.000, mean reward:  0.155 [ 0.000, 20.000], mean action: 2.409 [0.000, 5.000],  loss: 0.461618, mean_q: 5.665067, mean_eps: 0.100000\n",
      " 11983/100000: episode: 18, duration: 696.030s, episode steps: 836, steps per second:   1, episode reward: 230.000, mean reward:  0.275 [ 0.000, 30.000], mean action: 3.126 [0.000, 5.000],  loss: 0.650514, mean_q: 5.100923, mean_eps: 0.100000\n",
      " 12461/100000: episode: 19, duration: 395.450s, episode steps: 478, steps per second:   1, episode reward: 105.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.513 [0.000, 5.000],  loss: 0.736310, mean_q: 5.237313, mean_eps: 0.100000\n",
      " 13670/100000: episode: 20, duration: 1010.837s, episode steps: 1209, steps per second:   1, episode reward: 405.000, mean reward:  0.335 [ 0.000, 200.000], mean action: 2.461 [0.000, 5.000],  loss: 4.162957, mean_q: 5.492758, mean_eps: 0.100000\n",
      " 14361/100000: episode: 21, duration: 570.069s, episode steps: 691, steps per second:   1, episode reward: 30.000, mean reward:  0.043 [ 0.000, 10.000], mean action: 2.398 [0.000, 5.000],  loss: 0.634332, mean_q: 5.055111, mean_eps: 0.100000\n",
      " 15169/100000: episode: 22, duration: 666.701s, episode steps: 808, steps per second:   1, episode reward: 185.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.251 [0.000, 5.000],  loss: 0.713799, mean_q: 5.171555, mean_eps: 0.100000\n",
      " 16001/100000: episode: 23, duration: 686.542s, episode steps: 832, steps per second:   1, episode reward: 180.000, mean reward:  0.216 [ 0.000, 30.000], mean action: 2.264 [0.000, 5.000],  loss: 0.352307, mean_q: 5.288660, mean_eps: 0.100000\n",
      " 16854/100000: episode: 24, duration: 723.944s, episode steps: 853, steps per second:   1, episode reward: 210.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.649 [0.000, 5.000],  loss: 0.353228, mean_q: 4.888262, mean_eps: 0.100000\n",
      " 17813/100000: episode: 25, duration: 820.432s, episode steps: 959, steps per second:   1, episode reward: 260.000, mean reward:  0.271 [ 0.000, 30.000], mean action: 3.038 [0.000, 5.000],  loss: 0.391648, mean_q: 5.144691, mean_eps: 0.100000\n",
      " 18457/100000: episode: 26, duration: 582.508s, episode steps: 644, steps per second:   1, episode reward: 80.000, mean reward:  0.124 [ 0.000, 25.000], mean action: 2.693 [0.000, 5.000],  loss: 0.191673, mean_q: 5.087424, mean_eps: 0.100000\n",
      " 19091/100000: episode: 27, duration: 573.355s, episode steps: 634, steps per second:   1, episode reward: 135.000, mean reward:  0.213 [ 0.000, 30.000], mean action: 2.057 [0.000, 5.000],  loss: 0.398891, mean_q: 5.041421, mean_eps: 0.100000\n",
      " 19648/100000: episode: 28, duration: 498.010s, episode steps: 557, steps per second:   1, episode reward: 80.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 1.883 [0.000, 5.000],  loss: 0.283847, mean_q: 5.065940, mean_eps: 0.100000\n",
      " 20632/100000: episode: 29, duration: 882.886s, episode steps: 984, steps per second:   1, episode reward: 200.000, mean reward:  0.203 [ 0.000, 25.000], mean action: 2.763 [0.000, 5.000],  loss: 0.352839, mean_q: 5.386457, mean_eps: 0.100000\n",
      " 21307/100000: episode: 30, duration: 634.724s, episode steps: 675, steps per second:   1, episode reward: 105.000, mean reward:  0.156 [ 0.000, 25.000], mean action: 3.047 [0.000, 5.000],  loss: 0.235826, mean_q: 5.680508, mean_eps: 0.100000\n",
      " 21953/100000: episode: 31, duration: 615.064s, episode steps: 646, steps per second:   1, episode reward: 135.000, mean reward:  0.209 [ 0.000, 30.000], mean action: 2.184 [0.000, 5.000],  loss: 0.327175, mean_q: 5.337884, mean_eps: 0.100000\n",
      " 23090/100000: episode: 32, duration: 1052.346s, episode steps: 1137, steps per second:   1, episode reward: 315.000, mean reward:  0.277 [ 0.000, 30.000], mean action: 2.741 [0.000, 5.000],  loss: 0.313725, mean_q: 5.368422, mean_eps: 0.100000\n",
      " 23476/100000: episode: 33, duration: 370.858s, episode steps: 386, steps per second:   1, episode reward: 65.000, mean reward:  0.168 [ 0.000, 20.000], mean action: 2.699 [0.000, 5.000],  loss: 0.118734, mean_q: 5.513231, mean_eps: 0.100000\n",
      " 23833/100000: episode: 34, duration: 341.034s, episode steps: 357, steps per second:   1, episode reward: 75.000, mean reward:  0.210 [ 0.000, 25.000], mean action: 2.468 [0.000, 5.000],  loss: 0.239273, mean_q: 5.082445, mean_eps: 0.100000\n",
      " 24820/100000: episode: 35, duration: 887.129s, episode steps: 987, steps per second:   1, episode reward: 240.000, mean reward:  0.243 [ 0.000, 30.000], mean action: 2.200 [0.000, 5.000],  loss: 0.359097, mean_q: 5.502385, mean_eps: 0.100000\n",
      " 25557/100000: episode: 36, duration: 644.284s, episode steps: 737, steps per second:   1, episode reward: 135.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.635 [0.000, 5.000],  loss: 0.250901, mean_q: 5.833831, mean_eps: 0.100000\n",
      " 26255/100000: episode: 37, duration: 615.281s, episode steps: 698, steps per second:   1, episode reward: 120.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.162 [0.000, 5.000],  loss: 0.219615, mean_q: 5.788247, mean_eps: 0.100000\n",
      " 26912/100000: episode: 38, duration: 579.792s, episode steps: 657, steps per second:   1, episode reward: 120.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 1.872 [0.000, 5.000],  loss: 0.224163, mean_q: 5.481719, mean_eps: 0.100000\n",
      " 27923/100000: episode: 39, duration: 871.596s, episode steps: 1011, steps per second:   1, episode reward: 265.000, mean reward:  0.262 [ 0.000, 30.000], mean action: 2.285 [0.000, 5.000],  loss: 0.421570, mean_q: 5.662385, mean_eps: 0.100000\n",
      " 28868/100000: episode: 40, duration: 817.772s, episode steps: 945, steps per second:   1, episode reward: 445.000, mean reward:  0.471 [ 0.000, 200.000], mean action: 2.790 [0.000, 5.000],  loss: 3.236199, mean_q: 6.154782, mean_eps: 0.100000\n",
      " 29670/100000: episode: 41, duration: 678.698s, episode steps: 802, steps per second:   1, episode reward: 185.000, mean reward:  0.231 [ 0.000, 30.000], mean action: 3.349 [0.000, 5.000],  loss: 0.386185, mean_q: 6.252954, mean_eps: 0.100000\n",
      " 30156/100000: episode: 42, duration: 402.108s, episode steps: 486, steps per second:   1, episode reward: 50.000, mean reward:  0.103 [ 0.000, 15.000], mean action: 2.848 [0.000, 5.000],  loss: 0.457937, mean_q: 5.702576, mean_eps: 0.100000\n",
      " 30917/100000: episode: 43, duration: 631.270s, episode steps: 761, steps per second:   1, episode reward: 235.000, mean reward:  0.309 [ 0.000, 30.000], mean action: 2.905 [0.000, 5.000],  loss: 0.437989, mean_q: 5.683847, mean_eps: 0.100000\n",
      " 31437/100000: episode: 44, duration: 431.074s, episode steps: 520, steps per second:   1, episode reward: 95.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 3.277 [0.000, 5.000],  loss: 0.265587, mean_q: 5.864161, mean_eps: 0.100000\n",
      " 31927/100000: episode: 45, duration: 406.245s, episode steps: 490, steps per second:   1, episode reward: 45.000, mean reward:  0.092 [ 0.000, 15.000], mean action: 2.786 [0.000, 5.000],  loss: 0.124777, mean_q: 5.591795, mean_eps: 0.100000\n",
      " 32825/100000: episode: 46, duration: 743.988s, episode steps: 898, steps per second:   1, episode reward: 185.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.788 [0.000, 5.000],  loss: 0.317729, mean_q: 5.317323, mean_eps: 0.100000\n",
      " 33790/100000: episode: 47, duration: 797.715s, episode steps: 965, steps per second:   1, episode reward: 325.000, mean reward:  0.337 [ 0.000, 30.000], mean action: 2.415 [0.000, 5.000],  loss: 0.536924, mean_q: 6.493216, mean_eps: 0.100000\n",
      " 34713/100000: episode: 48, duration: 765.058s, episode steps: 923, steps per second:   1, episode reward: 120.000, mean reward:  0.130 [ 0.000, 30.000], mean action: 2.311 [0.000, 5.000],  loss: 0.278681, mean_q: 6.448273, mean_eps: 0.100000\n",
      " 35361/100000: episode: 49, duration: 537.559s, episode steps: 648, steps per second:   1, episode reward: 135.000, mean reward:  0.208 [ 0.000, 30.000], mean action: 2.778 [0.000, 5.000],  loss: 0.368657, mean_q: 6.201849, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36603/100000: episode: 50, duration: 1030.170s, episode steps: 1242, steps per second:   1, episode reward: 350.000, mean reward:  0.282 [ 0.000, 30.000], mean action: 2.898 [0.000, 5.000],  loss: 0.357804, mean_q: 6.037867, mean_eps: 0.100000\n",
      " 37103/100000: episode: 51, duration: 414.974s, episode steps: 500, steps per second:   1, episode reward: 95.000, mean reward:  0.190 [ 0.000, 20.000], mean action: 2.972 [0.000, 5.000],  loss: 0.280628, mean_q: 5.681791, mean_eps: 0.100000\n",
      " 37607/100000: episode: 52, duration: 418.699s, episode steps: 504, steps per second:   1, episode reward: 45.000, mean reward:  0.089 [ 0.000, 15.000], mean action: 2.683 [0.000, 5.000],  loss: 0.138025, mean_q: 5.459441, mean_eps: 0.100000\n",
      " 38124/100000: episode: 53, duration: 429.497s, episode steps: 517, steps per second:   1, episode reward: 110.000, mean reward:  0.213 [ 0.000, 30.000], mean action: 2.692 [0.000, 5.000],  loss: 0.192133, mean_q: 5.598152, mean_eps: 0.100000\n",
      " 39088/100000: episode: 54, duration: 800.146s, episode steps: 964, steps per second:   1, episode reward: 390.000, mean reward:  0.405 [ 0.000, 30.000], mean action: 3.132 [0.000, 5.000],  loss: 0.677600, mean_q: 6.005615, mean_eps: 0.100000\n",
      " 39634/100000: episode: 55, duration: 453.155s, episode steps: 546, steps per second:   1, episode reward: 65.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.826 [0.000, 5.000],  loss: 0.305545, mean_q: 6.263338, mean_eps: 0.100000\n",
      " 40310/100000: episode: 56, duration: 560.538s, episode steps: 676, steps per second:   1, episode reward: 135.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.448 [0.000, 5.000],  loss: 1.293512, mean_q: 6.637223, mean_eps: 0.100000\n",
      " 40880/100000: episode: 57, duration: 472.420s, episode steps: 570, steps per second:   1, episode reward: 135.000, mean reward:  0.237 [ 0.000, 30.000], mean action: 2.225 [0.000, 5.000],  loss: 0.430559, mean_q: 6.860769, mean_eps: 0.100000\n",
      " 41663/100000: episode: 58, duration: 650.251s, episode steps: 783, steps per second:   1, episode reward: 220.000, mean reward:  0.281 [ 0.000, 30.000], mean action: 2.054 [0.000, 5.000],  loss: 0.397282, mean_q: 6.800901, mean_eps: 0.100000\n",
      " 42080/100000: episode: 59, duration: 345.803s, episode steps: 417, steps per second:   1, episode reward: 80.000, mean reward:  0.192 [ 0.000, 25.000], mean action: 1.892 [0.000, 5.000],  loss: 0.396884, mean_q: 6.862304, mean_eps: 0.100000\n",
      " 42620/100000: episode: 60, duration: 448.530s, episode steps: 540, steps per second:   1, episode reward: 135.000, mean reward:  0.250 [ 0.000, 30.000], mean action: 2.163 [0.000, 5.000],  loss: 0.316781, mean_q: 6.158396, mean_eps: 0.100000\n",
      " 43476/100000: episode: 61, duration: 710.583s, episode steps: 856, steps per second:   1, episode reward: 55.000, mean reward:  0.064 [ 0.000, 10.000], mean action: 2.110 [0.000, 5.000],  loss: 0.155456, mean_q: 6.241158, mean_eps: 0.100000\n",
      " 44051/100000: episode: 62, duration: 477.928s, episode steps: 575, steps per second:   1, episode reward: 55.000, mean reward:  0.096 [ 0.000, 20.000], mean action: 2.513 [0.000, 5.000],  loss: 0.156844, mean_q: 6.520842, mean_eps: 0.100000\n",
      " 44425/100000: episode: 63, duration: 311.162s, episode steps: 374, steps per second:   1, episode reward: 45.000, mean reward:  0.120 [ 0.000, 15.000], mean action: 2.826 [0.000, 5.000],  loss: 0.159922, mean_q: 6.185110, mean_eps: 0.100000\n",
      " 45099/100000: episode: 64, duration: 559.503s, episode steps: 674, steps per second:   1, episode reward: 110.000, mean reward:  0.163 [ 0.000, 30.000], mean action: 2.335 [0.000, 5.000],  loss: 0.233138, mean_q: 6.153726, mean_eps: 0.100000\n",
      " 45731/100000: episode: 65, duration: 524.126s, episode steps: 632, steps per second:   1, episode reward: 100.000, mean reward:  0.158 [ 0.000, 20.000], mean action: 2.233 [0.000, 5.000],  loss: 0.211474, mean_q: 6.420263, mean_eps: 0.100000\n",
      " 46369/100000: episode: 66, duration: 529.472s, episode steps: 638, steps per second:   1, episode reward: 135.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.041 [0.000, 5.000],  loss: 0.333421, mean_q: 6.639420, mean_eps: 0.100000\n",
      " 46769/100000: episode: 67, duration: 332.654s, episode steps: 400, steps per second:   1, episode reward: 30.000, mean reward:  0.075 [ 0.000, 15.000], mean action: 2.425 [0.000, 5.000],  loss: 0.189985, mean_q: 6.370754, mean_eps: 0.100000\n",
      " 47172/100000: episode: 68, duration: 335.055s, episode steps: 403, steps per second:   1, episode reward: 50.000, mean reward:  0.124 [ 0.000, 20.000], mean action: 2.888 [0.000, 5.000],  loss: 0.198739, mean_q: 5.907980, mean_eps: 0.100000\n",
      " 48123/100000: episode: 69, duration: 789.745s, episode steps: 951, steps per second:   1, episode reward: 200.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.669 [0.000, 5.000],  loss: 0.255917, mean_q: 6.139018, mean_eps: 0.100000\n",
      " 48951/100000: episode: 70, duration: 687.527s, episode steps: 828, steps per second:   1, episode reward: 435.000, mean reward:  0.525 [ 0.000, 200.000], mean action: 2.021 [0.000, 5.000],  loss: 2.335740, mean_q: 6.860560, mean_eps: 0.100000\n",
      " 49659/100000: episode: 71, duration: 588.114s, episode steps: 708, steps per second:   1, episode reward: 160.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 1.665 [0.000, 5.000],  loss: 1.522704, mean_q: 7.219938, mean_eps: 0.100000\n",
      " 50630/100000: episode: 72, duration: 806.498s, episode steps: 971, steps per second:   1, episode reward: 260.000, mean reward:  0.268 [ 0.000, 30.000], mean action: 2.613 [0.000, 5.000],  loss: 0.661149, mean_q: 7.368491, mean_eps: 0.100000\n",
      " 51404/100000: episode: 73, duration: 642.009s, episode steps: 774, steps per second:   1, episode reward: 110.000, mean reward:  0.142 [ 0.000, 30.000], mean action: 2.147 [0.000, 5.000],  loss: 0.430336, mean_q: 7.634827, mean_eps: 0.100000\n",
      " 52026/100000: episode: 74, duration: 516.881s, episode steps: 622, steps per second:   1, episode reward: 50.000, mean reward:  0.080 [ 0.000, 20.000], mean action: 2.227 [0.000, 5.000],  loss: 0.181553, mean_q: 7.291030, mean_eps: 0.100000\n",
      " 52675/100000: episode: 75, duration: 539.450s, episode steps: 649, steps per second:   1, episode reward: 110.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.669 [0.000, 5.000],  loss: 0.374138, mean_q: 7.251546, mean_eps: 0.100000\n",
      " 53477/100000: episode: 76, duration: 667.086s, episode steps: 802, steps per second:   1, episode reward: 155.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.597 [0.000, 5.000],  loss: 0.422242, mean_q: 7.868194, mean_eps: 0.100000\n",
      " 54397/100000: episode: 77, duration: 765.055s, episode steps: 920, steps per second:   1, episode reward: 270.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 3.005 [0.000, 5.000],  loss: 0.509887, mean_q: 7.847300, mean_eps: 0.100000\n",
      " 55670/100000: episode: 78, duration: 1056.665s, episode steps: 1273, steps per second:   1, episode reward: 485.000, mean reward:  0.381 [ 0.000, 200.000], mean action: 3.229 [0.000, 5.000],  loss: 1.834384, mean_q: 8.322119, mean_eps: 0.100000\n",
      " 56445/100000: episode: 79, duration: 644.089s, episode steps: 775, steps per second:   1, episode reward: 260.000, mean reward:  0.335 [ 0.000, 30.000], mean action: 3.459 [0.000, 5.000],  loss: 0.532575, mean_q: 7.557382, mean_eps: 0.100000\n",
      " 57076/100000: episode: 80, duration: 524.905s, episode steps: 631, steps per second:   1, episode reward: 65.000, mean reward:  0.103 [ 0.000, 30.000], mean action: 2.797 [0.000, 5.000],  loss: 0.333313, mean_q: 6.898437, mean_eps: 0.100000\n",
      " 57847/100000: episode: 81, duration: 641.016s, episode steps: 771, steps per second:   1, episode reward: 130.000, mean reward:  0.169 [ 0.000, 20.000], mean action: 2.231 [0.000, 5.000],  loss: 0.263220, mean_q: 6.954530, mean_eps: 0.100000\n",
      " 58762/100000: episode: 82, duration: 760.282s, episode steps: 915, steps per second:   1, episode reward: 150.000, mean reward:  0.164 [ 0.000, 25.000], mean action: 2.368 [0.000, 5.000],  loss: 0.323496, mean_q: 7.359471, mean_eps: 0.100000\n",
      " 59394/100000: episode: 83, duration: 525.215s, episode steps: 632, steps per second:   1, episode reward: 155.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 2.044 [0.000, 5.000],  loss: 0.407581, mean_q: 7.309616, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59920/100000: episode: 84, duration: 436.974s, episode steps: 526, steps per second:   1, episode reward: 75.000, mean reward:  0.143 [ 0.000, 25.000], mean action: 2.211 [0.000, 5.000],  loss: 0.353090, mean_q: 7.667947, mean_eps: 0.100000\n",
      " 60754/100000: episode: 85, duration: 699.374s, episode steps: 834, steps per second:   1, episode reward: 210.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.535 [0.000, 5.000],  loss: 0.836739, mean_q: 8.080201, mean_eps: 0.100000\n",
      " 61170/100000: episode: 86, duration: 355.252s, episode steps: 416, steps per second:   1, episode reward: 65.000, mean reward:  0.156 [ 0.000, 20.000], mean action: 2.399 [0.000, 5.000],  loss: 0.271210, mean_q: 7.801833, mean_eps: 0.100000\n",
      " 61868/100000: episode: 87, duration: 596.259s, episode steps: 698, steps per second:   1, episode reward: 125.000, mean reward:  0.179 [ 0.000, 25.000], mean action: 2.021 [0.000, 5.000],  loss: 0.340848, mean_q: 7.728512, mean_eps: 0.100000\n",
      " 62799/100000: episode: 88, duration: 789.737s, episode steps: 931, steps per second:   1, episode reward: 120.000, mean reward:  0.129 [ 0.000, 30.000], mean action: 1.634 [0.000, 5.000],  loss: 0.352784, mean_q: 8.314430, mean_eps: 0.100000\n",
      " 63292/100000: episode: 89, duration: 423.272s, episode steps: 493, steps per second:   1, episode reward: 70.000, mean reward:  0.142 [ 0.000, 20.000], mean action: 1.728 [0.000, 5.000],  loss: 0.178570, mean_q: 7.954252, mean_eps: 0.100000\n",
      " 64105/100000: episode: 90, duration: 697.180s, episode steps: 813, steps per second:   1, episode reward: 240.000, mean reward:  0.295 [ 0.000, 30.000], mean action: 1.940 [0.000, 5.000],  loss: 0.411613, mean_q: 7.247809, mean_eps: 0.100000\n",
      " 64763/100000: episode: 91, duration: 562.778s, episode steps: 658, steps per second:   1, episode reward: 135.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 1.921 [0.000, 5.000],  loss: 0.331789, mean_q: 7.818736, mean_eps: 0.100000\n",
      " 65324/100000: episode: 92, duration: 477.336s, episode steps: 561, steps per second:   1, episode reward: 75.000, mean reward:  0.134 [ 0.000, 30.000], mean action: 1.991 [0.000, 5.000],  loss: 0.307599, mean_q: 7.711185, mean_eps: 0.100000\n",
      " 65693/100000: episode: 93, duration: 315.975s, episode steps: 369, steps per second:   1, episode reward: 50.000, mean reward:  0.136 [ 0.000, 15.000], mean action: 2.347 [0.000, 5.000],  loss: 0.357732, mean_q: 7.441197, mean_eps: 0.100000\n",
      " 66051/100000: episode: 94, duration: 309.093s, episode steps: 358, steps per second:   1, episode reward: 25.000, mean reward:  0.070 [ 0.000, 10.000], mean action: 2.528 [0.000, 5.000],  loss: 0.169903, mean_q: 7.165758, mean_eps: 0.100000\n",
      " 66900/100000: episode: 95, duration: 709.132s, episode steps: 849, steps per second:   1, episode reward: 170.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.490 [0.000, 5.000],  loss: 0.398600, mean_q: 7.604479, mean_eps: 0.100000\n",
      " 67412/100000: episode: 96, duration: 426.805s, episode steps: 512, steps per second:   1, episode reward: 125.000, mean reward:  0.244 [ 0.000, 25.000], mean action: 2.582 [0.000, 5.000],  loss: 0.309419, mean_q: 8.483704, mean_eps: 0.100000\n",
      " 67811/100000: episode: 97, duration: 332.392s, episode steps: 399, steps per second:   1, episode reward: 50.000, mean reward:  0.125 [ 0.000, 20.000], mean action: 2.098 [0.000, 5.000],  loss: 0.257819, mean_q: 8.014283, mean_eps: 0.100000\n",
      " 68488/100000: episode: 98, duration: 562.723s, episode steps: 677, steps per second:   1, episode reward: 355.000, mean reward:  0.524 [ 0.000, 200.000], mean action: 3.115 [0.000, 5.000],  loss: 3.155843, mean_q: 8.196211, mean_eps: 0.100000\n",
      " 69137/100000: episode: 99, duration: 539.746s, episode steps: 649, steps per second:   1, episode reward: 175.000, mean reward:  0.270 [ 0.000, 30.000], mean action: 3.427 [0.000, 5.000],  loss: 0.596491, mean_q: 8.033327, mean_eps: 0.100000\n",
      " 69791/100000: episode: 100, duration: 545.138s, episode steps: 654, steps per second:   1, episode reward: 150.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 3.370 [0.000, 5.000],  loss: 0.407337, mean_q: 6.573489, mean_eps: 0.100000\n",
      " 70198/100000: episode: 101, duration: 338.440s, episode steps: 407, steps per second:   1, episode reward: 45.000, mean reward:  0.111 [ 0.000, 15.000], mean action: 3.310 [0.000, 5.000],  loss: 0.721203, mean_q: 6.754981, mean_eps: 0.100000\n",
      " 70529/100000: episode: 102, duration: 275.869s, episode steps: 331, steps per second:   1, episode reward: 50.000, mean reward:  0.151 [ 0.000, 20.000], mean action: 2.580 [0.000, 5.000],  loss: 0.242636, mean_q: 7.193165, mean_eps: 0.100000\n",
      " 71191/100000: episode: 103, duration: 550.081s, episode steps: 662, steps per second:   1, episode reward: 155.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.934 [0.000, 5.000],  loss: 0.296790, mean_q: 7.011992, mean_eps: 0.100000\n",
      " 71865/100000: episode: 104, duration: 560.240s, episode steps: 674, steps per second:   1, episode reward: 210.000, mean reward:  0.312 [ 0.000, 30.000], mean action: 2.233 [0.000, 5.000],  loss: 0.649684, mean_q: 7.176294, mean_eps: 0.100000\n",
      " 72783/100000: episode: 105, duration: 761.398s, episode steps: 918, steps per second:   1, episode reward: 335.000, mean reward:  0.365 [ 0.000, 200.000], mean action: 1.913 [0.000, 5.000],  loss: 2.982972, mean_q: 8.489576, mean_eps: 0.100000\n",
      " 73563/100000: episode: 106, duration: 650.885s, episode steps: 780, steps per second:   1, episode reward: 305.000, mean reward:  0.391 [ 0.000, 200.000], mean action: 1.781 [0.000, 5.000],  loss: 4.599913, mean_q: 8.258150, mean_eps: 0.100000\n",
      " 74328/100000: episode: 107, duration: 637.086s, episode steps: 765, steps per second:   1, episode reward: 120.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 1.795 [0.000, 5.000],  loss: 16.891365, mean_q: 8.215168, mean_eps: 0.100000\n",
      " 75135/100000: episode: 108, duration: 671.871s, episode steps: 807, steps per second:   1, episode reward: 155.000, mean reward:  0.192 [ 0.000, 30.000], mean action: 1.784 [0.000, 5.000],  loss: 7.632985, mean_q: 8.112005, mean_eps: 0.100000\n",
      " 75646/100000: episode: 109, duration: 424.838s, episode steps: 511, steps per second:   1, episode reward: 50.000, mean reward:  0.098 [ 0.000, 20.000], mean action: 2.763 [0.000, 5.000],  loss: 0.397971, mean_q: 8.145155, mean_eps: 0.100000\n",
      " 77074/100000: episode: 110, duration: 1186.174s, episode steps: 1428, steps per second:   1, episode reward: 350.000, mean reward:  0.245 [ 0.000, 30.000], mean action: 1.954 [0.000, 5.000],  loss: 0.599830, mean_q: 7.526476, mean_eps: 0.100000\n",
      " 77678/100000: episode: 111, duration: 502.691s, episode steps: 604, steps per second:   1, episode reward: 115.000, mean reward:  0.190 [ 0.000, 30.000], mean action: 2.863 [0.000, 5.000],  loss: 0.517558, mean_q: 6.253970, mean_eps: 0.100000\n",
      " 78435/100000: episode: 112, duration: 629.392s, episode steps: 757, steps per second:   1, episode reward: 315.000, mean reward:  0.416 [ 0.000, 200.000], mean action: 2.828 [0.000, 5.000],  loss: 0.372740, mean_q: 7.204479, mean_eps: 0.100000\n",
      " 79261/100000: episode: 113, duration: 686.289s, episode steps: 826, steps per second:   1, episode reward: 210.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 2.303 [0.000, 5.000],  loss: 4.384086, mean_q: 9.446330, mean_eps: 0.100000\n",
      " 79917/100000: episode: 114, duration: 546.228s, episode steps: 656, steps per second:   1, episode reward: 110.000, mean reward:  0.168 [ 0.000, 30.000], mean action: 1.857 [0.000, 5.000],  loss: 0.840435, mean_q: 8.468964, mean_eps: 0.100000\n",
      " 80723/100000: episode: 115, duration: 667.419s, episode steps: 806, steps per second:   1, episode reward: 180.000, mean reward:  0.223 [ 0.000, 30.000], mean action: 1.627 [0.000, 5.000],  loss: 1.397582, mean_q: 9.454183, mean_eps: 0.100000\n",
      " 81600/100000: episode: 116, duration: 736.704s, episode steps: 877, steps per second:   1, episode reward: 180.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.552 [0.000, 5.000],  loss: 0.876795, mean_q: 9.524438, mean_eps: 0.100000\n",
      " 82314/100000: episode: 117, duration: 602.113s, episode steps: 714, steps per second:   1, episode reward: 155.000, mean reward:  0.217 [ 0.000, 30.000], mean action: 3.014 [0.000, 5.000],  loss: 0.775565, mean_q: 9.618460, mean_eps: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83124/100000: episode: 118, duration: 686.210s, episode steps: 810, steps per second:   1, episode reward: 165.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.995 [0.000, 5.000],  loss: 0.881244, mean_q: 10.636079, mean_eps: 0.100000\n",
      " 84072/100000: episode: 119, duration: 798.872s, episode steps: 948, steps per second:   1, episode reward: 245.000, mean reward:  0.258 [ 0.000, 30.000], mean action: 2.890 [0.000, 5.000],  loss: 0.726823, mean_q: 9.786154, mean_eps: 0.100000\n",
      " 84765/100000: episode: 120, duration: 584.198s, episode steps: 693, steps per second:   1, episode reward: 120.000, mean reward:  0.173 [ 0.000, 30.000], mean action: 2.778 [0.000, 5.000],  loss: 0.502907, mean_q: 9.694742, mean_eps: 0.100000\n",
      " 85148/100000: episode: 121, duration: 323.466s, episode steps: 383, steps per second:   1, episode reward: 65.000, mean reward:  0.170 [ 0.000, 20.000], mean action: 2.569 [0.000, 5.000],  loss: 0.692597, mean_q: 9.378683, mean_eps: 0.100000\n",
      " 85882/100000: episode: 122, duration: 633.054s, episode steps: 734, steps per second:   1, episode reward: 100.000, mean reward:  0.136 [ 0.000, 25.000], mean action: 2.516 [0.000, 5.000],  loss: 0.561028, mean_q: 9.032850, mean_eps: 0.100000\n",
      " 87480/100000: episode: 123, duration: 1364.793s, episode steps: 1598, steps per second:   1, episode reward: 455.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 1.934 [0.000, 5.000],  loss: 0.911798, mean_q: 8.613832, mean_eps: 0.100000\n",
      " 88071/100000: episode: 124, duration: 509.093s, episode steps: 591, steps per second:   1, episode reward: 110.000, mean reward:  0.186 [ 0.000, 20.000], mean action: 2.964 [0.000, 5.000],  loss: 1.807992, mean_q: 6.976389, mean_eps: 0.100000\n",
      " 88962/100000: episode: 125, duration: 762.280s, episode steps: 891, steps per second:   1, episode reward: 125.000, mean reward:  0.140 [ 0.000, 25.000], mean action: 2.478 [0.000, 5.000],  loss: 0.937558, mean_q: 9.157477, mean_eps: 0.100000\n",
      " 89910/100000: episode: 126, duration: 790.937s, episode steps: 948, steps per second:   1, episode reward: 245.000, mean reward:  0.258 [ 0.000, 30.000], mean action: 2.722 [0.000, 5.000],  loss: 0.679023, mean_q: 9.678237, mean_eps: 0.100000\n",
      " 90946/100000: episode: 127, duration: 869.486s, episode steps: 1036, steps per second:   1, episode reward: 435.000, mean reward:  0.420 [ 0.000, 200.000], mean action: 3.166 [0.000, 5.000],  loss: 3.268347, mean_q: 9.856251, mean_eps: 0.100000\n",
      " 91348/100000: episode: 128, duration: 334.965s, episode steps: 402, steps per second:   1, episode reward: 80.000, mean reward:  0.199 [ 0.000, 25.000], mean action: 2.828 [0.000, 5.000],  loss: 0.849770, mean_q: 8.622326, mean_eps: 0.100000\n",
      " 92008/100000: episode: 129, duration: 551.236s, episode steps: 660, steps per second:   1, episode reward: 135.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 2.641 [0.000, 5.000],  loss: 0.477602, mean_q: 8.462941, mean_eps: 0.100000\n",
      " 92359/100000: episode: 130, duration: 293.303s, episode steps: 351, steps per second:   1, episode reward: 50.000, mean reward:  0.142 [ 0.000, 15.000], mean action: 1.895 [0.000, 5.000],  loss: 0.396490, mean_q: 9.329588, mean_eps: 0.100000\n",
      " 92989/100000: episode: 131, duration: 536.620s, episode steps: 630, steps per second:   1, episode reward: 45.000, mean reward:  0.071 [ 0.000, 30.000], mean action: 2.022 [0.000, 5.000],  loss: 0.350307, mean_q: 9.123538, mean_eps: 0.100000\n",
      " 93828/100000: episode: 132, duration: 705.516s, episode steps: 839, steps per second:   1, episode reward: 170.000, mean reward:  0.203 [ 0.000, 30.000], mean action: 1.884 [0.000, 5.000],  loss: 0.566179, mean_q: 9.128846, mean_eps: 0.100000\n",
      " 94499/100000: episode: 133, duration: 559.625s, episode steps: 671, steps per second:   1, episode reward: 115.000, mean reward:  0.171 [ 0.000, 20.000], mean action: 1.697 [0.000, 5.000],  loss: 0.304450, mean_q: 9.108556, mean_eps: 0.100000\n",
      " 94852/100000: episode: 134, duration: 294.542s, episode steps: 353, steps per second:   1, episode reward: 65.000, mean reward:  0.184 [ 0.000, 20.000], mean action: 1.422 [0.000, 5.000],  loss: 0.335897, mean_q: 8.367757, mean_eps: 0.100000\n",
      " 95583/100000: episode: 135, duration: 609.396s, episode steps: 731, steps per second:   1, episode reward: 155.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 1.680 [0.000, 5.000],  loss: 0.523855, mean_q: 8.728360, mean_eps: 0.100000\n",
      " 96027/100000: episode: 136, duration: 371.159s, episode steps: 444, steps per second:   1, episode reward: 20.000, mean reward:  0.045 [ 0.000, 10.000], mean action: 1.745 [0.000, 5.000],  loss: 0.302337, mean_q: 9.809157, mean_eps: 0.100000\n",
      " 96559/100000: episode: 137, duration: 445.491s, episode steps: 532, steps per second:   1, episode reward: 60.000, mean reward:  0.113 [ 0.000, 25.000], mean action: 2.045 [0.000, 5.000],  loss: 0.326385, mean_q: 9.853450, mean_eps: 0.100000\n",
      " 97511/100000: episode: 138, duration: 800.613s, episode steps: 952, steps per second:   1, episode reward: 200.000, mean reward:  0.210 [ 0.000, 30.000], mean action: 2.285 [0.000, 5.000],  loss: 0.432033, mean_q: 9.970503, mean_eps: 0.100000\n",
      " 97894/100000: episode: 139, duration: 321.922s, episode steps: 383, steps per second:   1, episode reward: 40.000, mean reward:  0.104 [ 0.000, 15.000], mean action: 1.992 [0.000, 5.000],  loss: 0.353928, mean_q: 9.263560, mean_eps: 0.100000\n",
      " 98853/100000: episode: 140, duration: 800.755s, episode steps: 959, steps per second:   1, episode reward: 285.000, mean reward:  0.297 [ 0.000, 30.000], mean action: 1.927 [0.000, 5.000],  loss: 0.520569, mean_q: 8.409521, mean_eps: 0.100000\n",
      " 99498/100000: episode: 141, duration: 538.911s, episode steps: 645, steps per second:   1, episode reward: 120.000, mean reward:  0.186 [ 0.000, 25.000], mean action: 2.234 [0.000, 5.000],  loss: 0.464788, mean_q: 7.852051, mean_eps: 0.100000\n",
      " 99870/100000: episode: 142, duration: 315.797s, episode steps: 372, steps per second:   1, episode reward: 50.000, mean reward:  0.134 [ 0.000, 20.000], mean action: 2.199 [0.000, 5.000],  loss: 0.314688, mean_q: 8.194736, mean_eps: 0.100000\n",
      "done, took 76199.280 seconds\n",
      "CPU times: user 3d 6h 25min 41s, sys: 3h 41min 49s, total: 3d 10h 7min 30s\n",
      "Wall time: 21h 10min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3f96d6940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dqn = build_agent(model, actions)   # setup an agent\n",
    "\n",
    "dqn.compile(Adam(lr=1e-4))          # use Adam optimizer \n",
    "\n",
    "dqn.fit(env, nb_steps = 100000, visualize = False, verbose = 2)  # DeepMind Advise to train 10 Million to 40 Million steps as state of the art model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e54584",
   "metadata": {},
   "source": [
    "Time spend to traing the agent with 100k timesteps by CPU: 21hrs 10mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a6fc8",
   "metadata": {},
   "source": [
    "# 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641a8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('Savedweights/SpaceInvaders_dqn_weights_100ktimesteps.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd527b2",
   "metadata": {},
   "source": [
    "# 7. Evaluate the Agent's Performance after Training for 100k Timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "015c4d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:37:38.730 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa37fac4c00>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 16:37:38.730 python[43361:1411145] Warning: Expected min height of view: (<NSButton: 0x7fa3b75f4920>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 16:37:38.733 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3b75f74a0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 16:37:38.735 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3800875e0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 625.000, steps: 1356\n",
      "Episode 2: reward: 210.000, steps: 814\n",
      "Episode 3: reward: 145.000, steps: 937\n",
      "Episode 4: reward: 155.000, steps: 806\n",
      "Episode 5: reward: 170.000, steps: 927\n",
      "Episode 6: reward: 240.000, steps: 785\n",
      "Episode 7: reward: 110.000, steps: 704\n",
      "Episode 8: reward: 150.000, steps: 643\n",
      "Episode 9: reward: 105.000, steps: 637\n",
      "Episode 10: reward: 45.000, steps: 406\n",
      "195.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize = True)\n",
    "\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179ab6d",
   "metadata": {},
   "source": [
    "Not much improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225b777",
   "metadata": {},
   "source": [
    "# 8. Video Record and Play the Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2ea0627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env):\n",
    "    \n",
    "    scores = dqn.test(env, nb_episodes = 10, visualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7954c671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 17:06:28.563 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3e7512460>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 17:06:28.564 python[43361:1411145] Warning: Expected min height of view: (<NSButton: 0x7fa351a5ca50>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 17:06:28.566 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351a4bd50>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 17:06:28.569 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa35157e720>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 260.000, steps: 991\n",
      "Episode 2: reward: 120.000, steps: 764\n",
      "Episode 3: reward: 80.000, steps: 384\n",
      "Episode 4: reward: 365.000, steps: 1097\n",
      "Episode 5: reward: 110.000, steps: 671\n",
      "Episode 6: reward: 65.000, steps: 383\n",
      "Episode 7: reward: 185.000, steps: 794\n",
      "Episode 8: reward: 110.000, steps: 678\n",
      "Episode 9: reward: 195.000, steps: 789\n",
      "Episode 10: reward: 215.000, steps: 814\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 125.000, steps: 654\n",
      "Episode 2: reward: 115.000, steps: 682\n",
      "Episode 3: reward: 375.000, steps: 792\n",
      "Episode 4: reward: 110.000, steps: 633\n",
      "Episode 5: reward: 75.000, steps: 556\n",
      "Episode 6: reward: 130.000, steps: 827\n",
      "Episode 7: reward: 485.000, steps: 1393\n",
      "Episode 8: reward: 210.000, steps: 794\n",
      "Episode 9: reward: 55.000, steps: 450\n",
      "Episode 10: reward: 120.000, steps: 663\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 440.000, steps: 1022\n",
      "Episode 2: reward: 320.000, steps: 702\n",
      "Episode 3: reward: 675.000, steps: 1004\n",
      "Episode 4: reward: 210.000, steps: 676\n",
      "Episode 5: reward: 215.000, steps: 817\n",
      "Episode 6: reward: 315.000, steps: 1030\n",
      "Episode 7: reward: 160.000, steps: 642\n",
      "Episode 8: reward: 415.000, steps: 1170\n",
      "Episode 9: reward: 30.000, steps: 410\n",
      "Episode 10: reward: 315.000, steps: 1120\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 285.000, steps: 972\n",
      "Episode 2: reward: 125.000, steps: 663\n",
      "Episode 3: reward: 320.000, steps: 713\n",
      "Episode 4: reward: 410.000, steps: 810\n",
      "Episode 5: reward: 210.000, steps: 1435\n",
      "Episode 6: reward: 210.000, steps: 637\n",
      "Episode 7: reward: 210.000, steps: 828\n",
      "Episode 8: reward: 260.000, steps: 1246\n",
      "Episode 9: reward: 40.000, steps: 626\n",
      "Episode 10: reward: 135.000, steps: 723\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 410.000, steps: 811\n",
      "Episode 2: reward: 120.000, steps: 773\n",
      "Episode 3: reward: 35.000, steps: 362\n",
      "Episode 4: reward: 210.000, steps: 673\n",
      "Episode 5: reward: 115.000, steps: 663\n",
      "Episode 6: reward: 75.000, steps: 527\n",
      "Episode 7: reward: 125.000, steps: 631\n",
      "Episode 8: reward: 110.000, steps: 651\n",
      "Episode 9: reward: 50.000, steps: 382\n",
      "Episode 10: reward: 135.000, steps: 690\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 120.000, steps: 649\n",
      "Episode 2: reward: 160.000, steps: 869\n",
      "Episode 3: reward: 70.000, steps: 755\n",
      "Episode 4: reward: 480.000, steps: 1150\n",
      "Episode 5: reward: 210.000, steps: 806\n",
      "Episode 6: reward: 135.000, steps: 684\n",
      "Episode 7: reward: 125.000, steps: 591\n",
      "Episode 8: reward: 460.000, steps: 943\n",
      "Episode 9: reward: 185.000, steps: 760\n",
      "Episode 10: reward: 185.000, steps: 800\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 95.000, steps: 388\n",
      "Episode 2: reward: 185.000, steps: 921\n",
      "Episode 3: reward: 320.000, steps: 674\n",
      "Episode 4: reward: 580.000, steps: 1249\n",
      "Episode 5: reward: 70.000, steps: 622\n",
      "Episode 6: reward: 30.000, steps: 355\n",
      "Episode 7: reward: 20.000, steps: 385\n",
      "Episode 8: reward: 120.000, steps: 631\n",
      "Episode 9: reward: 135.000, steps: 659\n",
      "Episode 10: reward: 240.000, steps: 888\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 45.000, steps: 352\n",
      "Episode 2: reward: 160.000, steps: 803\n",
      "Episode 3: reward: 340.000, steps: 1337\n",
      "Episode 4: reward: 15.000, steps: 351\n",
      "Episode 5: reward: 195.000, steps: 1236\n",
      "Episode 6: reward: 205.000, steps: 880\n",
      "Episode 7: reward: 135.000, steps: 650\n",
      "Episode 8: reward: 15.000, steps: 334\n",
      "Episode 9: reward: 140.000, steps: 729\n",
      "Episode 10: reward: 50.000, steps: 521\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 230.000, steps: 983\n",
      "Episode 2: reward: 75.000, steps: 362\n",
      "Episode 3: reward: 95.000, steps: 497\n",
      "Episode 4: reward: 20.000, steps: 526\n",
      "Episode 5: reward: 5.000, steps: 394\n",
      "Episode 6: reward: 210.000, steps: 866\n",
      "Episode 7: reward: 140.000, steps: 846\n",
      "Episode 8: reward: 55.000, steps: 491\n",
      "Episode 9: reward: 155.000, steps: 867\n",
      "Episode 10: reward: 225.000, steps: 1286\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 125.000, steps: 411\n",
      "Episode 2: reward: 40.000, steps: 486\n",
      "Episode 3: reward: 180.000, steps: 1083\n",
      "Episode 4: reward: 120.000, steps: 683\n",
      "Episode 5: reward: 180.000, steps: 741\n",
      "Episode 6: reward: 155.000, steps: 795\n",
      "Episode 7: reward: 80.000, steps: 620\n",
      "Episode 8: reward: 410.000, steps: 824\n",
      "Episode 9: reward: 105.000, steps: 760\n",
      "Episode 10: reward: 135.000, steps: 729\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"SpaceInvaders-v0\"), directory=\"SpaceInvaders-v0_videos\", force=True) as env_monitor:\n",
    "    \n",
    "    sessions = [generate_session(env_monitor) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c469bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"SpaceInvaders-v0_videos/openaigym.video.8.43361.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play the recorded video\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('SpaceInvaders-v0_videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-2]))  # Play the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db4117",
   "metadata": {},
   "source": [
    "# 9. Evaluate the Agent's Performance after Training for 1M Timesteps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3efff783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(height, width, channels, actions):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(32,                                         # For image base model, we start with convolution2D layer with 32 convolution2D filters \n",
    "                            (8,8),                                      # Filter size 8 x 8\n",
    "                            strides = (4,4),                            # 4 x 4 means move 4 pixels to the right, and then 4 pixels to downward\n",
    "                            activation = 'relu',                       # use 'relu' function\n",
    "                            input_shape = (3, height, width, channels)  # pass through the image's heights, width, and channels as input into the model\n",
    "                           ))\n",
    "    \n",
    "    model.add(Convolution2D(64,                                         # 64 convolution2D filters in the second layer\n",
    "                            (4,4),                                      # Filter size 4 x 4\n",
    "                            strides = (2,2),                            # 2 x 2 means move 2 pixels to the right, and then 2 pixels to downward\n",
    "                            activation = 'relu'\n",
    "                           ))\n",
    "    \n",
    "    model.add(Convolution2D(64,                                         # 64 convolution2D filters in the second layer\n",
    "                            (3,3),                                      # Filter size 3 x 3\n",
    "                            strides = (1,1),                            # 1 x 1 means move 1 pixels to the right, and then 1 pixels to downward\n",
    "                            activation = 'relu'         \n",
    "                           ))\n",
    "    \n",
    "    model.add(Flatten())    # Flatten down all the above layers into a single layer, so that we can than pass it through to next Dense layer\n",
    "    \n",
    "    model.add(Dense(512, activation = 'relu'))                          # compress all the above input image into 512 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(256, activation = 'relu'))                          # compress down to 256 units of Dense layer\n",
    "    \n",
    "    #model.add(Dense(128, activation = 'relu'))                          # compress down to 128 units of Dense layer\n",
    "    \n",
    "    #model.add(Dense(64, activation = 'relu'))                           # compress down to 64 units of Dense layer\n",
    "    \n",
    "    model.add(Dense(actions, activation = 'linear'))                    # compress down to number of actions as output of the model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca66d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = build_model_2(height, width, channels, actions)\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb610027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn_2 = build_agent(model_2, actions)   # setup an agent\n",
    "\n",
    "dqn_2.compile(Adam(lr=1e-4))          # use Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62da6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_2.load_weights('Savedweights/1m/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "964cf64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoikinyu/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 180.000, steps: 1137\n",
      "Episode 2: reward: 240.000, steps: 820\n",
      "Episode 3: reward: 410.000, steps: 1030\n",
      "Episode 4: reward: 230.000, steps: 1081\n",
      "Episode 5: reward: 250.000, steps: 1131\n",
      "Episode 6: reward: 190.000, steps: 690\n",
      "Episode 7: reward: 175.000, steps: 671\n",
      "Episode 8: reward: 120.000, steps: 472\n",
      "Episode 9: reward: 465.000, steps: 1550\n",
      "Episode 10: reward: 530.000, steps: 1292\n",
      "279.0\n"
     ]
    }
   ],
   "source": [
    "scores_2 = dqn_2.test(env, nb_episodes = 10, visualize = True)\n",
    "\n",
    "print(np.mean(scores_2.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f281e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 19:26:18.441 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa3268a9b00>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 19:26:18.442 python[43361:1411145] Warning: Expected min height of view: (<NSButton: 0x7fa351b0b210>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 19:26:18.445 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351b0dc90>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-03-14 19:26:18.447 python[43361:1411145] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fa351b19000>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 175.000, steps: 701\n",
      "Episode 2: reward: 240.000, steps: 1356\n",
      "Episode 3: reward: 280.000, steps: 1211\n",
      "Episode 4: reward: 165.000, steps: 665\n",
      "Episode 5: reward: 275.000, steps: 1125\n",
      "Episode 6: reward: 245.000, steps: 860\n",
      "Episode 7: reward: 140.000, steps: 848\n",
      "Episode 8: reward: 310.000, steps: 736\n",
      "Episode 9: reward: 230.000, steps: 800\n",
      "Episode 10: reward: 165.000, steps: 632\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 215.000, steps: 1149\n",
      "Episode 2: reward: 165.000, steps: 648\n",
      "Episode 3: reward: 125.000, steps: 663\n",
      "Episode 4: reward: 310.000, steps: 929\n",
      "Episode 5: reward: 260.000, steps: 873\n",
      "Episode 6: reward: 425.000, steps: 1100\n",
      "Episode 7: reward: 235.000, steps: 583\n",
      "Episode 8: reward: 755.000, steps: 1583\n",
      "Episode 9: reward: 215.000, steps: 1148\n",
      "Episode 10: reward: 495.000, steps: 1046\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 370.000, steps: 1072\n",
      "Episode 2: reward: 210.000, steps: 864\n",
      "Episode 3: reward: 275.000, steps: 924\n",
      "Episode 4: reward: 210.000, steps: 680\n",
      "Episode 5: reward: 175.000, steps: 884\n",
      "Episode 6: reward: 355.000, steps: 969\n",
      "Episode 7: reward: 225.000, steps: 1095\n",
      "Episode 8: reward: 350.000, steps: 1080\n",
      "Episode 9: reward: 225.000, steps: 1159\n",
      "Episode 10: reward: 225.000, steps: 1148\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 335.000, steps: 955\n",
      "Episode 2: reward: 285.000, steps: 655\n",
      "Episode 3: reward: 320.000, steps: 884\n",
      "Episode 4: reward: 230.000, steps: 562\n",
      "Episode 5: reward: 230.000, steps: 597\n",
      "Episode 6: reward: 240.000, steps: 838\n",
      "Episode 7: reward: 240.000, steps: 1153\n",
      "Episode 8: reward: 300.000, steps: 797\n",
      "Episode 9: reward: 515.000, steps: 1284\n",
      "Episode 10: reward: 210.000, steps: 642\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 540.000, steps: 1275\n",
      "Episode 2: reward: 205.000, steps: 671\n",
      "Episode 3: reward: 155.000, steps: 626\n",
      "Episode 4: reward: 155.000, steps: 700\n",
      "Episode 5: reward: 195.000, steps: 1001\n",
      "Episode 6: reward: 375.000, steps: 1296\n",
      "Episode 7: reward: 160.000, steps: 596\n",
      "Episode 8: reward: 340.000, steps: 722\n",
      "Episode 9: reward: 400.000, steps: 1138\n",
      "Episode 10: reward: 30.000, steps: 372\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 155.000, steps: 637\n",
      "Episode 2: reward: 655.000, steps: 1529\n",
      "Episode 3: reward: 255.000, steps: 684\n",
      "Episode 4: reward: 175.000, steps: 657\n",
      "Episode 5: reward: 300.000, steps: 853\n",
      "Episode 6: reward: 380.000, steps: 1231\n",
      "Episode 7: reward: 290.000, steps: 961\n",
      "Episode 8: reward: 430.000, steps: 808\n",
      "Episode 9: reward: 205.000, steps: 658\n",
      "Episode 10: reward: 375.000, steps: 982\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 285.000, steps: 766\n",
      "Episode 2: reward: 180.000, steps: 658\n",
      "Episode 3: reward: 110.000, steps: 674\n",
      "Episode 4: reward: 65.000, steps: 401\n",
      "Episode 5: reward: 245.000, steps: 1207\n",
      "Episode 6: reward: 135.000, steps: 515\n",
      "Episode 7: reward: 220.000, steps: 1139\n",
      "Episode 8: reward: 255.000, steps: 1184\n",
      "Episode 9: reward: 185.000, steps: 745\n",
      "Episode 10: reward: 160.000, steps: 627\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 185.000, steps: 846\n",
      "Episode 2: reward: 160.000, steps: 642\n",
      "Episode 3: reward: 260.000, steps: 1162\n",
      "Episode 4: reward: 395.000, steps: 725\n",
      "Episode 5: reward: 205.000, steps: 867\n",
      "Episode 6: reward: 95.000, steps: 416\n",
      "Episode 7: reward: 120.000, steps: 666\n",
      "Episode 8: reward: 400.000, steps: 1188\n",
      "Episode 9: reward: 425.000, steps: 859\n",
      "Episode 10: reward: 355.000, steps: 1211\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 445.000, steps: 1107\n",
      "Episode 2: reward: 310.000, steps: 736\n",
      "Episode 3: reward: 215.000, steps: 1127\n",
      "Episode 4: reward: 240.000, steps: 1069\n",
      "Episode 5: reward: 445.000, steps: 1466\n",
      "Episode 6: reward: 675.000, steps: 1367\n",
      "Episode 7: reward: 245.000, steps: 1113\n",
      "Episode 8: reward: 240.000, steps: 1000\n",
      "Episode 9: reward: 350.000, steps: 1325\n",
      "Episode 10: reward: 150.000, steps: 657\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 400.000, steps: 1132\n",
      "Episode 2: reward: 410.000, steps: 1135\n",
      "Episode 3: reward: 405.000, steps: 1287\n",
      "Episode 4: reward: 230.000, steps: 1234\n",
      "Episode 5: reward: 170.000, steps: 675\n",
      "Episode 6: reward: 420.000, steps: 1124\n",
      "Episode 7: reward: 555.000, steps: 1486\n",
      "Episode 8: reward: 370.000, steps: 1069\n",
      "Episode 9: reward: 200.000, steps: 1117\n",
      "Episode 10: reward: 195.000, steps: 1086\n"
     ]
    }
   ],
   "source": [
    "# Record sessions\n",
    "\n",
    "def generate_session_2(env):\n",
    "    \n",
    "    scores = dqn_2.test(env, nb_episodes = 10, visualize = True)\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"SpaceInvaders-v0\"), directory=\"SpaceInvaders-v0_1Mtimesteps_videos\", force=True) as env_monitor:\n",
    "    \n",
    "    sessions = [generate_session_2(env_monitor) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee646785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"SpaceInvaders-v0_1Mtimesteps_videos/openaigym.video.9.43361.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play the recorded video\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('SpaceInvaders-v0_1Mtimesteps_videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # Play the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c79152",
   "metadata": {},
   "source": [
    "***End of Page***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
